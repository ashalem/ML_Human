{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Tz1PZ2VWkwFv"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashalem/ML_Human/blob/main/WS1_W2024_students_5823.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mLCUCd09-Y2"
      },
      "source": [
        "\n",
        "<div>Machine Learning and Human Behavior - 236667 - Winter 2024-2025</div>\n",
        "<font size=\"6\">Workshop #1 - Binary Choice ‚öñÔ∏è</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions and submission guidelines\n",
        "\n",
        "* Clone this notebook and complete the exercise:\n",
        "    * Aim for clear and concise solutions.\n",
        "    * Indicate clearly with a text block the sections of your solutions.\n",
        "    * Answer dry questions in text (markdown) blocks, and wet questions in code blocks.\n",
        "* Submission guidelines:\n",
        "    * When you're done, restart the notebook, and make sure that everything runs smoothly (Runtime->\"Restart and Run All\")\n",
        "    * Add a text block in the beginning of your notebook with your IDs.\n",
        "    * Export your notebook as ipynb (File->Download->\"Download .ipynb\")\n",
        "    * If you need to attach additional files to your submission (e.g images), add them to a zip file together with the notebook ipynb file.\n",
        "    * Submit through the course website. Remember to list partner IDs when you submit.\n",
        "* **Due date**: Sunday 01/12/2024, 10:00\n",
        "* For any questions regarding this workshop task, contact [Eden](mailto:edens@campus.technion.ac.il).\n"
      ],
      "metadata": {
        "id": "yUMsz9ROZ85F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ8O1MKGeH0T"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "* **Setting**: In the binary choice setting, users $u\\in U$ make a binary decision of whether or not to consume items $x \\in X$. Each item is represented by a vector $x\\in\\mathbb{R}^n$, and the outcome is represented using binary variable $y\\in\\left\\{0,1\\right\\}$, such that $y=1$ when the user chose to consume the item.\n",
        "\n",
        "\n",
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import sklearn.metrics\n",
        "import sklearn.linear_model\n",
        "import sklearn.metrics\n",
        "\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "wczZ4yxtPxsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract population models\n",
        "For the implementation of behavioral models, we define the abstract classes which handle data generation and formatting. These are similar to the abstract classes defined in HW1.\n",
        "\n",
        "As we will mostly use these classes through their public interface, there is no need to go through the implementation in detail.\n",
        "\n",
        "üîµ Run the cell below to define the `DiscreteChoiceEnvironment` and `InnerProductTrueValueEnvironment` classes:"
      ],
      "metadata": {
        "id": "Tz1PZ2VWkwFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscreteChoiceEnvironment:\n",
        "    \"\"\"\n",
        "    Generic class for discrete-choice dataset generation\n",
        "    \"\"\"\n",
        "    n_features = 16\n",
        "    observations_per_user = 10\n",
        "    n_users_eval = 50\n",
        "\n",
        "    def _generate_user_attributes(self, n_users):\n",
        "        \"\"\"\n",
        "        Generate latent parameters for users.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_users : int\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        users : ndarray of shape (n_users, n_features)\n",
        "        \"\"\"\n",
        "        return np.random.normal(\n",
        "            loc=1,\n",
        "            scale=0.1,\n",
        "            size=(\n",
        "                n_users,\n",
        "                self.n_features,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def _generate_item_attributes(self, n_users):\n",
        "        \"\"\"\n",
        "        Generate latent parameters for items.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_users : int\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        items : ndarray of shape\n",
        "                (n_users, observations_per_user, n_features)\n",
        "        \"\"\"\n",
        "        return np.random.normal(\n",
        "            size=(\n",
        "                n_users,\n",
        "                self.observations_per_user,\n",
        "                self.n_features,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def _choice(self, users, items):\n",
        "        \"\"\"\n",
        "        Discrete choice function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        users : ndarray of shape (n_users, n_features)\n",
        "        items : ndarray of shape\n",
        "                (n_users, observations_per_user, n_features)\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        choice : Dict[str -> ndarray of shape(n_users, observations_per_user)]\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _generate_choice_dataset(self, n_users, uid_offset):\n",
        "        \"\"\"\n",
        "        Generate choice dataset, formatted as pandas dataframe.\n",
        "        \"\"\"\n",
        "        users = self._generate_user_attributes(n_users)\n",
        "        items = self._generate_item_attributes(n_users)\n",
        "        choice_dct = self._choice(users, items)\n",
        "        rows = []\n",
        "        for i in range(n_users):\n",
        "            for j in range(self.observations_per_user):\n",
        "                dct = {}\n",
        "                dct['user_id'] = i+uid_offset\n",
        "                for k in range(self.n_features):\n",
        "                        dct[f'x{k}'] = items[i,j,k]\n",
        "                for choice_type, choice_matrix in choice_dct.items():\n",
        "                    dct[choice_type] = choice_matrix[i,j]\n",
        "                rows.append(dct)\n",
        "        df = pd.DataFrame(rows)\n",
        "        return df\n",
        "\n",
        "    def generate_train_eval_datasets(self, n_users_train):\n",
        "        return (\n",
        "            self._generate_choice_dataset(n_users_train, uid_offset=0),\n",
        "            self._generate_choice_dataset(self.n_users_eval, uid_offset=n_users_train),\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def feature_columns(self):\n",
        "        return [\n",
        "            f'x{k}'\n",
        "            for k in range(self.n_features)\n",
        "        ]\n",
        "\n",
        "\n",
        "class InnerProductTrueValueEnvironment(DiscreteChoiceEnvironment):\n",
        "    @staticmethod\n",
        "    def _true_value(users, items):\n",
        "        # true_value is an inner product u@x.\n",
        "        # Calculate using np.einsum, where:\n",
        "        # * i: user index\n",
        "        # * j: observation (item) index\n",
        "        # * k: feature\n",
        "        true_value = np.einsum('ik,ijk->ij', users, items)\n",
        "        return true_value\n"
      ],
      "metadata": {
        "id": "j3tgZFC-y5pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task \\#1: Prediction with stated and revealed preferences\n"
      ],
      "metadata": {
        "id": "sZpLOZz3tzv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In our first task, we will investigate the relation between predictive performance and the type of feedback obtained from users.\n",
        "\n",
        "We assume that consumption decisions are made according to the a random utility model. Each user is represented by a vector $u\\in\\mathbb{R}^d$, and each item is represented by a vector $x\\in\\mathbb{R}^d$. The true utility experienced by user $u$ from consuming item $x$ is assumed to be the inner product $v_u(x)=u^Tx$. We denote the indicator function by $\\mathbb{1}(\\cdot)\\in\\{0,1\\}$.\n",
        "\n",
        "\n",
        "We distinguish between three types of feedback:\n",
        "\n",
        "* **Rational preference**: When user $u$ is queried in an ideal environment, they choose to consume the item if its utility is larger than zero. Their rational choice is to consume if the utility is larger than zero, hence $y_\\mathrm{rational} = \\mathbb{1}(v_u(x) \\ge 0)$.\n",
        "\n",
        "* **Stated preference**: When user $u$ is questioned explicitly about item $x$ (e.g in a survey), they tend to under-estimate the value of the item. Therefore, their stated consumption choice is given by $y_\\mathrm{stated} = \\mathbb{1}(v_u(x)-b \\ge 0)$, where $b\\ge 0$ is a fixed and latent bias term.\n",
        "\n",
        "* **Revealed preference**: When $u$ is presented with item $x$, they reply according to a noisy evaluation $y_\\mathrm{revealed} = \\mathbb{1}(v_u(x)+\\varepsilon \\ge 0)$, where $\\varepsilon\\sim N(0,\\sigma)$.\n",
        "\n"
      ],
      "metadata": {
        "id": "KrG8O1lCDPXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1.1: Simulating user behavior\n",
        "\n",
        "The `NoisyBinaryChoiceEnvironment` class implements the preference models described above, and will be used for dataset generation.\n",
        "The class inherits from the `InnerProductTrueValueEnvironment` class defined above, and provides a simple interface which will be useful for simulation.\n",
        "\n"
      ],
      "metadata": {
        "id": "RYxGnvDZfF8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisyBinaryChoiceEnvironment(InnerProductTrueValueEnvironment):\n",
        "    \"\"\"\n",
        "    Dataset generator for binary choice with decision noise\n",
        "    \"\"\"\n",
        "    def __init__(self, noise_scale, stated_preference_bias):\n",
        "        self.noise_scale = noise_scale\n",
        "        self.stated_preference_bias = stated_preference_bias\n",
        "\n",
        "    def _choice(self, users, items):\n",
        "        true_value = self._true_value(users, items)\n",
        "        decision_noise = np.random.normal(\n",
        "            size=true_value.shape,\n",
        "            scale=self.noise_scale,\n",
        "        )\n",
        "        stated_value = true_value-self.stated_preference_bias\n",
        "        perceived_value = true_value + decision_noise\n",
        "        return {\n",
        "            'true_value': true_value,\n",
        "            'rational_choice': true_value >= 0,\n",
        "            'stated_choice': stated_value >= 0,\n",
        "            'revealed_choice': perceived_value >= 0,\n",
        "        }"
      ],
      "metadata": {
        "id": "xapLYncPJySz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "As an example, here we instantiate an environment with noise magnitude $\\sigma=2$ and bias $b=1$ for stated preferences. We generate training and evaluation datasets with 100 users for training. Note that the training and evaluation datasets are pandas DataFrames:"
      ],
      "metadata": {
        "id": "4tT5w6tiJxY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_noisy_choice_env = NoisyBinaryChoiceEnvironment(\n",
        "    noise_scale=2,\n",
        "    stated_preference_bias=1,\n",
        ")\n",
        "example_train_df, example_eval_df = example_noisy_choice_env.generate_train_eval_datasets(\n",
        "    n_users_train=100,\n",
        ")\n",
        "example_train_df.head()"
      ],
      "metadata": {
        "id": "eWlkyaRCSglw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We extract features and train models using `sklearn`. Here we use the training set to fit a Logistic Regression model, and predict on the evaluation set:"
      ],
      "metadata": {
        "id": "jXq09m0moNyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_model = sklearn.linear_model.LogisticRegression().fit(\n",
        "    X=example_train_df[example_noisy_choice_env.feature_columns],\n",
        "    y=example_train_df['revealed_choice'],\n",
        ")\n",
        "# Predict outcomes for first 10 rows\n",
        "example_model.predict(\n",
        "    X=example_eval_df.iloc[:10][example_noisy_choice_env.feature_columns],\n",
        ")"
      ],
      "metadata": {
        "id": "a9vyetgzezYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warm-up question**:\n",
        "\n",
        "In the `example_eval_df` dataset:\n",
        "* What is the proportion of positive consumption choices under the `rational_choice` criteria ($y_\\mathrm{rational}=1$)?\n",
        "* What is the proportion of positive consumption choices under the `stated_choice` criteria ($y_\\mathrm{stated}=1$)?\n",
        "* What is the proportion of positive consumption choices under the `revealed_choice` criteria ($y_\\mathrm{revealed}=1$)?\n",
        "\n",
        "üîµ **Answer**:"
      ],
      "metadata": {
        "id": "HmZA-a4bXNO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "n2QtE9S3XWrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1.2: Comparison graph\n",
        "\n",
        "If users make consumption decisions in alignment with $y_\\mathrm{revealed}$, which signal would be most effective for training the prediction model?\n",
        "\n",
        "Plot three lines (on the same axes) representing the accuracy of a logistic regression model, for variable $\\sigma$. Plot one line graph for *stated preferences*, and another line for *revealed preferences*.  \n",
        "\n",
        "For each decision noise magnitude $\\sigma$ in range $\\sigma\\in[0.1,10]$:\n",
        "* Instantiate a `NoisyBinaryChoiceEnvironment` environment with the given noise magnitude, and set the bias term as $b=1$.\n",
        "* Generate training/evaluation datasets with `n_users_train=20`.\n",
        "* Use the training set to train three Logistic Regression models, using $y_\\mathrm{rational}$, $y_\\mathrm{stated}$ and $y_\\mathrm{revealed}$ as training labels.\n",
        "* For each trained model, evaluate test accuracy on the task of predicting $y_\\mathrm{revealed}$.\n",
        "\n",
        "To reduce randomization noise, repeat the experiment 50 times for each $\\sigma$, and average the results.\n",
        "\n",
        "Hints:\n",
        "* Code should be simple and concise. Separation between data collection and data analysis usually makes it easier to debug.\n",
        "* Start with a small number of repetitions to make the debugging cycle quicker, and increase the number of repetitions to the final value when you are ready for analysis.\n",
        "* Given an environment `env` and a generated dataset `train_df`:\n",
        "  * `train_df['rational_choice']` is the user's rational choice $y_\\mathrm{rational}$. Similarly for $y_\\mathrm{stated}$, $y_\\mathrm{revealed}$.\n",
        "  * `train_df[env.feature_columns]` extracts the feature columns from the DataFrame.\n",
        "* Use `sklearn.linear_model.LogisticRegression` as the prediction model:\n",
        "  * Note that `pandas.DataFrame` and `pandas.Series` are valid datatypes for sklearn's `X` and `y` arguments, so is no need to convert the objects returned by `env.generate_train_eval_datasets`.\n",
        "  * Given a trained Logistic Regression model `m`, the command `m.score(X,y)` returns the mean accuracy on the given test data and labels.\n",
        "* Figures should be clear and organized. Make sure that title, axis labels, and legend are added and clearly labeled.\n",
        "\n",
        "üîµ **Answer**:"
      ],
      "metadata": {
        "id": "ss6i9HrgP6Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_scale_vec = np.linspace(0.1,10,10)\n",
        "task12_n_repetitions = 50\n",
        "task12_n_users = 20\n",
        "\n",
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "PclEQ6jtybAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain the results in detail. In your answer, relate to:\n",
        "* Overall trends:\n",
        "  * Are the lines increasing/decreasing/constant as a function of $\\sigma$? Why?\n",
        "* Relation between the lines:\n",
        "  * Assuming that $y_\\mathrm{rational}$ can't be measured in practice -\n",
        "  Which of the remaining training labels ($y_\\mathrm{stated}$, $y_\\mathrm{revealed}$) is better when $\\sigma\\to 0$? which is better when $\\sigma\\to \\infty$? Why?\n",
        "  * If lines cross each other, when and why do they cross?\n",
        "  * If lines coincide, when and why do they coincide?\n",
        "* Range of values:\n",
        "  * What is the range of accuracy values obtained?\n",
        "  * How do they relate to upper/lower bounds of predictive performance?\n",
        "\n"
      ],
      "metadata": {
        "id": "0AqNrFftoqdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "üîµ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ],
      "metadata": {
        "id": "iTG-hUXoohG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After describing your observations, try to answer the initial question: If users make consumption decisions in alignment with $y_\\mathrm{revealed}$, which signal would be most effective for training the prediction model?\n"
      ],
      "metadata": {
        "id": "2pEztXWp2r-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "üîµ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ],
      "metadata": {
        "id": "K_Qebzz82wzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1.3: Evaluating Welfare\n",
        "\n",
        "Plot a similar graph for the welfare metric you created in HW1:\n",
        "\n",
        "$$\n",
        "\\mathrm{welfare}(f, S)=\\frac{1}{|S|}\\sum_{ (u,x) \\in S } f(x) v_u(x)\n",
        "$$\n",
        "\n",
        "üîµ **Answer**:"
      ],
      "metadata": {
        "id": "DXoWHCAgOlcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "Xs9y8kbUOuFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain your results:\n",
        "\n",
        "üîµ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ],
      "metadata": {
        "id": "duBbsnybOuFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1.4: Welfare counter-example\n",
        "\n",
        "**Assuming we use a linear classifer** (i.e. $f(x) = \\mathrm{sign}(w^Tx + b)$)\n",
        "\n",
        "Find a dataset for which the classifier which yields the best accuracy, does not promote optimal welfare. Explain your answer.\n",
        "\n",
        "In your answer, you should provide:\n",
        "* Items and features\n",
        "* Utility function (utility doen't have to be linear)\n",
        "* Present a classifier, and explain why it's optimal in terms of accuracy.\n",
        "* Present another classifier which may have worse accuracy but better welfare.\n",
        "\n",
        "Note: you can sketch a 1D/2D dataset using Power-Point (taking a screenshot), or through this website: https://app.diagrams.net/. Describe the solution in words below.\n",
        "\n",
        "Attach the diagram inside a zip file together with your notebook when you submit your solution.\n",
        "\n",
        "üîµ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ],
      "metadata": {
        "id": "Oq91WJDZBqTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1.5: Mixed signals"
      ],
      "metadata": {
        "id": "LWIr-KrGqug4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following scenario:\n",
        "You manage a recommendation system and have a dataset capturing the consumption choices ($y_\\mathrm{revealed}$) of 10 users.\n",
        "Since this is a small user base (even with multiple observations per user), you are offered the opportunity to gather additional survey data. The survey provides information about items and consumption decisions, assuming that users answer the survey according to $y_\\mathrm{stated}$. You can survey up to 30 additional users. How many users should you include in the survey?\n",
        "\n",
        "Assume a `NoisyBinaryChoiceEnvironment` with $\\sigma=0.1$ and $b=1$. Support your claims with data, and explain your results.\n",
        "\n",
        "Hints:\n",
        "* The dataframes generated by `env.generate_train_eval_datasets` include a `user_id` column. Training set user IDs are numbered consecutively from `0` to `n_users_train-1` (inclusive).\n",
        "* Evaluate survey sizes in increments of 5 by iterating over the vector provided below.\n",
        "* To minimize estimation noise, repeat the experiment 100 times and average the results.\n",
        "\n",
        "üîµ **Answer**:\n"
      ],
      "metadata": {
        "id": "BgBkCUkq1MR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task_15_n_revealed = 10\n",
        "task_15_n_additional_users_vec = np.arange(0,31,5)\n",
        "task_15_n_repetitions = 100\n",
        "\n",
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "yypUj568pcCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain your results:\n",
        "\n",
        "üîµ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ],
      "metadata": {
        "id": "lf0wHs_0twgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without additional computations, how would your answer change if $b=0$ or $b\\to \\infty$? If changes occur, explain how and why, providing justification for your reasoning.\n"
      ],
      "metadata": {
        "id": "Rn_kFHWGqlY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "üîµ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ],
      "metadata": {
        "id": "N_pbT228BbIK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD7iSr_MlTPd"
      },
      "source": [
        "# Task \\#2: Irrational choice predition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa80uj8SmX4b"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this task, we will evaluate the performance of a standard (\"rational\") prediction model when decision-makers are loss-averse.\n",
        "\n",
        "In this section, users make decisions under uncertainty. Each user is associated with two utility functions, $u_a$ and $u_b$, and a probability parameter $p\\in\\left[0,1\\right]$. The user's utility from consuming an item $x$ is:\n",
        "- $u_a(x)\\in\\mathbb{R}$ with probability $p$, and\n",
        "- $u_b(x)\\in\\mathbb{R}$ with probability $(1-p)$.\n",
        "\n",
        "When user behavior is *rational*, the decision is made by comparing the *expected utility* of the two alternatives:\n",
        "\n",
        "$$\n",
        "y_\\text{rational}=\\begin{cases}\n",
        "1&p \\cdot u_a(x) + (1-p) \\cdot u_b(x) \\ge 0\\\\\n",
        "0&\\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "When user decisions are subject to *behavioral bias*, choice deviates from the expected optiumum. In particular, we will focus on a setting where the users are *loss-averse*. In the spirit of [Prospect Theory](https://en.wikipedia.org/wiki/Prospect_theory), we assume there exist two functions $\\pi, v$ such that the perceived value from consuming the item is:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "V_{\\pi, v}\\left(x\\right)\n",
        "&=\\sum_i \\pi\\left(p_i\\right) v\\left(u_i(x)\\right)\\\\\n",
        "&=\\pi(p) \\cdot v(u_a(x)) + \\pi(1-p) \\cdot v(u_b(x))\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The function $v$ captures the loss-aversion property, and it is s-shaped and asymmetrical. The function $\\pi$ is a probability weighting function and captures the idea that people tend to overreact to small probability events, but underreact to large probabilities. Assuming $v(0)=0$, consumption decisions are made according to the following rule:\n",
        "\n",
        "$$\n",
        "y_\\text{prospect}=\\begin{cases}\n",
        "1&V_{\\pi,v}(x) \\ge 0\\\\\n",
        "0&\\text{otherwise}\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnXadiQab3hl"
      },
      "source": [
        "## Exercise 2.1: The functional form of $v$\n",
        "\n",
        "\n",
        "The user valuation bias can be modeled using an S-shaped assymetrical function $v:\\mathbb{R}\\to\\mathbb{R}$. Following [Maggi (2004, Section 4.1)](https://www.econstor.eu/bitstream/10419/87132/1/472515071.pdf), we assume that $v$ is a power S-shaped utility function, and its functional form is given by:\n",
        "\n",
        "\n",
        "$$\n",
        "v(u)=\\begin{cases}\n",
        "u^\\alpha& u \\ge 0 \\\\\n",
        "-\\gamma \\left(-u\\right)^\\beta& u < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where $u=u(x)$ is the objective utility from consuming item $x$, and $0< \\alpha \\le \\beta \\le 1$, $\\gamma\\ge 1$ are constants.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation**\n",
        "\n",
        "Implement the class `PowerLossAversion`. The class constructor will receive three scalar constants `alpha`, `beta`, `gamma`.  The `__call__` function will calculate $v(u)$ as defined above.\n",
        "\n",
        "Hint: Make your code concise and more efficient by using numpy vectorized operations. Try to avoid explicit loops and `if` statements.\n",
        "\n",
        "üîµ **Answer**:"
      ],
      "metadata": {
        "id": "ik5BKrVECxQg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4cfUhgvG-dY"
      },
      "source": [
        "class PowerLossAversion:\n",
        "    \"\"\"\n",
        "    The power S-shaped utility function, as defined by Maggi (2014)\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha, beta, gamma):\n",
        "        assert 0 < alpha <= 1\n",
        "        assert 0 < beta <= 1\n",
        "        assert alpha <= beta\n",
        "        assert gamma >= 1\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def __call__(self, u):\n",
        "        \"\"\"\n",
        "        Compute the power S-shaped utility function for a vector of utilities.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        u : ndarray of shape (n)\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        v : ndarray of shape (n)\n",
        "        \"\"\"\n",
        "        ## YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8D62Fpxm9zk"
      },
      "source": [
        "Using the implementation above, plot the function $\\nu$ for values of $u$ in the range $[-2,2]$, and for the given sets of parameters:\n",
        "\n",
        "  1. $\\left(\\alpha_1, \\beta_1, \\gamma_1\\right) = \\left(1, 1, 1\\right)$\n",
        "  2. $\\left(\\alpha_2, \\beta_2, \\gamma_2\\right) = \\left(1, 1, 2.5\\right)$\n",
        "  3. $\\left(\\alpha_3, \\beta_3, \\gamma_3\\right) = \\left(0.88, 0.88, 2.5\\right)$\n",
        "  4. $\\left(\\alpha_4, \\beta_4, \\gamma_4\\right) = \\left(0.2, 0.88, 1.8\\right)$\n",
        "\n",
        "üîµ **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oChQJkNmzee"
      },
      "source": [
        "prospect_params_lst = [\n",
        "    (1,1,1),\n",
        "    (1,1,2.5),\n",
        "    (0.88,0.88,2.5),\n",
        "    (0.2,0.8,1.8),\n",
        "]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(6,3))\n",
        "\n",
        "u_vec = np.linspace(-2,2,200)\n",
        "for alpha, beta, gamma in prospect_params_lst:\n",
        "    v = PowerLossAversion(alpha, beta, gamma)\n",
        "    perceived_value = v(u_vec)\n",
        "    ax.plot(\n",
        "        u_vec,\n",
        "        perceived_value,\n",
        "        label=f'$\\\\alpha={alpha}, \\\\beta={beta}, \\\\gamma={gamma}$',\n",
        "    )\n",
        "\n",
        "ax.set_title('Illustration of Loss Aversion Functions $v(u; \\\\alpha, \\\\beta, \\\\gamma)$')\n",
        "ax.set_xlabel('objective utility ($u$)')\n",
        "ax.set_ylabel(r'perceived value ($v(u)$)')\n",
        "ax.axhline(0,linestyle=':',zorder=-1,alpha=0.3)\n",
        "ax.axvline(0,linestyle=':',zorder=-1,alpha=0.3)\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4B_uSzKqi17"
      },
      "source": [
        "Given the results -\n",
        "\n",
        "What type of behavior is characterized by the curve parametrized by $\\left(\\alpha, \\beta, \\gamma\\right) = \\left(1, 1, 1\\right)$?\n",
        "\n",
        "üîµ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "What is the interpretation of the parameter $\\gamma$? Which behavioral traits are represented by high/low values of $\\gamma$? What aspect of prospect theory does it correspond to?\n",
        "\n",
        "üîµ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)\n",
        "\n"
      ],
      "metadata": {
        "id": "MEbT-TFJQbyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the meaning of the parameters $\\alpha,\\beta$? What aspects of prospect theory do they correspond to?\n",
        "\n",
        "üîµ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)\n",
        "\n"
      ],
      "metadata": {
        "id": "jEw77v4TQfHI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzcoyoM6PeGF"
      },
      "source": [
        "## Exercise 2.2: Simulating user behavior"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example**"
      ],
      "metadata": {
        "id": "FtRd0z0qfF9X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWzpj8cLJNdu"
      },
      "source": [
        "For the implementation of this behavioral model, we inherit from the `InnerProductTrueValueEnvironment` defined at the start of this notebook, and define the following abstract class:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ProspectEnvironment(InnerProductTrueValueEnvironment):\n",
        "    def _generate_user_attributes(self, n_users):\n",
        "        \"\"\"\n",
        "        Generate latent parameters for users.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_users : int\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        users : ndarray of shape (n_outcomes, n_users, n_features)\n",
        "        \"\"\"\n",
        "        return np.stack(\n",
        "            [\n",
        "                np.random.normal(\n",
        "                    loc=1,\n",
        "                    scale=0.1,\n",
        "                    size=(n_users, self.n_features),\n",
        "                ),\n",
        "                np.random.normal(\n",
        "                    loc=-0.1,\n",
        "                    scale=0.1,\n",
        "                    size=(n_users, self.n_features),\n",
        "                ),\n",
        "            ],\n",
        "            axis=0,\n",
        "        )"
      ],
      "metadata": {
        "id": "1cOwNd4cc-hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can inherit from these classes to create specific behavioral models. For example, here is a class which models unbiased decision making:"
      ],
      "metadata": {
        "id": "-q0IqcuYc-Fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RationalProspectEnvironmentExample(ProspectEnvironment):\n",
        "    def __init__(self):\n",
        "        p_a = 0.3\n",
        "        self.p = [p_a, 1-p_a]\n",
        "        super().__init__()\n",
        "\n",
        "    def _choice(self, users, items):\n",
        "        \"\"\"\n",
        "        Simulate choice\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        users : ndarray of shape (n_outcomes, n_users, n_features)\n",
        "        items : ndarray of shape (n_users, n_observations, n_features)\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "        choice : Dict[str -> ndarray of shape(n_users, observations_per_user)]\n",
        "        \"\"\"\n",
        "        # Calculate true value based on inner product\n",
        "        u_a = self._true_value(users[0], items)\n",
        "        u_b = self._true_value(users[1], items)\n",
        "        return {\n",
        "            'u_a': u_a,  # u_a(x)\n",
        "            'u_b': u_b,  # u_b(x)\n",
        "            'rational_choice': self.p[0]*u_a + self.p[1]*u_b >= 0,\n",
        "        }\n",
        "\n",
        "rational_env_example = RationalProspectEnvironmentExample()\n",
        "rational_train_df, rational_eval_df = rational_env_example.generate_train_eval_datasets(n_users_train=100)\n",
        "rational_train_df.sample(5)"
      ],
      "metadata": {
        "id": "1C3nxIBHczmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXadaa3mPZkP"
      },
      "source": [
        "**Implementation**\n",
        "\n",
        "Based on the example above, implement the `BehavioralProspectEnvironment` class for simulating choice in a behavioral environment:\n",
        "* Class should inherit from `ProspectEnvironment`\n",
        "* Prospect value function $v(u)$ and a probability weighting function $\\pi(p)$ should be given in the class constructor.\n",
        "* Value of $p$ should be constant: $p=0.3$\n",
        "* Implement the binary choice inside the `_choice` function. Function returns a dictionary mapping column names to numpy arrays containing their contents (see example above).\n",
        "\n",
        "üîµ **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "ab_eRt7jD58T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxa6aSrGshTc"
      },
      "source": [
        "## Exercise 2.3: Predicting under behavioral bias\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhNb9u_Yvt8O"
      },
      "source": [
        "For each set of behavioral parameters $\\left(\\alpha_1, \\beta_1, \\gamma_1\\right),\\dots,\\left(\\alpha_4, \\beta_4, \\gamma_4\\right)$ given above, and for a neutral probability weighting ($\\pi(p)=p$), train and evaluate a Logistic Regression model on data generated by the corresponding `BehavioralProspectEnvironment`, with `n_users=100`.\n",
        "\n",
        "Report evaluation set accuracy for each set of parameters, averaged over 10 repetitions of the simulation.\n",
        "\n",
        "üîµ **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoNMaks5x-Ro"
      },
      "source": [
        "task_2_3_n_repetitions = 10\n",
        "task_2_3_n_users = 100\n",
        "\n",
        "## YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yi6vf5ky5aW"
      },
      "source": [
        "Plot a line graph representing the accuracy, for fixed $\\alpha=\\beta=1$ and variable $\\gamma\\in[1,5]$. Repeat each simulation 10 times for each value of $\\gamma$, and use the average value for the plot.\n",
        "\n",
        "üîµ **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCLjej7uxdLO"
      },
      "source": [
        "alpha = 1\n",
        "beta = 1\n",
        "gamma_vec = np.linspace(1,5,10)\n",
        "\n",
        "## YOUR SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehCgoCVD1J00"
      },
      "source": [
        "Explain the results:\n",
        "\n",
        "üîµ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc5nHH982FTR"
      },
      "source": [
        "Similarly, plot two lines representing the accuracy, for fixed $\\gamma=\\{1,2\\}$ and variable $\\alpha=\\beta\\in[0.4,1]$. Repeat each simulation 10 times for each value of $\\gamma$, and average results.\n",
        "\n",
        "üîµ **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alphabeta_vec = np.linspace(0.5,1,20)\n",
        "gamma=[1,2]\n",
        "\n",
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "fXdXRxnRBJNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain your results:\n",
        "\n",
        "üîµ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ],
      "metadata": {
        "id": "A4HdE0EINOMm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGBjVdMB6A23"
      },
      "source": [
        "\n",
        "What can we conclude about the performance of a logistic regression classifier on behavioral data? What can explain the above observations?\n",
        "\n",
        "üîµ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYtTZyoz31qF"
      },
      "source": [
        "# Task 3: Exploratory Analysis (Open-Ended)\n",
        "\n",
        "We present three open-ended questions. Please choose one of them and answer in detail. **Bonus will be given for solving more than one question**.\n",
        "\n",
        "This task is exploratory, and we encourage you to try different and creative approaches to solve it. In your answers, you should design and run appropriate experiment(s) - state your hypotheses, show plots that support your claim, and explain them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Option 1: Exploratory analysis (for Task \\#1)\n",
        "\n",
        "\n",
        "Looking at both measures (accuracy and welfare), try to vary the parameters of the experiment (\\#users, \\#items, \\#features, environment parameters, etc.) in ways that show interesting trends. Explain your results in detail and support your claims.\n"
      ],
      "metadata": {
        "id": "d4Pqix8lBmu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Option 2: Estimating behavioral deviations (for Task \\#2)\n",
        "\n",
        "Assuming neutral $\\pi$ ($\\pi(p)=p$) and a power S-shaped utility function $v$ (as described in Ex. 2.1), propose a way to estimate the functional parameters $\\alpha,\\beta,\\gamma$ from data. Support your claims using simulated data.\n"
      ],
      "metadata": {
        "id": "a024KGKUBoGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Option 3: Accounting for behavioral deviations (for Task \\#2)\n",
        "\n",
        "Assuming neutral $\\pi$ ($\\pi(p)=p$) and a power S-shaped utility function $v$ (as described in Ex. 2.1), improve predictive performance compared to the naive logistic regression baseline.\n",
        "\n",
        "Support your claims using simulated data, and evaluate performance on behavioral models with parameters $\\alpha, \\beta, \\gamma$ as defined in `prospect_params_lst` above.\n",
        "Explain your methods. How did you train your model? Why?\n",
        "\n"
      ],
      "metadata": {
        "id": "Sxl7gSkhBpAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open-ended question: Solution\n",
        "\n",
        "Support your claims with data, explain your results, and maintain clarity. You are free to add code and Markdown cells."
      ],
      "metadata": {
        "id": "ruqT1nzjB8Ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "üîµ **Answer**:"
      ],
      "metadata": {
        "id": "4nJBilQTB7aP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "WTYNP6SLNkTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain your results:\n",
        "\n",
        "üîµ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ],
      "metadata": {
        "id": "nYN2j6uiVsRe"
      }
    }
  ]
}