{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Tuple, Dict\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants:\n",
    "\n",
    "FACULTY_NAMES = ['Computer Science', 'Economics', 'Psychology', 'Law', 'Art']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FacultyParams:\n",
    "    \"\"\"Parameters for each faculty\"\"\"\n",
    "    name: str\n",
    "    utility_vector: np.ndarray  # Hidden vector that determines student success\n",
    "    capacity: int  # Number of spots available (can be infinite)\n",
    "\n",
    "@dataclass\n",
    "class SupplierParams:\n",
    "    \"\"\"Parameters for each preparation supplier\"\"\"\n",
    "    name: str\n",
    "    diff_vector: np.ndarray  # How this supplier modifies student features\n",
    "\n",
    "class UniversityEnvironment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int = 5,  # Number of student features (e.g., math, english, etc.)\n",
    "        n_faculties: int = 5,  # Number of different faculties\n",
    "        n_suppliers: int = 4,  # Number of preparation suppliers\n",
    "        noise_range: Tuple[float, float] = (-5, 5)  # Range for uniform noise\n",
    "    ):\n",
    "        self.n_features = n_features\n",
    "        self.n_faculties = n_faculties\n",
    "        self.n_suppliers = n_suppliers\n",
    "        self.noise_range = noise_range\n",
    "        \n",
    "        # Initialize faculties with random utility vectors\n",
    "        self.faculties = [\n",
    "            FacultyParams(\n",
    "                name=f\"Faculty_{i}\",\n",
    "                utility_vector=np.random.uniform(0, 1, n_features),\n",
    "                capacity=np.inf  # As per description, infinite capacity\n",
    "            )\n",
    "            for i in range(n_faculties)\n",
    "        ]\n",
    "        \n",
    "        # Initialize suppliers with random modification vectors\n",
    "        self.suppliers = [\n",
    "          SupplierParams(\n",
    "              name=f\"Supplier_{i}\",\n",
    "              diff_vector=np.array([\n",
    "                  0.2 if j == idx1 else 0.1 if j == idx2 else 0\n",
    "                  for j in range(n_features)\n",
    "              ]),\n",
    "          )\n",
    "          for i in range(n_suppliers)\n",
    "          for idx1, idx2 in [np.random.choice(n_features, size=2, replace=False)]\n",
    "        ]\n",
    "        \n",
    "        self.past_applicants_df = None\n",
    "        self.current_applicants_df = None\n",
    "\n",
    "    def generate_past_applicants(\n",
    "        self,\n",
    "        n_applicants: int = 1000\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Generate dataset of past applicants with their outcomes\"\"\"\n",
    "        # Generate random feature vectors\n",
    "        features = np.random.normal(0, 1, (n_applicants, self.n_features))\n",
    "        \n",
    "        # Randomly assign faculty for each applicant\n",
    "        df = pd.DataFrame(features, columns=[f\"feature_{i}\" for i in range(self.n_features)])\n",
    "        df['assigned_faculty'] = np.random.randint(0, self.n_faculties, n_applicants)\n",
    "        \n",
    "        # Calculate grade only for assigned faculty\n",
    "        faculty_vectors = np.array([f.utility_vector for f in self.faculties])\n",
    "        grades = np.zeros(n_applicants)\n",
    "        # Get faculty vectors for each applicant based on their assigned faculty\n",
    "        faculty_vectors_per_applicant = faculty_vectors[df['assigned_faculty']]\n",
    "        \n",
    "        # Calculate base grades using matrix multiplication\n",
    "        base_grades = np.sum(features * faculty_vectors_per_applicant, axis=1)\n",
    "        \n",
    "        # Generate noise for all applicants at once\n",
    "        noise = np.random.uniform(*self.noise_range, size=n_applicants)\n",
    "        \n",
    "        # Calculate final grades\n",
    "        grades = base_grades + noise\n",
    "            \n",
    "        df['final_grade'] = grades\n",
    "        self.past_applicants_df = df\n",
    "        return df\n",
    "\n",
    "    def generate_current_applicants(\n",
    "        self,\n",
    "        n_applicants: int = 100\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Generate dataset of current applicants\"\"\"\n",
    "        # Generate random feature vectors\n",
    "        features = np.random.normal(0, 1, (n_applicants, self.n_features))\n",
    "        \n",
    "        # Create DataFrame\n",
    "        feature_cols = [f\"feature_{i}\" for i in range(self.n_features)]\n",
    "        df = pd.DataFrame(features, columns=feature_cols)\n",
    "        \n",
    "        # Add desired faculty (random)\n",
    "        df['desired_faculty'] = np.random.randint(0, self.n_faculties, n_applicants)\n",
    "        \n",
    "        self.current_applicants_df = df\n",
    "        return df\n",
    "\n",
    "    def choose_supplier_for_applicant(\n",
    "        self,\n",
    "        applicant_features: np.ndarray,\n",
    "        desired_faculty: int,\n",
    "        past_data: pd.DataFrame = None\n",
    "    ) -> Tuple[int, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Choose the best supplier for an applicant based on past data and supplier effects.\n",
    "        \n",
    "        Args:\n",
    "            applicant_features: The current features of the applicant\n",
    "            desired_faculty: The faculty index the applicant wants to get into\n",
    "            past_data: Optional past data to train on. If None, uses self.past_applicants_df\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (chosen_supplier_idx, modified_features)\n",
    "        \"\"\"\n",
    "        if past_data is None:\n",
    "            past_data = self.past_applicants_df\n",
    "        \n",
    "        if past_data is None:\n",
    "            raise ValueError(\"No past data available. Generate past applicants first.\")\n",
    "        \n",
    "        # Create and train applicant's MLP model\n",
    "        feature_cols = [f\"feature_{i}\" for i in range(self.n_features)]\n",
    "        X_train = torch.FloatTensor(past_data[feature_cols].values)\n",
    "        y_train = torch.LongTensor(past_data['assigned_faculty'].values)\n",
    "        \n",
    "        model = ApplicantMLP(self.n_features, self.n_faculties)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        \n",
    "        # Train the model\n",
    "        model.train()\n",
    "        for epoch in range(100):  # Quick training, adjust epochs as needed\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate each supplier's effect\n",
    "        model.eval()\n",
    "        best_probability = -1\n",
    "        best_supplier_idx = -1\n",
    "        best_modified_features = None\n",
    "        \n",
    "        original_features = torch.FloatTensor(applicant_features).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Try each supplier\n",
    "            for i, supplier in enumerate(self.suppliers):\n",
    "                # Apply supplier's modification\n",
    "                modified_features = original_features + torch.FloatTensor(supplier.diff_vector)\n",
    "                \n",
    "                # Get probability distribution over faculties\n",
    "                probabilities = model(modified_features)\n",
    "                \n",
    "                # Check probability for desired faculty\n",
    "                prob_desired = probabilities[0, int(desired_faculty)].item()\n",
    "                \n",
    "                if prob_desired > best_probability:\n",
    "                    best_probability = prob_desired\n",
    "                    best_supplier_idx = i\n",
    "                    best_modified_features = modified_features.squeeze(0).numpy()\n",
    "        \n",
    "        if best_supplier_idx == -1:\n",
    "            # If no supplier improves probability, return original features with no supplier\n",
    "            return (-1, applicant_features)\n",
    "        \n",
    "        return (best_supplier_idx, best_modified_features)\n",
    "        \n",
    "    def recommend(\n",
    "        self,\n",
    "        student_features: np.ndarray,\n",
    "        recommended_faculties: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Calculate final grades for students given their features and recommended faculties\n",
    "        \n",
    "        Args:\n",
    "            student_features: Features matrix of shape (n_students, n_features)\n",
    "            recommended_faculties: Array of faculty indices of shape (n_students,)\n",
    "            \n",
    "        Returns:\n",
    "            Array of final grades of shape (n_students,)\n",
    "        \"\"\"\n",
    "        # Get utility vectors for all recommended faculties\n",
    "        faculty_vectors = np.array([self.faculties[f].utility_vector for f in recommended_faculties])\n",
    "        \n",
    "        # Calculate base grades using batch matrix multiplication\n",
    "        base_grades = np.sum(student_features * faculty_vectors, axis=1)\n",
    "        \n",
    "        # Generate noise for all students at once\n",
    "        noise = np.random.uniform(*self.noise_range, size=len(student_features))\n",
    "        \n",
    "        return base_grades + noise\n",
    "\n",
    "    def university_decision_process(\n",
    "        self,\n",
    "        current_applicants_features: np.ndarray,\n",
    "        past_data: pd.DataFrame = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Train university model on past data and make faculty recommendations for current applicants.\n",
    "        \n",
    "        Args:\n",
    "            current_applicants_features: Modified features of current applicants (n_applicants x n_features)\n",
    "            past_data: Optional past data to train on. If None, uses self.past_applicants_df\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (chosen_faculties, final_grades, mean_grade)\n",
    "            - chosen_faculties: Array of faculty indices chosen for each applicant\n",
    "            - final_grades: Array of final grades received by each applicant\n",
    "            - mean_grade: Average grade across all applicants\n",
    "        \"\"\"\n",
    "        if past_data is None:\n",
    "            past_data = self.past_applicants_df\n",
    "        \n",
    "        if past_data is None:\n",
    "            raise ValueError(\"No past data available. Generate past applicants first.\")\n",
    "        \n",
    "        # Prepare training data \n",
    "        feature_cols = [f\"feature_{i}\" for i in range(self.n_features)]\n",
    "        \n",
    "        # Create one-hot encoding for faculty\n",
    "        faculty_one_hot = torch.nn.functional.one_hot(torch.LongTensor(past_data['assigned_faculty'].values), num_classes=self.n_faculties)\n",
    "        \n",
    "        # Combine features with one-hot faculty encoding\n",
    "        X_train = torch.FloatTensor(past_data[feature_cols].values)\n",
    "        X_train = torch.cat([X_train, faculty_one_hot.float()], dim=1)\n",
    "        \n",
    "        y_train = torch.FloatTensor(past_data['final_grade'].values)\n",
    "        \n",
    "        # Create and train university model\n",
    "        model = UniversityMLP(self.n_features, self.n_faculties)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        \n",
    "        # Train the model\n",
    "        model.train()\n",
    "        for epoch in range(200):  # More epochs for better training\n",
    "            optimizer.zero_grad()\n",
    "            predicted_grades = model(X_train)\n",
    "            loss = criterion(predicted_grades, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Make predictions for current applicants\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predicted_grades = model(current_applicants_features)\n",
    "            \n",
    "            # Choose best faculty for each applicant based on predicted grades\n",
    "            chosen_faculties = torch.argmax(predicted_grades, dim=1).numpy()\n",
    "        \n",
    "        # Get actual final grades using recommend function for all applicants at once\n",
    "        final_grades = self.recommend(current_applicants_features, chosen_faculties)\n",
    "        \n",
    "        mean_grade = np.mean(final_grades)\n",
    "        \n",
    "        return chosen_faculties, final_grades, mean_grade\n",
    "\n",
    "class UniversityMLP(nn.Module):\n",
    "    \"\"\"Simple MLP for university decisions\"\"\"\n",
    "    def __init__(self, n_features: int, n_faculties: int):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # Add one-hot encoded faculty to features\n",
    "            nn.Linear(n_features + n_faculties, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_faculties)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class ApplicantMLP(nn.Module):\n",
    "    \"\"\"MLP for applicant decisions with softmax output\"\"\"\n",
    "    def __init__(self, n_features: int, n_faculties: int):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_faculties),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "def run_example():\n",
    "# Create environment\n",
    "    env = UniversityEnvironment()\n",
    "    \n",
    "    # Generate past applicants\n",
    "    past_df = env.generate_past_applicants(1000)\n",
    "    print(\"Past applicants shape:\", past_df.shape)\n",
    "    \n",
    "    # Generate current applicants\n",
    "    current_df = env.generate_current_applicants(100)\n",
    "    print(\"Current applicants shape:\", current_df.shape)\n",
    "    \n",
    "    # Get modified features for all current applicants\n",
    "    feature_cols = [f\"feature_{i}\" for i in range(env.n_features)]\n",
    "    modified_features = []\n",
    "    \n",
    "    for idx in range(len(current_df)):\n",
    "        student_features = current_df.iloc[idx][feature_cols].values\n",
    "        desired_faculty = current_df.iloc[idx]['desired_faculty']\n",
    "        \n",
    "        _, modified_student_features = env.choose_supplier_for_applicant(\n",
    "            student_features,\n",
    "            desired_faculty\n",
    "        )\n",
    "        modified_features.append(modified_student_features)\n",
    "    \n",
    "    modified_features = np.array(modified_features)\n",
    "    \n",
    "    # Run university decision process\n",
    "    chosen_faculties, final_grades, mean_grade = env.university_decision_process(modified_features)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Mean grade across all applicants: {mean_grade:.2f}\")\n",
    "    \n",
    "    # Print detailed results for first 5 applicants\n",
    "    print(\"\\nDetailed results for first 5 applicants:\")\n",
    "    for i in range(5):\n",
    "        desired_faculty = current_df.iloc[i]['desired_faculty']\n",
    "        print(f\"\\nApplicant {i}:\")\n",
    "        print(f\"Desired faculty: {desired_faculty}\")\n",
    "        print(f\"Assigned faculty: {chosen_faculties[i]}\")\n",
    "        print(f\"Final grade: {final_grades[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past applicants shape: (1000, 7)\n",
      "Current applicants shape: (100, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arielshalem/miniconda3/envs/cs236781-hw2/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (1000) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m, in \u001b[0;36mrun_example\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m modified_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(modified_features)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Run university decision process\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m chosen_faculties, final_grades, mean_grade \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniversity_decision_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodified_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResults:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 245\u001b[0m, in \u001b[0;36mUniversityEnvironment.university_decision_process\u001b[0;34m(self, current_applicants_features, past_data)\u001b[0m\n\u001b[1;32m    243\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    244\u001b[0m predicted_grades \u001b[38;5;241m=\u001b[39m model(X_train)\n\u001b[0;32m--> 245\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_grades\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    247\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw2/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw2/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw2/lib/python3.8/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw2/lib/python3.8/site-packages/torch/nn/functional.py:3338\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3336\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3338\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/miniconda3/envs/cs236781-hw2/lib/python3.8/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (1000) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "run_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs236781-hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
