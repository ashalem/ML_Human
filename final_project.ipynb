{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5Shw-_FB-qET"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gVXNFGMv-qEU"
      },
      "outputs": [],
      "source": [
        "\n",
        "class UniversityMLP(nn.Module):\n",
        "    \"\"\"Simple MLP for university decisions\"\"\"\n",
        "    def __init__(self, n_features: int, n_faculties: int):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            # Add one-hot encoded faculty to features\n",
        "            nn.Linear(n_features, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, n_faculties)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class ApplicantMLP(nn.Module):\n",
        "    \"\"\"MLP for applicant decisions with softmax output\"\"\"\n",
        "    def __init__(self, n_features: int, n_faculties: int):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(n_features, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, n_faculties),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Jt4ptqyg-qEV"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class FacultyParams:\n",
        "    \"\"\"Parameters for each faculty\"\"\"\n",
        "    name: str\n",
        "    utility_vector: np.ndarray  # Hidden vector that determines student success\n",
        "    capacity: int  # Number of spots available (can be infinite)\n",
        "\n",
        "@dataclass\n",
        "class SupplierParams:\n",
        "    \"\"\"Parameters for each preparation supplier\"\"\"\n",
        "    name: str\n",
        "    diff_vector: np.ndarray  # How this supplier modifies student features\n",
        "\n",
        "@dataclass\n",
        "class MechinaParams:\n",
        "    \"\"\"Parameters for each Mechina supplier.\"\"\"\n",
        "    def __init__(self, faculty_name: str, utility_vector: np.ndarray):\n",
        "        self.name = f\"Mechina_{faculty_name}\"\n",
        "        self.diff_vector = self._generate_diff_vector(utility_vector)\n",
        "\n",
        "    def _generate_diff_vector(self, utility_vector: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Create a diff vector based on faculty utility vector.\"\"\"\n",
        "        strongest_idx = np.argmax(utility_vector)  # Feature with highest utility\n",
        "        weakest_idx = np.argmin(utility_vector)  # Feature with lowest utility\n",
        "        diff_vector = np.zeros_like(utility_vector)\n",
        "        diff_vector[strongest_idx] = 20  # Boost strongest feature\n",
        "        diff_vector[weakest_idx] = -5   # Reduce weakest feature\n",
        "        return diff_vector\n",
        "\n",
        "class UniversityEnvironment:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_features: int = 5,  # Number of student features (e.g., math, english, etc.)\n",
        "        n_faculties: int = 5,  # Number of different faculties\n",
        "        n_suppliers: int = 20,  # Number of preparation suppliers\n",
        "        noise_range: Tuple[float, float] = (0,0)  # Range for uniform noise\n",
        "    ):\n",
        "        self.n_features = n_features\n",
        "        self.n_faculties = n_faculties\n",
        "        self.n_suppliers = n_suppliers\n",
        "        self.noise_range = noise_range\n",
        "\n",
        "        # Initialize faculties with random utility vectors\n",
        "        # Initialize faculties with normalized random utility vectors\n",
        "        self.faculties = [\n",
        "            FacultyParams(\n",
        "                name=f\"faculty_{i}\",  # Using the predefined faculty names\n",
        "                utility_vector=self._create_normalized_vector(n_features),\n",
        "                capacity=np.inf  # As per description, infinite capacity\n",
        "            )\n",
        "            for i in range(n_faculties)\n",
        "        ]\n",
        "\n",
        "        # Initialize suppliers with random modification vectors\n",
        "        self.suppliers = [\n",
        "          SupplierParams(\n",
        "              name=f\"Supplier_{i}\",\n",
        "              diff_vector=np.array([\n",
        "                  15 if j == idx1 else 5 if j == idx2 else -5 if j == idx3 else 0\n",
        "                  for j in range(n_features)\n",
        "              ]),\n",
        "          )\n",
        "          for i in range(n_suppliers)\n",
        "          for idx1, idx2, idx3 in [np.random.choice(n_features, size=3, replace=False)]\n",
        "        ]\n",
        "\n",
        "        # Initialize Mechina suppliers (one per faculty)\n",
        "        self.mechina_suppliers = [\n",
        "            MechinaParams(faculty.name, faculty.utility_vector)\n",
        "            for faculty in self.faculties\n",
        "        ]\n",
        "\n",
        "        self.past_applicants_df = None\n",
        "        self.current_applicants_df = None\n",
        "\n",
        "    def _create_normalized_vector(self, size: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Create a normalized random vector of given size.\n",
        "        Normalization ensures ||vector|| = 1\n",
        "        \"\"\"\n",
        "        # Create vector with some high and some low values\n",
        "        vector = np.random.uniform(0.05, 0.2, size)  # Base small values\n",
        "\n",
        "        # Randomly select ~40% of elements to be higher values\n",
        "        high_value_indices = np.random.choice(size, size=max(1, size // 3), replace=False)\n",
        "        vector[high_value_indices] = np.random.uniform(0.4, 0.8, size=len(high_value_indices))\n",
        "\n",
        "        # Normalize to sum to 1 while preserving relative differences\n",
        "        return vector / np.sum(vector)\n",
        "\n",
        "    def _generate_truncated_normal_features(self, n_samples: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate features using truncated normal distribution between 55 and 100.\n",
        "        Uses mean at center of range (77.5) and std that makes the distribution fit well in the range.\n",
        "        \"\"\"\n",
        "        # Generate features with uniform distribution between 40 and 100\n",
        "        features = np.random.uniform(40, 100, (n_samples, self.n_features))\n",
        "\n",
        "        return features\n",
        "\n",
        "    def generate_past_applicants(\n",
        "        self,\n",
        "        n_applicants: int = 1000\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"Generate dataset of past applicants with their outcomes\"\"\"\n",
        "        # Generate random feature vectors\n",
        "        features = self._generate_truncated_normal_features(n_applicants)\n",
        "\n",
        "        # Randomly assign faculty for each applicant\n",
        "        df = pd.DataFrame(features, columns=[f\"feature_{i}\" for i in range(self.n_features)])\n",
        "        df['assigned_faculty'] = np.random.randint(0, self.n_faculties, n_applicants)\n",
        "\n",
        "        # Calculate grade only for assigned faculty\n",
        "        faculty_vectors = np.array([f.utility_vector for f in self.faculties])\n",
        "        grades = np.zeros(n_applicants)\n",
        "        # Get faculty vectors for each applicant based on their assigned faculty\n",
        "        faculty_vectors_per_applicant = faculty_vectors[df['assigned_faculty']]\n",
        "\n",
        "        # Calculate base grades using matrix multiplication\n",
        "        base_grades = np.sum(features * faculty_vectors_per_applicant, axis=1)\n",
        "\n",
        "        # Generate noise for all applicants at once\n",
        "        noise = np.random.uniform(*self.noise_range, size=n_applicants)\n",
        "\n",
        "        # Calculate final grades\n",
        "        grades = base_grades + noise\n",
        "\n",
        "        df['final_grade'] = grades\n",
        "        self.past_applicants_df = df\n",
        "        return df\n",
        "\n",
        "    def generate_current_applicants(\n",
        "        self,\n",
        "        n_applicants: int = 100\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"Generate dataset of current applicants\"\"\"\n",
        "        # Generate random feature vectors\n",
        "        features = self._generate_truncated_normal_features(n_applicants)\n",
        "\n",
        "        # Create DataFrame\n",
        "        feature_cols = [f\"feature_{i}\" for i in range(self.n_features)]\n",
        "        df = pd.DataFrame(features, columns=feature_cols)\n",
        "\n",
        "        # Add desired faculty (random)\n",
        "        df['desired_faculty'] = np.random.randint(0, self.n_faculties, n_applicants)\n",
        "\n",
        "        self.current_applicants_df = df\n",
        "        return df\n",
        "    def reconstruct_original_features(\n",
        "        self,\n",
        "        modified_features: np.ndarray,\n",
        "        desired_faculties: np.ndarray\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Reconstruct approximate original features using group-wise mean vectors.\n",
        "\n",
        "        Args:\n",
        "            modified_features: Modified feature vectors of shape (n_students, n_features)\n",
        "            desired_faculties: Array of desired faculty indices for each student\n",
        "\n",
        "        Returns:\n",
        "            Reconstructed original feature vectors\n",
        "        \"\"\"\n",
        "        # Calculate global mean vector\n",
        "        global_mean = np.mean(modified_features, axis=0)\n",
        "\n",
        "        # Initialize reconstructed features array\n",
        "        reconstructed_features = np.zeros_like(modified_features)\n",
        "\n",
        "        # Process each faculty group\n",
        "        for faculty in range(self.n_faculties):\n",
        "            # Get indices of students who desire this faculty\n",
        "            faculty_mask = desired_faculties == faculty\n",
        "            if not np.any(faculty_mask):\n",
        "                continue\n",
        "\n",
        "            # Calculate mean vector for this faculty group\n",
        "            faculty_mean = np.mean(modified_features[faculty_mask], axis=0)\n",
        "\n",
        "            # Calculate proportion vector (avoiding division by zero)\n",
        "            proportion_vector = np.ones_like(global_mean)\n",
        "            non_zero_mask = global_mean != 0\n",
        "            proportion_vector[non_zero_mask] = faculty_mean[non_zero_mask] / global_mean[non_zero_mask]\n",
        "\n",
        "            # Apply inverse proportion to reconstruct original features\n",
        "            reconstructed_features[faculty_mask] = modified_features[faculty_mask] / proportion_vector\n",
        "\n",
        "        return reconstructed_features\n",
        "\n",
        "    def assign_applicants_to_faculties_with_reconstruction(\n",
        "        self,\n",
        "        model: UniversityMLP,\n",
        "        modified_features: np.ndarray,\n",
        "        desired_faculties: np.ndarray\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Assign applicants to faculties using reconstructed original features.\n",
        "\n",
        "        Args:\n",
        "            model: Trained UniversityMLP model\n",
        "            modified_features: Modified features of current applicants\n",
        "            desired_faculties: Array of desired faculty indices\n",
        "\n",
        "        Returns:\n",
        "            Array of assigned faculty indices\n",
        "        \"\"\"\n",
        "        # First reconstruct the approximate original features\n",
        "        reconstructed_features = self.reconstruct_original_features(modified_features, desired_faculties)\n",
        "\n",
        "        # Use reconstructed features for prediction\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            features_tensor = torch.FloatTensor(reconstructed_features)\n",
        "            predicted_grades = model(features_tensor)\n",
        "            chosen_faculties = torch.argmax(predicted_grades, dim=1).numpy()\n",
        "\n",
        "        return chosen_faculties\n",
        "\n",
        "    def train_applicant_model(\n",
        "        self,\n",
        "        past_data: pd.DataFrame = None\n",
        "    ) -> ApplicantMLP:\n",
        "        \"\"\"Train applicant model on past data\"\"\"\n",
        "        if past_data is None:\n",
        "            past_data = self.past_applicants_df\n",
        "\n",
        "        if past_data is None:\n",
        "            raise ValueError(\"No past data available. Generate past applicants first.\")\n",
        "\n",
        "        # Create and train applicant's MLP model\n",
        "        feature_cols = [f\"feature_{i}\" for i in range(self.n_features)]\n",
        "        X_train = torch.FloatTensor(past_data[feature_cols].values)\n",
        "        y_train = torch.LongTensor(past_data['assigned_faculty'].values)\n",
        "\n",
        "        model = ApplicantMLP(self.n_features, self.n_faculties)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Reduced learning rate\n",
        "\n",
        "        # Train the model\n",
        "        model.train()\n",
        "\n",
        "        for epoch in range(500):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_train)\n",
        "            loss = criterion(outputs, y_train)\n",
        "            print(f'loss: {loss.item():.6f} at epoch {epoch} at applicants training')\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        return model\n",
        "\n",
        "    def choose_supplier_for_applicant(\n",
        "        self,\n",
        "        applicant_features: np.ndarray,\n",
        "        desired_faculty: int,\n",
        "        applicant_model: ApplicantMLP = None\n",
        "    ) -> Tuple[int, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Choose the best supplier for an applicant based on past data and supplier effects.\n",
        "\n",
        "        Args:\n",
        "            applicant_features: The current features of the applicant\n",
        "            desired_faculty: The faculty index the applicant wants to get into\n",
        "            past_data: Optional past data to train on. If None, uses self.past_applicants_df\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (chosen_supplier_idx, modified_features)\n",
        "        \"\"\"\n",
        "        # Evaluate each supplier's effect\n",
        "        applicant_model.eval()\n",
        "        best_probability = -1\n",
        "        best_supplier_idx = -1\n",
        "        best_modified_features = None\n",
        "\n",
        "        original_features = torch.FloatTensor(applicant_features).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Try each supplier\n",
        "            for i, supplier in enumerate(self.suppliers):\n",
        "                # Apply supplier's modification\n",
        "                modified_features_unclipped = original_features + torch.FloatTensor(supplier.diff_vector)\n",
        "                modified_features = np.clip(modified_features_unclipped, 40, 100)\n",
        "\n",
        "                # Get probability distribution over faculties\n",
        "                probabilities = applicant_model(modified_features)\n",
        "\n",
        "                # Check probability for desired faculty\n",
        "                prob_desired = probabilities[0, int(desired_faculty)].item()\n",
        "\n",
        "                if prob_desired > best_probability:\n",
        "                    best_probability = prob_desired\n",
        "                    best_supplier_idx = i\n",
        "                    best_modified_features = modified_features.squeeze(0).numpy()\n",
        "\n",
        "        if best_supplier_idx == -1:\n",
        "            # If no supplier improves probability, return original features with no supplier\n",
        "            return (-1, applicant_features)\n",
        "\n",
        "        return (best_supplier_idx, best_modified_features)\n",
        "\n",
        "    def choose_supplier_or_mechina_for_applicant(self, applicant_features, desired_faculty, applicant_model):\n",
        "        \"\"\"\n",
        "        Choose the best supplier for an applicant, allowing selection of either a regular supplier or a Mechina supplier.\n",
        "        \"\"\"\n",
        "        applicant_model.eval()\n",
        "        best_supplier_idx = -1\n",
        "        best_modified_features = applicant_features.copy()\n",
        "        best_probability = -1\n",
        "        used_mechina = False\n",
        "\n",
        "        original_features = torch.FloatTensor(applicant_features).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            # Evaluate regular suppliers\n",
        "            for i, supplier in enumerate(self.suppliers):\n",
        "                modified_features = np.clip(original_features + torch.FloatTensor(supplier.diff_vector), 40, 100)\n",
        "                probabilities = applicant_model(modified_features)\n",
        "                prob_desired = probabilities[0, int(desired_faculty)].item()\n",
        "\n",
        "                if prob_desired > best_probability:\n",
        "                    best_probability = prob_desired\n",
        "                    best_supplier_idx = i\n",
        "                    best_modified_features = modified_features.squeeze(0).numpy()\n",
        "                    used_mechina = False\n",
        "\n",
        "            # Evaluate Mechina suppliers\n",
        "            mechina_supplier = self.mechina_suppliers[int(desired_faculty)]\n",
        "            mechina_modified_features = np.clip(original_features + torch.FloatTensor(mechina_supplier.diff_vector), 40, 100)\n",
        "            probabilities = applicant_model(mechina_modified_features)\n",
        "            prob_desired_mechina = probabilities[0, int(desired_faculty)].item()\n",
        "\n",
        "            if prob_desired_mechina > best_probability:\n",
        "                best_probability = prob_desired_mechina\n",
        "                best_supplier_idx = desired_faculty  # Identifies Mechina supplier\n",
        "                best_modified_features = mechina_modified_features.squeeze(0).numpy()\n",
        "                used_mechina = True\n",
        "\n",
        "        return best_supplier_idx, best_modified_features, used_mechina\n",
        "\n",
        "    def assign_applicants_to_faculties_fully_exposed(\n",
        "        self,\n",
        "        model: UniversityMLP,\n",
        "        current_applicants_features: np.ndarray\n",
        "    ) -> Tuple[np.ndarray, np.ndarray, float]:\n",
        "        \"\"\"\n",
        "        Use trained model to make faculty recommendations for current applicants.\n",
        "\n",
        "        Args:\n",
        "            model: Trained UniversityMLP model\n",
        "            current_applicants_features: Modified features of current applicants (n_applicants x n_features)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (chosen_faculties, final_grades, mean_grade)\n",
        "            - chosen_faculties: Array of faculty indices chosen for each applicant\n",
        "            - final_grades: Array of final grades received by each applicant\n",
        "            - mean_grade: Average grade across all applicants\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            current_features = torch.FloatTensor(current_applicants_features)\n",
        "            predicted_grades = model(current_features)\n",
        "\n",
        "            # Choose best faculty for each applicant based on predicted grades\n",
        "            chosen_faculties = torch.argmax(predicted_grades).numpy()\n",
        "\n",
        "        return chosen_faculties, predicted_grades\n",
        "\n",
        "    def choose_supplier_for_applicant_fully_exposed(\n",
        "        self,\n",
        "        applicant_features: np.ndarray,\n",
        "        desired_faculty: int,\n",
        "        trained_model: UniversityMLP\n",
        "    ) -> Tuple[int, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Choose the best supplier for an applicant based on faculty utility vectors instead of a model.\n",
        "\n",
        "        The function ensures that the modified features lead to the **desired faculty** having the\n",
        "        highest grade among all faculties.\n",
        "\n",
        "        Args:\n",
        "            applicant_features: The current features of the applicant\n",
        "            desired_faculty: The faculty index the applicant wants to get into\n",
        "            faculty_utility_vectors: A numpy array (n_faculties x n_features) containing utility vectors for faculties\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (chosen_supplier_idx, modified_features)\n",
        "        \"\"\"\n",
        "        best_supplier_idx = -1\n",
        "        best_modified_features = None\n",
        "\n",
        "\n",
        "        # Get final assignments and grades using modified features\n",
        "        chosen_faculty, predicted_grades = self.assign_applicants_to_faculties_fully_exposed(\n",
        "            trained_model,\n",
        "            applicant_features\n",
        "        )\n",
        "\n",
        "\n",
        "        # If the desired faculty is already the best, return without any modifications\n",
        "        if chosen_faculty == desired_faculty:\n",
        "            return -1, applicant_features\n",
        "\n",
        "        # Iterate over suppliers and check if applying their modifications makes the desired faculty the best\n",
        "        for i, supplier in enumerate(self.suppliers):\n",
        "            # Apply supplier's modifications to features\n",
        "            modified_features = applicant_features + supplier.diff_vector\n",
        "            modified_features = np.clip(modified_features, 40, 100)  # Ensure within valid range\n",
        "\n",
        "             # Get final assignments and grades using modified features\n",
        "            chosen_faculty, predicted_grades = self.assign_applicants_to_faculties_fully_exposed(\n",
        "                trained_model,\n",
        "                modified_features\n",
        "            )\n",
        "\n",
        "            # Check if the desired faculty is now the highest-ranked one\n",
        "            if chosen_faculty == desired_faculty:\n",
        "                return i, modified_features  # Return the first supplier that achieves this\n",
        "\n",
        "        # If no supplier achieves the goal, return the original features\n",
        "        return -1, applicant_features\n",
        "\n",
        "\n",
        "    def recommend(\n",
        "        self,\n",
        "        student_features: np.ndarray,\n",
        "        recommended_faculties: np.ndarray\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Calculate final grades for students given their features and recommended faculties\n",
        "\n",
        "        Args:\n",
        "            student_features: Features matrix of shape (n_students, n_features)\n",
        "            recommended_faculties: Array of faculty indices of shape (n_students,)\n",
        "\n",
        "        Returns:\n",
        "            Array of final grades of shape (n_students,)\n",
        "        \"\"\"\n",
        "        # Get utility vectors for all recommended faculties\n",
        "        faculty_vectors = np.array([self.faculties[f].utility_vector for f in recommended_faculties])\n",
        "\n",
        "        print(f'faculty_vectors: {faculty_vectors}')\n",
        "        print(f'student_features: {student_features}')\n",
        "\n",
        "        # Calculate base grades using batch matrix multiplication\n",
        "        base_grades = np.sum(student_features * faculty_vectors, axis=1)\n",
        "\n",
        "        # Generate noise for all students at once\n",
        "        noise = np.random.uniform(*self.noise_range, size=len(student_features))\n",
        "\n",
        "        return base_grades + noise\n",
        "\n",
        "    def recommend_mechina(self, student_features: np.ndarray, modified_student_features: np.ndarray, used_mechina_array: np.ndarray, recommended_faculties: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculate final grades for students given their features and recommended faculties.\n",
        "        If a student used Mechina, their modified features are used instead of the original ones.\n",
        "        \"\"\"\n",
        "        # Ensure arrays are copied to avoid modifying original data\n",
        "        student_features = student_features.copy()\n",
        "        modified_student_features = modified_student_features.copy()\n",
        "\n",
        "        # Apply Mechina modifications where applicable\n",
        "        student_features[used_mechina_array] = modified_student_features[used_mechina_array]\n",
        "\n",
        "        # Get utility vectors for all recommended faculties\n",
        "        faculty_vectors = np.array([self.faculties[f].utility_vector for f in recommended_faculties])\n",
        "\n",
        "        print(f'faculty_vectors: {faculty_vectors}')\n",
        "        print(f'student_features: {student_features}')\n",
        "\n",
        "        # Calculate base grades using batch matrix multiplication\n",
        "        base_grades = np.sum(student_features * faculty_vectors, axis=1)\n",
        "\n",
        "        # Generate noise for all students at once\n",
        "        noise = np.random.uniform(*self.noise_range, size=len(student_features))\n",
        "\n",
        "        return base_grades + noise\n",
        "\n",
        "\n",
        "    def train_university_model(\n",
        "        self,\n",
        "        past_data: pd.DataFrame = None\n",
        "    ) -> UniversityMLP:\n",
        "        \"\"\"\n",
        "        Train university model on past data.\n",
        "\n",
        "        Args:\n",
        "            past_data: Optional past data to train on. If None, uses self.past_applicants_df\n",
        "\n",
        "        Returns:\n",
        "            Trained UniversityMLP model\n",
        "        \"\"\"\n",
        "        if past_data is None:\n",
        "            past_data = self.past_applicants_df\n",
        "\n",
        "        if past_data is None:\n",
        "            raise ValueError(\"No past data available. Generate past applicants first.\")\n",
        "\n",
        "        # Prepare training data\n",
        "        feature_cols = [f\"feature_{i}\" for i in range(self.n_features)]\n",
        "        X_train = torch.FloatTensor(past_data[feature_cols].values)\n",
        "\n",
        "        # Create and train university model\n",
        "        model = UniversityMLP(self.n_features, self.n_faculties)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "        # Custom loss function that only considers the assigned faculty's grade\n",
        "        def custom_loss(predictions, targets, assigned_faculties):\n",
        "            batch_size = predictions.size(0)\n",
        "            indices = torch.arange(batch_size)\n",
        "            predicted_assigned_grades = predictions[indices, assigned_faculties]\n",
        "            return torch.mean((predicted_assigned_grades - targets) ** 2)\n",
        "\n",
        "        # Train the model\n",
        "        model.train()\n",
        "        batch_size = 128\n",
        "        n_epochs = 100\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Process in batches\n",
        "            permutation = torch.randperm(len(X_train))\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                indices = permutation[i:i + batch_size]\n",
        "                batch_x = X_train[indices]\n",
        "                batch_y = torch.FloatTensor(past_data['final_grade'].values[indices])\n",
        "                batch_assigned = torch.LongTensor(past_data['assigned_faculty'].values[indices])\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                predictions = model(batch_x)\n",
        "                loss = custom_loss(predictions, batch_y, batch_assigned)\n",
        "                print(f'loss: {loss} at epoch {epoch}')\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        return model\n",
        "\n",
        "    def assign_applicants_to_faculties(\n",
        "        self,\n",
        "        model: UniversityMLP,\n",
        "        current_applicants_features: np.ndarray\n",
        "    ) -> Tuple[np.ndarray, np.ndarray, float]:\n",
        "        \"\"\"\n",
        "        Use trained model to make faculty recommendations for current applicants.\n",
        "\n",
        "        Args:\n",
        "            model: Trained UniversityMLP model\n",
        "            current_applicants_features: Modified features of current applicants (n_applicants x n_features)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (chosen_faculties, final_grades, mean_grade)\n",
        "            - chosen_faculties: Array of faculty indices chosen for each applicant\n",
        "            - final_grades: Array of final grades received by each applicant\n",
        "            - mean_grade: Average grade across all applicants\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            current_features = torch.FloatTensor(current_applicants_features)\n",
        "            predicted_grades = model(current_features)\n",
        "\n",
        "            # Choose best faculty for each applicant based on predicted grades\n",
        "            chosen_faculties = torch.argmax(predicted_grades, dim=1).numpy()\n",
        "\n",
        "        return chosen_faculties\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KZj7x-Ot-qEW"
      },
      "outputs": [],
      "source": [
        "def run_example():\n",
        "    # Create environment\n",
        "    env = UniversityEnvironment()\n",
        "\n",
        "    # Generate past applicants\n",
        "    past_df = env.generate_past_applicants(1000)\n",
        "    print(\"Past applicants shape:\", past_df.shape)\n",
        "\n",
        "    # Generate current applicants\n",
        "    current_df = env.generate_current_applicants(100)\n",
        "    print(\"Current applicants shape:\", current_df.shape)\n",
        "    print(f'current_df: {current_df}')\n",
        "\n",
        "    # Get modified features for all current applicants\n",
        "    feature_cols = [f\"feature_{i}\" for i in range(env.n_features)]\n",
        "    modified_features = []\n",
        "    original_features = current_df[feature_cols].values\n",
        "\n",
        "    for idx in range(len(current_df)):\n",
        "        student_features = current_df.iloc[idx][feature_cols].values\n",
        "        desired_faculty = current_df.iloc[idx]['desired_faculty']\n",
        "\n",
        "        _, modified_student_features = env.choose_supplier_for_applicant(\n",
        "            student_features,\n",
        "            desired_faculty\n",
        "        )\n",
        "        modified_features.append(modified_student_features)\n",
        "\n",
        "    modified_features = np.array(modified_features)\n",
        "\n",
        "    # Train university model\n",
        "    trained_model = env.train_university_model(past_df)\n",
        "\n",
        "    # Make predictions using trained model\n",
        "    chosen_faculties = env.assign_applicants_to_faculties(\n",
        "        trained_model,\n",
        "        modified_features\n",
        "    )\n",
        "    # Calculate percentage of students accepted into their desired faculty\n",
        "    desired_faculties = current_df['desired_faculty'].values\n",
        "    matches = (chosen_faculties == desired_faculties)\n",
        "    acceptance_rate = (np.sum(matches) / len(desired_faculties)) * 100\n",
        "\n",
        "    # Calculate final grades using original features\n",
        "    final_grades = env.recommend(original_features, chosen_faculties)\n",
        "    mean_grade = np.mean(final_grades)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nResults:\")\n",
        "    print(f\"Mean grade across all applicants: {mean_grade:.2f}\")\n",
        "    print(f\"\\nPercentage of students accepted to desired faculty: {acceptance_rate:.2f}%\")\n",
        "\n",
        "\n",
        "    # Print detailed results for first 5 applicants\n",
        "    print(\"\\nDetailed results for first 5 applicants:\")\n",
        "    for i in range(5):\n",
        "        desired_faculty = current_df.iloc[i]['desired_faculty']\n",
        "        print(f\"\\nApplicant {i}:\")\n",
        "        print(f\"Desired faculty: {desired_faculty}\")\n",
        "        print(f\"Assigned faculty: {chosen_faculties[i]}\")\n",
        "        print(f\"Final grade: {final_grades[i]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fqYcUXkb-qEW"
      },
      "outputs": [],
      "source": [
        "def calculate_desired_faculty_stats(assigned_faculties, desired_faculties):\n",
        "        total_students = len(desired_faculties)\n",
        "        matches = sum(assigned == desired for assigned, desired in zip(assigned_faculties, desired_faculties))\n",
        "        percentage = (matches / total_students) * 100\n",
        "        return matches, percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "69T-N5mx-qEX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fmy-HDgg-qEX"
      },
      "outputs": [],
      "source": [
        "def run_multi_iteration_example():\n",
        "    # Create environment\n",
        "    env = UniversityEnvironment()\n",
        "    feature_cols = [f\"feature_{i}\" for i in range(env.n_features)]\n",
        "\n",
        "    # Iteration -1: Initial University Training\n",
        "    print(\"\\n=== Iteration -1: Initial University Training ===\")\n",
        "    past_df = env.generate_past_applicants(10000)\n",
        "    trained_model = env.train_university_model(past_df)\n",
        "\n",
        "    # Generate students that will be used in iterations 0\n",
        "    iteration0_applicants_df = env.generate_current_applicants(10000)\n",
        "    original_features = iteration0_applicants_df[feature_cols].values\n",
        "    original_copy = iteration0_applicants_df[feature_cols].values\n",
        "\n",
        "\n",
        "\n",
        "    # Iteration 0: Pure Assignment\n",
        "    print(\"\\n=== Iteration 0: Pure Assignment ===\")\n",
        "    # Assign faculties using original features\n",
        "    iteration0_faculties = env.assign_applicants_to_faculties(\n",
        "        trained_model,\n",
        "        original_features\n",
        "    )\n",
        "\n",
        "    # Get real grades for these assignments\n",
        "    iteration0_grades = env.recommend(original_features, iteration0_faculties)\n",
        "\n",
        "    # Create training data for students from iteration 0\n",
        "    iteration0_df = pd.DataFrame(original_features, columns=feature_cols)\n",
        "    iteration0_df['assigned_faculty'] = iteration0_faculties\n",
        "    iteration0_df['final_grade'] = iteration0_grades\n",
        "\n",
        "    # Iteration 1: Student Learning\n",
        "    print(\"\\n=== Iteration 1: Student Learning ===\")\n",
        "    iteration1_applicants_df = env.generate_current_applicants(10000)\n",
        "    modified_features_with_features_knowledge = []\n",
        "    modified_features_without_features_knowledge = []\n",
        "    modified_features_with_mechina = []\n",
        "    used_mechina_array = []\n",
        "    desired_faculties = iteration1_applicants_df['desired_faculty'].values\n",
        "\n",
        "    applicant_model = env.train_applicant_model(iteration0_df)\n",
        "\n",
        "    for idx in range(len(iteration1_applicants_df)):\n",
        "        student_features = iteration1_applicants_df.iloc[idx][feature_cols].values\n",
        "        desired_faculty = iteration1_applicants_df.iloc[idx]['desired_faculty']\n",
        "\n",
        "        # Now students learn from iteration0 data instead of past_df\n",
        "        _, modified_student_features = env.choose_supplier_for_applicant(\n",
        "            student_features,\n",
        "            desired_faculty,\n",
        "            applicant_model\n",
        "        )\n",
        "        modified_features_with_features_knowledge.append(modified_student_features)\n",
        "        _, modified_student_features_without_features_knowledge = env.choose_supplier_for_applicant(\n",
        "            np.zeros_like(student_features),\n",
        "            desired_faculty,\n",
        "            applicant_model\n",
        "        )\n",
        "        modified_student_features_without_features_knowledge = modified_student_features_without_features_knowledge + student_features\n",
        "        modified_features_without_features_knowledge.append(modified_student_features_without_features_knowledge)\n",
        "\n",
        "        _, modified_student_features_with_mechina, used_machina  = env.choose_supplier_or_mechina_for_applicant(\n",
        "            student_features,\n",
        "            desired_faculty,\n",
        "            applicant_model\n",
        "        )\n",
        "        modified_features_with_mechina.append(modified_student_features)\n",
        "        used_mechina_array.append(used_machina)\n",
        "\n",
        "\n",
        "    modified_features_with_features_knowledge = np.array(modified_features_with_features_knowledge)\n",
        "    modified_features_without_features_knowledge = np.array(modified_features_without_features_knowledge)\n",
        "    modified_features_with_mechina = np.array(modified_features_with_mechina)\n",
        "\n",
        "    # Get final assignments and grades using modified features\n",
        "    final_faculties_modified = env.assign_applicants_to_faculties(\n",
        "        trained_model,\n",
        "        modified_features_with_features_knowledge\n",
        "    )\n",
        "\n",
        "    final_faculties_modified_without_features_knowledge = env.assign_applicants_to_faculties(\n",
        "        trained_model,\n",
        "        modified_features_without_features_knowledge\n",
        "    )\n",
        "\n",
        "    final_faculties_original = env.assign_applicants_to_faculties(\n",
        "        trained_model,\n",
        "        original_features\n",
        "    )\n",
        "\n",
        "    final_faculties_with_reconstruction = env.assign_applicants_to_faculties_with_reconstruction(\n",
        "        trained_model,\n",
        "        modified_features_with_features_knowledge,\n",
        "        desired_faculties\n",
        "    )\n",
        "\n",
        "    final_faculties_with_mechina = env.assign_applicants_to_faculties(\n",
        "        trained_model,\n",
        "        modified_features_with_mechina,\n",
        "    )\n",
        "\n",
        "    # Calculate final grades using original features\n",
        "    final_grades_original = env.recommend(original_features, final_faculties_original)\n",
        "    final_grades_modified = env.recommend(original_features, final_faculties_modified)\n",
        "    final_grades_modified_without_features_knowledge = env.recommend(original_features, final_faculties_modified_without_features_knowledge)\n",
        "    final_grades_with_reconstruction = env.recommend(original_features, final_faculties_with_reconstruction)\n",
        "    final_grades_with_mechina = env.recommend_mechina(original_features, modified_features_with_mechina,used_mechina_array, final_faculties_with_mechina)\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate stats for both iterations\n",
        "    desired_faculties = iteration1_applicants_df['desired_faculty'].values\n",
        "    final_matches_original, final_percentage_original = calculate_desired_faculty_stats(final_faculties_original, desired_faculties)\n",
        "    final_matches_modified, final_percentage_modified = calculate_desired_faculty_stats(final_faculties_modified, desired_faculties)\n",
        "    final_matches_modified_without_features_knowledge, final_percentage_modified_without_features_knowledge = calculate_desired_faculty_stats(final_faculties_modified_without_features_knowledge, desired_faculties)\n",
        "    final_matches_with_reconstruction, final_percentage_with_reconstruction = calculate_desired_faculty_stats(final_faculties_with_reconstruction, desired_faculties)\n",
        "    final_matches_with_mechina, final_percentage_with_mechina = calculate_desired_faculty_stats(final_faculties_with_mechina, desired_faculties)\n",
        "    # # Print comparison of results\n",
        "    # print(\"\\nResults Comparison:\")\n",
        "    # print(\"\\nIteration 0 (No Gaming):\")\n",
        "    # print(f\"Mean grade: {np.mean(iteration0_grades):.2f}\")\n",
        "    # print(f\"Faculty distribution: {np.bincount(iteration0_faculties)}\")\n",
        "    # print(f\"Students who got desired faculty: {iter0_matches} ({iter0_percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"Mean grade: {np.mean(final_grades_original):.2f}\")\n",
        "    print(f\"Faculty distribution: {np.bincount(final_faculties_original)}\")\n",
        "    print(f\"Students who got desired faculty: {final_matches_original} ({final_percentage_original:.1f}%)\")\n",
        "    print(f\"Mean grade: {np.mean(final_grades_modified):.2f}\")\n",
        "    print(f\"Faculty distribution: {np.bincount(final_faculties_modified)}\")\n",
        "    print(f\"Students who got desired faculty: {final_matches_modified} ({final_percentage_modified:.1f}%)\")\n",
        "    print(f\"Mean grade: {np.mean(final_grades_modified_without_features_knowledge):.2f}\")\n",
        "    print(f\"Faculty distribution: {np.bincount(final_faculties_modified_without_features_knowledge)}\")\n",
        "    print(f\"Students who got desired faculty: {final_matches_modified_without_features_knowledge} ({final_percentage_modified_without_features_knowledge:.1f}%)\")\n",
        "    print(f\"Mean grade: {np.mean(final_grades_with_reconstruction):.2f}\")\n",
        "    print(f\"Faculty distribution: {np.bincount(final_faculties_with_reconstruction)}\")\n",
        "    print(f\"Students who got desired faculty: {final_matches_with_reconstruction} ({final_percentage_with_reconstruction:.1f}%)\")\n",
        "    print(f'_Adding Mechina to the process_')\n",
        "    print(f\"Mean grade: {np.mean(final_grades_with_mechina):.2f}\")\n",
        "    print(f\"Faculty distribution: {np.bincount(final_faculties_with_mechina)}\")\n",
        "    print(f\"Students who got desired faculty: {final_matches_with_mechina} ({final_percentage_with_mechina:.1f}%)\")\n",
        "\n",
        "\n",
        "\n",
        "    return iteration1_applicants_df, feature_cols, env, trained_model, original_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbOsJ_nR-qEX",
        "outputId": "57064bc1-b4e3-417a-b837-f520c9b8c620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "loss: 0.0038728034123778343 at epoch 44\n",
            "loss: 0.0014590874779969454 at epoch 44\n",
            "loss: 0.005538171622902155 at epoch 44\n",
            "loss: 0.0012169842375442386 at epoch 44\n",
            "loss: 0.003069664817303419 at epoch 44\n",
            "loss: 0.006537904497236013 at epoch 44\n",
            "loss: 0.0040681916289031506 at epoch 44\n",
            "loss: 0.0042342450469732285 at epoch 44\n",
            "loss: 0.001992420759052038 at epoch 44\n",
            "loss: 0.003295635338872671 at epoch 44\n",
            "loss: 0.0018878413829952478 at epoch 44\n",
            "loss: 0.0025126889813691378 at epoch 44\n",
            "loss: 0.002544065937399864 at epoch 44\n",
            "loss: 0.0020072392653673887 at epoch 44\n",
            "loss: 0.0019545068498700857 at epoch 44\n",
            "loss: 0.0029814739245921373 at epoch 44\n",
            "loss: 0.0010455093579366803 at epoch 44\n",
            "loss: 0.002674537245184183 at epoch 44\n",
            "loss: 0.0017428374849259853 at epoch 44\n",
            "loss: 0.0018171060364693403 at epoch 44\n",
            "loss: 0.001395488390699029 at epoch 44\n",
            "loss: 0.0016025510849431157 at epoch 44\n",
            "loss: 0.005519432947039604 at epoch 44\n",
            "loss: 0.00397039158269763 at epoch 44\n",
            "loss: 0.005266161635518074 at epoch 44\n",
            "loss: 0.005324950907379389 at epoch 44\n",
            "loss: 0.001717720995657146 at epoch 44\n",
            "loss: 0.004041873384267092 at epoch 44\n",
            "loss: 0.0071562896482646465 at epoch 44\n",
            "loss: 0.002643470885232091 at epoch 44\n",
            "loss: 0.0023738713935017586 at epoch 44\n",
            "loss: 0.004899302497506142 at epoch 44\n",
            "loss: 0.0024828664027154446 at epoch 44\n",
            "loss: 0.0007509231800213456 at epoch 44\n",
            "loss: 0.002311481162905693 at epoch 44\n",
            "loss: 0.0051435548812150955 at epoch 44\n",
            "loss: 0.0016286332393065095 at epoch 44\n",
            "loss: 0.0016962075605988503 at epoch 44\n",
            "loss: 0.004586956463754177 at epoch 44\n",
            "loss: 0.006910707801580429 at epoch 44\n",
            "loss: 0.0013762296875938773 at epoch 44\n",
            "loss: 0.0026572896167635918 at epoch 44\n",
            "loss: 0.0044279382564127445 at epoch 44\n",
            "loss: 0.0022808010689914227 at epoch 44\n",
            "loss: 0.002342972671613097 at epoch 44\n",
            "loss: 0.006265650503337383 at epoch 44\n",
            "loss: 0.0029248653445392847 at epoch 44\n",
            "loss: 0.0034856884740293026 at epoch 44\n",
            "loss: 0.009362316690385342 at epoch 44\n",
            "loss: 0.0027336636558175087 at epoch 44\n",
            "loss: 0.0016920045018196106 at epoch 44\n",
            "loss: 0.008154325187206268 at epoch 45\n",
            "loss: 0.006374204531311989 at epoch 45\n",
            "loss: 0.006731603294610977 at epoch 45\n",
            "loss: 0.008472548797726631 at epoch 45\n",
            "loss: 0.008054479025304317 at epoch 45\n",
            "loss: 0.014615696854889393 at epoch 45\n",
            "loss: 0.003642700845375657 at epoch 45\n",
            "loss: 0.009248275309801102 at epoch 45\n",
            "loss: 0.008885486982762814 at epoch 45\n",
            "loss: 0.010719826444983482 at epoch 45\n",
            "loss: 0.003924557939171791 at epoch 45\n",
            "loss: 0.004763856064528227 at epoch 45\n",
            "loss: 0.007774571888148785 at epoch 45\n",
            "loss: 0.0024791196919977665 at epoch 45\n",
            "loss: 0.006879211403429508 at epoch 45\n",
            "loss: 0.005281677003949881 at epoch 45\n",
            "loss: 0.005253287963569164 at epoch 45\n",
            "loss: 0.0028874145355075598 at epoch 45\n",
            "loss: 0.004142810590565205 at epoch 45\n",
            "loss: 0.0018758117221295834 at epoch 45\n",
            "loss: 0.002268549520522356 at epoch 45\n",
            "loss: 0.0018712574383243918 at epoch 45\n",
            "loss: 0.0017740856856107712 at epoch 45\n",
            "loss: 0.004622170235961676 at epoch 45\n",
            "loss: 0.0044957781210541725 at epoch 45\n",
            "loss: 0.00406055711209774 at epoch 45\n",
            "loss: 0.0068746646866202354 at epoch 45\n",
            "loss: 0.0035940431989729404 at epoch 45\n",
            "loss: 0.011331030167639256 at epoch 45\n",
            "loss: 0.006251119542866945 at epoch 45\n",
            "loss: 0.008208836428821087 at epoch 45\n",
            "loss: 0.017202308401465416 at epoch 45\n",
            "loss: 0.007936051115393639 at epoch 45\n",
            "loss: 0.009659228846430779 at epoch 45\n",
            "loss: 0.012761148624122143 at epoch 45\n",
            "loss: 0.011768866330385208 at epoch 45\n",
            "loss: 0.008919795975089073 at epoch 45\n",
            "loss: 0.012366621755063534 at epoch 45\n",
            "loss: 0.04140710458159447 at epoch 45\n",
            "loss: 0.031263336539268494 at epoch 45\n",
            "loss: 0.027305277064442635 at epoch 45\n",
            "loss: 0.0261823870241642 at epoch 45\n",
            "loss: 0.032065168023109436 at epoch 45\n",
            "loss: 0.020862838253378868 at epoch 45\n",
            "loss: 0.03006443940103054 at epoch 45\n",
            "loss: 0.030427929013967514 at epoch 45\n",
            "loss: 0.02546832337975502 at epoch 45\n",
            "loss: 0.027618614956736565 at epoch 45\n",
            "loss: 0.011615082621574402 at epoch 45\n",
            "loss: 0.01782297156751156 at epoch 45\n",
            "loss: 0.01711566559970379 at epoch 45\n",
            "loss: 0.01962493173778057 at epoch 45\n",
            "loss: 0.0065152328461408615 at epoch 45\n",
            "loss: 0.01603543758392334 at epoch 45\n",
            "loss: 0.01194495614618063 at epoch 45\n",
            "loss: 0.010746875777840614 at epoch 45\n",
            "loss: 0.008845965377986431 at epoch 45\n",
            "loss: 0.007648089900612831 at epoch 45\n",
            "loss: 0.007359534502029419 at epoch 45\n",
            "loss: 0.009299889206886292 at epoch 45\n",
            "loss: 0.00830361433327198 at epoch 45\n",
            "loss: 0.00519253546372056 at epoch 45\n",
            "loss: 0.005550901405513287 at epoch 45\n",
            "loss: 0.005165004637092352 at epoch 45\n",
            "loss: 0.0066683003678917885 at epoch 45\n",
            "loss: 0.002554977312684059 at epoch 45\n",
            "loss: 0.007403316907584667 at epoch 45\n",
            "loss: 0.0026160827837884426 at epoch 45\n",
            "loss: 0.004857887513935566 at epoch 45\n",
            "loss: 0.002529790857806802 at epoch 45\n",
            "loss: 0.005627886857837439 at epoch 45\n",
            "loss: 0.005179446190595627 at epoch 45\n",
            "loss: 0.01229103747755289 at epoch 45\n",
            "loss: 0.0027445789892226458 at epoch 45\n",
            "loss: 0.005898746661841869 at epoch 45\n",
            "loss: 0.007133683189749718 at epoch 45\n",
            "loss: 0.008039676584303379 at epoch 45\n",
            "loss: 0.0018754824995994568 at epoch 45\n",
            "loss: 0.009132971055805683 at epoch 45\n",
            "loss: 0.010439474135637283 at epoch 46\n",
            "loss: 0.006212322972714901 at epoch 46\n",
            "loss: 0.012428517453372478 at epoch 46\n",
            "loss: 0.01671512983739376 at epoch 46\n",
            "loss: 0.01792016625404358 at epoch 46\n",
            "loss: 0.013682262971997261 at epoch 46\n",
            "loss: 0.014773880131542683 at epoch 46\n",
            "loss: 0.017820952460169792 at epoch 46\n",
            "loss: 0.028115125373005867 at epoch 46\n",
            "loss: 0.006376453675329685 at epoch 46\n",
            "loss: 0.01806192845106125 at epoch 46\n",
            "loss: 0.01534798089414835 at epoch 46\n",
            "loss: 0.008324047550559044 at epoch 46\n",
            "loss: 0.014167648740112782 at epoch 46\n",
            "loss: 0.018194060772657394 at epoch 46\n",
            "loss: 0.015659309923648834 at epoch 46\n",
            "loss: 0.02970188669860363 at epoch 46\n",
            "loss: 0.027220837771892548 at epoch 46\n",
            "loss: 0.016737446188926697 at epoch 46\n",
            "loss: 0.020084083080291748 at epoch 46\n",
            "loss: 0.04104571416974068 at epoch 46\n",
            "loss: 0.01590878888964653 at epoch 46\n",
            "loss: 0.0165264792740345 at epoch 46\n",
            "loss: 0.043947603553533554 at epoch 46\n",
            "loss: 0.028947800397872925 at epoch 46\n",
            "loss: 0.014961335808038712 at epoch 46\n",
            "loss: 0.05420346558094025 at epoch 46\n",
            "loss: 0.0258997343480587 at epoch 46\n",
            "loss: 0.039433978497982025 at epoch 46\n",
            "loss: 0.07587992399930954 at epoch 46\n",
            "loss: 0.012582107447087765 at epoch 46\n",
            "loss: 0.04131324216723442 at epoch 46\n",
            "loss: 0.01869259402155876 at epoch 46\n",
            "loss: 0.02349894866347313 at epoch 46\n",
            "loss: 0.025598032400012016 at epoch 46\n",
            "loss: 0.02704496681690216 at epoch 46\n",
            "loss: 0.018040306866168976 at epoch 46\n",
            "loss: 0.026584066450595856 at epoch 46\n",
            "loss: 0.015332619659602642 at epoch 46\n",
            "loss: 0.021849587559700012 at epoch 46\n",
            "loss: 0.02779484912753105 at epoch 46\n",
            "loss: 0.01266711950302124 at epoch 46\n",
            "loss: 0.015874508768320084 at epoch 46\n",
            "loss: 0.012378621846437454 at epoch 46\n",
            "loss: 0.024108484387397766 at epoch 46\n",
            "loss: 0.008535407483577728 at epoch 46\n",
            "loss: 0.013584474101662636 at epoch 46\n",
            "loss: 0.009324424900114536 at epoch 46\n",
            "loss: 0.02144911140203476 at epoch 46\n",
            "loss: 0.012250581756234169 at epoch 46\n",
            "loss: 0.005121689755469561 at epoch 46\n",
            "loss: 0.014044132083654404 at epoch 46\n",
            "loss: 0.031793732196092606 at epoch 46\n",
            "loss: 0.022781362757086754 at epoch 46\n",
            "loss: 0.0090261185541749 at epoch 46\n",
            "loss: 0.016620822250843048 at epoch 46\n",
            "loss: 0.03175553306937218 at epoch 46\n",
            "loss: 0.013739599846303463 at epoch 46\n",
            "loss: 0.013037823140621185 at epoch 46\n",
            "loss: 0.012841491028666496 at epoch 46\n",
            "loss: 0.020015520974993706 at epoch 46\n",
            "loss: 0.0195462666451931 at epoch 46\n",
            "loss: 0.012733561918139458 at epoch 46\n",
            "loss: 0.008336823433637619 at epoch 46\n",
            "loss: 0.004625275731086731 at epoch 46\n",
            "loss: 0.004564100876450539 at epoch 46\n",
            "loss: 0.00863569788634777 at epoch 46\n",
            "loss: 0.008776361122727394 at epoch 46\n",
            "loss: 0.018250420689582825 at epoch 46\n",
            "loss: 0.019598746672272682 at epoch 46\n",
            "loss: 0.008012945763766766 at epoch 46\n",
            "loss: 0.014179001562297344 at epoch 46\n",
            "loss: 0.019550122320652008 at epoch 46\n",
            "loss: 0.013518624939024448 at epoch 46\n",
            "loss: 0.007722587324678898 at epoch 46\n",
            "loss: 0.02700795792043209 at epoch 46\n",
            "loss: 0.02117699384689331 at epoch 46\n",
            "loss: 0.008930429816246033 at epoch 46\n",
            "loss: 0.020814858376979828 at epoch 46\n",
            "loss: 0.012189588509500027 at epoch 47\n",
            "loss: 0.02034936659038067 at epoch 47\n",
            "loss: 0.019415415823459625 at epoch 47\n",
            "loss: 0.025641009211540222 at epoch 47\n",
            "loss: 0.03823800012469292 at epoch 47\n",
            "loss: 0.021739695221185684 at epoch 47\n",
            "loss: 0.009587922133505344 at epoch 47\n",
            "loss: 0.03839586675167084 at epoch 47\n",
            "loss: 0.019813524559140205 at epoch 47\n",
            "loss: 0.02363685518503189 at epoch 47\n",
            "loss: 0.0087181581184268 at epoch 47\n",
            "loss: 0.015772761777043343 at epoch 47\n",
            "loss: 0.011844979599118233 at epoch 47\n",
            "loss: 0.004578134510666132 at epoch 47\n",
            "loss: 0.0030811442993581295 at epoch 47\n",
            "loss: 0.0024032858200371265 at epoch 47\n",
            "loss: 0.0036754123866558075 at epoch 47\n",
            "loss: 0.001805775100365281 at epoch 47\n",
            "loss: 0.00248130620457232 at epoch 47\n",
            "loss: 0.00195989734493196 at epoch 47\n",
            "loss: 0.002386968582868576 at epoch 47\n",
            "loss: 0.003910521976649761 at epoch 47\n",
            "loss: 0.004294471349567175 at epoch 47\n",
            "loss: 0.002382848644629121 at epoch 47\n",
            "loss: 0.00492599094286561 at epoch 47\n",
            "loss: 0.003696299158036709 at epoch 47\n",
            "loss: 0.0037028773222118616 at epoch 47\n",
            "loss: 0.007735817693173885 at epoch 47\n",
            "loss: 0.006517789326608181 at epoch 47\n",
            "loss: 0.001883704331703484 at epoch 47\n",
            "loss: 0.010043592192232609 at epoch 47\n",
            "loss: 0.004027171526104212 at epoch 47\n",
            "loss: 0.006612617056816816 at epoch 47\n",
            "loss: 0.008891346864402294 at epoch 47\n",
            "loss: 0.003681014757603407 at epoch 47\n",
            "loss: 0.0035990276373922825 at epoch 47\n",
            "loss: 0.01412542350590229 at epoch 47\n",
            "loss: 0.014178270474076271 at epoch 47\n",
            "loss: 0.012835465371608734 at epoch 47\n",
            "loss: 0.006263455841690302 at epoch 47\n",
            "loss: 0.0019702655263245106 at epoch 47\n",
            "loss: 0.01478379126638174 at epoch 47\n",
            "loss: 0.01907457411289215 at epoch 47\n",
            "loss: 0.012187766842544079 at epoch 47\n",
            "loss: 0.009739017114043236 at epoch 47\n",
            "loss: 0.014210261404514313 at epoch 47\n",
            "loss: 0.0173496063798666 at epoch 47\n",
            "loss: 0.01012472901493311 at epoch 47\n",
            "loss: 0.0023753358982503414 at epoch 47\n",
            "loss: 0.013825422152876854 at epoch 47\n",
            "loss: 0.015360487625002861 at epoch 47\n",
            "loss: 0.005017379764467478 at epoch 47\n",
            "loss: 0.01019739918410778 at epoch 47\n",
            "loss: 0.01885060779750347 at epoch 47\n",
            "loss: 0.010599980130791664 at epoch 47\n",
            "loss: 0.006196320056915283 at epoch 47\n",
            "loss: 0.01606336422264576 at epoch 47\n",
            "loss: 0.0032795784063637257 at epoch 47\n",
            "loss: 0.01183249894529581 at epoch 47\n",
            "loss: 0.0017668462824076414 at epoch 47\n",
            "loss: 0.010077848099172115 at epoch 47\n",
            "loss: 0.007620601449161768 at epoch 47\n",
            "loss: 0.007519284263253212 at epoch 47\n",
            "loss: 0.006408833432942629 at epoch 47\n",
            "loss: 0.010209125466644764 at epoch 47\n",
            "loss: 0.00940832868218422 at epoch 47\n",
            "loss: 0.005703945178538561 at epoch 47\n",
            "loss: 0.013472783379256725 at epoch 47\n",
            "loss: 0.006376277655363083 at epoch 47\n",
            "loss: 0.007818545214831829 at epoch 47\n",
            "loss: 0.011767144314944744 at epoch 47\n",
            "loss: 0.015167223289608955 at epoch 47\n",
            "loss: 0.01332052331417799 at epoch 47\n",
            "loss: 0.016179271042346954 at epoch 47\n",
            "loss: 0.0738934576511383 at epoch 47\n",
            "loss: 0.06292615085840225 at epoch 47\n",
            "loss: 0.03334852308034897 at epoch 47\n",
            "loss: 0.04419846087694168 at epoch 47\n",
            "loss: 0.017163239419460297 at epoch 47\n",
            "loss: 0.0616735965013504 at epoch 48\n",
            "loss: 0.05217990651726723 at epoch 48\n",
            "loss: 0.0877586156129837 at epoch 48\n",
            "loss: 0.04588981717824936 at epoch 48\n",
            "loss: 0.05936875566840172 at epoch 48\n",
            "loss: 0.10984610766172409 at epoch 48\n",
            "loss: 0.09770539402961731 at epoch 48\n",
            "loss: 0.08136840909719467 at epoch 48\n",
            "loss: 0.04731706902384758 at epoch 48\n",
            "loss: 0.11276978999376297 at epoch 48\n",
            "loss: 0.10666738450527191 at epoch 48\n",
            "loss: 0.08465174585580826 at epoch 48\n",
            "loss: 0.06181883439421654 at epoch 48\n",
            "loss: 0.13480594754219055 at epoch 48\n",
            "loss: 0.07665136456489563 at epoch 48\n",
            "loss: 0.06310343742370605 at epoch 48\n",
            "loss: 0.08931541442871094 at epoch 48\n",
            "loss: 0.19587859511375427 at epoch 48\n",
            "loss: 0.1276681125164032 at epoch 48\n",
            "loss: 0.048085033893585205 at epoch 48\n",
            "loss: 0.11524266004562378 at epoch 48\n",
            "loss: 0.16374444961547852 at epoch 48\n",
            "loss: 0.08575893193483353 at epoch 48\n",
            "loss: 0.02849404513835907 at epoch 48\n",
            "loss: 0.1481669396162033 at epoch 48\n",
            "loss: 0.19267341494560242 at epoch 48\n",
            "loss: 0.2652076482772827 at epoch 48\n",
            "loss: 0.09248756617307663 at epoch 48\n",
            "loss: 0.08617434650659561 at epoch 48\n",
            "loss: 0.13489091396331787 at epoch 48\n",
            "loss: 0.3549467623233795 at epoch 48\n",
            "loss: 0.12954293191432953 at epoch 48\n",
            "loss: 0.09922899305820465 at epoch 48\n",
            "loss: 0.16669154167175293 at epoch 48\n",
            "loss: 0.1015695333480835 at epoch 48\n",
            "loss: 0.08607371896505356 at epoch 48\n",
            "loss: 0.026012562215328217 at epoch 48\n",
            "loss: 0.08539941906929016 at epoch 48\n",
            "loss: 0.06453897804021835 at epoch 48\n",
            "loss: 0.03174443542957306 at epoch 48\n",
            "loss: 0.055967748165130615 at epoch 48\n",
            "loss: 0.12851734459400177 at epoch 48\n",
            "loss: 0.10583469271659851 at epoch 48\n",
            "loss: 0.07355361431837082 at epoch 48\n",
            "loss: 0.039319176226854324 at epoch 48\n",
            "loss: 0.07290036231279373 at epoch 48\n",
            "loss: 0.04696054011583328 at epoch 48\n",
            "loss: 0.11682286858558655 at epoch 48\n",
            "loss: 0.07439980655908585 at epoch 48\n",
            "loss: 0.04825694113969803 at epoch 48\n",
            "loss: 0.08645438402891159 at epoch 48\n",
            "loss: 0.08288165926933289 at epoch 48\n",
            "loss: 0.09768466651439667 at epoch 48\n",
            "loss: 0.07334031164646149 at epoch 48\n",
            "loss: 0.05386427789926529 at epoch 48\n",
            "loss: 0.1216505840420723 at epoch 48\n",
            "loss: 0.057393986731767654 at epoch 48\n",
            "loss: 0.12940570712089539 at epoch 48\n",
            "loss: 0.2810683846473694 at epoch 48\n",
            "loss: 0.10030728578567505 at epoch 48\n",
            "loss: 0.19093452394008636 at epoch 48\n",
            "loss: 0.11779449135065079 at epoch 48\n",
            "loss: 0.10797181725502014 at epoch 48\n",
            "loss: 0.06766247004270554 at epoch 48\n",
            "loss: 0.0988934338092804 at epoch 48\n",
            "loss: 0.05395721271634102 at epoch 48\n",
            "loss: 0.1717793196439743 at epoch 48\n",
            "loss: 0.10393165051937103 at epoch 48\n",
            "loss: 0.16131538152694702 at epoch 48\n",
            "loss: 0.1106140986084938 at epoch 48\n",
            "loss: 0.11821666359901428 at epoch 48\n",
            "loss: 0.06772144138813019 at epoch 48\n",
            "loss: 0.20905598998069763 at epoch 48\n",
            "loss: 0.07811914384365082 at epoch 48\n",
            "loss: 0.13160386681556702 at epoch 48\n",
            "loss: 0.08701014518737793 at epoch 48\n",
            "loss: 0.10938744992017746 at epoch 48\n",
            "loss: 0.10498704016208649 at epoch 48\n",
            "loss: 0.10130354762077332 at epoch 48\n",
            "loss: 0.07989053428173065 at epoch 49\n",
            "loss: 0.16432930529117584 at epoch 49\n",
            "loss: 0.13397647440433502 at epoch 49\n",
            "loss: 0.05764675512909889 at epoch 49\n",
            "loss: 0.07192622125148773 at epoch 49\n",
            "loss: 0.20901896059513092 at epoch 49\n",
            "loss: 0.39319726824760437 at epoch 49\n",
            "loss: 0.1854967474937439 at epoch 49\n",
            "loss: 0.07696104794740677 at epoch 49\n",
            "loss: 0.3082204759120941 at epoch 49\n",
            "loss: 0.4057618975639343 at epoch 49\n",
            "loss: 0.16341198980808258 at epoch 49\n",
            "loss: 0.17670799791812897 at epoch 49\n",
            "loss: 0.7239740490913391 at epoch 49\n",
            "loss: 0.9792078137397766 at epoch 49\n",
            "loss: 1.0610251426696777 at epoch 49\n",
            "loss: 0.16538940370082855 at epoch 49\n",
            "loss: 0.844307541847229 at epoch 49\n",
            "loss: 0.5526248812675476 at epoch 49\n",
            "loss: 0.9454834461212158 at epoch 49\n",
            "loss: 0.18773162364959717 at epoch 49\n",
            "loss: 1.0378146171569824 at epoch 49\n",
            "loss: 0.6989033222198486 at epoch 49\n",
            "loss: 0.48740944266319275 at epoch 49\n",
            "loss: 0.8664740324020386 at epoch 49\n",
            "loss: 2.324215888977051 at epoch 49\n",
            "loss: 2.0316710472106934 at epoch 49\n",
            "loss: 0.5694015026092529 at epoch 49\n",
            "loss: 1.8057736158370972 at epoch 49\n",
            "loss: 1.8206839561462402 at epoch 49\n",
            "loss: 0.8937112092971802 at epoch 49\n",
            "loss: 1.4676358699798584 at epoch 49\n",
            "loss: 2.592057704925537 at epoch 49\n",
            "loss: 0.6286556720733643 at epoch 49\n",
            "loss: 2.8289413452148438 at epoch 49\n",
            "loss: 3.0132088661193848 at epoch 49\n",
            "loss: 0.8934551477432251 at epoch 49\n",
            "loss: 2.0211703777313232 at epoch 49\n",
            "loss: 2.3054518699645996 at epoch 49\n",
            "loss: 0.32479751110076904 at epoch 49\n",
            "loss: 1.3457794189453125 at epoch 49\n",
            "loss: 0.5976946949958801 at epoch 49\n",
            "loss: 0.5893286466598511 at epoch 49\n",
            "loss: 0.8145377039909363 at epoch 49\n",
            "loss: 0.5353885889053345 at epoch 49\n",
            "loss: 0.1760985255241394 at epoch 49\n",
            "loss: 0.3813283443450928 at epoch 49\n",
            "loss: 0.2769146263599396 at epoch 49\n",
            "loss: 0.12721408903598785 at epoch 49\n",
            "loss: 0.46944794058799744 at epoch 49\n",
            "loss: 0.09793132543563843 at epoch 49\n",
            "loss: 0.36925631761550903 at epoch 49\n",
            "loss: 0.6466640830039978 at epoch 49\n",
            "loss: 0.11987413465976715 at epoch 49\n",
            "loss: 0.9657212495803833 at epoch 49\n",
            "loss: 0.3899557590484619 at epoch 49\n",
            "loss: 0.2520666718482971 at epoch 49\n",
            "loss: 0.32701238989830017 at epoch 49\n",
            "loss: 0.4944009780883789 at epoch 49\n",
            "loss: 0.4332970678806305 at epoch 49\n",
            "loss: 0.4526127278804779 at epoch 49\n",
            "loss: 0.3714526891708374 at epoch 49\n",
            "loss: 0.21608306467533112 at epoch 49\n",
            "loss: 0.464078426361084 at epoch 49\n",
            "loss: 0.0846472904086113 at epoch 49\n",
            "loss: 0.5354510545730591 at epoch 49\n",
            "loss: 0.040267642587423325 at epoch 49\n",
            "loss: 0.3149952292442322 at epoch 49\n",
            "loss: 0.14167089760303497 at epoch 49\n",
            "loss: 0.5752507448196411 at epoch 49\n",
            "loss: 0.05924249067902565 at epoch 49\n",
            "loss: 0.344325989484787 at epoch 49\n",
            "loss: 0.06526505202054977 at epoch 49\n",
            "loss: 0.4125005006790161 at epoch 49\n",
            "loss: 0.026647157967090607 at epoch 49\n",
            "loss: 0.2782590687274933 at epoch 49\n",
            "loss: 0.14671820402145386 at epoch 49\n",
            "loss: 0.2915460467338562 at epoch 49\n",
            "loss: 0.05561506375670433 at epoch 49\n",
            "loss: 0.2542632520198822 at epoch 50\n",
            "loss: 0.08075789362192154 at epoch 50\n",
            "loss: 0.13976210355758667 at epoch 50\n",
            "loss: 0.07117100059986115 at epoch 50\n",
            "loss: 0.11666881293058395 at epoch 50\n",
            "loss: 0.047067925333976746 at epoch 50\n",
            "loss: 0.12496442347764969 at epoch 50\n",
            "loss: 0.05941983312368393 at epoch 50\n",
            "loss: 0.08154251426458359 at epoch 50\n",
            "loss: 0.041470348834991455 at epoch 50\n",
            "loss: 0.030471492558717728 at epoch 50\n",
            "loss: 0.040484458208084106 at epoch 50\n",
            "loss: 0.01714743860065937 at epoch 50\n",
            "loss: 0.03930193930864334 at epoch 50\n",
            "loss: 0.013325637206435204 at epoch 50\n",
            "loss: 0.02649007737636566 at epoch 50\n",
            "loss: 0.013235281221568584 at epoch 50\n",
            "loss: 0.014235977083444595 at epoch 50\n",
            "loss: 0.018981438130140305 at epoch 50\n",
            "loss: 0.01395661011338234 at epoch 50\n",
            "loss: 0.019598135724663734 at epoch 50\n",
            "loss: 0.009716170839965343 at epoch 50\n",
            "loss: 0.028317425400018692 at epoch 50\n",
            "loss: 0.0176934115588665 at epoch 50\n",
            "loss: 0.018811751157045364 at epoch 50\n",
            "loss: 0.022207245230674744 at epoch 50\n",
            "loss: 0.015217513777315617 at epoch 50\n",
            "loss: 0.028864435851573944 at epoch 50\n",
            "loss: 0.011834671720862389 at epoch 50\n",
            "loss: 0.030097343027591705 at epoch 50\n",
            "loss: 0.012001764960587025 at epoch 50\n",
            "loss: 0.019971847534179688 at epoch 50\n",
            "loss: 0.027674710378050804 at epoch 50\n",
            "loss: 0.017720770090818405 at epoch 50\n",
            "loss: 0.03321389481425285 at epoch 50\n",
            "loss: 0.00815702136605978 at epoch 50\n",
            "loss: 0.03284188359975815 at epoch 50\n",
            "loss: 0.0047675431706011295 at epoch 50\n",
            "loss: 0.022265344858169556 at epoch 50\n",
            "loss: 0.008516063913702965 at epoch 50\n",
            "loss: 0.009821699932217598 at epoch 50\n",
            "loss: 0.009435628540813923 at epoch 50\n",
            "loss: 0.009946724399924278 at epoch 50\n",
            "loss: 0.010268686339259148 at epoch 50\n",
            "loss: 0.008738086558878422 at epoch 50\n",
            "loss: 0.0038989922031760216 at epoch 50\n",
            "loss: 0.008123265579342842 at epoch 50\n",
            "loss: 0.0038976508658379316 at epoch 50\n",
            "loss: 0.008144252002239227 at epoch 50\n",
            "loss: 0.00658836867660284 at epoch 50\n",
            "loss: 0.007899869233369827 at epoch 50\n",
            "loss: 0.01905953697860241 at epoch 50\n",
            "loss: 0.011165222153067589 at epoch 50\n",
            "loss: 0.021676726639270782 at epoch 50\n",
            "loss: 0.016037682071328163 at epoch 50\n",
            "loss: 0.004786945879459381 at epoch 50\n",
            "loss: 0.008070038631558418 at epoch 50\n",
            "loss: 0.005618470720946789 at epoch 50\n",
            "loss: 0.004122342448681593 at epoch 50\n",
            "loss: 0.00803102646023035 at epoch 50\n",
            "loss: 0.003063046373426914 at epoch 50\n",
            "loss: 0.007600128650665283 at epoch 50\n",
            "loss: 0.013868371956050396 at epoch 50\n",
            "loss: 0.007050369866192341 at epoch 50\n",
            "loss: 0.0018427282338961959 at epoch 50\n",
            "loss: 0.0065604024566709995 at epoch 50\n",
            "loss: 0.0030722222290933132 at epoch 50\n",
            "loss: 0.004795861896127462 at epoch 50\n",
            "loss: 0.004577687010169029 at epoch 50\n",
            "loss: 0.0037028584629297256 at epoch 50\n",
            "loss: 0.003268101019784808 at epoch 50\n",
            "loss: 0.0033375993371009827 at epoch 50\n",
            "loss: 0.0038194190710783005 at epoch 50\n",
            "loss: 0.0020007998682558537 at epoch 50\n",
            "loss: 0.008718486875295639 at epoch 50\n",
            "loss: 0.0041897837072610855 at epoch 50\n",
            "loss: 0.0037682410329580307 at epoch 50\n",
            "loss: 0.0006144815706647933 at epoch 50\n",
            "loss: 0.0026921529788523912 at epoch 50\n",
            "loss: 0.008480675518512726 at epoch 51\n",
            "loss: 0.007821097038686275 at epoch 51\n",
            "loss: 0.00686119357123971 at epoch 51\n",
            "loss: 0.01661030948162079 at epoch 51\n",
            "loss: 0.01681673154234886 at epoch 51\n",
            "loss: 0.0066350470297038555 at epoch 51\n",
            "loss: 0.01647518202662468 at epoch 51\n",
            "loss: 0.008023514412343502 at epoch 51\n",
            "loss: 0.006740823853760958 at epoch 51\n",
            "loss: 0.011747174896299839 at epoch 51\n",
            "loss: 0.01247810572385788 at epoch 51\n",
            "loss: 0.010774333029985428 at epoch 51\n",
            "loss: 0.007089714519679546 at epoch 51\n",
            "loss: 0.016469506546854973 at epoch 51\n",
            "loss: 0.0055845193564891815 at epoch 51\n",
            "loss: 0.010336114093661308 at epoch 51\n",
            "loss: 0.015155733563005924 at epoch 51\n",
            "loss: 0.008046522736549377 at epoch 51\n",
            "loss: 0.006688529159873724 at epoch 51\n",
            "loss: 0.01818455569446087 at epoch 51\n",
            "loss: 0.002656596014276147 at epoch 51\n",
            "loss: 0.02023898810148239 at epoch 51\n",
            "loss: 0.005315632093697786 at epoch 51\n",
            "loss: 0.01890963688492775 at epoch 51\n",
            "loss: 0.01206246018409729 at epoch 51\n",
            "loss: 0.01105752307921648 at epoch 51\n",
            "loss: 0.009687790647149086 at epoch 51\n",
            "loss: 0.02024211175739765 at epoch 51\n",
            "loss: 0.001923802075907588 at epoch 51\n",
            "loss: 0.014083972200751305 at epoch 51\n",
            "loss: 0.01211170107126236 at epoch 51\n",
            "loss: 0.007362218573689461 at epoch 51\n",
            "loss: 0.017187325283885002 at epoch 51\n",
            "loss: 0.02217904105782509 at epoch 51\n",
            "loss: 0.0031370532233268023 at epoch 51\n",
            "loss: 0.02500474825501442 at epoch 51\n",
            "loss: 0.00995364598929882 at epoch 51\n",
            "loss: 0.010922957211732864 at epoch 51\n",
            "loss: 0.007442149333655834 at epoch 51\n",
            "loss: 0.01424990501254797 at epoch 51\n",
            "loss: 0.001289949519559741 at epoch 51\n",
            "loss: 0.011191709898412228 at epoch 51\n",
            "loss: 0.007702792063355446 at epoch 51\n",
            "loss: 0.004079695791006088 at epoch 51\n",
            "loss: 0.009280727244913578 at epoch 51\n",
            "loss: 0.008745836094021797 at epoch 51\n",
            "loss: 0.008547145873308182 at epoch 51\n",
            "loss: 0.01075806189328432 at epoch 51\n",
            "loss: 0.02041517198085785 at epoch 51\n",
            "loss: 0.007511856500059366 at epoch 51\n",
            "loss: 0.01665845327079296 at epoch 51\n",
            "loss: 0.008817975409328938 at epoch 51\n",
            "loss: 0.0023282745387405157 at epoch 51\n",
            "loss: 0.006718343589454889 at epoch 51\n",
            "loss: 0.0027932061348110437 at epoch 51\n",
            "loss: 0.008286001160740852 at epoch 51\n",
            "loss: 0.0030497456900775433 at epoch 51\n",
            "loss: 0.003400635439902544 at epoch 51\n",
            "loss: 0.0034896903671324253 at epoch 51\n",
            "loss: 0.008358452469110489 at epoch 51\n",
            "loss: 0.004328396171331406 at epoch 51\n",
            "loss: 0.006853760685771704 at epoch 51\n",
            "loss: 0.006503220181912184 at epoch 51\n",
            "loss: 0.004859490320086479 at epoch 51\n",
            "loss: 0.004247091710567474 at epoch 51\n",
            "loss: 0.009984975680708885 at epoch 51\n",
            "loss: 0.002341663930565119 at epoch 51\n",
            "loss: 0.007670429535210133 at epoch 51\n",
            "loss: 0.007860448211431503 at epoch 51\n",
            "loss: 0.007239385042339563 at epoch 51\n",
            "loss: 0.008040329441428185 at epoch 51\n",
            "loss: 0.006871890742331743 at epoch 51\n",
            "loss: 0.0054314350709319115 at epoch 51\n",
            "loss: 0.0033324575051665306 at epoch 51\n",
            "loss: 0.007854349911212921 at epoch 51\n",
            "loss: 0.006812341045588255 at epoch 51\n",
            "loss: 0.0008136347169056535 at epoch 51\n",
            "loss: 0.0066788895055651665 at epoch 51\n",
            "loss: 0.009190339595079422 at epoch 51\n",
            "loss: 0.007479286752641201 at epoch 52\n",
            "loss: 0.0015152590349316597 at epoch 52\n",
            "loss: 0.007261470425873995 at epoch 52\n",
            "loss: 0.006677594035863876 at epoch 52\n",
            "loss: 0.008897264488041401 at epoch 52\n",
            "loss: 0.004344183485955 at epoch 52\n",
            "loss: 0.003853782080113888 at epoch 52\n",
            "loss: 0.005220934748649597 at epoch 52\n",
            "loss: 0.008152910508215427 at epoch 52\n",
            "loss: 0.00748056173324585 at epoch 52\n",
            "loss: 0.006602524779736996 at epoch 52\n",
            "loss: 0.016147561371326447 at epoch 52\n",
            "loss: 0.0037602605298161507 at epoch 52\n",
            "loss: 0.012977443635463715 at epoch 52\n",
            "loss: 0.015993067994713783 at epoch 52\n",
            "loss: 0.021985294297337532 at epoch 52\n",
            "loss: 0.004666451830416918 at epoch 52\n",
            "loss: 0.01602363958954811 at epoch 52\n",
            "loss: 0.02537991851568222 at epoch 52\n",
            "loss: 0.029005836695432663 at epoch 52\n",
            "loss: 0.007644337601959705 at epoch 52\n",
            "loss: 0.014193500392138958 at epoch 52\n",
            "loss: 0.029361700639128685 at epoch 52\n",
            "loss: 0.011760477907955647 at epoch 52\n",
            "loss: 0.009222926571965218 at epoch 52\n",
            "loss: 0.019390325993299484 at epoch 52\n",
            "loss: 0.03385797142982483 at epoch 52\n",
            "loss: 0.013135487213730812 at epoch 52\n",
            "loss: 0.010443122126162052 at epoch 52\n",
            "loss: 0.019054275006055832 at epoch 52\n",
            "loss: 0.03453266620635986 at epoch 52\n",
            "loss: 0.009973517619073391 at epoch 52\n",
            "loss: 0.016551459208130836 at epoch 52\n",
            "loss: 0.02718961052596569 at epoch 52\n",
            "loss: 0.026479605585336685 at epoch 52\n",
            "loss: 0.018734343349933624 at epoch 52\n",
            "loss: 0.007625708356499672 at epoch 52\n",
            "loss: 0.01268705539405346 at epoch 52\n",
            "loss: 0.009844405576586723 at epoch 52\n",
            "loss: 0.031992748379707336 at epoch 52\n",
            "loss: 0.022528979927301407 at epoch 52\n",
            "loss: 0.020840030163526535 at epoch 52\n",
            "loss: 0.02843925543129444 at epoch 52\n",
            "loss: 0.0222167931497097 at epoch 52\n",
            "loss: 0.026579590514302254 at epoch 52\n",
            "loss: 0.004167007748037577 at epoch 52\n",
            "loss: 0.03782223165035248 at epoch 52\n",
            "loss: 0.028237400576472282 at epoch 52\n",
            "loss: 0.027851330116391182 at epoch 52\n",
            "loss: 0.01717161014676094 at epoch 52\n",
            "loss: 0.046267494559288025 at epoch 52\n",
            "loss: 0.010514069348573685 at epoch 52\n",
            "loss: 0.0207765344530344 at epoch 52\n",
            "loss: 0.04398028552532196 at epoch 52\n",
            "loss: 0.025296783074736595 at epoch 52\n",
            "loss: 0.004212329164147377 at epoch 52\n",
            "loss: 0.031360723078250885 at epoch 52\n",
            "loss: 0.03650375455617905 at epoch 52\n",
            "loss: 0.020605729892849922 at epoch 52\n",
            "loss: 0.005987790413200855 at epoch 52\n",
            "loss: 0.025665871798992157 at epoch 52\n",
            "loss: 0.015490267425775528 at epoch 52\n",
            "loss: 0.020661627873778343 at epoch 52\n",
            "loss: 0.010793960653245449 at epoch 52\n",
            "loss: 0.015531869605183601 at epoch 52\n",
            "loss: 0.02148752100765705 at epoch 52\n",
            "loss: 0.006580396555364132 at epoch 52\n",
            "loss: 0.01226117555052042 at epoch 52\n",
            "loss: 0.006399281322956085 at epoch 52\n",
            "loss: 0.014058299362659454 at epoch 52\n",
            "loss: 0.010669846087694168 at epoch 52\n",
            "loss: 0.002821671310812235 at epoch 52\n",
            "loss: 0.005561486817896366 at epoch 52\n",
            "loss: 0.0076703582890331745 at epoch 52\n",
            "loss: 0.00921692419797182 at epoch 52\n",
            "loss: 0.0028321819845587015 at epoch 52\n",
            "loss: 0.01125376857817173 at epoch 52\n",
            "loss: 0.0040224106051027775 at epoch 52\n",
            "loss: 0.005703555420041084 at epoch 52\n",
            "loss: 0.007654143497347832 at epoch 53\n",
            "loss: 0.003624639241024852 at epoch 53\n",
            "loss: 0.0051199463196098804 at epoch 53\n",
            "loss: 0.003885480808094144 at epoch 53\n",
            "loss: 0.005918972194194794 at epoch 53\n",
            "loss: 0.001656275475397706 at epoch 53\n",
            "loss: 0.006559058558195829 at epoch 53\n",
            "loss: 0.007497446611523628 at epoch 53\n",
            "loss: 0.005907464772462845 at epoch 53\n",
            "loss: 0.002011084696277976 at epoch 53\n",
            "loss: 0.00266148685477674 at epoch 53\n",
            "loss: 0.0016225348226726055 at epoch 53\n",
            "loss: 0.0035663116723299026 at epoch 53\n",
            "loss: 0.0024410164915025234 at epoch 53\n",
            "loss: 0.0012558128219097853 at epoch 53\n",
            "loss: 0.0004974094335921109 at epoch 53\n",
            "loss: 0.0019772760570049286 at epoch 53\n",
            "loss: 0.0029476836789399385 at epoch 53\n",
            "loss: 0.000760816503316164 at epoch 53\n",
            "loss: 0.0011937023373320699 at epoch 53\n",
            "loss: 0.0014965272275730968 at epoch 53\n",
            "loss: 0.0023384669329971075 at epoch 53\n",
            "loss: 0.0007168578449636698 at epoch 53\n",
            "loss: 0.001372972852550447 at epoch 53\n",
            "loss: 0.002520006150007248 at epoch 53\n",
            "loss: 0.0012917806161567569 at epoch 53\n",
            "loss: 0.0008214355329982936 at epoch 53\n",
            "loss: 0.0038214419037103653 at epoch 53\n",
            "loss: 0.0079118013381958 at epoch 53\n",
            "loss: 0.0032270196825265884 at epoch 53\n",
            "loss: 0.000631579605396837 at epoch 53\n",
            "loss: 0.0008367844275198877 at epoch 53\n",
            "loss: 0.006762095261365175 at epoch 53\n",
            "loss: 0.003166128881275654 at epoch 53\n",
            "loss: 0.0013020113110542297 at epoch 53\n",
            "loss: 0.0007560823578387499 at epoch 53\n",
            "loss: 0.0014952144119888544 at epoch 53\n",
            "loss: 0.0019001479959115386 at epoch 53\n",
            "loss: 0.0037571201100945473 at epoch 53\n",
            "loss: 0.0007560218800790608 at epoch 53\n",
            "loss: 0.0025679427199065685 at epoch 53\n",
            "loss: 0.0010700610000640154 at epoch 53\n",
            "loss: 0.0005513296346180141 at epoch 53\n",
            "loss: 0.00202916725538671 at epoch 53\n",
            "loss: 0.004618191625922918 at epoch 53\n",
            "loss: 0.0004845551447942853 at epoch 53\n",
            "loss: 0.000780987786129117 at epoch 53\n",
            "loss: 0.0009127206867560744 at epoch 53\n",
            "loss: 0.001060195849277079 at epoch 53\n",
            "loss: 0.0009976865258067846 at epoch 53\n",
            "loss: 0.0022292768117040396 at epoch 53\n",
            "loss: 0.002655645366758108 at epoch 53\n",
            "loss: 0.0015786966541782022 at epoch 53\n",
            "loss: 0.0005393374594859779 at epoch 53\n",
            "loss: 0.0007029065163806081 at epoch 53\n",
            "loss: 0.00033292293664999306 at epoch 53\n",
            "loss: 0.00039852800546213984 at epoch 53\n",
            "loss: 0.00025160511722788215 at epoch 53\n",
            "loss: 0.0005121006979607046 at epoch 53\n",
            "loss: 0.0004713218950200826 at epoch 53\n",
            "loss: 0.0013658992247655988 at epoch 53\n",
            "loss: 0.0003232852031942457 at epoch 53\n",
            "loss: 0.0003759083920158446 at epoch 53\n",
            "loss: 0.0006771103944629431 at epoch 53\n",
            "loss: 0.00037687894655391574 at epoch 53\n",
            "loss: 0.0003434857935644686 at epoch 53\n",
            "loss: 0.00034104305086657405 at epoch 53\n",
            "loss: 0.00016103792586363852 at epoch 53\n",
            "loss: 0.00019710038031917065 at epoch 53\n",
            "loss: 0.0006950672250241041 at epoch 53\n",
            "loss: 0.0007862109923735261 at epoch 53\n",
            "loss: 0.0004923776723444462 at epoch 53\n",
            "loss: 0.0012182890204712749 at epoch 53\n",
            "loss: 0.000735742854885757 at epoch 53\n",
            "loss: 0.0016798893921077251 at epoch 53\n",
            "loss: 0.0011217251885682344 at epoch 53\n",
            "loss: 0.0005015620263293386 at epoch 53\n",
            "loss: 0.0006465507321991026 at epoch 53\n",
            "loss: 0.0016381118912249804 at epoch 53\n",
            "loss: 0.0035385857336223125 at epoch 54\n",
            "loss: 0.00038419137126766145 at epoch 54\n",
            "loss: 0.003072098596021533 at epoch 54\n",
            "loss: 0.0035447943955659866 at epoch 54\n",
            "loss: 0.004422192927449942 at epoch 54\n",
            "loss: 0.0038114632479846478 at epoch 54\n",
            "loss: 0.0033194597344845533 at epoch 54\n",
            "loss: 0.002137138042598963 at epoch 54\n",
            "loss: 0.002903413726016879 at epoch 54\n",
            "loss: 0.002525600604712963 at epoch 54\n",
            "loss: 0.001536509720608592 at epoch 54\n",
            "loss: 0.0027296659536659718 at epoch 54\n",
            "loss: 0.0010252122301608324 at epoch 54\n",
            "loss: 0.001707497169263661 at epoch 54\n",
            "loss: 0.002733295550569892 at epoch 54\n",
            "loss: 0.002763696713373065 at epoch 54\n",
            "loss: 0.0027423284482210875 at epoch 54\n",
            "loss: 0.000379948498448357 at epoch 54\n",
            "loss: 0.0008236650610342622 at epoch 54\n",
            "loss: 0.001917869783937931 at epoch 54\n",
            "loss: 0.0027675700839608908 at epoch 54\n",
            "loss: 0.0011039618402719498 at epoch 54\n",
            "loss: 0.0004695374227594584 at epoch 54\n",
            "loss: 0.0026341562625020742 at epoch 54\n",
            "loss: 0.0026982296258211136 at epoch 54\n",
            "loss: 0.003254312789067626 at epoch 54\n",
            "loss: 0.0020894238259643316 at epoch 54\n",
            "loss: 0.005697405897080898 at epoch 54\n",
            "loss: 0.006631175521761179 at epoch 54\n",
            "loss: 0.0007554012117907405 at epoch 54\n",
            "loss: 0.0029816129244863987 at epoch 54\n",
            "loss: 0.007931043393909931 at epoch 54\n",
            "loss: 0.008222738280892372 at epoch 54\n",
            "loss: 0.001738379942253232 at epoch 54\n",
            "loss: 0.0035576687660068274 at epoch 54\n",
            "loss: 0.01000401470810175 at epoch 54\n",
            "loss: 0.008455849252641201 at epoch 54\n",
            "loss: 0.00258849048987031 at epoch 54\n",
            "loss: 0.0027176826260983944 at epoch 54\n",
            "loss: 0.005920924246311188 at epoch 54\n",
            "loss: 0.01910429075360298 at epoch 54\n",
            "loss: 0.025574538856744766 at epoch 54\n",
            "loss: 0.020751772448420525 at epoch 54\n",
            "loss: 0.00963868573307991 at epoch 54\n",
            "loss: 0.007972866296768188 at epoch 54\n",
            "loss: 0.0073324465192854404 at epoch 54\n",
            "loss: 0.010658187791705132 at epoch 54\n",
            "loss: 0.008233074098825455 at epoch 54\n",
            "loss: 0.0053031048737466335 at epoch 54\n",
            "loss: 0.009238386526703835 at epoch 54\n",
            "loss: 0.012798149138689041 at epoch 54\n",
            "loss: 0.009944165125489235 at epoch 54\n",
            "loss: 0.0031377593986690044 at epoch 54\n",
            "loss: 0.008882008492946625 at epoch 54\n",
            "loss: 0.023656683042645454 at epoch 54\n",
            "loss: 0.0024871353525668383 at epoch 54\n",
            "loss: 0.02599431201815605 at epoch 54\n",
            "loss: 0.009090999141335487 at epoch 54\n",
            "loss: 0.011951044201850891 at epoch 54\n",
            "loss: 0.01620311290025711 at epoch 54\n",
            "loss: 0.03079802915453911 at epoch 54\n",
            "loss: 0.04315638169646263 at epoch 54\n",
            "loss: 0.0438474640250206 at epoch 54\n",
            "loss: 0.0327567458152771 at epoch 54\n",
            "loss: 0.008392025716602802 at epoch 54\n",
            "loss: 0.06301888823509216 at epoch 54\n",
            "loss: 0.10661591589450836 at epoch 54\n",
            "loss: 0.11073410511016846 at epoch 54\n",
            "loss: 0.10611332952976227 at epoch 54\n",
            "loss: 0.18326309323310852 at epoch 54\n",
            "loss: 0.30764201283454895 at epoch 54\n",
            "loss: 0.22842639684677124 at epoch 54\n",
            "loss: 0.1292186975479126 at epoch 54\n",
            "loss: 0.20174987614154816 at epoch 54\n",
            "loss: 0.4641494154930115 at epoch 54\n",
            "loss: 0.5848361849784851 at epoch 54\n",
            "loss: 0.6621118783950806 at epoch 54\n",
            "loss: 0.8859258890151978 at epoch 54\n",
            "loss: 1.6117101907730103 at epoch 54\n",
            "loss: 2.941796064376831 at epoch 55\n",
            "loss: 1.5434819459915161 at epoch 55\n",
            "loss: 2.0677592754364014 at epoch 55\n",
            "loss: 1.2359548807144165 at epoch 55\n",
            "loss: 2.907684326171875 at epoch 55\n",
            "loss: 2.3154001235961914 at epoch 55\n",
            "loss: 1.6186655759811401 at epoch 55\n",
            "loss: 3.2221693992614746 at epoch 55\n",
            "loss: 2.197186231613159 at epoch 55\n",
            "loss: 1.753195881843567 at epoch 55\n",
            "loss: 1.4427859783172607 at epoch 55\n",
            "loss: 1.069093942642212 at epoch 55\n",
            "loss: 1.4801586866378784 at epoch 55\n",
            "loss: 2.5789575576782227 at epoch 55\n",
            "loss: 0.7406761646270752 at epoch 55\n",
            "loss: 1.4999103546142578 at epoch 55\n",
            "loss: 1.570000171661377 at epoch 55\n",
            "loss: 1.8399765491485596 at epoch 55\n",
            "loss: 0.4208584427833557 at epoch 55\n",
            "loss: 3.023728132247925 at epoch 55\n",
            "loss: 3.796834707260132 at epoch 55\n",
            "loss: 2.3392603397369385 at epoch 55\n",
            "loss: 2.2174642086029053 at epoch 55\n",
            "loss: 3.3504133224487305 at epoch 55\n",
            "loss: 0.6763530373573303 at epoch 55\n",
            "loss: 4.182994842529297 at epoch 55\n",
            "loss: 3.848280191421509 at epoch 55\n",
            "loss: 2.9249720573425293 at epoch 55\n",
            "loss: 0.8391907215118408 at epoch 55\n",
            "loss: 3.3473920822143555 at epoch 55\n",
            "loss: 1.0615277290344238 at epoch 55\n",
            "loss: 1.192888617515564 at epoch 55\n",
            "loss: 2.3888003826141357 at epoch 55\n",
            "loss: 1.4791114330291748 at epoch 55\n",
            "loss: 0.7318233847618103 at epoch 55\n",
            "loss: 1.3248517513275146 at epoch 55\n",
            "loss: 0.7452268600463867 at epoch 55\n",
            "loss: 0.7159643769264221 at epoch 55\n",
            "loss: 1.1474637985229492 at epoch 55\n",
            "loss: 0.407641738653183 at epoch 55\n",
            "loss: 0.7115000486373901 at epoch 55\n",
            "loss: 0.9653603434562683 at epoch 55\n",
            "loss: 0.29422304034233093 at epoch 55\n",
            "loss: 0.530755341053009 at epoch 55\n",
            "loss: 0.5417465567588806 at epoch 55\n",
            "loss: 0.3548879623413086 at epoch 55\n",
            "loss: 0.12369650602340698 at epoch 55\n",
            "loss: 0.40633153915405273 at epoch 55\n",
            "loss: 0.2376188039779663 at epoch 55\n",
            "loss: 0.28530243039131165 at epoch 55\n",
            "loss: 0.3185802102088928 at epoch 55\n",
            "loss: 0.3019005358219147 at epoch 55\n",
            "loss: 0.1320066899061203 at epoch 55\n",
            "loss: 0.177546888589859 at epoch 55\n",
            "loss: 0.20993220806121826 at epoch 55\n",
            "loss: 0.05733826756477356 at epoch 55\n",
            "loss: 0.1437995880842209 at epoch 55\n",
            "loss: 0.10425661504268646 at epoch 55\n",
            "loss: 0.1336735337972641 at epoch 55\n",
            "loss: 0.07582509517669678 at epoch 55\n",
            "loss: 0.0828319638967514 at epoch 55\n",
            "loss: 0.1010984256863594 at epoch 55\n",
            "loss: 0.03247343376278877 at epoch 55\n",
            "loss: 0.10839710384607315 at epoch 55\n",
            "loss: 0.020193345844745636 at epoch 55\n",
            "loss: 0.10243362188339233 at epoch 55\n",
            "loss: 0.037981946021318436 at epoch 55\n",
            "loss: 0.04024984687566757 at epoch 55\n",
            "loss: 0.09116492420434952 at epoch 55\n",
            "loss: 0.05898310989141464 at epoch 55\n",
            "loss: 0.04006890580058098 at epoch 55\n",
            "loss: 0.045295413583517075 at epoch 55\n",
            "loss: 0.0427401140332222 at epoch 55\n",
            "loss: 0.0442996583878994 at epoch 55\n",
            "loss: 0.03862600028514862 at epoch 55\n",
            "loss: 0.05159708857536316 at epoch 55\n",
            "loss: 0.0362788550555706 at epoch 55\n",
            "loss: 0.035195011645555496 at epoch 55\n",
            "loss: 0.04076474532485008 at epoch 55\n",
            "loss: 0.022594556212425232 at epoch 56\n",
            "loss: 0.026362154632806778 at epoch 56\n",
            "loss: 0.015212356112897396 at epoch 56\n",
            "loss: 0.02245589718222618 at epoch 56\n",
            "loss: 0.04010288789868355 at epoch 56\n",
            "loss: 0.03860482573509216 at epoch 56\n",
            "loss: 0.050793375819921494 at epoch 56\n",
            "loss: 0.036413852125406265 at epoch 56\n",
            "loss: 0.03138484060764313 at epoch 56\n",
            "loss: 0.03136536106467247 at epoch 56\n",
            "loss: 0.06605943292379379 at epoch 56\n",
            "loss: 0.020479438826441765 at epoch 56\n",
            "loss: 0.026185445487499237 at epoch 56\n",
            "loss: 0.027240868657827377 at epoch 56\n",
            "loss: 0.013855437748134136 at epoch 56\n",
            "loss: 0.021948980167508125 at epoch 56\n",
            "loss: 0.02134651131927967 at epoch 56\n",
            "loss: 0.004765540361404419 at epoch 56\n",
            "loss: 0.02251637727022171 at epoch 56\n",
            "loss: 0.018125584349036217 at epoch 56\n",
            "loss: 0.012874436564743519 at epoch 56\n",
            "loss: 0.004667091183364391 at epoch 56\n",
            "loss: 0.009317852556705475 at epoch 56\n",
            "loss: 0.0087531553581357 at epoch 56\n",
            "loss: 0.005946549586951733 at epoch 56\n",
            "loss: 0.007520241662859917 at epoch 56\n",
            "loss: 0.005511653609573841 at epoch 56\n",
            "loss: 0.007855254225432873 at epoch 56\n",
            "loss: 0.01187499426305294 at epoch 56\n",
            "loss: 0.013405399397015572 at epoch 56\n",
            "loss: 0.010852791368961334 at epoch 56\n",
            "loss: 0.0030810306780040264 at epoch 56\n",
            "loss: 0.011699702590703964 at epoch 56\n",
            "loss: 0.006248375866562128 at epoch 56\n",
            "loss: 0.00638192892074585 at epoch 56\n",
            "loss: 0.005024481564760208 at epoch 56\n",
            "loss: 0.013630564324557781 at epoch 56\n",
            "loss: 0.008020849898457527 at epoch 56\n",
            "loss: 0.005065220408141613 at epoch 56\n",
            "loss: 0.011940051801502705 at epoch 56\n",
            "loss: 0.007091455161571503 at epoch 56\n",
            "loss: 0.006475638598203659 at epoch 56\n",
            "loss: 0.014278007671236992 at epoch 56\n",
            "loss: 0.010007179342210293 at epoch 56\n",
            "loss: 0.005997675471007824 at epoch 56\n",
            "loss: 0.0194388460367918 at epoch 56\n",
            "loss: 0.007128812372684479 at epoch 56\n",
            "loss: 0.010434068739414215 at epoch 56\n",
            "loss: 0.011222489178180695 at epoch 56\n",
            "loss: 0.0020912259351462126 at epoch 56\n",
            "loss: 0.0061230044811964035 at epoch 56\n",
            "loss: 0.006672220770269632 at epoch 56\n",
            "loss: 0.002660750411450863 at epoch 56\n",
            "loss: 0.008746778592467308 at epoch 56\n",
            "loss: 0.006690322421491146 at epoch 56\n",
            "loss: 0.0013920888304710388 at epoch 56\n",
            "loss: 0.013402394019067287 at epoch 56\n",
            "loss: 0.005291234701871872 at epoch 56\n",
            "loss: 0.004296969622373581 at epoch 56\n",
            "loss: 0.00284609105437994 at epoch 56\n",
            "loss: 0.012010921724140644 at epoch 56\n",
            "loss: 0.006496804300695658 at epoch 56\n",
            "loss: 0.00728758005425334 at epoch 56\n",
            "loss: 0.00888162013143301 at epoch 56\n",
            "loss: 0.006933079566806555 at epoch 56\n",
            "loss: 0.007674792781472206 at epoch 56\n",
            "loss: 0.005131298676133156 at epoch 56\n",
            "loss: 0.0038360566832125187 at epoch 56\n",
            "loss: 0.006306551396846771 at epoch 56\n",
            "loss: 0.0049851154908537865 at epoch 56\n",
            "loss: 0.006946058012545109 at epoch 56\n",
            "loss: 0.009465368464589119 at epoch 56\n",
            "loss: 0.007464782800525427 at epoch 56\n",
            "loss: 0.0038628855254501104 at epoch 56\n",
            "loss: 0.008550514467060566 at epoch 56\n",
            "loss: 0.008163798600435257 at epoch 56\n",
            "loss: 0.008875157684087753 at epoch 56\n",
            "loss: 0.003882860764861107 at epoch 56\n",
            "loss: 0.009240672923624516 at epoch 56\n",
            "loss: 0.011979935690760612 at epoch 57\n",
            "loss: 0.015860866755247116 at epoch 57\n",
            "loss: 0.005908415652811527 at epoch 57\n",
            "loss: 0.025211850181221962 at epoch 57\n",
            "loss: 0.009336077608168125 at epoch 57\n",
            "loss: 0.013247968629002571 at epoch 57\n",
            "loss: 0.015985073521733284 at epoch 57\n",
            "loss: 0.010926353745162487 at epoch 57\n",
            "loss: 0.007190183270722628 at epoch 57\n",
            "loss: 0.006249121855944395 at epoch 57\n",
            "loss: 0.005541946738958359 at epoch 57\n",
            "loss: 0.012087976559996605 at epoch 57\n",
            "loss: 0.00436966260895133 at epoch 57\n",
            "loss: 0.01439564861357212 at epoch 57\n",
            "loss: 0.003910589497536421 at epoch 57\n",
            "loss: 0.013713907450437546 at epoch 57\n",
            "loss: 0.004110779147595167 at epoch 57\n",
            "loss: 0.02017909474670887 at epoch 57\n",
            "loss: 0.02660546451807022 at epoch 57\n",
            "loss: 0.02262023650109768 at epoch 57\n",
            "loss: 0.00469215726479888 at epoch 57\n",
            "loss: 0.02779160439968109 at epoch 57\n",
            "loss: 0.016345500946044922 at epoch 57\n",
            "loss: 0.00795380212366581 at epoch 57\n",
            "loss: 0.012115329504013062 at epoch 57\n",
            "loss: 0.026841167360544205 at epoch 57\n",
            "loss: 0.012178000994026661 at epoch 57\n",
            "loss: 0.025678634643554688 at epoch 57\n",
            "loss: 0.02263764664530754 at epoch 57\n",
            "loss: 0.013379259034991264 at epoch 57\n",
            "loss: 0.0034905248321592808 at epoch 57\n",
            "loss: 0.02898576483130455 at epoch 57\n",
            "loss: 0.017819318920373917 at epoch 57\n",
            "loss: 0.010418380610644817 at epoch 57\n",
            "loss: 0.017209205776453018 at epoch 57\n",
            "loss: 0.019435323774814606 at epoch 57\n",
            "loss: 0.00753397773951292 at epoch 57\n",
            "loss: 0.008351021446287632 at epoch 57\n",
            "loss: 0.02574319951236248 at epoch 57\n",
            "loss: 0.00951506569981575 at epoch 57\n",
            "loss: 0.01754794828593731 at epoch 57\n",
            "loss: 0.023889506235718727 at epoch 57\n",
            "loss: 0.010099655017256737 at epoch 57\n",
            "loss: 0.00678613455966115 at epoch 57\n",
            "loss: 0.01226950902491808 at epoch 57\n",
            "loss: 0.007770801894366741 at epoch 57\n",
            "loss: 0.009978143498301506 at epoch 57\n",
            "loss: 0.004360254853963852 at epoch 57\n",
            "loss: 0.008269771002233028 at epoch 57\n",
            "loss: 0.009371448308229446 at epoch 57\n",
            "loss: 0.0042451829649508 at epoch 57\n",
            "loss: 0.0048247636295855045 at epoch 57\n",
            "loss: 0.005778381135314703 at epoch 57\n",
            "loss: 0.005098402500152588 at epoch 57\n",
            "loss: 0.0048673879355192184 at epoch 57\n",
            "loss: 0.012048267759382725 at epoch 57\n",
            "loss: 0.0030029716435819864 at epoch 57\n",
            "loss: 0.007110712118446827 at epoch 57\n",
            "loss: 0.0032566306181252003 at epoch 57\n",
            "loss: 0.004251337610185146 at epoch 57\n",
            "loss: 0.0026364086661487818 at epoch 57\n",
            "loss: 0.003771639196202159 at epoch 57\n",
            "loss: 0.004086615983396769 at epoch 57\n",
            "loss: 0.00295315682888031 at epoch 57\n",
            "loss: 0.0036626311484724283 at epoch 57\n",
            "loss: 0.006768426857888699 at epoch 57\n",
            "loss: 0.003441320965066552 at epoch 57\n",
            "loss: 0.004635046701878309 at epoch 57\n",
            "loss: 0.0026252043899148703 at epoch 57\n",
            "loss: 0.008473294787108898 at epoch 57\n",
            "loss: 0.0033373672049492598 at epoch 57\n",
            "loss: 0.0030117551796138287 at epoch 57\n",
            "loss: 0.006649906747043133 at epoch 57\n",
            "loss: 0.011120903305709362 at epoch 57\n",
            "loss: 0.008746229112148285 at epoch 57\n",
            "loss: 0.005695519037544727 at epoch 57\n",
            "loss: 0.009848276153206825 at epoch 57\n",
            "loss: 0.014585815370082855 at epoch 57\n",
            "loss: 0.003108822973445058 at epoch 57\n",
            "loss: 0.007525750435888767 at epoch 58\n",
            "loss: 0.01393734011799097 at epoch 58\n",
            "loss: 0.011469794437289238 at epoch 58\n",
            "loss: 0.008010564371943474 at epoch 58\n",
            "loss: 0.013421483337879181 at epoch 58\n",
            "loss: 0.015460123308002949 at epoch 58\n",
            "loss: 0.0020680816378444433 at epoch 58\n",
            "loss: 0.01705924980342388 at epoch 58\n",
            "loss: 0.010230335406959057 at epoch 58\n",
            "loss: 0.010538756847381592 at epoch 58\n",
            "loss: 0.005766840651631355 at epoch 58\n",
            "loss: 0.013844605535268784 at epoch 58\n",
            "loss: 0.013506163842976093 at epoch 58\n",
            "loss: 0.008768617175519466 at epoch 58\n",
            "loss: 0.007132356055080891 at epoch 58\n",
            "loss: 0.010844164527952671 at epoch 58\n",
            "loss: 0.01611091010272503 at epoch 58\n",
            "loss: 0.0030077167320996523 at epoch 58\n",
            "loss: 0.013962050899863243 at epoch 58\n",
            "loss: 0.027671700343489647 at epoch 58\n",
            "loss: 0.013327809050679207 at epoch 58\n",
            "loss: 0.006685392931103706 at epoch 58\n",
            "loss: 0.0170588418841362 at epoch 58\n",
            "loss: 0.013937651179730892 at epoch 58\n",
            "loss: 0.005262376274913549 at epoch 58\n",
            "loss: 0.008709153160452843 at epoch 58\n",
            "loss: 0.017823901027441025 at epoch 58\n",
            "loss: 0.011786533519625664 at epoch 58\n",
            "loss: 0.004479375202208757 at epoch 58\n",
            "loss: 0.006517990492284298 at epoch 58\n",
            "loss: 0.020048832520842552 at epoch 58\n",
            "loss: 0.0045623439364135265 at epoch 58\n",
            "loss: 0.007432200945913792 at epoch 58\n",
            "loss: 0.005546617787331343 at epoch 58\n",
            "loss: 0.00790091697126627 at epoch 58\n",
            "loss: 0.00515283877030015 at epoch 58\n",
            "loss: 0.005332907661795616 at epoch 58\n",
            "loss: 0.001701243338175118 at epoch 58\n",
            "loss: 0.006496782880276442 at epoch 58\n",
            "loss: 0.009307279251515865 at epoch 58\n",
            "loss: 0.006112793460488319 at epoch 58\n",
            "loss: 0.002582096727564931 at epoch 58\n",
            "loss: 0.006706245243549347 at epoch 58\n",
            "loss: 0.007972982712090015 at epoch 58\n",
            "loss: 0.002155060414224863 at epoch 58\n",
            "loss: 0.00599774532020092 at epoch 58\n",
            "loss: 0.004821433685719967 at epoch 58\n",
            "loss: 0.0025774298701435328 at epoch 58\n",
            "loss: 0.00973445549607277 at epoch 58\n",
            "loss: 0.01319657452404499 at epoch 58\n",
            "loss: 0.01067928597331047 at epoch 58\n",
            "loss: 0.004188762046396732 at epoch 58\n",
            "loss: 0.013085092417895794 at epoch 58\n",
            "loss: 0.004342942498624325 at epoch 58\n",
            "loss: 0.005760803818702698 at epoch 58\n",
            "loss: 0.005624208599328995 at epoch 58\n",
            "loss: 0.007766433991491795 at epoch 58\n",
            "loss: 0.005155728664249182 at epoch 58\n",
            "loss: 0.004183056764304638 at epoch 58\n",
            "loss: 0.010272589512169361 at epoch 58\n",
            "loss: 0.002781401854008436 at epoch 58\n",
            "loss: 0.011232515797019005 at epoch 58\n",
            "loss: 0.010832065716385841 at epoch 58\n",
            "loss: 0.01007135771214962 at epoch 58\n",
            "loss: 0.0060564144514501095 at epoch 58\n",
            "loss: 0.00497952476143837 at epoch 58\n",
            "loss: 0.001734777120873332 at epoch 58\n",
            "loss: 0.0061072418466210365 at epoch 58\n",
            "loss: 0.0018913327949121594 at epoch 58\n",
            "loss: 0.005468210205435753 at epoch 58\n",
            "loss: 0.0035871220752596855 at epoch 58\n",
            "loss: 0.0031514973379671574 at epoch 58\n",
            "loss: 0.0037211868911981583 at epoch 58\n",
            "loss: 0.003907300531864166 at epoch 58\n",
            "loss: 0.004492147825658321 at epoch 58\n",
            "loss: 0.006914118770509958 at epoch 58\n",
            "loss: 0.008000021800398827 at epoch 58\n",
            "loss: 0.003462729509919882 at epoch 58\n",
            "loss: 0.06371375173330307 at epoch 58\n",
            "loss: 0.07351499050855637 at epoch 59\n",
            "loss: 0.03740312159061432 at epoch 59\n",
            "loss: 0.042593833059072495 at epoch 59\n",
            "loss: 0.053426291793584824 at epoch 59\n",
            "loss: 0.04392210394144058 at epoch 59\n",
            "loss: 0.0494028776884079 at epoch 59\n",
            "loss: 0.0662197545170784 at epoch 59\n",
            "loss: 0.04828256368637085 at epoch 59\n",
            "loss: 0.022499779239296913 at epoch 59\n",
            "loss: 0.09220188111066818 at epoch 59\n",
            "loss: 0.07651478052139282 at epoch 59\n",
            "loss: 0.029467709362506866 at epoch 59\n",
            "loss: 0.016465457156300545 at epoch 59\n",
            "loss: 0.06894764304161072 at epoch 59\n",
            "loss: 0.047046054154634476 at epoch 59\n",
            "loss: 0.024175643920898438 at epoch 59\n",
            "loss: 0.013779181987047195 at epoch 59\n",
            "loss: 0.045213304460048676 at epoch 59\n",
            "loss: 0.024837525561451912 at epoch 59\n",
            "loss: 0.007841002196073532 at epoch 59\n",
            "loss: 0.016114309430122375 at epoch 59\n",
            "loss: 0.03621980547904968 at epoch 59\n",
            "loss: 0.041489068418741226 at epoch 59\n",
            "loss: 0.012410268187522888 at epoch 59\n",
            "loss: 0.015504841692745686 at epoch 59\n",
            "loss: 0.028082072734832764 at epoch 59\n",
            "loss: 0.032320015132427216 at epoch 59\n",
            "loss: 0.013841396197676659 at epoch 59\n",
            "loss: 0.022850418463349342 at epoch 59\n",
            "loss: 0.025900859385728836 at epoch 59\n",
            "loss: 0.03147459775209427 at epoch 59\n",
            "loss: 0.004205696284770966 at epoch 59\n",
            "loss: 0.01772134006023407 at epoch 59\n",
            "loss: 0.014334512874484062 at epoch 59\n",
            "loss: 0.010814467445015907 at epoch 59\n",
            "loss: 0.012438427656888962 at epoch 59\n",
            "loss: 0.012774198316037655 at epoch 59\n",
            "loss: 0.01915733702480793 at epoch 59\n",
            "loss: 0.002316799247637391 at epoch 59\n",
            "loss: 0.01210678368806839 at epoch 59\n",
            "loss: 0.013450133614242077 at epoch 59\n",
            "loss: 0.02357352338731289 at epoch 59\n",
            "loss: 0.00453215604647994 at epoch 59\n",
            "loss: 0.011640957556664944 at epoch 59\n",
            "loss: 0.015477219596505165 at epoch 59\n",
            "loss: 0.00914721004664898 at epoch 59\n",
            "loss: 0.00880142766982317 at epoch 59\n",
            "loss: 0.016731135547161102 at epoch 59\n",
            "loss: 0.013642479665577412 at epoch 59\n",
            "loss: 0.005352812819182873 at epoch 59\n",
            "loss: 0.004904416389763355 at epoch 59\n",
            "loss: 0.013423031195998192 at epoch 59\n",
            "loss: 0.00945581216365099 at epoch 59\n",
            "loss: 0.001823365455493331 at epoch 59\n",
            "loss: 0.008746255189180374 at epoch 59\n",
            "loss: 0.018496744334697723 at epoch 59\n",
            "loss: 0.018312009051442146 at epoch 59\n",
            "loss: 0.006568155251443386 at epoch 59\n",
            "loss: 0.010286780074238777 at epoch 59\n",
            "loss: 0.009615656919777393 at epoch 59\n",
            "loss: 0.0080370232462883 at epoch 59\n",
            "loss: 0.0026462594978511333 at epoch 59\n",
            "loss: 0.0068585872650146484 at epoch 59\n",
            "loss: 0.007999769411981106 at epoch 59\n",
            "loss: 0.005985671654343605 at epoch 59\n",
            "loss: 0.0029943271074444056 at epoch 59\n",
            "loss: 0.004355433396995068 at epoch 59\n",
            "loss: 0.007989986799657345 at epoch 59\n",
            "loss: 0.0021224357187747955 at epoch 59\n",
            "loss: 0.004382824990898371 at epoch 59\n",
            "loss: 0.0018295601475983858 at epoch 59\n",
            "loss: 0.0054746950045228004 at epoch 59\n",
            "loss: 0.0009944752091541886 at epoch 59\n",
            "loss: 0.002707243897020817 at epoch 59\n",
            "loss: 0.0014966290909796953 at epoch 59\n",
            "loss: 0.0026549547910690308 at epoch 59\n",
            "loss: 0.0049124895595014095 at epoch 59\n",
            "loss: 0.0018439636332914233 at epoch 59\n",
            "loss: 0.0023215932305902243 at epoch 59\n",
            "loss: 0.0017019830411300063 at epoch 60\n",
            "loss: 0.004744089674204588 at epoch 60\n",
            "loss: 0.002766201738268137 at epoch 60\n",
            "loss: 0.0016831596149131656 at epoch 60\n",
            "loss: 0.0037569678388535976 at epoch 60\n",
            "loss: 0.0023526838049292564 at epoch 60\n",
            "loss: 0.0021648576948791742 at epoch 60\n",
            "loss: 0.0008280880283564329 at epoch 60\n",
            "loss: 0.0015848003095015883 at epoch 60\n",
            "loss: 0.0016126971459016204 at epoch 60\n",
            "loss: 0.0009454060345888138 at epoch 60\n",
            "loss: 0.0041481368243694305 at epoch 60\n",
            "loss: 0.0015508122742176056 at epoch 60\n",
            "loss: 0.0006829328485764563 at epoch 60\n",
            "loss: 0.0013171931495890021 at epoch 60\n",
            "loss: 0.0018324842676520348 at epoch 60\n",
            "loss: 0.003114534541964531 at epoch 60\n",
            "loss: 0.000997300143353641 at epoch 60\n",
            "loss: 0.0017607881454750896 at epoch 60\n",
            "loss: 0.0009876415133476257 at epoch 60\n",
            "loss: 0.003361765993759036 at epoch 60\n",
            "loss: 0.002610299736261368 at epoch 60\n",
            "loss: 0.0009791350457817316 at epoch 60\n",
            "loss: 0.0013091329019516706 at epoch 60\n",
            "loss: 0.004840901121497154 at epoch 60\n",
            "loss: 0.004948893096297979 at epoch 60\n",
            "loss: 0.0035641270224004984 at epoch 60\n",
            "loss: 0.0007709982455708086 at epoch 60\n",
            "loss: 0.0057735140435397625 at epoch 60\n",
            "loss: 0.009636028669774532 at epoch 60\n",
            "loss: 0.007190648466348648 at epoch 60\n",
            "loss: 0.003802230814471841 at epoch 60\n",
            "loss: 0.0034727631136775017 at epoch 60\n",
            "loss: 0.004402052611112595 at epoch 60\n",
            "loss: 0.00443427124992013 at epoch 60\n",
            "loss: 0.0059806425124406815 at epoch 60\n",
            "loss: 0.0012106450740247965 at epoch 60\n",
            "loss: 0.0032775134313851595 at epoch 60\n",
            "loss: 0.001720914733596146 at epoch 60\n",
            "loss: 0.004478686489164829 at epoch 60\n",
            "loss: 0.001022039563395083 at epoch 60\n",
            "loss: 0.008712355978786945 at epoch 60\n",
            "loss: 0.0022273387294262648 at epoch 60\n",
            "loss: 0.003450834657996893 at epoch 60\n",
            "loss: 0.0006136998999863863 at epoch 60\n",
            "loss: 0.004120549187064171 at epoch 60\n",
            "loss: 0.0009487415663897991 at epoch 60\n",
            "loss: 0.006870889104902744 at epoch 60\n",
            "loss: 0.004492270760238171 at epoch 60\n",
            "loss: 0.004370727576315403 at epoch 60\n",
            "loss: 0.0027135126292705536 at epoch 60\n",
            "loss: 0.0013745621545240283 at epoch 60\n",
            "loss: 0.003550044260919094 at epoch 60\n",
            "loss: 0.002689740853384137 at epoch 60\n",
            "loss: 0.0016860427567735314 at epoch 60\n",
            "loss: 0.00046589080011472106 at epoch 60\n",
            "loss: 0.0024412814527750015 at epoch 60\n",
            "loss: 0.000712502165697515 at epoch 60\n",
            "loss: 0.0008396683260798454 at epoch 60\n",
            "loss: 0.0004695206880569458 at epoch 60\n",
            "loss: 0.001338378875516355 at epoch 60\n",
            "loss: 0.001674499362707138 at epoch 60\n",
            "loss: 0.0008490374311804771 at epoch 60\n",
            "loss: 0.0005087234312668443 at epoch 60\n",
            "loss: 0.0005562078440561891 at epoch 60\n",
            "loss: 0.0004459418705664575 at epoch 60\n",
            "loss: 0.0008224872290156782 at epoch 60\n",
            "loss: 0.0012243713717907667 at epoch 60\n",
            "loss: 0.0065473332069814205 at epoch 60\n",
            "loss: 0.005952100735157728 at epoch 60\n",
            "loss: 0.0048186141066253185 at epoch 60\n",
            "loss: 0.007445330731570721 at epoch 60\n",
            "loss: 0.000752172723878175 at epoch 60\n",
            "loss: 0.0019956340547651052 at epoch 60\n",
            "loss: 0.0017311410047113895 at epoch 60\n",
            "loss: 0.0019628149457275867 at epoch 60\n",
            "loss: 0.0011685340432450175 at epoch 60\n",
            "loss: 0.0009656704496592283 at epoch 60\n",
            "loss: 0.0015510382363572717 at epoch 60\n",
            "loss: 0.002233368344604969 at epoch 61\n",
            "loss: 0.0013017257442697883 at epoch 61\n",
            "loss: 0.0006004431052133441 at epoch 61\n",
            "loss: 0.0012474149698391557 at epoch 61\n",
            "loss: 0.002221653237938881 at epoch 61\n",
            "loss: 0.0010271937353536487 at epoch 61\n",
            "loss: 0.0012797784293070436 at epoch 61\n",
            "loss: 0.002833852544426918 at epoch 61\n",
            "loss: 0.008294003084301949 at epoch 61\n",
            "loss: 0.003000119002535939 at epoch 61\n",
            "loss: 0.001209229463711381 at epoch 61\n",
            "loss: 0.004140024539083242 at epoch 61\n",
            "loss: 0.006050638854503632 at epoch 61\n",
            "loss: 0.008711101487278938 at epoch 61\n",
            "loss: 0.005897747352719307 at epoch 61\n",
            "loss: 0.002272358164191246 at epoch 61\n",
            "loss: 0.0016672986093908548 at epoch 61\n",
            "loss: 0.00896440353244543 at epoch 61\n",
            "loss: 0.007434704340994358 at epoch 61\n",
            "loss: 0.001852162298746407 at epoch 61\n",
            "loss: 0.005833877250552177 at epoch 61\n",
            "loss: 0.0072682686150074005 at epoch 61\n",
            "loss: 0.008989972993731499 at epoch 61\n",
            "loss: 0.004648669622838497 at epoch 61\n",
            "loss: 0.004164362326264381 at epoch 61\n",
            "loss: 0.005040384829044342 at epoch 61\n",
            "loss: 0.004604552872478962 at epoch 61\n",
            "loss: 0.008392997086048126 at epoch 61\n",
            "loss: 0.005549455527216196 at epoch 61\n",
            "loss: 0.008145690895617008 at epoch 61\n",
            "loss: 0.007735313847661018 at epoch 61\n",
            "loss: 0.004260390996932983 at epoch 61\n",
            "loss: 0.005786807741969824 at epoch 61\n",
            "loss: 0.0025949962437152863 at epoch 61\n",
            "loss: 0.007073851302266121 at epoch 61\n",
            "loss: 0.005146948155015707 at epoch 61\n",
            "loss: 0.006226871162652969 at epoch 61\n",
            "loss: 0.002002152381464839 at epoch 61\n",
            "loss: 0.006395670585334301 at epoch 61\n",
            "loss: 0.002700165845453739 at epoch 61\n",
            "loss: 0.007874798029661179 at epoch 61\n",
            "loss: 0.012762031517922878 at epoch 61\n",
            "loss: 0.012241599150002003 at epoch 61\n",
            "loss: 0.006905370391905308 at epoch 61\n",
            "loss: 0.0015679147327318788 at epoch 61\n",
            "loss: 0.003314624074846506 at epoch 61\n",
            "loss: 0.0054397257044911385 at epoch 61\n",
            "loss: 0.00557092996314168 at epoch 61\n",
            "loss: 0.003441996406763792 at epoch 61\n",
            "loss: 0.0014697372680529952 at epoch 61\n",
            "loss: 0.003987312316894531 at epoch 61\n",
            "loss: 0.006183686666190624 at epoch 61\n",
            "loss: 0.002557413186877966 at epoch 61\n",
            "loss: 0.0047128209844231606 at epoch 61\n",
            "loss: 0.0018380937399342656 at epoch 61\n",
            "loss: 0.0031522451899945736 at epoch 61\n",
            "loss: 0.0028581134974956512 at epoch 61\n",
            "loss: 0.001086450065486133 at epoch 61\n",
            "loss: 0.00158770103007555 at epoch 61\n",
            "loss: 0.0008039491949602962 at epoch 61\n",
            "loss: 0.0014874780317768455 at epoch 61\n",
            "loss: 0.0012242574011906981 at epoch 61\n",
            "loss: 0.00047885277308523655 at epoch 61\n",
            "loss: 0.0020696185529232025 at epoch 61\n",
            "loss: 0.002375628100708127 at epoch 61\n",
            "loss: 0.0032560089603066444 at epoch 61\n",
            "loss: 0.002515372121706605 at epoch 61\n",
            "loss: 0.0015223317313939333 at epoch 61\n",
            "loss: 0.0017569437623023987 at epoch 61\n",
            "loss: 0.0026381220668554306 at epoch 61\n",
            "loss: 0.0038157403469085693 at epoch 61\n",
            "loss: 0.0023199347779154778 at epoch 61\n",
            "loss: 0.0013476274907588959 at epoch 61\n",
            "loss: 0.000415979913668707 at epoch 61\n",
            "loss: 0.0014721278566867113 at epoch 61\n",
            "loss: 0.003298413008451462 at epoch 61\n",
            "loss: 0.004719560034573078 at epoch 61\n",
            "loss: 0.0031722739804536104 at epoch 61\n",
            "loss: 0.0012840837007388473 at epoch 61\n",
            "loss: 0.0007601768593303859 at epoch 62\n",
            "loss: 0.002168364590033889 at epoch 62\n",
            "loss: 0.0018460561987012625 at epoch 62\n",
            "loss: 0.0025193002074956894 at epoch 62\n",
            "loss: 0.0005433124024420977 at epoch 62\n",
            "loss: 0.0019651614129543304 at epoch 62\n",
            "loss: 0.0052131302654743195 at epoch 62\n",
            "loss: 0.005827026441693306 at epoch 62\n",
            "loss: 0.004923461005091667 at epoch 62\n",
            "loss: 0.005933065433055162 at epoch 62\n",
            "loss: 0.0006956318393349648 at epoch 62\n",
            "loss: 0.0026151719503104687 at epoch 62\n",
            "loss: 0.003899558912962675 at epoch 62\n",
            "loss: 0.006150972098112106 at epoch 62\n",
            "loss: 0.004414310213178396 at epoch 62\n",
            "loss: 0.00411524623632431 at epoch 62\n",
            "loss: 0.0011641047894954681 at epoch 62\n",
            "loss: 0.002119634998962283 at epoch 62\n",
            "loss: 0.0017388381529599428 at epoch 62\n",
            "loss: 0.0012878382112830877 at epoch 62\n",
            "loss: 0.0037006614729762077 at epoch 62\n",
            "loss: 0.001423724228516221 at epoch 62\n",
            "loss: 0.005122206173837185 at epoch 62\n",
            "loss: 0.0010022131027653813 at epoch 62\n",
            "loss: 0.0028073585126549006 at epoch 62\n",
            "loss: 0.0017374282469972968 at epoch 62\n",
            "loss: 0.002814946696162224 at epoch 62\n",
            "loss: 0.002064729342237115 at epoch 62\n",
            "loss: 0.0016179573722183704 at epoch 62\n",
            "loss: 0.001989167183637619 at epoch 62\n",
            "loss: 0.0006318689556792378 at epoch 62\n",
            "loss: 0.0021517665591090918 at epoch 62\n",
            "loss: 0.0005158617859706283 at epoch 62\n",
            "loss: 0.004424327053129673 at epoch 62\n",
            "loss: 0.0014862939715385437 at epoch 62\n",
            "loss: 0.0025610094889998436 at epoch 62\n",
            "loss: 0.001772698131389916 at epoch 62\n",
            "loss: 0.0020146104507148266 at epoch 62\n",
            "loss: 0.002496447879821062 at epoch 62\n",
            "loss: 0.003185771405696869 at epoch 62\n",
            "loss: 0.004547442309558392 at epoch 62\n",
            "loss: 0.004197046626359224 at epoch 62\n",
            "loss: 0.001219553523696959 at epoch 62\n",
            "loss: 0.0021768275182694197 at epoch 62\n",
            "loss: 0.005760731641203165 at epoch 62\n",
            "loss: 0.005736215505748987 at epoch 62\n",
            "loss: 0.004552173428237438 at epoch 62\n",
            "loss: 0.0016103112138807774 at epoch 62\n",
            "loss: 0.0011538435937836766 at epoch 62\n",
            "loss: 0.0023822681978344917 at epoch 62\n",
            "loss: 0.00546052772551775 at epoch 62\n",
            "loss: 0.009625669568777084 at epoch 62\n",
            "loss: 0.011285326443612576 at epoch 62\n",
            "loss: 0.0123464809730649 at epoch 62\n",
            "loss: 0.00889438483864069 at epoch 62\n",
            "loss: 0.004838243592530489 at epoch 62\n",
            "loss: 0.004205102100968361 at epoch 62\n",
            "loss: 0.00746546033769846 at epoch 62\n",
            "loss: 0.014071641489863396 at epoch 62\n",
            "loss: 0.021244505420327187 at epoch 62\n",
            "loss: 0.020312732085585594 at epoch 62\n",
            "loss: 0.013650305569171906 at epoch 62\n",
            "loss: 0.025153864175081253 at epoch 62\n",
            "loss: 0.0145807433873415 at epoch 62\n",
            "loss: 0.016489513218402863 at epoch 62\n",
            "loss: 0.006155077368021011 at epoch 62\n",
            "loss: 0.01617942936718464 at epoch 62\n",
            "loss: 0.026568643748760223 at epoch 62\n",
            "loss: 0.028348781168460846 at epoch 62\n",
            "loss: 0.054398395121097565 at epoch 62\n",
            "loss: 0.07009519636631012 at epoch 62\n",
            "loss: 0.15482620894908905 at epoch 62\n",
            "loss: 0.07005695253610611 at epoch 62\n",
            "loss: 0.07491586357355118 at epoch 62\n",
            "loss: 0.02715672180056572 at epoch 62\n",
            "loss: 0.09346330165863037 at epoch 62\n",
            "loss: 0.1143021509051323 at epoch 62\n",
            "loss: 0.12352733314037323 at epoch 62\n",
            "loss: 0.08042227476835251 at epoch 62\n",
            "loss: 0.11687950044870377 at epoch 63\n",
            "loss: 0.04031045734882355 at epoch 63\n",
            "loss: 0.1362546980381012 at epoch 63\n",
            "loss: 0.08769325911998749 at epoch 63\n",
            "loss: 0.05277407541871071 at epoch 63\n",
            "loss: 0.08226285874843597 at epoch 63\n",
            "loss: 0.05809653922915459 at epoch 63\n",
            "loss: 0.17385968565940857 at epoch 63\n",
            "loss: 0.2596083879470825 at epoch 63\n",
            "loss: 0.38345152139663696 at epoch 63\n",
            "loss: 0.28781476616859436 at epoch 63\n",
            "loss: 0.23916074633598328 at epoch 63\n",
            "loss: 0.20518580079078674 at epoch 63\n",
            "loss: 0.0685727447271347 at epoch 63\n",
            "loss: 0.15386517345905304 at epoch 63\n",
            "loss: 0.05166415125131607 at epoch 63\n",
            "loss: 0.1033012792468071 at epoch 63\n",
            "loss: 0.06980068981647491 at epoch 63\n",
            "loss: 0.12031812220811844 at epoch 63\n",
            "loss: 0.09806594252586365 at epoch 63\n",
            "loss: 0.12911102175712585 at epoch 63\n",
            "loss: 0.19655941426753998 at epoch 63\n",
            "loss: 0.1683097779750824 at epoch 63\n",
            "loss: 0.17106613516807556 at epoch 63\n",
            "loss: 0.0788140818476677 at epoch 63\n",
            "loss: 0.21516120433807373 at epoch 63\n",
            "loss: 0.4059912860393524 at epoch 63\n",
            "loss: 0.4581466317176819 at epoch 63\n",
            "loss: 0.6113699078559875 at epoch 63\n",
            "loss: 0.6671261787414551 at epoch 63\n",
            "loss: 0.41246461868286133 at epoch 63\n",
            "loss: 0.4271523058414459 at epoch 63\n",
            "loss: 0.2973617911338806 at epoch 63\n",
            "loss: 0.6754483580589294 at epoch 63\n",
            "loss: 0.7643187046051025 at epoch 63\n",
            "loss: 0.37288898229599 at epoch 63\n",
            "loss: 0.893237829208374 at epoch 63\n",
            "loss: 0.6247487664222717 at epoch 63\n",
            "loss: 0.963134765625 at epoch 63\n",
            "loss: 0.716009259223938 at epoch 63\n",
            "loss: 0.7226371169090271 at epoch 63\n",
            "loss: 0.32085156440734863 at epoch 63\n",
            "loss: 0.49339741468429565 at epoch 63\n",
            "loss: 1.1561057567596436 at epoch 63\n",
            "loss: 0.9349908828735352 at epoch 63\n",
            "loss: 0.813976526260376 at epoch 63\n",
            "loss: 0.1848040074110031 at epoch 63\n",
            "loss: 0.5607902407646179 at epoch 63\n",
            "loss: 0.3019676208496094 at epoch 63\n",
            "loss: 0.6422489285469055 at epoch 63\n",
            "loss: 0.4908614456653595 at epoch 63\n",
            "loss: 0.4085330367088318 at epoch 63\n",
            "loss: 0.5543113350868225 at epoch 63\n",
            "loss: 0.5721871256828308 at epoch 63\n",
            "loss: 0.5081897377967834 at epoch 63\n",
            "loss: 0.4753625988960266 at epoch 63\n",
            "loss: 0.2653951942920685 at epoch 63\n",
            "loss: 0.3100929856300354 at epoch 63\n",
            "loss: 0.2709779739379883 at epoch 63\n",
            "loss: 0.3982194662094116 at epoch 63\n",
            "loss: 0.3860307037830353 at epoch 63\n",
            "loss: 1.1056957244873047 at epoch 63\n",
            "loss: 1.1247031688690186 at epoch 63\n",
            "loss: 0.9209586381912231 at epoch 63\n",
            "loss: 0.3351191282272339 at epoch 63\n",
            "loss: 0.5024948120117188 at epoch 63\n",
            "loss: 0.5715184807777405 at epoch 63\n",
            "loss: 0.4937889575958252 at epoch 63\n",
            "loss: 0.7468323707580566 at epoch 63\n",
            "loss: 0.27170297503471375 at epoch 63\n",
            "loss: 0.5744935274124146 at epoch 63\n",
            "loss: 0.8110814094543457 at epoch 63\n",
            "loss: 0.7496064901351929 at epoch 63\n",
            "loss: 0.8384697437286377 at epoch 63\n",
            "loss: 0.4560009241104126 at epoch 63\n",
            "loss: 0.24545824527740479 at epoch 63\n",
            "loss: 0.5993038415908813 at epoch 63\n",
            "loss: 0.5263143181800842 at epoch 63\n",
            "loss: 0.36427736282348633 at epoch 63\n",
            "loss: 0.7859295010566711 at epoch 64\n",
            "loss: 1.7275444269180298 at epoch 64\n",
            "loss: 1.8048343658447266 at epoch 64\n",
            "loss: 0.8166145086288452 at epoch 64\n",
            "loss: 0.5100536942481995 at epoch 64\n",
            "loss: 0.7103881239891052 at epoch 64\n",
            "loss: 0.9337201118469238 at epoch 64\n",
            "loss: 0.5131032466888428 at epoch 64\n",
            "loss: 0.250758558511734 at epoch 64\n",
            "loss: 0.5469105243682861 at epoch 64\n",
            "loss: 0.2265719622373581 at epoch 64\n",
            "loss: 0.2899862825870514 at epoch 64\n",
            "loss: 0.19369840621948242 at epoch 64\n",
            "loss: 0.6221636533737183 at epoch 64\n",
            "loss: 0.2221405953168869 at epoch 64\n",
            "loss: 0.6360602974891663 at epoch 64\n",
            "loss: 0.3933779299259186 at epoch 64\n",
            "loss: 0.5904765129089355 at epoch 64\n",
            "loss: 0.2004966139793396 at epoch 64\n",
            "loss: 0.2502903342247009 at epoch 64\n",
            "loss: 0.2525281608104706 at epoch 64\n",
            "loss: 0.38552340865135193 at epoch 64\n",
            "loss: 0.2058086395263672 at epoch 64\n",
            "loss: 0.22911123931407928 at epoch 64\n",
            "loss: 0.05318160355091095 at epoch 64\n",
            "loss: 0.10041128098964691 at epoch 64\n",
            "loss: 0.09664972126483917 at epoch 64\n",
            "loss: 0.1515219658613205 at epoch 64\n",
            "loss: 0.09362819790840149 at epoch 64\n",
            "loss: 0.07846449315547943 at epoch 64\n",
            "loss: 0.07924942672252655 at epoch 64\n",
            "loss: 0.05268579721450806 at epoch 64\n",
            "loss: 0.08937723934650421 at epoch 64\n",
            "loss: 0.07222308963537216 at epoch 64\n",
            "loss: 0.05772474408149719 at epoch 64\n",
            "loss: 0.07747828960418701 at epoch 64\n",
            "loss: 0.04261484742164612 at epoch 64\n",
            "loss: 0.037954363971948624 at epoch 64\n",
            "loss: 0.026150718331336975 at epoch 64\n",
            "loss: 0.04735896736383438 at epoch 64\n",
            "loss: 0.06611388921737671 at epoch 64\n",
            "loss: 0.05976639315485954 at epoch 64\n",
            "loss: 0.027601201087236404 at epoch 64\n",
            "loss: 0.013424593955278397 at epoch 64\n",
            "loss: 0.0399039052426815 at epoch 64\n",
            "loss: 0.03351293504238129 at epoch 64\n",
            "loss: 0.021896768361330032 at epoch 64\n",
            "loss: 0.009667539969086647 at epoch 64\n",
            "loss: 0.02155235782265663 at epoch 64\n",
            "loss: 0.03484131395816803 at epoch 64\n",
            "loss: 0.03276003897190094 at epoch 64\n",
            "loss: 0.010183586739003658 at epoch 64\n",
            "loss: 0.022966478019952774 at epoch 64\n",
            "loss: 0.06743911653757095 at epoch 64\n",
            "loss: 0.06831304728984833 at epoch 64\n",
            "loss: 0.035880181938409805 at epoch 64\n",
            "loss: 0.007112047169357538 at epoch 64\n",
            "loss: 0.058989934623241425 at epoch 64\n",
            "loss: 0.09573626518249512 at epoch 64\n",
            "loss: 0.09074302762746811 at epoch 64\n",
            "loss: 0.021794158965349197 at epoch 64\n",
            "loss: 0.044013381004333496 at epoch 64\n",
            "loss: 0.06858230382204056 at epoch 64\n",
            "loss: 0.07029972970485687 at epoch 64\n",
            "loss: 0.029735226184129715 at epoch 64\n",
            "loss: 0.03005610965192318 at epoch 64\n",
            "loss: 0.05780751630663872 at epoch 64\n",
            "loss: 0.10200190544128418 at epoch 64\n",
            "loss: 0.047912392765283585 at epoch 64\n",
            "loss: 0.05076918005943298 at epoch 64\n",
            "loss: 0.021160170435905457 at epoch 64\n",
            "loss: 0.0630962923169136 at epoch 64\n",
            "loss: 0.006033234763890505 at epoch 64\n",
            "loss: 0.05428805947303772 at epoch 64\n",
            "loss: 0.021821314468979836 at epoch 64\n",
            "loss: 0.05042996630072594 at epoch 64\n",
            "loss: 0.019711855798959732 at epoch 64\n",
            "loss: 0.03872508928179741 at epoch 64\n",
            "loss: 0.0462418757379055 at epoch 64\n",
            "loss: 0.05501565337181091 at epoch 65\n",
            "loss: 0.050678692758083344 at epoch 65\n",
            "loss: 0.09936922043561935 at epoch 65\n",
            "loss: 0.11836385726928711 at epoch 65\n",
            "loss: 0.11430121958255768 at epoch 65\n",
            "loss: 0.07214522361755371 at epoch 65\n",
            "loss: 0.043134283274412155 at epoch 65\n",
            "loss: 0.11532749980688095 at epoch 65\n",
            "loss: 0.1028042733669281 at epoch 65\n",
            "loss: 0.1696881204843521 at epoch 65\n",
            "loss: 0.041031718254089355 at epoch 65\n",
            "loss: 0.18129593133926392 at epoch 65\n",
            "loss: 0.14365270733833313 at epoch 65\n",
            "loss: 0.1071472093462944 at epoch 65\n",
            "loss: 0.057929571717977524 at epoch 65\n",
            "loss: 0.07295931875705719 at epoch 65\n",
            "loss: 0.1049242913722992 at epoch 65\n",
            "loss: 0.06601888686418533 at epoch 65\n",
            "loss: 0.08159279823303223 at epoch 65\n",
            "loss: 0.0742323026061058 at epoch 65\n",
            "loss: 0.10301513969898224 at epoch 65\n",
            "loss: 0.060387954115867615 at epoch 65\n",
            "loss: 0.039269909262657166 at epoch 65\n",
            "loss: 0.04961227625608444 at epoch 65\n",
            "loss: 0.04840139299631119 at epoch 65\n",
            "loss: 0.15117500722408295 at epoch 65\n",
            "loss: 0.11958863586187363 at epoch 65\n",
            "loss: 0.1412154883146286 at epoch 65\n",
            "loss: 0.07410308718681335 at epoch 65\n",
            "loss: 0.1079321876168251 at epoch 65\n",
            "loss: 0.05532434582710266 at epoch 65\n",
            "loss: 0.06229458004236221 at epoch 65\n",
            "loss: 0.05254742130637169 at epoch 65\n",
            "loss: 0.06637663394212723 at epoch 65\n",
            "loss: 0.06444279104471207 at epoch 65\n",
            "loss: 0.06660883873701096 at epoch 65\n",
            "loss: 0.09736786782741547 at epoch 65\n",
            "loss: 0.07496531307697296 at epoch 65\n",
            "loss: 0.055103618651628494 at epoch 65\n",
            "loss: 0.030225399881601334 at epoch 65\n",
            "loss: 0.045904602855443954 at epoch 65\n",
            "loss: 0.0689113438129425 at epoch 65\n",
            "loss: 0.0522744320333004 at epoch 65\n",
            "loss: 0.05305467173457146 at epoch 65\n",
            "loss: 0.048250652849674225 at epoch 65\n",
            "loss: 0.019680604338645935 at epoch 65\n",
            "loss: 0.03260534256696701 at epoch 65\n",
            "loss: 0.00864716712385416 at epoch 65\n",
            "loss: 0.03224273771047592 at epoch 65\n",
            "loss: 0.005827122367918491 at epoch 65\n",
            "loss: 0.02449960820376873 at epoch 65\n",
            "loss: 0.005890160333365202 at epoch 65\n",
            "loss: 0.01934119313955307 at epoch 65\n",
            "loss: 0.006673010066151619 at epoch 65\n",
            "loss: 0.02725653164088726 at epoch 65\n",
            "loss: 0.03588071092963219 at epoch 65\n",
            "loss: 0.05709199607372284 at epoch 65\n",
            "loss: 0.06605203449726105 at epoch 65\n",
            "loss: 0.07027066498994827 at epoch 65\n",
            "loss: 0.03208755701780319 at epoch 65\n",
            "loss: 0.02173163928091526 at epoch 65\n",
            "loss: 0.014444535598158836 at epoch 65\n",
            "loss: 0.0329965315759182 at epoch 65\n",
            "loss: 0.04141407832503319 at epoch 65\n",
            "loss: 0.04660644382238388 at epoch 65\n",
            "loss: 0.023015905171632767 at epoch 65\n",
            "loss: 0.0211557075381279 at epoch 65\n",
            "loss: 0.03344987332820892 at epoch 65\n",
            "loss: 0.033734098076820374 at epoch 65\n",
            "loss: 0.015447809360921383 at epoch 65\n",
            "loss: 0.009383526630699635 at epoch 65\n",
            "loss: 0.00822429172694683 at epoch 65\n",
            "loss: 0.020468885079026222 at epoch 65\n",
            "loss: 0.01946120709180832 at epoch 65\n",
            "loss: 0.024476543068885803 at epoch 65\n",
            "loss: 0.01431418489664793 at epoch 65\n",
            "loss: 0.03006458654999733 at epoch 65\n",
            "loss: 0.02534518390893936 at epoch 65\n",
            "loss: 0.035622261464595795 at epoch 65\n",
            "loss: 0.10906441509723663 at epoch 66\n",
            "loss: 0.1712767630815506 at epoch 66\n",
            "loss: 0.15491123497486115 at epoch 66\n",
            "loss: 0.0983845517039299 at epoch 66\n",
            "loss: 0.05611268803477287 at epoch 66\n",
            "loss: 0.043297234922647476 at epoch 66\n",
            "loss: 0.06722525507211685 at epoch 66\n",
            "loss: 0.04457918182015419 at epoch 66\n",
            "loss: 0.08210387080907822 at epoch 66\n",
            "loss: 0.08996476978063583 at epoch 66\n",
            "loss: 0.12507909536361694 at epoch 66\n",
            "loss: 0.09803945571184158 at epoch 66\n",
            "loss: 0.1825450360774994 at epoch 66\n",
            "loss: 0.144359290599823 at epoch 66\n",
            "loss: 0.16709932684898376 at epoch 66\n",
            "loss: 0.07115410268306732 at epoch 66\n",
            "loss: 0.30105894804000854 at epoch 66\n",
            "loss: 0.6595651507377625 at epoch 66\n",
            "loss: 0.6192692518234253 at epoch 66\n",
            "loss: 0.34626010060310364 at epoch 66\n",
            "loss: 0.11788211762905121 at epoch 66\n",
            "loss: 0.6108095645904541 at epoch 66\n",
            "loss: 1.1826939582824707 at epoch 66\n",
            "loss: 1.9446420669555664 at epoch 66\n",
            "loss: 1.2726218700408936 at epoch 66\n",
            "loss: 0.5409450531005859 at epoch 66\n",
            "loss: 0.2396889626979828 at epoch 66\n",
            "loss: 0.5502649545669556 at epoch 66\n",
            "loss: 1.2518693208694458 at epoch 66\n",
            "loss: 1.011826992034912 at epoch 66\n",
            "loss: 0.8975294232368469 at epoch 66\n",
            "loss: 0.12422654777765274 at epoch 66\n",
            "loss: 1.0237503051757812 at epoch 66\n",
            "loss: 1.941136121749878 at epoch 66\n",
            "loss: 1.4457306861877441 at epoch 66\n",
            "loss: 0.8638373613357544 at epoch 66\n",
            "loss: 0.23535756766796112 at epoch 66\n",
            "loss: 0.6226685047149658 at epoch 66\n",
            "loss: 0.9136404991149902 at epoch 66\n",
            "loss: 0.8256602883338928 at epoch 66\n",
            "loss: 0.3652994930744171 at epoch 66\n",
            "loss: 0.13005845248699188 at epoch 66\n",
            "loss: 0.5246745347976685 at epoch 66\n",
            "loss: 0.5585117936134338 at epoch 66\n",
            "loss: 0.34888070821762085 at epoch 66\n",
            "loss: 0.182479590177536 at epoch 66\n",
            "loss: 0.5072359442710876 at epoch 66\n",
            "loss: 0.9433740377426147 at epoch 66\n",
            "loss: 0.5427681803703308 at epoch 66\n",
            "loss: 0.3134289085865021 at epoch 66\n",
            "loss: 0.2771223187446594 at epoch 66\n",
            "loss: 0.7592803835868835 at epoch 66\n",
            "loss: 0.6351993680000305 at epoch 66\n",
            "loss: 0.34270599484443665 at epoch 66\n",
            "loss: 0.3806222975254059 at epoch 66\n",
            "loss: 1.2159347534179688 at epoch 66\n",
            "loss: 0.6776026487350464 at epoch 66\n",
            "loss: 0.4446261525154114 at epoch 66\n",
            "loss: 0.17223361134529114 at epoch 66\n",
            "loss: 0.361446738243103 at epoch 66\n",
            "loss: 0.19201317429542542 at epoch 66\n",
            "loss: 0.15091295540332794 at epoch 66\n",
            "loss: 0.2298203855752945 at epoch 66\n",
            "loss: 0.10595403611660004 at epoch 66\n",
            "loss: 0.11008976399898529 at epoch 66\n",
            "loss: 0.17849263548851013 at epoch 66\n",
            "loss: 0.1267026960849762 at epoch 66\n",
            "loss: 0.04974541440606117 at epoch 66\n",
            "loss: 0.043395403772592545 at epoch 66\n",
            "loss: 0.03360651805996895 at epoch 66\n",
            "loss: 0.05884217843413353 at epoch 66\n",
            "loss: 0.017205335199832916 at epoch 66\n",
            "loss: 0.03637945279479027 at epoch 66\n",
            "loss: 0.048148464411497116 at epoch 66\n",
            "loss: 0.060045842081308365 at epoch 66\n",
            "loss: 0.029666662216186523 at epoch 66\n",
            "loss: 0.02817711792886257 at epoch 66\n",
            "loss: 0.01874837651848793 at epoch 66\n",
            "loss: 0.0554756298661232 at epoch 66\n",
            "loss: 0.052992403507232666 at epoch 67\n",
            "loss: 0.035496026277542114 at epoch 67\n",
            "loss: 0.03634665906429291 at epoch 67\n",
            "loss: 0.06043548882007599 at epoch 67\n",
            "loss: 0.07832297682762146 at epoch 67\n",
            "loss: 0.03578503802418709 at epoch 67\n",
            "loss: 0.032861754298210144 at epoch 67\n",
            "loss: 0.04042107239365578 at epoch 67\n",
            "loss: 0.07136272639036179 at epoch 67\n",
            "loss: 0.041939955204725266 at epoch 67\n",
            "loss: 0.024971656501293182 at epoch 67\n",
            "loss: 0.07332205772399902 at epoch 67\n",
            "loss: 0.12544643878936768 at epoch 67\n",
            "loss: 0.0400579608976841 at epoch 67\n",
            "loss: 0.05226357281208038 at epoch 67\n",
            "loss: 0.06221744045615196 at epoch 67\n",
            "loss: 0.12718285620212555 at epoch 67\n",
            "loss: 0.11053188145160675 at epoch 67\n",
            "loss: 0.026927463710308075 at epoch 67\n",
            "loss: 0.1402708888053894 at epoch 67\n",
            "loss: 0.13330376148223877 at epoch 67\n",
            "loss: 0.0883185863494873 at epoch 67\n",
            "loss: 0.051959071308374405 at epoch 67\n",
            "loss: 0.08431556820869446 at epoch 67\n",
            "loss: 0.09061824530363083 at epoch 67\n",
            "loss: 0.04938187077641487 at epoch 67\n",
            "loss: 0.05890209227800369 at epoch 67\n",
            "loss: 0.05434824526309967 at epoch 67\n",
            "loss: 0.08370070159435272 at epoch 67\n",
            "loss: 0.045412711799144745 at epoch 67\n",
            "loss: 0.10899978131055832 at epoch 67\n",
            "loss: 0.02074354514479637 at epoch 67\n",
            "loss: 0.12430131435394287 at epoch 67\n",
            "loss: 0.04274474456906319 at epoch 67\n",
            "loss: 0.11138955503702164 at epoch 67\n",
            "loss: 0.0495712049305439 at epoch 67\n",
            "loss: 0.0838593915104866 at epoch 67\n",
            "loss: 0.17444927990436554 at epoch 67\n",
            "loss: 0.11060266196727753 at epoch 67\n",
            "loss: 0.047116491943597794 at epoch 67\n",
            "loss: 0.06144193187355995 at epoch 67\n",
            "loss: 0.1879855990409851 at epoch 67\n",
            "loss: 0.06241532415151596 at epoch 67\n",
            "loss: 0.062071364372968674 at epoch 67\n",
            "loss: 0.14921288192272186 at epoch 67\n",
            "loss: 0.1531548798084259 at epoch 67\n",
            "loss: 0.03100738301873207 at epoch 67\n",
            "loss: 0.1017054095864296 at epoch 67\n",
            "loss: 0.0500478595495224 at epoch 67\n",
            "loss: 0.07555703073740005 at epoch 67\n",
            "loss: 0.013747922144830227 at epoch 67\n",
            "loss: 0.06302313506603241 at epoch 67\n",
            "loss: 0.03905584663152695 at epoch 67\n",
            "loss: 0.05546713247895241 at epoch 67\n",
            "loss: 0.020480027422308922 at epoch 67\n",
            "loss: 0.09784115105867386 at epoch 67\n",
            "loss: 0.11680547147989273 at epoch 67\n",
            "loss: 0.045509807765483856 at epoch 67\n",
            "loss: 0.039129357784986496 at epoch 67\n",
            "loss: 0.07741972804069519 at epoch 67\n",
            "loss: 0.13064733147621155 at epoch 67\n",
            "loss: 0.05330945551395416 at epoch 67\n",
            "loss: 0.03113279864192009 at epoch 67\n",
            "loss: 0.08512639999389648 at epoch 67\n",
            "loss: 0.12150263041257858 at epoch 67\n",
            "loss: 0.07760987430810928 at epoch 67\n",
            "loss: 0.057703856378793716 at epoch 67\n",
            "loss: 0.037431761622428894 at epoch 67\n",
            "loss: 0.07247387617826462 at epoch 67\n",
            "loss: 0.020928580313920975 at epoch 67\n",
            "loss: 0.05513921007514 at epoch 67\n",
            "loss: 0.03624308481812477 at epoch 67\n",
            "loss: 0.07239789515733719 at epoch 67\n",
            "loss: 0.025884168222546577 at epoch 67\n",
            "loss: 0.021931512281298637 at epoch 67\n",
            "loss: 0.051033515483140945 at epoch 67\n",
            "loss: 0.08757888525724411 at epoch 67\n",
            "loss: 0.06548330187797546 at epoch 67\n",
            "loss: 0.039933253079652786 at epoch 67\n",
            "loss: 0.03738037869334221 at epoch 68\n",
            "loss: 0.05804556608200073 at epoch 68\n",
            "loss: 0.029777448624372482 at epoch 68\n",
            "loss: 0.02421877533197403 at epoch 68\n",
            "loss: 0.006330318748950958 at epoch 68\n",
            "loss: 0.026369992643594742 at epoch 68\n",
            "loss: 0.00860280729830265 at epoch 68\n",
            "loss: 0.016148746013641357 at epoch 68\n",
            "loss: 0.018384741619229317 at epoch 68\n",
            "loss: 0.015292341820895672 at epoch 68\n",
            "loss: 0.018061906099319458 at epoch 68\n",
            "loss: 0.010557182133197784 at epoch 68\n",
            "loss: 0.020469672977924347 at epoch 68\n",
            "loss: 0.01211182028055191 at epoch 68\n",
            "loss: 0.014177246019244194 at epoch 68\n",
            "loss: 0.00610191049054265 at epoch 68\n",
            "loss: 0.007187922950834036 at epoch 68\n",
            "loss: 0.007674156688153744 at epoch 68\n",
            "loss: 0.012275240384042263 at epoch 68\n",
            "loss: 0.011319532059133053 at epoch 68\n",
            "loss: 0.005618801340460777 at epoch 68\n",
            "loss: 0.004540457855910063 at epoch 68\n",
            "loss: 0.0019008376402780414 at epoch 68\n",
            "loss: 0.00408557103946805 at epoch 68\n",
            "loss: 0.0024264208041131496 at epoch 68\n",
            "loss: 0.0031008331570774317 at epoch 68\n",
            "loss: 0.003596259979531169 at epoch 68\n",
            "loss: 0.006819851230829954 at epoch 68\n",
            "loss: 0.007416835054755211 at epoch 68\n",
            "loss: 0.003194019664078951 at epoch 68\n",
            "loss: 0.0034862791653722525 at epoch 68\n",
            "loss: 0.0022926623933017254 at epoch 68\n",
            "loss: 0.004322987049818039 at epoch 68\n",
            "loss: 0.0037144653033465147 at epoch 68\n",
            "loss: 0.008826622739434242 at epoch 68\n",
            "loss: 0.002006498398259282 at epoch 68\n",
            "loss: 0.007878548465669155 at epoch 68\n",
            "loss: 0.0016931358259171247 at epoch 68\n",
            "loss: 0.003026195801794529 at epoch 68\n",
            "loss: 0.00471368758007884 at epoch 68\n",
            "loss: 0.002168624196201563 at epoch 68\n",
            "loss: 0.003354990156367421 at epoch 68\n",
            "loss: 0.0014655522536486387 at epoch 68\n",
            "loss: 0.0068000913597643375 at epoch 68\n",
            "loss: 0.0007994844345375896 at epoch 68\n",
            "loss: 0.002514595165848732 at epoch 68\n",
            "loss: 0.0045843119733035564 at epoch 68\n",
            "loss: 0.0036716251634061337 at epoch 68\n",
            "loss: 0.0038362834602594376 at epoch 68\n",
            "loss: 0.001135488273575902 at epoch 68\n",
            "loss: 0.008677948266267776 at epoch 68\n",
            "loss: 0.0023476770147681236 at epoch 68\n",
            "loss: 0.0038090550806373358 at epoch 68\n",
            "loss: 0.01412886194884777 at epoch 68\n",
            "loss: 0.001991573954001069 at epoch 68\n",
            "loss: 0.005616196896880865 at epoch 68\n",
            "loss: 0.01787603087723255 at epoch 68\n",
            "loss: 0.022657498717308044 at epoch 68\n",
            "loss: 0.023912731558084488 at epoch 68\n",
            "loss: 0.009460296481847763 at epoch 68\n",
            "loss: 0.009523805230855942 at epoch 68\n",
            "loss: 0.008506199344992638 at epoch 68\n",
            "loss: 0.010045036673545837 at epoch 68\n",
            "loss: 0.008897178806364536 at epoch 68\n",
            "loss: 0.002382530365139246 at epoch 68\n",
            "loss: 0.005112692713737488 at epoch 68\n",
            "loss: 0.015080360695719719 at epoch 68\n",
            "loss: 0.022014522925019264 at epoch 68\n",
            "loss: 0.014528339728713036 at epoch 68\n",
            "loss: 0.011905836872756481 at epoch 68\n",
            "loss: 0.005449581891298294 at epoch 68\n",
            "loss: 0.007216340862214565 at epoch 68\n",
            "loss: 0.005777208134531975 at epoch 68\n",
            "loss: 0.005901019088923931 at epoch 68\n",
            "loss: 0.01010920386761427 at epoch 68\n",
            "loss: 0.01078485045582056 at epoch 68\n",
            "loss: 0.009458009153604507 at epoch 68\n",
            "loss: 0.0035701736342161894 at epoch 68\n",
            "loss: 0.01362741831690073 at epoch 68\n",
            "loss: 0.030729088932275772 at epoch 69\n",
            "loss: 0.03473515436053276 at epoch 69\n",
            "loss: 0.02530374936759472 at epoch 69\n",
            "loss: 0.005506178829818964 at epoch 69\n",
            "loss: 0.02046898752450943 at epoch 69\n",
            "loss: 0.007854010909795761 at epoch 69\n",
            "loss: 0.016462212428450584 at epoch 69\n",
            "loss: 0.026320530101656914 at epoch 69\n",
            "loss: 0.05362682044506073 at epoch 69\n",
            "loss: 0.05296235904097557 at epoch 69\n",
            "loss: 0.03810105472803116 at epoch 69\n",
            "loss: 0.01615571416914463 at epoch 69\n",
            "loss: 0.011901749297976494 at epoch 69\n",
            "loss: 0.04665328562259674 at epoch 69\n",
            "loss: 0.04486612603068352 at epoch 69\n",
            "loss: 0.06620416045188904 at epoch 69\n",
            "loss: 0.03876084089279175 at epoch 69\n",
            "loss: 0.046547677367925644 at epoch 69\n",
            "loss: 0.023846950381994247 at epoch 69\n",
            "loss: 0.01427143719047308 at epoch 69\n",
            "loss: 0.03862730786204338 at epoch 69\n",
            "loss: 0.005720202811062336 at epoch 69\n",
            "loss: 0.0246201753616333 at epoch 69\n",
            "loss: 0.023258456960320473 at epoch 69\n",
            "loss: 0.0781535878777504 at epoch 69\n",
            "loss: 0.08837945759296417 at epoch 69\n",
            "loss: 0.09867626428604126 at epoch 69\n",
            "loss: 0.06241299584507942 at epoch 69\n",
            "loss: 0.009996089152991772 at epoch 69\n",
            "loss: 0.03915053606033325 at epoch 69\n",
            "loss: 0.007121602073311806 at epoch 69\n",
            "loss: 0.021684525534510612 at epoch 69\n",
            "loss: 0.009095177054405212 at epoch 69\n",
            "loss: 0.02028430625796318 at epoch 69\n",
            "loss: 0.02023192122578621 at epoch 69\n",
            "loss: 0.02058589644730091 at epoch 69\n",
            "loss: 0.014744319953024387 at epoch 69\n",
            "loss: 0.013837737962603569 at epoch 69\n",
            "loss: 0.022670365869998932 at epoch 69\n",
            "loss: 0.03102416917681694 at epoch 69\n",
            "loss: 0.0236704982817173 at epoch 69\n",
            "loss: 0.017769258469343185 at epoch 69\n",
            "loss: 0.028774727135896683 at epoch 69\n",
            "loss: 0.05133341997861862 at epoch 69\n",
            "loss: 0.03368634358048439 at epoch 69\n",
            "loss: 0.037304043769836426 at epoch 69\n",
            "loss: 0.02378656156361103 at epoch 69\n",
            "loss: 0.012090257368981838 at epoch 69\n",
            "loss: 0.015392681583762169 at epoch 69\n",
            "loss: 0.022786861285567284 at epoch 69\n",
            "loss: 0.03390001878142357 at epoch 69\n",
            "loss: 0.01733124628663063 at epoch 69\n",
            "loss: 0.02720721811056137 at epoch 69\n",
            "loss: 0.030339905992150307 at epoch 69\n",
            "loss: 0.04859274625778198 at epoch 69\n",
            "loss: 0.05031656473875046 at epoch 69\n",
            "loss: 0.03376477584242821 at epoch 69\n",
            "loss: 0.011847030371427536 at epoch 69\n",
            "loss: 0.013423819094896317 at epoch 69\n",
            "loss: 0.019246142357587814 at epoch 69\n",
            "loss: 0.034790195524692535 at epoch 69\n",
            "loss: 0.0195392407476902 at epoch 69\n",
            "loss: 0.02032669633626938 at epoch 69\n",
            "loss: 0.018226195126771927 at epoch 69\n",
            "loss: 0.015333248302340508 at epoch 69\n",
            "loss: 0.01854799874126911 at epoch 69\n",
            "loss: 0.019157692790031433 at epoch 69\n",
            "loss: 0.025494810193777084 at epoch 69\n",
            "loss: 0.02549983747303486 at epoch 69\n",
            "loss: 0.011785658076405525 at epoch 69\n",
            "loss: 0.0027618850581347942 at epoch 69\n",
            "loss: 0.006644358392804861 at epoch 69\n",
            "loss: 0.010250681079924107 at epoch 69\n",
            "loss: 0.01522243395447731 at epoch 69\n",
            "loss: 0.009713400155305862 at epoch 69\n",
            "loss: 0.011190729215741158 at epoch 69\n",
            "loss: 0.0032052535098046064 at epoch 69\n",
            "loss: 0.00870356336236 at epoch 69\n",
            "loss: 0.0019414486596360803 at epoch 69\n",
            "loss: 0.009373986162245274 at epoch 70\n",
            "loss: 0.016616683453321457 at epoch 70\n",
            "loss: 0.012276146560907364 at epoch 70\n",
            "loss: 0.005540830548852682 at epoch 70\n",
            "loss: 0.0019896388985216618 at epoch 70\n",
            "loss: 0.01028036791831255 at epoch 70\n",
            "loss: 0.02410334162414074 at epoch 70\n",
            "loss: 0.032690420746803284 at epoch 70\n",
            "loss: 0.018615689128637314 at epoch 70\n",
            "loss: 0.003022945486009121 at epoch 70\n",
            "loss: 0.023828353732824326 at epoch 70\n",
            "loss: 0.0723385289311409 at epoch 70\n",
            "loss: 0.12292465567588806 at epoch 70\n",
            "loss: 0.141829252243042 at epoch 70\n",
            "loss: 0.11757706850767136 at epoch 70\n",
            "loss: 0.04138023778796196 at epoch 70\n",
            "loss: 0.030351141467690468 at epoch 70\n",
            "loss: 0.06126515194773674 at epoch 70\n",
            "loss: 0.1311718374490738 at epoch 70\n",
            "loss: 0.17476491630077362 at epoch 70\n",
            "loss: 0.16809961199760437 at epoch 70\n",
            "loss: 0.08981642127037048 at epoch 70\n",
            "loss: 0.042262963950634 at epoch 70\n",
            "loss: 0.053217798471450806 at epoch 70\n",
            "loss: 0.08923882246017456 at epoch 70\n",
            "loss: 0.17244839668273926 at epoch 70\n",
            "loss: 0.18409119546413422 at epoch 70\n",
            "loss: 0.1549077332019806 at epoch 70\n",
            "loss: 0.15986193716526031 at epoch 70\n",
            "loss: 0.14591021835803986 at epoch 70\n",
            "loss: 0.07907038927078247 at epoch 70\n",
            "loss: 0.055976249277591705 at epoch 70\n",
            "loss: 0.07812220603227615 at epoch 70\n",
            "loss: 0.10887467861175537 at epoch 70\n",
            "loss: 0.15915779769420624 at epoch 70\n",
            "loss: 0.07620196044445038 at epoch 70\n",
            "loss: 0.14564543962478638 at epoch 70\n",
            "loss: 0.18217895925045013 at epoch 70\n",
            "loss: 0.20235693454742432 at epoch 70\n",
            "loss: 0.06037922948598862 at epoch 70\n",
            "loss: 0.09159152209758759 at epoch 70\n",
            "loss: 0.10586649179458618 at epoch 70\n",
            "loss: 0.15849703550338745 at epoch 70\n",
            "loss: 0.19465196132659912 at epoch 70\n",
            "loss: 0.06565782427787781 at epoch 70\n",
            "loss: 0.11468171328306198 at epoch 70\n",
            "loss: 0.04430452361702919 at epoch 70\n",
            "loss: 0.08707486838102341 at epoch 70\n",
            "loss: 0.021706916391849518 at epoch 70\n",
            "loss: 0.09193256497383118 at epoch 70\n",
            "loss: 0.0384204238653183 at epoch 70\n",
            "loss: 0.09294171631336212 at epoch 70\n",
            "loss: 0.08229992538690567 at epoch 70\n",
            "loss: 0.12142346054315567 at epoch 70\n",
            "loss: 0.03238248825073242 at epoch 70\n",
            "loss: 0.051188401877880096 at epoch 70\n",
            "loss: 0.04746526852250099 at epoch 70\n",
            "loss: 0.07954070717096329 at epoch 70\n",
            "loss: 0.07685067504644394 at epoch 70\n",
            "loss: 0.10829517245292664 at epoch 70\n",
            "loss: 0.06420975178480148 at epoch 70\n",
            "loss: 0.028686147183179855 at epoch 70\n",
            "loss: 0.016792356967926025 at epoch 70\n",
            "loss: 0.08660373091697693 at epoch 70\n",
            "loss: 0.1724841445684433 at epoch 70\n",
            "loss: 0.28196024894714355 at epoch 70\n",
            "loss: 0.2677137851715088 at epoch 70\n",
            "loss: 0.23599712550640106 at epoch 70\n",
            "loss: 0.0895518958568573 at epoch 70\n",
            "loss: 0.0551876574754715 at epoch 70\n",
            "loss: 0.09395325928926468 at epoch 70\n",
            "loss: 0.2854219973087311 at epoch 70\n",
            "loss: 0.3753981590270996 at epoch 70\n",
            "loss: 0.2841973900794983 at epoch 70\n",
            "loss: 0.46050700545310974 at epoch 70\n",
            "loss: 0.41376686096191406 at epoch 70\n",
            "loss: 0.38145679235458374 at epoch 70\n",
            "loss: 0.15672430396080017 at epoch 70\n",
            "loss: 0.29184481501579285 at epoch 70\n",
            "loss: 0.8790830373764038 at epoch 71\n",
            "loss: 0.6587418913841248 at epoch 71\n",
            "loss: 1.5274399518966675 at epoch 71\n",
            "loss: 1.4282227754592896 at epoch 71\n",
            "loss: 1.5794413089752197 at epoch 71\n",
            "loss: 1.2298946380615234 at epoch 71\n",
            "loss: 0.8999168276786804 at epoch 71\n",
            "loss: 0.49677136540412903 at epoch 71\n",
            "loss: 0.3408059775829315 at epoch 71\n",
            "loss: 0.7529239654541016 at epoch 71\n",
            "loss: 1.1652451753616333 at epoch 71\n",
            "loss: 1.0585228204727173 at epoch 71\n",
            "loss: 0.338947594165802 at epoch 71\n",
            "loss: 0.8735548257827759 at epoch 71\n",
            "loss: 0.9653609991073608 at epoch 71\n",
            "loss: 1.3356754779815674 at epoch 71\n",
            "loss: 1.316890835762024 at epoch 71\n",
            "loss: 0.5596666932106018 at epoch 71\n",
            "loss: 2.677659273147583 at epoch 71\n",
            "loss: 1.9441965818405151 at epoch 71\n",
            "loss: 0.8069373369216919 at epoch 71\n",
            "loss: 0.8758578300476074 at epoch 71\n",
            "loss: 1.485373854637146 at epoch 71\n",
            "loss: 2.673976421356201 at epoch 71\n",
            "loss: 0.997673511505127 at epoch 71\n",
            "loss: 0.902266800403595 at epoch 71\n",
            "loss: 1.1320425271987915 at epoch 71\n",
            "loss: 2.121115207672119 at epoch 71\n",
            "loss: 0.7822006940841675 at epoch 71\n",
            "loss: 3.1632113456726074 at epoch 71\n",
            "loss: 3.480325937271118 at epoch 71\n",
            "loss: 1.4896316528320312 at epoch 71\n",
            "loss: 1.2268887758255005 at epoch 71\n",
            "loss: 5.014090061187744 at epoch 71\n",
            "loss: 6.0496063232421875 at epoch 71\n",
            "loss: 0.9528694748878479 at epoch 71\n",
            "loss: 3.5317649841308594 at epoch 71\n",
            "loss: 2.4929041862487793 at epoch 71\n",
            "loss: 2.8371598720550537 at epoch 71\n",
            "loss: 1.3018317222595215 at epoch 71\n",
            "loss: 2.3648688793182373 at epoch 71\n",
            "loss: 2.365978956222534 at epoch 71\n",
            "loss: 1.0589677095413208 at epoch 71\n",
            "loss: 2.2065088748931885 at epoch 71\n",
            "loss: 3.358764410018921 at epoch 71\n",
            "loss: 0.43793022632598877 at epoch 71\n",
            "loss: 2.125807762145996 at epoch 71\n",
            "loss: 2.5536983013153076 at epoch 71\n",
            "loss: 2.031230926513672 at epoch 71\n",
            "loss: 2.9292635917663574 at epoch 71\n",
            "loss: 4.301224708557129 at epoch 71\n",
            "loss: 2.0866951942443848 at epoch 71\n",
            "loss: 2.0652267932891846 at epoch 71\n",
            "loss: 3.0034444332122803 at epoch 71\n",
            "loss: 3.7272651195526123 at epoch 71\n",
            "loss: 0.6359710097312927 at epoch 71\n",
            "loss: 2.5794405937194824 at epoch 71\n",
            "loss: 1.2114388942718506 at epoch 71\n",
            "loss: 0.9758841395378113 at epoch 71\n",
            "loss: 2.4420113563537598 at epoch 71\n",
            "loss: 0.40323469042778015 at epoch 71\n",
            "loss: 1.581437110900879 at epoch 71\n",
            "loss: 2.0442492961883545 at epoch 71\n",
            "loss: 0.42607685923576355 at epoch 71\n",
            "loss: 2.1907217502593994 at epoch 71\n",
            "loss: 2.189962148666382 at epoch 71\n",
            "loss: 0.7511597871780396 at epoch 71\n",
            "loss: 0.6828434467315674 at epoch 71\n",
            "loss: 1.5968750715255737 at epoch 71\n",
            "loss: 0.473869264125824 at epoch 71\n",
            "loss: 0.8586273789405823 at epoch 71\n",
            "loss: 0.7420743703842163 at epoch 71\n",
            "loss: 0.5088819861412048 at epoch 71\n",
            "loss: 0.8040257692337036 at epoch 71\n",
            "loss: 0.5548152923583984 at epoch 71\n",
            "loss: 0.7569997906684875 at epoch 71\n",
            "loss: 0.9266096353530884 at epoch 71\n",
            "loss: 0.5583188533782959 at epoch 71\n",
            "loss: 0.7811815142631531 at epoch 71\n",
            "loss: 0.5689483880996704 at epoch 72\n",
            "loss: 0.33161765336990356 at epoch 72\n",
            "loss: 0.4930574893951416 at epoch 72\n",
            "loss: 0.6769330501556396 at epoch 72\n",
            "loss: 0.23800839483737946 at epoch 72\n",
            "loss: 0.4593021869659424 at epoch 72\n",
            "loss: 0.6389869451522827 at epoch 72\n",
            "loss: 0.21709184348583221 at epoch 72\n",
            "loss: 0.5400748252868652 at epoch 72\n",
            "loss: 0.21227674186229706 at epoch 72\n",
            "loss: 0.4236603379249573 at epoch 72\n",
            "loss: 0.3085128962993622 at epoch 72\n",
            "loss: 0.28037694096565247 at epoch 72\n",
            "loss: 0.3682639002799988 at epoch 72\n",
            "loss: 0.1800215244293213 at epoch 72\n",
            "loss: 0.4724372327327728 at epoch 72\n",
            "loss: 0.16982793807983398 at epoch 72\n",
            "loss: 0.18145467340946198 at epoch 72\n",
            "loss: 0.32111695408821106 at epoch 72\n",
            "loss: 0.15955865383148193 at epoch 72\n",
            "loss: 0.22533488273620605 at epoch 72\n",
            "loss: 0.33425164222717285 at epoch 72\n",
            "loss: 0.15465907752513885 at epoch 72\n",
            "loss: 0.3476046323776245 at epoch 72\n",
            "loss: 0.5942366719245911 at epoch 72\n",
            "loss: 0.2087751030921936 at epoch 72\n",
            "loss: 0.44651442766189575 at epoch 72\n",
            "loss: 0.3501109480857849 at epoch 72\n",
            "loss: 0.09649591147899628 at epoch 72\n",
            "loss: 0.41244909167289734 at epoch 72\n",
            "loss: 0.08398421108722687 at epoch 72\n",
            "loss: 0.29041552543640137 at epoch 72\n",
            "loss: 0.17786478996276855 at epoch 72\n",
            "loss: 0.17196711897850037 at epoch 72\n",
            "loss: 0.37626034021377563 at epoch 72\n",
            "loss: 0.19229604303836823 at epoch 72\n",
            "loss: 0.33181437849998474 at epoch 72\n",
            "loss: 0.33298563957214355 at epoch 72\n",
            "loss: 0.2665279805660248 at epoch 72\n",
            "loss: 0.3833324611186981 at epoch 72\n",
            "loss: 0.36862102150917053 at epoch 72\n",
            "loss: 0.30690425634384155 at epoch 72\n",
            "loss: 0.5138429403305054 at epoch 72\n",
            "loss: 0.24557550251483917 at epoch 72\n",
            "loss: 0.23464040458202362 at epoch 72\n",
            "loss: 0.21678173542022705 at epoch 72\n",
            "loss: 0.2134268879890442 at epoch 72\n",
            "loss: 0.16677799820899963 at epoch 72\n",
            "loss: 0.3444630801677704 at epoch 72\n",
            "loss: 0.2654145359992981 at epoch 72\n",
            "loss: 0.16614367067813873 at epoch 72\n",
            "loss: 0.30178794264793396 at epoch 72\n",
            "loss: 0.15942104160785675 at epoch 72\n",
            "loss: 0.27258163690567017 at epoch 72\n",
            "loss: 0.3624740540981293 at epoch 72\n",
            "loss: 0.10201990604400635 at epoch 72\n",
            "loss: 0.30063843727111816 at epoch 72\n",
            "loss: 0.3236762285232544 at epoch 72\n",
            "loss: 0.03843314200639725 at epoch 72\n",
            "loss: 0.18775855004787445 at epoch 72\n",
            "loss: 0.172566220164299 at epoch 72\n",
            "loss: 0.09077989310026169 at epoch 72\n",
            "loss: 0.2888178527355194 at epoch 72\n",
            "loss: 0.16211876273155212 at epoch 72\n",
            "loss: 0.11468082666397095 at epoch 72\n",
            "loss: 0.313112735748291 at epoch 72\n",
            "loss: 0.20569637417793274 at epoch 72\n",
            "loss: 0.07069126516580582 at epoch 72\n",
            "loss: 0.1272648721933365 at epoch 72\n",
            "loss: 0.06344661861658096 at epoch 72\n",
            "loss: 0.20976673066616058 at epoch 72\n",
            "loss: 0.33549049496650696 at epoch 72\n",
            "loss: 0.13289594650268555 at epoch 72\n",
            "loss: 0.12840062379837036 at epoch 72\n",
            "loss: 0.11429767310619354 at epoch 72\n",
            "loss: 0.14162257313728333 at epoch 72\n",
            "loss: 0.20053516328334808 at epoch 72\n",
            "loss: 0.17069005966186523 at epoch 72\n",
            "loss: 0.14962342381477356 at epoch 72\n",
            "loss: 0.3292458951473236 at epoch 73\n",
            "loss: 0.33703505992889404 at epoch 73\n",
            "loss: 0.10756363719701767 at epoch 73\n",
            "loss: 0.2775748372077942 at epoch 73\n",
            "loss: 0.1846970021724701 at epoch 73\n",
            "loss: 0.14163632690906525 at epoch 73\n",
            "loss: 0.31227314472198486 at epoch 73\n",
            "loss: 0.1877465844154358 at epoch 73\n",
            "loss: 0.1337571144104004 at epoch 73\n",
            "loss: 0.3851422071456909 at epoch 73\n",
            "loss: 0.17798969149589539 at epoch 73\n",
            "loss: 0.19829323887825012 at epoch 73\n",
            "loss: 0.3931005895137787 at epoch 73\n",
            "loss: 0.2144453078508377 at epoch 73\n",
            "loss: 0.15452903509140015 at epoch 73\n",
            "loss: 0.24421697854995728 at epoch 73\n",
            "loss: 0.2102140188217163 at epoch 73\n",
            "loss: 0.07218313217163086 at epoch 73\n",
            "loss: 0.17498180270195007 at epoch 73\n",
            "loss: 0.06699209660291672 at epoch 73\n",
            "loss: 0.07970590144395828 at epoch 73\n",
            "loss: 0.10170779377222061 at epoch 73\n",
            "loss: 0.03417959064245224 at epoch 73\n",
            "loss: 0.10536820441484451 at epoch 73\n",
            "loss: 0.009092080406844616 at epoch 73\n",
            "loss: 0.07040940970182419 at epoch 73\n",
            "loss: 0.050115879625082016 at epoch 73\n",
            "loss: 0.02764592133462429 at epoch 73\n",
            "loss: 0.06579980254173279 at epoch 73\n",
            "loss: 0.052307650446891785 at epoch 73\n",
            "loss: 0.06995323300361633 at epoch 73\n",
            "loss: 0.02322397381067276 at epoch 73\n",
            "loss: 0.06967370212078094 at epoch 73\n",
            "loss: 0.028683550655841827 at epoch 73\n",
            "loss: 0.07595153152942657 at epoch 73\n",
            "loss: 0.05303457751870155 at epoch 73\n",
            "loss: 0.08448302745819092 at epoch 73\n",
            "loss: 0.039776600897312164 at epoch 73\n",
            "loss: 0.046311650425195694 at epoch 73\n",
            "loss: 0.06960837543010712 at epoch 73\n",
            "loss: 0.013434904627501965 at epoch 73\n",
            "loss: 0.05898183956742287 at epoch 73\n",
            "loss: 0.042541395872831345 at epoch 73\n",
            "loss: 0.03768518194556236 at epoch 73\n",
            "loss: 0.03654426336288452 at epoch 73\n",
            "loss: 0.01858951896429062 at epoch 73\n",
            "loss: 0.03344832360744476 at epoch 73\n",
            "loss: 0.012050249613821507 at epoch 73\n",
            "loss: 0.040108200162649155 at epoch 73\n",
            "loss: 0.008944365195930004 at epoch 73\n",
            "loss: 0.029962357133626938 at epoch 73\n",
            "loss: 0.016416892409324646 at epoch 73\n",
            "loss: 0.03698797523975372 at epoch 73\n",
            "loss: 0.0337243489921093 at epoch 73\n",
            "loss: 0.05757769197225571 at epoch 73\n",
            "loss: 0.06432446092367172 at epoch 73\n",
            "loss: 0.028790676966309547 at epoch 73\n",
            "loss: 0.06964445859193802 at epoch 73\n",
            "loss: 0.04410272464156151 at epoch 73\n",
            "loss: 0.06673965603113174 at epoch 73\n",
            "loss: 0.09725881367921829 at epoch 73\n",
            "loss: 0.08355438709259033 at epoch 73\n",
            "loss: 0.02770976535975933 at epoch 73\n",
            "loss: 0.06258469074964523 at epoch 73\n",
            "loss: 0.04447716102004051 at epoch 73\n",
            "loss: 0.012318374589085579 at epoch 73\n",
            "loss: 0.06373018026351929 at epoch 73\n",
            "loss: 0.035616327077150345 at epoch 73\n",
            "loss: 0.02186841331422329 at epoch 73\n",
            "loss: 0.03067721053957939 at epoch 73\n",
            "loss: 0.011533118784427643 at epoch 73\n",
            "loss: 0.03889778256416321 at epoch 73\n",
            "loss: 0.021250570192933083 at epoch 73\n",
            "loss: 0.01982293650507927 at epoch 73\n",
            "loss: 0.040236398577690125 at epoch 73\n",
            "loss: 0.04719327390193939 at epoch 73\n",
            "loss: 0.020863134413957596 at epoch 73\n",
            "loss: 0.04771162569522858 at epoch 73\n",
            "loss: 0.04326244443655014 at epoch 73\n",
            "loss: 0.05418037995696068 at epoch 74\n",
            "loss: 0.09541190415620804 at epoch 74\n",
            "loss: 0.04558742791414261 at epoch 74\n",
            "loss: 0.04990562051534653 at epoch 74\n",
            "loss: 0.035584162920713425 at epoch 74\n",
            "loss: 0.039926886558532715 at epoch 74\n",
            "loss: 0.03844514116644859 at epoch 74\n",
            "loss: 0.05265941098332405 at epoch 74\n",
            "loss: 0.05358723923563957 at epoch 74\n",
            "loss: 0.03216550126671791 at epoch 74\n",
            "loss: 0.01866837777197361 at epoch 74\n",
            "loss: 0.02867351472377777 at epoch 74\n",
            "loss: 0.018150893971323967 at epoch 74\n",
            "loss: 0.026244347915053368 at epoch 74\n",
            "loss: 0.013773223385214806 at epoch 74\n",
            "loss: 0.021202649921178818 at epoch 74\n",
            "loss: 0.0330674946308136 at epoch 74\n",
            "loss: 0.010742291808128357 at epoch 74\n",
            "loss: 0.018089763820171356 at epoch 74\n",
            "loss: 0.018323594704270363 at epoch 74\n",
            "loss: 0.014203103259205818 at epoch 74\n",
            "loss: 0.00422262167558074 at epoch 74\n",
            "loss: 0.007324776612222195 at epoch 74\n",
            "loss: 0.0059696221724152565 at epoch 74\n",
            "loss: 0.007420457899570465 at epoch 74\n",
            "loss: 0.007454130332916975 at epoch 74\n",
            "loss: 0.0020049733575433493 at epoch 74\n",
            "loss: 0.003481070278212428 at epoch 74\n",
            "loss: 0.005392720922827721 at epoch 74\n",
            "loss: 0.003887976286932826 at epoch 74\n",
            "loss: 0.0040207854472100735 at epoch 74\n",
            "loss: 0.006390232592821121 at epoch 74\n",
            "loss: 0.0029695620760321617 at epoch 74\n",
            "loss: 0.005320493131875992 at epoch 74\n",
            "loss: 0.012456908822059631 at epoch 74\n",
            "loss: 0.003579226788133383 at epoch 74\n",
            "loss: 0.0018822499550879002 at epoch 74\n",
            "loss: 0.0011076475493609905 at epoch 74\n",
            "loss: 0.002041817642748356 at epoch 74\n",
            "loss: 0.002082410966977477 at epoch 74\n",
            "loss: 0.001463118358515203 at epoch 74\n",
            "loss: 0.0016487521352246404 at epoch 74\n",
            "loss: 0.0012519946321845055 at epoch 74\n",
            "loss: 0.01468692347407341 at epoch 74\n",
            "loss: 0.0016125896945595741 at epoch 74\n",
            "loss: 0.004045738372951746 at epoch 74\n",
            "loss: 0.0009815138764679432 at epoch 74\n",
            "loss: 0.0009077542345039546 at epoch 74\n",
            "loss: 0.0007483268855139613 at epoch 74\n",
            "loss: 0.0019046217203140259 at epoch 74\n",
            "loss: 0.00887476559728384 at epoch 74\n",
            "loss: 0.0027494896203279495 at epoch 74\n",
            "loss: 0.0009896765695884824 at epoch 74\n",
            "loss: 0.0010766959749162197 at epoch 74\n",
            "loss: 0.0018906926270574331 at epoch 74\n",
            "loss: 0.0014850302832201123 at epoch 74\n",
            "loss: 0.005938287824392319 at epoch 74\n",
            "loss: 0.0025189679581671953 at epoch 74\n",
            "loss: 0.004223095253109932 at epoch 74\n",
            "loss: 0.002079355763271451 at epoch 74\n",
            "loss: 0.0037348021287471056 at epoch 74\n",
            "loss: 0.0017298206221312284 at epoch 74\n",
            "loss: 0.0032439706847071648 at epoch 74\n",
            "loss: 0.004212561063468456 at epoch 74\n",
            "loss: 0.003307514823973179 at epoch 74\n",
            "loss: 0.0034733281936496496 at epoch 74\n",
            "loss: 0.0009874022798612714 at epoch 74\n",
            "loss: 0.0032916138879954815 at epoch 74\n",
            "loss: 0.0023830002173781395 at epoch 74\n",
            "loss: 0.0020080606918781996 at epoch 74\n",
            "loss: 0.0007689767517149448 at epoch 74\n",
            "loss: 0.0014998241094872355 at epoch 74\n",
            "loss: 0.001347192912362516 at epoch 74\n",
            "loss: 0.0006211019353941083 at epoch 74\n",
            "loss: 0.0011926537845283747 at epoch 74\n",
            "loss: 0.008188488893210888 at epoch 74\n",
            "loss: 0.0011650638189166784 at epoch 74\n",
            "loss: 0.0016310130013152957 at epoch 74\n",
            "loss: 0.004819976165890694 at epoch 74\n",
            "loss: 0.00503319688141346 at epoch 75\n",
            "loss: 0.006610613316297531 at epoch 75\n",
            "loss: 0.003452406730502844 at epoch 75\n",
            "loss: 0.009480854496359825 at epoch 75\n",
            "loss: 0.002430347492918372 at epoch 75\n",
            "loss: 0.0032867430709302425 at epoch 75\n",
            "loss: 0.0029391455464065075 at epoch 75\n",
            "loss: 0.0030455703381448984 at epoch 75\n",
            "loss: 0.006108576897531748 at epoch 75\n",
            "loss: 0.00284294574521482 at epoch 75\n",
            "loss: 0.0015226604882627726 at epoch 75\n",
            "loss: 0.0037735013756901026 at epoch 75\n",
            "loss: 0.0005799731006845832 at epoch 75\n",
            "loss: 0.0020258373115211725 at epoch 75\n",
            "loss: 0.0014571438077837229 at epoch 75\n",
            "loss: 0.001218694495037198 at epoch 75\n",
            "loss: 0.00205391482450068 at epoch 75\n",
            "loss: 0.001153615303337574 at epoch 75\n",
            "loss: 0.004918312653899193 at epoch 75\n",
            "loss: 0.0018803148996084929 at epoch 75\n",
            "loss: 0.0048206280916929245 at epoch 75\n",
            "loss: 0.002246318617835641 at epoch 75\n",
            "loss: 0.004793618805706501 at epoch 75\n",
            "loss: 0.002871844917535782 at epoch 75\n",
            "loss: 0.0020566205494105816 at epoch 75\n",
            "loss: 0.0020832119043916464 at epoch 75\n",
            "loss: 0.01091146282851696 at epoch 75\n",
            "loss: 0.0008813401218503714 at epoch 75\n",
            "loss: 0.0027724700048565865 at epoch 75\n",
            "loss: 0.0025191917084157467 at epoch 75\n",
            "loss: 0.010710659436881542 at epoch 75\n",
            "loss: 0.0014999551931396127 at epoch 75\n",
            "loss: 0.0016687986208125949 at epoch 75\n",
            "loss: 0.0009517640573903918 at epoch 75\n",
            "loss: 0.001607805141247809 at epoch 75\n",
            "loss: 0.00434698723256588 at epoch 75\n",
            "loss: 0.006587956566363573 at epoch 75\n",
            "loss: 0.0009858181001618505 at epoch 75\n",
            "loss: 0.0015721380477771163 at epoch 75\n",
            "loss: 0.002415717812255025 at epoch 75\n",
            "loss: 0.002451669191941619 at epoch 75\n",
            "loss: 0.000841087254229933 at epoch 75\n",
            "loss: 0.000490093429107219 at epoch 75\n",
            "loss: 0.0007047138060443103 at epoch 75\n",
            "loss: 0.0004586241557262838 at epoch 75\n",
            "loss: 0.0009263521060347557 at epoch 75\n",
            "loss: 0.0006322264671325684 at epoch 75\n",
            "loss: 0.00049071095418185 at epoch 75\n",
            "loss: 0.000668373191729188 at epoch 75\n",
            "loss: 0.0016277780523523688 at epoch 75\n",
            "loss: 0.0018419143743813038 at epoch 75\n",
            "loss: 0.00078894681064412 at epoch 75\n",
            "loss: 0.0005479799001477659 at epoch 75\n",
            "loss: 0.0012930429074913263 at epoch 75\n",
            "loss: 0.0005124526214785874 at epoch 75\n",
            "loss: 0.0010983129031956196 at epoch 75\n",
            "loss: 0.0009867927292361856 at epoch 75\n",
            "loss: 0.00017325663066003472 at epoch 75\n",
            "loss: 0.0013520288048312068 at epoch 75\n",
            "loss: 0.0012436453253030777 at epoch 75\n",
            "loss: 0.0047865272499620914 at epoch 75\n",
            "loss: 0.0010545527329668403 at epoch 75\n",
            "loss: 0.0014066678704693913 at epoch 75\n",
            "loss: 0.0014243038604035974 at epoch 75\n",
            "loss: 0.004236728884279728 at epoch 75\n",
            "loss: 0.003897862508893013 at epoch 75\n",
            "loss: 0.0027347079012542963 at epoch 75\n",
            "loss: 0.0002716378658078611 at epoch 75\n",
            "loss: 0.002096574055030942 at epoch 75\n",
            "loss: 0.0014119495172053576 at epoch 75\n",
            "loss: 0.0034228095319122076 at epoch 75\n",
            "loss: 0.0018091273959726095 at epoch 75\n",
            "loss: 0.0029722312465310097 at epoch 75\n",
            "loss: 0.0016705631278455257 at epoch 75\n",
            "loss: 0.0004400841426104307 at epoch 75\n",
            "loss: 0.001954098464921117 at epoch 75\n",
            "loss: 0.005891155451536179 at epoch 75\n",
            "loss: 0.0008296500891447067 at epoch 75\n",
            "loss: 0.001161973224952817 at epoch 75\n",
            "loss: 0.001256338320672512 at epoch 76\n",
            "loss: 0.0003635025641415268 at epoch 76\n",
            "loss: 0.0012452012160792947 at epoch 76\n",
            "loss: 0.0036822170950472355 at epoch 76\n",
            "loss: 0.00042217480950057507 at epoch 76\n",
            "loss: 0.00909739825874567 at epoch 76\n",
            "loss: 0.004208302590996027 at epoch 76\n",
            "loss: 0.005563555285334587 at epoch 76\n",
            "loss: 0.000557346036657691 at epoch 76\n",
            "loss: 0.002859759610146284 at epoch 76\n",
            "loss: 0.003853646805509925 at epoch 76\n",
            "loss: 0.0037190329749137163 at epoch 76\n",
            "loss: 0.0046077873557806015 at epoch 76\n",
            "loss: 0.006405777297914028 at epoch 76\n",
            "loss: 0.002651026239618659 at epoch 76\n",
            "loss: 0.005518404766917229 at epoch 76\n",
            "loss: 0.005233862437307835 at epoch 76\n",
            "loss: 0.004799275659024715 at epoch 76\n",
            "loss: 0.002968067303299904 at epoch 76\n",
            "loss: 0.005406804848462343 at epoch 76\n",
            "loss: 0.006217154674232006 at epoch 76\n",
            "loss: 0.0032281377352774143 at epoch 76\n",
            "loss: 0.005041385069489479 at epoch 76\n",
            "loss: 0.004923678003251553 at epoch 76\n",
            "loss: 0.006183466874063015 at epoch 76\n",
            "loss: 0.0022327082697302103 at epoch 76\n",
            "loss: 0.0012556235305964947 at epoch 76\n",
            "loss: 0.005743302404880524 at epoch 76\n",
            "loss: 0.0038152672350406647 at epoch 76\n",
            "loss: 0.005864637903869152 at epoch 76\n",
            "loss: 0.000781111535616219 at epoch 76\n",
            "loss: 0.0019065228989347816 at epoch 76\n",
            "loss: 0.0017161197029054165 at epoch 76\n",
            "loss: 0.0013711185893043876 at epoch 76\n",
            "loss: 0.0025959124322980642 at epoch 76\n",
            "loss: 0.002472059102728963 at epoch 76\n",
            "loss: 0.001281862729229033 at epoch 76\n",
            "loss: 0.0012007532641291618 at epoch 76\n",
            "loss: 0.002741590840741992 at epoch 76\n",
            "loss: 0.0006915158592164516 at epoch 76\n",
            "loss: 0.0016725737368687987 at epoch 76\n",
            "loss: 0.002472798340022564 at epoch 76\n",
            "loss: 0.0017386634135618806 at epoch 76\n",
            "loss: 0.0019097879994660616 at epoch 76\n",
            "loss: 0.0035887728445231915 at epoch 76\n",
            "loss: 0.0040487321093678474 at epoch 76\n",
            "loss: 0.0007441597990691662 at epoch 76\n",
            "loss: 0.004255662672221661 at epoch 76\n",
            "loss: 0.004506430588662624 at epoch 76\n",
            "loss: 0.0033275315072387457 at epoch 76\n",
            "loss: 0.002951439004391432 at epoch 76\n",
            "loss: 0.004730943590402603 at epoch 76\n",
            "loss: 0.0024413971696048975 at epoch 76\n",
            "loss: 0.002931615337729454 at epoch 76\n",
            "loss: 0.010534708388149738 at epoch 76\n",
            "loss: 0.007266782224178314 at epoch 76\n",
            "loss: 0.0016271172789856791 at epoch 76\n",
            "loss: 0.011400565505027771 at epoch 76\n",
            "loss: 0.012900426983833313 at epoch 76\n",
            "loss: 0.012544042430818081 at epoch 76\n",
            "loss: 0.004412631969898939 at epoch 76\n",
            "loss: 0.011918742209672928 at epoch 76\n",
            "loss: 0.015583530068397522 at epoch 76\n",
            "loss: 0.010655857622623444 at epoch 76\n",
            "loss: 0.010798992589116096 at epoch 76\n",
            "loss: 0.018017973750829697 at epoch 76\n",
            "loss: 0.006535351276397705 at epoch 76\n",
            "loss: 0.007275170646607876 at epoch 76\n",
            "loss: 0.009820111095905304 at epoch 76\n",
            "loss: 0.007210099138319492 at epoch 76\n",
            "loss: 0.0032633226364851 at epoch 76\n",
            "loss: 0.00861210748553276 at epoch 76\n",
            "loss: 0.007319116033613682 at epoch 76\n",
            "loss: 0.0056860665790736675 at epoch 76\n",
            "loss: 0.0016826161881908774 at epoch 76\n",
            "loss: 0.005279902368783951 at epoch 76\n",
            "loss: 0.0031154246535152197 at epoch 76\n",
            "loss: 0.006710640154778957 at epoch 76\n",
            "loss: 0.0020413745660334826 at epoch 76\n",
            "loss: 0.005151407327502966 at epoch 77\n",
            "loss: 0.0029147444292902946 at epoch 77\n",
            "loss: 0.0024277083575725555 at epoch 77\n",
            "loss: 0.00361200631596148 at epoch 77\n",
            "loss: 0.0018019563285633922 at epoch 77\n",
            "loss: 0.0020752251148223877 at epoch 77\n",
            "loss: 0.001556080300360918 at epoch 77\n",
            "loss: 0.0032398495823144913 at epoch 77\n",
            "loss: 0.00514457281678915 at epoch 77\n",
            "loss: 0.0021844718139618635 at epoch 77\n",
            "loss: 0.002200526650995016 at epoch 77\n",
            "loss: 0.004281521774828434 at epoch 77\n",
            "loss: 0.0034440390300005674 at epoch 77\n",
            "loss: 0.002920642262324691 at epoch 77\n",
            "loss: 0.0031135976314544678 at epoch 77\n",
            "loss: 0.0015246489783748984 at epoch 77\n",
            "loss: 0.0036344726104289293 at epoch 77\n",
            "loss: 0.004374643787741661 at epoch 77\n",
            "loss: 0.002290448173880577 at epoch 77\n",
            "loss: 0.002967270789667964 at epoch 77\n",
            "loss: 0.0020618969574570656 at epoch 77\n",
            "loss: 0.0029169803019613028 at epoch 77\n",
            "loss: 0.0014612741069868207 at epoch 77\n",
            "loss: 0.0034355144016444683 at epoch 77\n",
            "loss: 0.008092896081507206 at epoch 77\n",
            "loss: 0.006307784467935562 at epoch 77\n",
            "loss: 0.002032731892541051 at epoch 77\n",
            "loss: 0.0037709863390773535 at epoch 77\n",
            "loss: 0.00663591455668211 at epoch 77\n",
            "loss: 0.006891414988785982 at epoch 77\n",
            "loss: 0.014393232762813568 at epoch 77\n",
            "loss: 0.013467603363096714 at epoch 77\n",
            "loss: 0.011312615126371384 at epoch 77\n",
            "loss: 0.010694565251469612 at epoch 77\n",
            "loss: 0.00608096132054925 at epoch 77\n",
            "loss: 0.005715749226510525 at epoch 77\n",
            "loss: 0.009972854517400265 at epoch 77\n",
            "loss: 0.005410387646406889 at epoch 77\n",
            "loss: 0.01646481640636921 at epoch 77\n",
            "loss: 0.004992679227143526 at epoch 77\n",
            "loss: 0.011427449993789196 at epoch 77\n",
            "loss: 0.00434545474126935 at epoch 77\n",
            "loss: 0.007215163204818964 at epoch 77\n",
            "loss: 0.011243345215916634 at epoch 77\n",
            "loss: 0.006641411688178778 at epoch 77\n",
            "loss: 0.007363616023212671 at epoch 77\n",
            "loss: 0.007623228244483471 at epoch 77\n",
            "loss: 0.008660156279802322 at epoch 77\n",
            "loss: 0.012073284946382046 at epoch 77\n",
            "loss: 0.0011571889044716954 at epoch 77\n",
            "loss: 0.006921153515577316 at epoch 77\n",
            "loss: 0.0033491235226392746 at epoch 77\n",
            "loss: 0.004026252776384354 at epoch 77\n",
            "loss: 0.00540108373388648 at epoch 77\n",
            "loss: 0.0019064692314714193 at epoch 77\n",
            "loss: 0.00554652838036418 at epoch 77\n",
            "loss: 0.002062295563519001 at epoch 77\n",
            "loss: 0.00683531304821372 at epoch 77\n",
            "loss: 0.009896357543766499 at epoch 77\n",
            "loss: 0.0060647595673799515 at epoch 77\n",
            "loss: 0.004812044091522694 at epoch 77\n",
            "loss: 0.004473801702260971 at epoch 77\n",
            "loss: 0.016138890758156776 at epoch 77\n",
            "loss: 0.01296975091099739 at epoch 77\n",
            "loss: 0.005479704588651657 at epoch 77\n",
            "loss: 0.0032298588193953037 at epoch 77\n",
            "loss: 0.010310905054211617 at epoch 77\n",
            "loss: 0.013023752719163895 at epoch 77\n",
            "loss: 0.0024726209230720997 at epoch 77\n",
            "loss: 0.0061084069311618805 at epoch 77\n",
            "loss: 0.018346693366765976 at epoch 77\n",
            "loss: 0.02011094056069851 at epoch 77\n",
            "loss: 0.00897407066076994 at epoch 77\n",
            "loss: 0.005989184137433767 at epoch 77\n",
            "loss: 0.019346371293067932 at epoch 77\n",
            "loss: 0.015259942971169949 at epoch 77\n",
            "loss: 0.00496899476274848 at epoch 77\n",
            "loss: 0.01884782873094082 at epoch 77\n",
            "loss: 0.02926085703074932 at epoch 77\n",
            "loss: 0.04055909812450409 at epoch 78\n",
            "loss: 0.011601235717535019 at epoch 78\n",
            "loss: 0.04057122766971588 at epoch 78\n",
            "loss: 0.07553201168775558 at epoch 78\n",
            "loss: 0.0825120136141777 at epoch 78\n",
            "loss: 0.014819841831922531 at epoch 78\n",
            "loss: 0.04071522876620293 at epoch 78\n",
            "loss: 0.06736662983894348 at epoch 78\n",
            "loss: 0.07863423228263855 at epoch 78\n",
            "loss: 0.08234567195177078 at epoch 78\n",
            "loss: 0.017844295129179955 at epoch 78\n",
            "loss: 0.05718763917684555 at epoch 78\n",
            "loss: 0.036104705184698105 at epoch 78\n",
            "loss: 0.07884113490581512 at epoch 78\n",
            "loss: 0.026543060317635536 at epoch 78\n",
            "loss: 0.046360716223716736 at epoch 78\n",
            "loss: 0.0202611293643713 at epoch 78\n",
            "loss: 0.06675496697425842 at epoch 78\n",
            "loss: 0.015710562467575073 at epoch 78\n",
            "loss: 0.029680244624614716 at epoch 78\n",
            "loss: 0.013126783072948456 at epoch 78\n",
            "loss: 0.037605803459882736 at epoch 78\n",
            "loss: 0.015049931593239307 at epoch 78\n",
            "loss: 0.026477230712771416 at epoch 78\n",
            "loss: 0.0033116431441158056 at epoch 78\n",
            "loss: 0.01920786127448082 at epoch 78\n",
            "loss: 0.011837637051939964 at epoch 78\n",
            "loss: 0.024449685588479042 at epoch 78\n",
            "loss: 0.008511311374604702 at epoch 78\n",
            "loss: 0.005864671431481838 at epoch 78\n",
            "loss: 0.00843681488186121 at epoch 78\n",
            "loss: 0.012281199917197227 at epoch 78\n",
            "loss: 0.00788818672299385 at epoch 78\n",
            "loss: 0.013949226588010788 at epoch 78\n",
            "loss: 0.0031645805574953556 at epoch 78\n",
            "loss: 0.015178442001342773 at epoch 78\n",
            "loss: 0.0015419740229845047 at epoch 78\n",
            "loss: 0.012825851328670979 at epoch 78\n",
            "loss: 0.007619765587151051 at epoch 78\n",
            "loss: 0.01711282506585121 at epoch 78\n",
            "loss: 0.009514430537819862 at epoch 78\n",
            "loss: 0.01076907105743885 at epoch 78\n",
            "loss: 0.007076943293213844 at epoch 78\n",
            "loss: 0.006957729812711477 at epoch 78\n",
            "loss: 0.0044402917847037315 at epoch 78\n",
            "loss: 0.00781793799251318 at epoch 78\n",
            "loss: 0.0014087104937061667 at epoch 78\n",
            "loss: 0.010125478729605675 at epoch 78\n",
            "loss: 0.004461885429918766 at epoch 78\n",
            "loss: 0.01804901473224163 at epoch 78\n",
            "loss: 0.008733857423067093 at epoch 78\n",
            "loss: 0.016003897413611412 at epoch 78\n",
            "loss: 0.00643028412014246 at epoch 78\n",
            "loss: 0.006193768698722124 at epoch 78\n",
            "loss: 0.004047825466841459 at epoch 78\n",
            "loss: 0.008513065055012703 at epoch 78\n",
            "loss: 0.010729060508310795 at epoch 78\n",
            "loss: 0.016832850873470306 at epoch 78\n",
            "loss: 0.00791425071656704 at epoch 78\n",
            "loss: 0.007285524159669876 at epoch 78\n",
            "loss: 0.01002463698387146 at epoch 78\n",
            "loss: 0.021420542150735855 at epoch 78\n",
            "loss: 0.019166382029652596 at epoch 78\n",
            "loss: 0.01623799279332161 at epoch 78\n",
            "loss: 0.00115109421312809 at epoch 78\n",
            "loss: 0.01262837927788496 at epoch 78\n",
            "loss: 0.01609642244875431 at epoch 78\n",
            "loss: 0.013221517205238342 at epoch 78\n",
            "loss: 0.006286436691880226 at epoch 78\n",
            "loss: 0.005290446802973747 at epoch 78\n",
            "loss: 0.015970559790730476 at epoch 78\n",
            "loss: 0.024918541312217712 at epoch 78\n",
            "loss: 0.011319714598357677 at epoch 78\n",
            "loss: 0.007276768796145916 at epoch 78\n",
            "loss: 0.012515230104327202 at epoch 78\n",
            "loss: 0.03370606526732445 at epoch 78\n",
            "loss: 0.027987172827124596 at epoch 78\n",
            "loss: 0.010138100944459438 at epoch 78\n",
            "loss: 0.004349016584455967 at epoch 78\n",
            "loss: 0.029321685433387756 at epoch 79\n",
            "loss: 0.04574970528483391 at epoch 79\n",
            "loss: 0.04570040851831436 at epoch 79\n",
            "loss: 0.014663062058389187 at epoch 79\n",
            "loss: 0.00574115663766861 at epoch 79\n",
            "loss: 0.0225897915661335 at epoch 79\n",
            "loss: 0.04893559217453003 at epoch 79\n",
            "loss: 0.04328635334968567 at epoch 79\n",
            "loss: 0.009283173829317093 at epoch 79\n",
            "loss: 0.02315153181552887 at epoch 79\n",
            "loss: 0.045865368098020554 at epoch 79\n",
            "loss: 0.031207621097564697 at epoch 79\n",
            "loss: 0.022526511922478676 at epoch 79\n",
            "loss: 0.012964918278157711 at epoch 79\n",
            "loss: 0.031100314110517502 at epoch 79\n",
            "loss: 0.006118459161370993 at epoch 79\n",
            "loss: 0.020646365359425545 at epoch 79\n",
            "loss: 0.009405814111232758 at epoch 79\n",
            "loss: 0.02520991489291191 at epoch 79\n",
            "loss: 0.01326159480959177 at epoch 79\n",
            "loss: 0.01361096277832985 at epoch 79\n",
            "loss: 0.018311310559511185 at epoch 79\n",
            "loss: 0.006432069465517998 at epoch 79\n",
            "loss: 0.014025223441421986 at epoch 79\n",
            "loss: 0.018889501690864563 at epoch 79\n",
            "loss: 0.008658204227685928 at epoch 79\n",
            "loss: 0.011381574906408787 at epoch 79\n",
            "loss: 0.018028415739536285 at epoch 79\n",
            "loss: 0.0269485916942358 at epoch 79\n",
            "loss: 0.023741940036416054 at epoch 79\n",
            "loss: 0.012661213055253029 at epoch 79\n",
            "loss: 0.013215607032179832 at epoch 79\n",
            "loss: 0.004895029589533806 at epoch 79\n",
            "loss: 0.024929605424404144 at epoch 79\n",
            "loss: 0.02085336297750473 at epoch 79\n",
            "loss: 0.029719149693846703 at epoch 79\n",
            "loss: 0.016918059438467026 at epoch 79\n",
            "loss: 0.011487513780593872 at epoch 79\n",
            "loss: 0.03636998310685158 at epoch 79\n",
            "loss: 0.03535838797688484 at epoch 79\n",
            "loss: 0.017002210021018982 at epoch 79\n",
            "loss: 0.019434351474046707 at epoch 79\n",
            "loss: 0.011386717669665813 at epoch 79\n",
            "loss: 0.03243815898895264 at epoch 79\n",
            "loss: 0.03215761482715607 at epoch 79\n",
            "loss: 0.039309293031692505 at epoch 79\n",
            "loss: 0.02702941931784153 at epoch 79\n",
            "loss: 0.013434164226055145 at epoch 79\n",
            "loss: 0.003578515723347664 at epoch 79\n",
            "loss: 0.01677943766117096 at epoch 79\n",
            "loss: 0.029478728771209717 at epoch 79\n",
            "loss: 0.03644603490829468 at epoch 79\n",
            "loss: 0.017216157168149948 at epoch 79\n",
            "loss: 0.02108188532292843 at epoch 79\n",
            "loss: 0.01088116504251957 at epoch 79\n",
            "loss: 0.010309617966413498 at epoch 79\n",
            "loss: 0.00815156102180481 at epoch 79\n",
            "loss: 0.012480998411774635 at epoch 79\n",
            "loss: 0.005965462885797024 at epoch 79\n",
            "loss: 0.014983758330345154 at epoch 79\n",
            "loss: 0.009589090943336487 at epoch 79\n",
            "loss: 0.014816585928201675 at epoch 79\n",
            "loss: 0.005166073329746723 at epoch 79\n",
            "loss: 0.011680228635668755 at epoch 79\n",
            "loss: 0.006355110090225935 at epoch 79\n",
            "loss: 0.007763467263430357 at epoch 79\n",
            "loss: 0.004734341520816088 at epoch 79\n",
            "loss: 0.008052653633058071 at epoch 79\n",
            "loss: 0.00552355358377099 at epoch 79\n",
            "loss: 0.008922834880650043 at epoch 79\n",
            "loss: 0.005836946424096823 at epoch 79\n",
            "loss: 0.006925391033291817 at epoch 79\n",
            "loss: 0.0016093410085886717 at epoch 79\n",
            "loss: 0.00453410018235445 at epoch 79\n",
            "loss: 0.005499180871993303 at epoch 79\n",
            "loss: 0.007294383365660906 at epoch 79\n",
            "loss: 0.0059568192809820175 at epoch 79\n",
            "loss: 0.0027249662671238184 at epoch 79\n",
            "loss: 0.006183004006743431 at epoch 79\n",
            "loss: 0.019170507788658142 at epoch 80\n",
            "loss: 0.03434019163250923 at epoch 80\n",
            "loss: 0.03459982946515083 at epoch 80\n",
            "loss: 0.03036671131849289 at epoch 80\n",
            "loss: 0.006007553543895483 at epoch 80\n",
            "loss: 0.01014296431094408 at epoch 80\n",
            "loss: 0.007114368490874767 at epoch 80\n",
            "loss: 0.01511540450155735 at epoch 80\n",
            "loss: 0.009470853954553604 at epoch 80\n",
            "loss: 0.006438551004976034 at epoch 80\n",
            "loss: 0.00237151887267828 at epoch 80\n",
            "loss: 0.0011395529145374894 at epoch 80\n",
            "loss: 0.0034204849507659674 at epoch 80\n",
            "loss: 0.007753062527626753 at epoch 80\n",
            "loss: 0.007176099810749292 at epoch 80\n",
            "loss: 0.007363557815551758 at epoch 80\n",
            "loss: 0.0009103399934247136 at epoch 80\n",
            "loss: 0.009032966569066048 at epoch 80\n",
            "loss: 0.021585123613476753 at epoch 80\n",
            "loss: 0.050365202128887177 at epoch 80\n",
            "loss: 0.05402810126543045 at epoch 80\n",
            "loss: 0.11509724706411362 at epoch 80\n",
            "loss: 0.10723836719989777 at epoch 80\n",
            "loss: 0.12920834124088287 at epoch 80\n",
            "loss: 0.09722337126731873 at epoch 80\n",
            "loss: 0.0787094458937645 at epoch 80\n",
            "loss: 0.0355788916349411 at epoch 80\n",
            "loss: 0.05144688859581947 at epoch 80\n",
            "loss: 0.15229685604572296 at epoch 80\n",
            "loss: 0.16251300275325775 at epoch 80\n",
            "loss: 0.18867045640945435 at epoch 80\n",
            "loss: 0.19500000774860382 at epoch 80\n",
            "loss: 0.17707626521587372 at epoch 80\n",
            "loss: 0.15979428589344025 at epoch 80\n",
            "loss: 0.07104136049747467 at epoch 80\n",
            "loss: 0.052369147539138794 at epoch 80\n",
            "loss: 0.03828291594982147 at epoch 80\n",
            "loss: 0.10922881215810776 at epoch 80\n",
            "loss: 0.15765933692455292 at epoch 80\n",
            "loss: 0.13769744336605072 at epoch 80\n",
            "loss: 0.03361526131629944 at epoch 80\n",
            "loss: 0.09525995701551437 at epoch 80\n",
            "loss: 0.08737672865390778 at epoch 80\n",
            "loss: 0.14167900383472443 at epoch 80\n",
            "loss: 0.15445780754089355 at epoch 80\n",
            "loss: 0.12291965633630753 at epoch 80\n",
            "loss: 0.06079952418804169 at epoch 80\n",
            "loss: 0.04136880859732628 at epoch 80\n",
            "loss: 0.06103581190109253 at epoch 80\n",
            "loss: 0.058203358203172684 at epoch 80\n",
            "loss: 0.12021185457706451 at epoch 80\n",
            "loss: 0.11734715104103088 at epoch 80\n",
            "loss: 0.073160320520401 at epoch 80\n",
            "loss: 0.03072577342391014 at epoch 80\n",
            "loss: 0.01891116239130497 at epoch 80\n",
            "loss: 0.034616317600011826 at epoch 80\n",
            "loss: 0.029051758348941803 at epoch 80\n",
            "loss: 0.036546383053064346 at epoch 80\n",
            "loss: 0.02209990844130516 at epoch 80\n",
            "loss: 0.012281607836484909 at epoch 80\n",
            "loss: 0.014604132622480392 at epoch 80\n",
            "loss: 0.015079181641340256 at epoch 80\n",
            "loss: 0.039476510137319565 at epoch 80\n",
            "loss: 0.03688802197575569 at epoch 80\n",
            "loss: 0.04034604877233505 at epoch 80\n",
            "loss: 0.017501894384622574 at epoch 80\n",
            "loss: 0.022783547639846802 at epoch 80\n",
            "loss: 0.003342881565913558 at epoch 80\n",
            "loss: 0.025584790855646133 at epoch 80\n",
            "loss: 0.00874657928943634 at epoch 80\n",
            "loss: 0.04577093571424484 at epoch 80\n",
            "loss: 0.01119987852871418 at epoch 80\n",
            "loss: 0.02466174215078354 at epoch 80\n",
            "loss: 0.0079758670181036 at epoch 80\n",
            "loss: 0.023581042885780334 at epoch 80\n",
            "loss: 0.018823396414518356 at epoch 80\n",
            "loss: 0.021018007770180702 at epoch 80\n",
            "loss: 0.012337464839220047 at epoch 80\n",
            "loss: 0.009122156538069248 at epoch 80\n",
            "loss: 0.034978386014699936 at epoch 81\n",
            "loss: 0.0743272453546524 at epoch 81\n",
            "loss: 0.11584821343421936 at epoch 81\n",
            "loss: 0.13343264162540436 at epoch 81\n",
            "loss: 0.11422830820083618 at epoch 81\n",
            "loss: 0.05276847258210182 at epoch 81\n",
            "loss: 0.05040818452835083 at epoch 81\n",
            "loss: 0.028110014274716377 at epoch 81\n",
            "loss: 0.06420458108186722 at epoch 81\n",
            "loss: 0.1883855164051056 at epoch 81\n",
            "loss: 0.3722640573978424 at epoch 81\n",
            "loss: 0.6528806686401367 at epoch 81\n",
            "loss: 0.8738038539886475 at epoch 81\n",
            "loss: 0.7830730080604553 at epoch 81\n",
            "loss: 0.38084593415260315 at epoch 81\n",
            "loss: 0.10565550625324249 at epoch 81\n",
            "loss: 0.28207534551620483 at epoch 81\n",
            "loss: 0.6334766149520874 at epoch 81\n",
            "loss: 0.8434656262397766 at epoch 81\n",
            "loss: 0.697709321975708 at epoch 81\n",
            "loss: 0.3902161419391632 at epoch 81\n",
            "loss: 0.1792600154876709 at epoch 81\n",
            "loss: 1.1139981746673584 at epoch 81\n",
            "loss: 1.4626495838165283 at epoch 81\n",
            "loss: 1.8631033897399902 at epoch 81\n",
            "loss: 1.2624661922454834 at epoch 81\n",
            "loss: 1.063427448272705 at epoch 81\n",
            "loss: 0.12180599570274353 at epoch 81\n",
            "loss: 0.5814551115036011 at epoch 81\n",
            "loss: 1.3854020833969116 at epoch 81\n",
            "loss: 1.9215399026870728 at epoch 81\n",
            "loss: 1.7656009197235107 at epoch 81\n",
            "loss: 1.1091787815093994 at epoch 81\n",
            "loss: 4.09316873550415 at epoch 81\n",
            "loss: 3.1419620513916016 at epoch 81\n",
            "loss: 0.960294783115387 at epoch 81\n",
            "loss: 1.3877403736114502 at epoch 81\n",
            "loss: 2.392179489135742 at epoch 81\n",
            "loss: 2.8855082988739014 at epoch 81\n",
            "loss: 1.2755396366119385 at epoch 81\n",
            "loss: 0.3451708257198334 at epoch 81\n",
            "loss: 1.1326850652694702 at epoch 81\n",
            "loss: 1.560257077217102 at epoch 81\n",
            "loss: 0.8129894733428955 at epoch 81\n",
            "loss: 0.8218146562576294 at epoch 81\n",
            "loss: 2.3728139400482178 at epoch 81\n",
            "loss: 2.3032174110412598 at epoch 81\n",
            "loss: 0.6717970967292786 at epoch 81\n",
            "loss: 0.6718516945838928 at epoch 81\n",
            "loss: 1.2178443670272827 at epoch 81\n",
            "loss: 0.41845619678497314 at epoch 81\n",
            "loss: 0.4553508758544922 at epoch 81\n",
            "loss: 1.3970216512680054 at epoch 81\n",
            "loss: 0.9385412931442261 at epoch 81\n",
            "loss: 0.2796165347099304 at epoch 81\n",
            "loss: 1.3654128313064575 at epoch 81\n",
            "loss: 1.4755613803863525 at epoch 81\n",
            "loss: 0.2747959494590759 at epoch 81\n",
            "loss: 0.3306784927845001 at epoch 81\n",
            "loss: 0.6473224759101868 at epoch 81\n",
            "loss: 0.2993139922618866 at epoch 81\n",
            "loss: 0.12323914468288422 at epoch 81\n",
            "loss: 0.23801766335964203 at epoch 81\n",
            "loss: 0.18472571671009064 at epoch 81\n",
            "loss: 0.0753561481833458 at epoch 81\n",
            "loss: 0.3119652271270752 at epoch 81\n",
            "loss: 0.31077542901039124 at epoch 81\n",
            "loss: 0.042113564908504486 at epoch 81\n",
            "loss: 0.19013403356075287 at epoch 81\n",
            "loss: 0.18074914813041687 at epoch 81\n",
            "loss: 0.08567231893539429 at epoch 81\n",
            "loss: 0.08271913975477219 at epoch 81\n",
            "loss: 0.1858486533164978 at epoch 81\n",
            "loss: 0.13917076587677002 at epoch 81\n",
            "loss: 0.12112350761890411 at epoch 81\n",
            "loss: 0.3477519750595093 at epoch 81\n",
            "loss: 0.2680858075618744 at epoch 81\n",
            "loss: 0.13008390367031097 at epoch 81\n",
            "loss: 0.27629756927490234 at epoch 81\n",
            "loss: 0.2362551987171173 at epoch 82\n",
            "loss: 0.09971201419830322 at epoch 82\n",
            "loss: 0.16775046288967133 at epoch 82\n",
            "loss: 0.37231186032295227 at epoch 82\n",
            "loss: 0.17076298594474792 at epoch 82\n",
            "loss: 0.1858292818069458 at epoch 82\n",
            "loss: 0.24935416877269745 at epoch 82\n",
            "loss: 0.3142504394054413 at epoch 82\n",
            "loss: 0.06320661306381226 at epoch 82\n",
            "loss: 0.2820122539997101 at epoch 82\n",
            "loss: 0.3232403099536896 at epoch 82\n",
            "loss: 0.07400023937225342 at epoch 82\n",
            "loss: 0.3052726984024048 at epoch 82\n",
            "loss: 0.5088869333267212 at epoch 82\n",
            "loss: 0.10854656249284744 at epoch 82\n",
            "loss: 0.19320878386497498 at epoch 82\n",
            "loss: 0.423953115940094 at epoch 82\n",
            "loss: 0.16265642642974854 at epoch 82\n",
            "loss: 0.06742018461227417 at epoch 82\n",
            "loss: 0.1158108115196228 at epoch 82\n",
            "loss: 0.12083502113819122 at epoch 82\n",
            "loss: 0.03393624722957611 at epoch 82\n",
            "loss: 0.07410375028848648 at epoch 82\n",
            "loss: 0.061098188161849976 at epoch 82\n",
            "loss: 0.03811662644147873 at epoch 82\n",
            "loss: 0.04014606773853302 at epoch 82\n",
            "loss: 0.08641764521598816 at epoch 82\n",
            "loss: 0.08751460164785385 at epoch 82\n",
            "loss: 0.0934864804148674 at epoch 82\n",
            "loss: 0.2466859221458435 at epoch 82\n",
            "loss: 0.12734317779541016 at epoch 82\n",
            "loss: 0.06520786136388779 at epoch 82\n",
            "loss: 0.2694506347179413 at epoch 82\n",
            "loss: 0.1370019018650055 at epoch 82\n",
            "loss: 0.09830689430236816 at epoch 82\n",
            "loss: 0.23056036233901978 at epoch 82\n",
            "loss: 0.33489349484443665 at epoch 82\n",
            "loss: 0.07955801486968994 at epoch 82\n",
            "loss: 0.09390106052160263 at epoch 82\n",
            "loss: 0.3320857882499695 at epoch 82\n",
            "loss: 0.25699976086616516 at epoch 82\n",
            "loss: 0.025985516607761383 at epoch 82\n",
            "loss: 0.14795877039432526 at epoch 82\n",
            "loss: 0.20524626970291138 at epoch 82\n",
            "loss: 0.07674550265073776 at epoch 82\n",
            "loss: 0.043158963322639465 at epoch 82\n",
            "loss: 0.07401670515537262 at epoch 82\n",
            "loss: 0.03696396201848984 at epoch 82\n",
            "loss: 0.022363584488630295 at epoch 82\n",
            "loss: 0.07254430651664734 at epoch 82\n",
            "loss: 0.036633267998695374 at epoch 82\n",
            "loss: 0.02049814537167549 at epoch 82\n",
            "loss: 0.06400971859693527 at epoch 82\n",
            "loss: 0.06132354214787483 at epoch 82\n",
            "loss: 0.007133259437978268 at epoch 82\n",
            "loss: 0.04695785045623779 at epoch 82\n",
            "loss: 0.07065921276807785 at epoch 82\n",
            "loss: 0.036470867693424225 at epoch 82\n",
            "loss: 0.00607523787766695 at epoch 82\n",
            "loss: 0.039480969309806824 at epoch 82\n",
            "loss: 0.03846107795834541 at epoch 82\n",
            "loss: 0.011268237605690956 at epoch 82\n",
            "loss: 0.015998948365449905 at epoch 82\n",
            "loss: 0.026684768497943878 at epoch 82\n",
            "loss: 0.014548308216035366 at epoch 82\n",
            "loss: 0.01297835074365139 at epoch 82\n",
            "loss: 0.03625905513763428 at epoch 82\n",
            "loss: 0.024837328121066093 at epoch 82\n",
            "loss: 0.012757159769535065 at epoch 82\n",
            "loss: 0.02867533266544342 at epoch 82\n",
            "loss: 0.014484158717095852 at epoch 82\n",
            "loss: 0.012655874714255333 at epoch 82\n",
            "loss: 0.022992871701717377 at epoch 82\n",
            "loss: 0.01758650131523609 at epoch 82\n",
            "loss: 0.018258050084114075 at epoch 82\n",
            "loss: 0.011795394122600555 at epoch 82\n",
            "loss: 0.014789674431085587 at epoch 82\n",
            "loss: 0.015195271000266075 at epoch 82\n",
            "loss: 0.007938300259411335 at epoch 82\n",
            "loss: 0.030487822368741035 at epoch 83\n",
            "loss: 0.01257354486733675 at epoch 83\n",
            "loss: 0.02818889543414116 at epoch 83\n",
            "loss: 0.04867132753133774 at epoch 83\n",
            "loss: 0.08044443279504776 at epoch 83\n",
            "loss: 0.04370271787047386 at epoch 83\n",
            "loss: 0.020490841940045357 at epoch 83\n",
            "loss: 0.042666591703891754 at epoch 83\n",
            "loss: 0.03032265603542328 at epoch 83\n",
            "loss: 0.04506063833832741 at epoch 83\n",
            "loss: 0.013486217707395554 at epoch 83\n",
            "loss: 0.03257143869996071 at epoch 83\n",
            "loss: 0.014615753665566444 at epoch 83\n",
            "loss: 0.03298886865377426 at epoch 83\n",
            "loss: 0.012713322415947914 at epoch 83\n",
            "loss: 0.02694256603717804 at epoch 83\n",
            "loss: 0.009550658985972404 at epoch 83\n",
            "loss: 0.02558823674917221 at epoch 83\n",
            "loss: 0.010944769717752934 at epoch 83\n",
            "loss: 0.017127864062786102 at epoch 83\n",
            "loss: 0.007248577196151018 at epoch 83\n",
            "loss: 0.014465130865573883 at epoch 83\n",
            "loss: 0.01657797396183014 at epoch 83\n",
            "loss: 0.01554710790514946 at epoch 83\n",
            "loss: 0.007508896291255951 at epoch 83\n",
            "loss: 0.006169659085571766 at epoch 83\n",
            "loss: 0.0033210075926035643 at epoch 83\n",
            "loss: 0.013442715629935265 at epoch 83\n",
            "loss: 0.00732145132496953 at epoch 83\n",
            "loss: 0.014371668919920921 at epoch 83\n",
            "loss: 0.00999356061220169 at epoch 83\n",
            "loss: 0.03199998661875725 at epoch 83\n",
            "loss: 0.021225642412900925 at epoch 83\n",
            "loss: 0.007214101031422615 at epoch 83\n",
            "loss: 0.005011065397411585 at epoch 83\n",
            "loss: 0.014501136727631092 at epoch 83\n",
            "loss: 0.005304428748786449 at epoch 83\n",
            "loss: 0.009635929018259048 at epoch 83\n",
            "loss: 0.014648115262389183 at epoch 83\n",
            "loss: 0.017997251823544502 at epoch 83\n",
            "loss: 0.007109895348548889 at epoch 83\n",
            "loss: 0.005306040868163109 at epoch 83\n",
            "loss: 0.009937251918017864 at epoch 83\n",
            "loss: 0.010119476355612278 at epoch 83\n",
            "loss: 0.005799885839223862 at epoch 83\n",
            "loss: 0.021105099469423294 at epoch 83\n",
            "loss: 0.016560686752200127 at epoch 83\n",
            "loss: 0.003968421835452318 at epoch 83\n",
            "loss: 0.017501138150691986 at epoch 83\n",
            "loss: 0.017764827236533165 at epoch 83\n",
            "loss: 0.007705975323915482 at epoch 83\n",
            "loss: 0.007904481142759323 at epoch 83\n",
            "loss: 0.012541444040834904 at epoch 83\n",
            "loss: 0.010012970305979252 at epoch 83\n",
            "loss: 0.016834110021591187 at epoch 83\n",
            "loss: 0.016858873888850212 at epoch 83\n",
            "loss: 0.01605895906686783 at epoch 83\n",
            "loss: 0.007158308289945126 at epoch 83\n",
            "loss: 0.001527829677797854 at epoch 83\n",
            "loss: 0.008207201957702637 at epoch 83\n",
            "loss: 0.0008572656661272049 at epoch 83\n",
            "loss: 0.008031072095036507 at epoch 83\n",
            "loss: 0.0049829198978841305 at epoch 83\n",
            "loss: 0.005634667817503214 at epoch 83\n",
            "loss: 0.0022036945447325706 at epoch 83\n",
            "loss: 0.003972963895648718 at epoch 83\n",
            "loss: 0.006513587199151516 at epoch 83\n",
            "loss: 0.004358179867267609 at epoch 83\n",
            "loss: 0.0012673406163230538 at epoch 83\n",
            "loss: 0.006744483020156622 at epoch 83\n",
            "loss: 0.0039263321086764336 at epoch 83\n",
            "loss: 0.005654086358845234 at epoch 83\n",
            "loss: 0.0026863806415349245 at epoch 83\n",
            "loss: 0.0020350180566310883 at epoch 83\n",
            "loss: 0.006880313623696566 at epoch 83\n",
            "loss: 0.004489913582801819 at epoch 83\n",
            "loss: 0.006765871774405241 at epoch 83\n",
            "loss: 0.0024402588605880737 at epoch 83\n",
            "loss: 0.0010196591028943658 at epoch 83\n",
            "loss: 0.0035873455926775932 at epoch 84\n",
            "loss: 0.005731065757572651 at epoch 84\n",
            "loss: 0.004223539959639311 at epoch 84\n",
            "loss: 0.0028968406841158867 at epoch 84\n",
            "loss: 0.0025320975109934807 at epoch 84\n",
            "loss: 0.0028856017161160707 at epoch 84\n",
            "loss: 0.00291925435885787 at epoch 84\n",
            "loss: 0.0023341537453234196 at epoch 84\n",
            "loss: 0.002827964024618268 at epoch 84\n",
            "loss: 0.002714404370635748 at epoch 84\n",
            "loss: 0.004270485602319241 at epoch 84\n",
            "loss: 0.002961567137390375 at epoch 84\n",
            "loss: 0.0029617284890264273 at epoch 84\n",
            "loss: 0.0017978271935135126 at epoch 84\n",
            "loss: 0.003534977324306965 at epoch 84\n",
            "loss: 0.003320396179333329 at epoch 84\n",
            "loss: 0.0011853061150759459 at epoch 84\n",
            "loss: 0.0013642108533531427 at epoch 84\n",
            "loss: 0.004036365542560816 at epoch 84\n",
            "loss: 0.003316265996545553 at epoch 84\n",
            "loss: 0.0012467149645090103 at epoch 84\n",
            "loss: 0.0021504038013517857 at epoch 84\n",
            "loss: 0.006232795305550098 at epoch 84\n",
            "loss: 0.004857729654759169 at epoch 84\n",
            "loss: 0.0013954914174973965 at epoch 84\n",
            "loss: 0.0025012758560478687 at epoch 84\n",
            "loss: 0.005780561361461878 at epoch 84\n",
            "loss: 0.005156262777745724 at epoch 84\n",
            "loss: 0.003136404324322939 at epoch 84\n",
            "loss: 0.002605125308036804 at epoch 84\n",
            "loss: 0.004601349588483572 at epoch 84\n",
            "loss: 0.003653116524219513 at epoch 84\n",
            "loss: 0.001642211340367794 at epoch 84\n",
            "loss: 0.002950068563222885 at epoch 84\n",
            "loss: 0.003508930327370763 at epoch 84\n",
            "loss: 0.0036145648919045925 at epoch 84\n",
            "loss: 0.002779594389721751 at epoch 84\n",
            "loss: 0.0013224204303696752 at epoch 84\n",
            "loss: 0.0037282805424183607 at epoch 84\n",
            "loss: 0.003223945153877139 at epoch 84\n",
            "loss: 0.005372092127799988 at epoch 84\n",
            "loss: 0.00048431268078275025 at epoch 84\n",
            "loss: 0.001304485835134983 at epoch 84\n",
            "loss: 0.0017584057059139013 at epoch 84\n",
            "loss: 0.0018810557667165995 at epoch 84\n",
            "loss: 0.0012295476626604795 at epoch 84\n",
            "loss: 0.0020420679356902838 at epoch 84\n",
            "loss: 0.0008744666702114046 at epoch 84\n",
            "loss: 0.0018365106079727411 at epoch 84\n",
            "loss: 0.0011149602942168713 at epoch 84\n",
            "loss: 0.000915138574782759 at epoch 84\n",
            "loss: 0.0009717352222651243 at epoch 84\n",
            "loss: 0.005205398425459862 at epoch 84\n",
            "loss: 0.00047730363439768553 at epoch 84\n",
            "loss: 0.0011199201690033078 at epoch 84\n",
            "loss: 0.00028908150852657855 at epoch 84\n",
            "loss: 0.0012972189579159021 at epoch 84\n",
            "loss: 0.0064900778234004974 at epoch 84\n",
            "loss: 0.0008634364348836243 at epoch 84\n",
            "loss: 0.0005406641867011786 at epoch 84\n",
            "loss: 0.0009635588503442705 at epoch 84\n",
            "loss: 0.001914310036227107 at epoch 84\n",
            "loss: 0.0007591285975649953 at epoch 84\n",
            "loss: 0.00027947663329541683 at epoch 84\n",
            "loss: 0.0020833483431488276 at epoch 84\n",
            "loss: 0.00704845180734992 at epoch 84\n",
            "loss: 0.00260066706687212 at epoch 84\n",
            "loss: 0.0010453953873366117 at epoch 84\n",
            "loss: 0.0020413673482835293 at epoch 84\n",
            "loss: 0.0018428319599479437 at epoch 84\n",
            "loss: 0.0034704103600233793 at epoch 84\n",
            "loss: 0.0022945741657167673 at epoch 84\n",
            "loss: 0.004593868274241686 at epoch 84\n",
            "loss: 0.0013140469091013074 at epoch 84\n",
            "loss: 0.0014817997580394149 at epoch 84\n",
            "loss: 0.0012267440324649215 at epoch 84\n",
            "loss: 0.0011980197159573436 at epoch 84\n",
            "loss: 0.0017060781829059124 at epoch 84\n",
            "loss: 0.0015771123580634594 at epoch 84\n",
            "loss: 0.0028516752645373344 at epoch 85\n",
            "loss: 0.003384650219231844 at epoch 85\n",
            "loss: 0.0020795585587620735 at epoch 85\n",
            "loss: 0.001531662535853684 at epoch 85\n",
            "loss: 0.0031854016706347466 at epoch 85\n",
            "loss: 0.008681297302246094 at epoch 85\n",
            "loss: 0.004082127008587122 at epoch 85\n",
            "loss: 0.001114010694436729 at epoch 85\n",
            "loss: 0.002118599135428667 at epoch 85\n",
            "loss: 0.007331577129662037 at epoch 85\n",
            "loss: 0.007620665710419416 at epoch 85\n",
            "loss: 0.003584829159080982 at epoch 85\n",
            "loss: 0.00247932318598032 at epoch 85\n",
            "loss: 0.004260182846337557 at epoch 85\n",
            "loss: 0.007524564396589994 at epoch 85\n",
            "loss: 0.00440038600936532 at epoch 85\n",
            "loss: 0.005252237897366285 at epoch 85\n",
            "loss: 0.023421794176101685 at epoch 85\n",
            "loss: 0.022704169154167175 at epoch 85\n",
            "loss: 0.016199268400669098 at epoch 85\n",
            "loss: 0.00301746535114944 at epoch 85\n",
            "loss: 0.007942181080579758 at epoch 85\n",
            "loss: 0.010841000825166702 at epoch 85\n",
            "loss: 0.003732746932655573 at epoch 85\n",
            "loss: 0.005555625073611736 at epoch 85\n",
            "loss: 0.0019948380067944527 at epoch 85\n",
            "loss: 0.00429083127528429 at epoch 85\n",
            "loss: 0.004121029749512672 at epoch 85\n",
            "loss: 0.0029384917579591274 at epoch 85\n",
            "loss: 0.0051361569203436375 at epoch 85\n",
            "loss: 0.0018074259860441089 at epoch 85\n",
            "loss: 0.006590919103473425 at epoch 85\n",
            "loss: 0.013100573793053627 at epoch 85\n",
            "loss: 0.013874522410333157 at epoch 85\n",
            "loss: 0.014047599397599697 at epoch 85\n",
            "loss: 0.0022378438152372837 at epoch 85\n",
            "loss: 0.011442502029240131 at epoch 85\n",
            "loss: 0.008902739733457565 at epoch 85\n",
            "loss: 0.0138040566816926 at epoch 85\n",
            "loss: 0.0065677594393491745 at epoch 85\n",
            "loss: 0.00333092687651515 at epoch 85\n",
            "loss: 0.008150910027325153 at epoch 85\n",
            "loss: 0.01549172680824995 at epoch 85\n",
            "loss: 0.018848352134227753 at epoch 85\n",
            "loss: 0.010204590857028961 at epoch 85\n",
            "loss: 0.005425033625215292 at epoch 85\n",
            "loss: 0.01878967136144638 at epoch 85\n",
            "loss: 0.021129414439201355 at epoch 85\n",
            "loss: 0.027076702564954758 at epoch 85\n",
            "loss: 0.009640274569392204 at epoch 85\n",
            "loss: 0.018665729090571404 at epoch 85\n",
            "loss: 0.03870275616645813 at epoch 85\n",
            "loss: 0.05125049501657486 at epoch 85\n",
            "loss: 0.04210948571562767 at epoch 85\n",
            "loss: 0.018512539565563202 at epoch 85\n",
            "loss: 0.008652561344206333 at epoch 85\n",
            "loss: 0.0175284706056118 at epoch 85\n",
            "loss: 0.03157605603337288 at epoch 85\n",
            "loss: 0.012216354720294476 at epoch 85\n",
            "loss: 0.011912079527974129 at epoch 85\n",
            "loss: 0.015338710509240627 at epoch 85\n",
            "loss: 0.02535158023238182 at epoch 85\n",
            "loss: 0.03744785860180855 at epoch 85\n",
            "loss: 0.011585752479732037 at epoch 85\n",
            "loss: 0.018556976690888405 at epoch 85\n",
            "loss: 0.014170361682772636 at epoch 85\n",
            "loss: 0.018168054521083832 at epoch 85\n",
            "loss: 0.012119598686695099 at epoch 85\n",
            "loss: 0.019288577139377594 at epoch 85\n",
            "loss: 0.025705918669700623 at epoch 85\n",
            "loss: 0.03202410414814949 at epoch 85\n",
            "loss: 0.024009766057133675 at epoch 85\n",
            "loss: 0.02034751884639263 at epoch 85\n",
            "loss: 0.010005591437220573 at epoch 85\n",
            "loss: 0.017784567549824715 at epoch 85\n",
            "loss: 0.033486928790807724 at epoch 85\n",
            "loss: 0.021433284506201744 at epoch 85\n",
            "loss: 0.02429257333278656 at epoch 85\n",
            "loss: 0.003650458063930273 at epoch 85\n",
            "loss: 0.023938558995723724 at epoch 86\n",
            "loss: 0.013866459019482136 at epoch 86\n",
            "loss: 0.03230682760477066 at epoch 86\n",
            "loss: 0.006986395455896854 at epoch 86\n",
            "loss: 0.014416266232728958 at epoch 86\n",
            "loss: 0.006777871400117874 at epoch 86\n",
            "loss: 0.011542011052370071 at epoch 86\n",
            "loss: 0.007757008541375399 at epoch 86\n",
            "loss: 0.012319065630435944 at epoch 86\n",
            "loss: 0.003855247050523758 at epoch 86\n",
            "loss: 0.011076457798480988 at epoch 86\n",
            "loss: 0.0033192294649779797 at epoch 86\n",
            "loss: 0.014620198868215084 at epoch 86\n",
            "loss: 0.012576963752508163 at epoch 86\n",
            "loss: 0.018708564341068268 at epoch 86\n",
            "loss: 0.015551066026091576 at epoch 86\n",
            "loss: 0.012820033356547356 at epoch 86\n",
            "loss: 0.004341386258602142 at epoch 86\n",
            "loss: 0.01390107348561287 at epoch 86\n",
            "loss: 0.01797570288181305 at epoch 86\n",
            "loss: 0.022538568824529648 at epoch 86\n",
            "loss: 0.008221154101192951 at epoch 86\n",
            "loss: 0.029979001730680466 at epoch 86\n",
            "loss: 0.05000685155391693 at epoch 86\n",
            "loss: 0.06800396740436554 at epoch 86\n",
            "loss: 0.015058993361890316 at epoch 86\n",
            "loss: 0.02168271318078041 at epoch 86\n",
            "loss: 0.020466046407818794 at epoch 86\n",
            "loss: 0.0675315111875534 at epoch 86\n",
            "loss: 0.0328359454870224 at epoch 86\n",
            "loss: 0.03394268453121185 at epoch 86\n",
            "loss: 0.004728619009256363 at epoch 86\n",
            "loss: 0.02376554161310196 at epoch 86\n",
            "loss: 0.007027863524854183 at epoch 86\n",
            "loss: 0.02411489374935627 at epoch 86\n",
            "loss: 0.017326897010207176 at epoch 86\n",
            "loss: 0.027871713042259216 at epoch 86\n",
            "loss: 0.024596119299530983 at epoch 86\n",
            "loss: 0.017764277756214142 at epoch 86\n",
            "loss: 0.005705039948225021 at epoch 86\n",
            "loss: 0.018182706087827682 at epoch 86\n",
            "loss: 0.024063315242528915 at epoch 86\n",
            "loss: 0.023896774277091026 at epoch 86\n",
            "loss: 0.024648364633321762 at epoch 86\n",
            "loss: 0.021230144426226616 at epoch 86\n",
            "loss: 0.013502930290997028 at epoch 86\n",
            "loss: 0.007798176258802414 at epoch 86\n",
            "loss: 0.008558287285268307 at epoch 86\n",
            "loss: 0.003560858080163598 at epoch 86\n",
            "loss: 0.0022609392181038857 at epoch 86\n",
            "loss: 0.003626427846029401 at epoch 86\n",
            "loss: 0.005068943370133638 at epoch 86\n",
            "loss: 0.004863213747739792 at epoch 86\n",
            "loss: 0.006247847341001034 at epoch 86\n",
            "loss: 0.004544299561530352 at epoch 86\n",
            "loss: 0.0028095764573663473 at epoch 86\n",
            "loss: 0.001950390636920929 at epoch 86\n",
            "loss: 0.0016654396895319223 at epoch 86\n",
            "loss: 0.0037658684886991978 at epoch 86\n",
            "loss: 0.0024872119538486004 at epoch 86\n",
            "loss: 0.0012035575928166509 at epoch 86\n",
            "loss: 0.0024466412141919136 at epoch 86\n",
            "loss: 0.005094893276691437 at epoch 86\n",
            "loss: 0.009152359329164028 at epoch 86\n",
            "loss: 0.006890251766890287 at epoch 86\n",
            "loss: 0.0039248839020729065 at epoch 86\n",
            "loss: 0.00255841133184731 at epoch 86\n",
            "loss: 0.0012904718751087785 at epoch 86\n",
            "loss: 0.0019147286657243967 at epoch 86\n",
            "loss: 0.003567825071513653 at epoch 86\n",
            "loss: 0.007401630282402039 at epoch 86\n",
            "loss: 0.008029144257307053 at epoch 86\n",
            "loss: 0.008809065446257591 at epoch 86\n",
            "loss: 0.004340101964771748 at epoch 86\n",
            "loss: 0.0034814057871699333 at epoch 86\n",
            "loss: 0.002309758448973298 at epoch 86\n",
            "loss: 0.003491038689389825 at epoch 86\n",
            "loss: 0.004407570231705904 at epoch 86\n",
            "loss: 0.003303411416709423 at epoch 86\n",
            "loss: 0.0036784568801522255 at epoch 87\n",
            "loss: 0.007293635979294777 at epoch 87\n",
            "loss: 0.0031930578406900167 at epoch 87\n",
            "loss: 0.002413718029856682 at epoch 87\n",
            "loss: 0.0014341480564326048 at epoch 87\n",
            "loss: 0.0028641715180128813 at epoch 87\n",
            "loss: 0.002959989011287689 at epoch 87\n",
            "loss: 0.00974790658801794 at epoch 87\n",
            "loss: 0.004467805847525597 at epoch 87\n",
            "loss: 0.0033513957168906927 at epoch 87\n",
            "loss: 0.002801094204187393 at epoch 87\n",
            "loss: 0.0022095986641943455 at epoch 87\n",
            "loss: 0.0018109657103195786 at epoch 87\n",
            "loss: 0.002043741522356868 at epoch 87\n",
            "loss: 0.0028748218901455402 at epoch 87\n",
            "loss: 0.002701550256460905 at epoch 87\n",
            "loss: 0.0025467362720519304 at epoch 87\n",
            "loss: 0.005115855019539595 at epoch 87\n",
            "loss: 0.0032846464309841394 at epoch 87\n",
            "loss: 0.008833557367324829 at epoch 87\n",
            "loss: 0.010234041139483452 at epoch 87\n",
            "loss: 0.015270727686583996 at epoch 87\n",
            "loss: 0.0075982180424034595 at epoch 87\n",
            "loss: 0.009431087411940098 at epoch 87\n",
            "loss: 0.00421659741550684 at epoch 87\n",
            "loss: 0.006464323960244656 at epoch 87\n",
            "loss: 0.005941478535532951 at epoch 87\n",
            "loss: 0.01109700370579958 at epoch 87\n",
            "loss: 0.008886820636689663 at epoch 87\n",
            "loss: 0.006874432787299156 at epoch 87\n",
            "loss: 0.0050041042268276215 at epoch 87\n",
            "loss: 0.005407076794654131 at epoch 87\n",
            "loss: 0.010237853974103928 at epoch 87\n",
            "loss: 0.019153151661157608 at epoch 87\n",
            "loss: 0.022162802517414093 at epoch 87\n",
            "loss: 0.014975317753851414 at epoch 87\n",
            "loss: 0.011936415918171406 at epoch 87\n",
            "loss: 0.004905129782855511 at epoch 87\n",
            "loss: 0.004538719542324543 at epoch 87\n",
            "loss: 0.005921442527323961 at epoch 87\n",
            "loss: 0.003681906033307314 at epoch 87\n",
            "loss: 0.009050356224179268 at epoch 87\n",
            "loss: 0.0029001529328525066 at epoch 87\n",
            "loss: 0.006519208662211895 at epoch 87\n",
            "loss: 0.006045964080840349 at epoch 87\n",
            "loss: 0.007140930742025375 at epoch 87\n",
            "loss: 0.014007669873535633 at epoch 87\n",
            "loss: 0.013689791783690453 at epoch 87\n",
            "loss: 0.018212981522083282 at epoch 87\n",
            "loss: 0.011704427190124989 at epoch 87\n",
            "loss: 0.009471087716519833 at epoch 87\n",
            "loss: 0.0033100079745054245 at epoch 87\n",
            "loss: 0.009319386444985867 at epoch 87\n",
            "loss: 0.011346360668540001 at epoch 87\n",
            "loss: 0.014584237709641457 at epoch 87\n",
            "loss: 0.006452989764511585 at epoch 87\n",
            "loss: 0.007204405497759581 at epoch 87\n",
            "loss: 0.010308956727385521 at epoch 87\n",
            "loss: 0.03432897850871086 at epoch 87\n",
            "loss: 0.05127228796482086 at epoch 87\n",
            "loss: 0.060937926173210144 at epoch 87\n",
            "loss: 0.034166306257247925 at epoch 87\n",
            "loss: 0.010531643405556679 at epoch 87\n",
            "loss: 0.04140762239694595 at epoch 87\n",
            "loss: 0.11511064320802689 at epoch 87\n",
            "loss: 0.21599696576595306 at epoch 87\n",
            "loss: 0.26165902614593506 at epoch 87\n",
            "loss: 0.3031803369522095 at epoch 87\n",
            "loss: 0.364670068025589 at epoch 87\n",
            "loss: 0.2644417881965637 at epoch 87\n",
            "loss: 0.31722986698150635 at epoch 87\n",
            "loss: 0.3606562912464142 at epoch 87\n",
            "loss: 0.3482203483581543 at epoch 87\n",
            "loss: 0.2535041272640228 at epoch 87\n",
            "loss: 0.09084830433130264 at epoch 87\n",
            "loss: 0.025071527808904648 at epoch 87\n",
            "loss: 0.04135692119598389 at epoch 87\n",
            "loss: 0.09235626459121704 at epoch 87\n",
            "loss: 0.19533580541610718 at epoch 87\n",
            "loss: 0.33539265394210815 at epoch 88\n",
            "loss: 0.4302535653114319 at epoch 88\n",
            "loss: 0.4510084390640259 at epoch 88\n",
            "loss: 0.3896937966346741 at epoch 88\n",
            "loss: 0.6127185821533203 at epoch 88\n",
            "loss: 0.3948093056678772 at epoch 88\n",
            "loss: 0.5996930003166199 at epoch 88\n",
            "loss: 0.13178694248199463 at epoch 88\n",
            "loss: 0.45423078536987305 at epoch 88\n",
            "loss: 0.26105356216430664 at epoch 88\n",
            "loss: 0.3626633286476135 at epoch 88\n",
            "loss: 0.4471105635166168 at epoch 88\n",
            "loss: 0.2979607582092285 at epoch 88\n",
            "loss: 0.5556601285934448 at epoch 88\n",
            "loss: 0.7571307420730591 at epoch 88\n",
            "loss: 0.39721745252609253 at epoch 88\n",
            "loss: 0.5087876319885254 at epoch 88\n",
            "loss: 0.18845586478710175 at epoch 88\n",
            "loss: 0.6804678440093994 at epoch 88\n",
            "loss: 0.9729361534118652 at epoch 88\n",
            "loss: 0.9438135623931885 at epoch 88\n",
            "loss: 0.7996214032173157 at epoch 88\n",
            "loss: 0.40526992082595825 at epoch 88\n",
            "loss: 0.4598492383956909 at epoch 88\n",
            "loss: 0.22117207944393158 at epoch 88\n",
            "loss: 0.21024274826049805 at epoch 88\n",
            "loss: 0.29486554861068726 at epoch 88\n",
            "loss: 0.525002658367157 at epoch 88\n",
            "loss: 0.4637310802936554 at epoch 88\n",
            "loss: 0.6145846247673035 at epoch 88\n",
            "loss: 0.5932570099830627 at epoch 88\n",
            "loss: 0.272111713886261 at epoch 88\n",
            "loss: 0.17688004672527313 at epoch 88\n",
            "loss: 0.10630742460489273 at epoch 88\n",
            "loss: 0.18552768230438232 at epoch 88\n",
            "loss: 0.13767839968204498 at epoch 88\n",
            "loss: 0.10530868917703629 at epoch 88\n",
            "loss: 0.12643857300281525 at epoch 88\n",
            "loss: 0.30464333295822144 at epoch 88\n",
            "loss: 0.5042039155960083 at epoch 88\n",
            "loss: 0.553406298160553 at epoch 88\n",
            "loss: 0.27432286739349365 at epoch 88\n",
            "loss: 0.4357284605503082 at epoch 88\n",
            "loss: 0.13185888528823853 at epoch 88\n",
            "loss: 0.2831127643585205 at epoch 88\n",
            "loss: 0.027735313400626183 at epoch 88\n",
            "loss: 0.1892595738172531 at epoch 88\n",
            "loss: 0.1905970424413681 at epoch 88\n",
            "loss: 0.27571412920951843 at epoch 88\n",
            "loss: 0.0681697204709053 at epoch 88\n",
            "loss: 0.11784029006958008 at epoch 88\n",
            "loss: 0.2504146695137024 at epoch 88\n",
            "loss: 0.45135632157325745 at epoch 88\n",
            "loss: 0.2910637855529785 at epoch 88\n",
            "loss: 0.1828267127275467 at epoch 88\n",
            "loss: 0.08458556234836578 at epoch 88\n",
            "loss: 0.02634439617395401 at epoch 88\n",
            "loss: 0.042159304022789 at epoch 88\n",
            "loss: 0.060005057603120804 at epoch 88\n",
            "loss: 0.09025685489177704 at epoch 88\n",
            "loss: 0.06134338676929474 at epoch 88\n",
            "loss: 0.03887419030070305 at epoch 88\n",
            "loss: 0.022508926689624786 at epoch 88\n",
            "loss: 0.04355956241488457 at epoch 88\n",
            "loss: 0.04001115262508392 at epoch 88\n",
            "loss: 0.026525892317295074 at epoch 88\n",
            "loss: 0.02764701284468174 at epoch 88\n",
            "loss: 0.05081375315785408 at epoch 88\n",
            "loss: 0.1315697580575943 at epoch 88\n",
            "loss: 0.18254293501377106 at epoch 88\n",
            "loss: 0.08796557039022446 at epoch 88\n",
            "loss: 0.01491834968328476 at epoch 88\n",
            "loss: 0.06824882328510284 at epoch 88\n",
            "loss: 0.15729878842830658 at epoch 88\n",
            "loss: 0.24542224407196045 at epoch 88\n",
            "loss: 0.08999717235565186 at epoch 88\n",
            "loss: 0.1280352920293808 at epoch 88\n",
            "loss: 0.15793490409851074 at epoch 88\n",
            "loss: 0.3198993504047394 at epoch 88\n",
            "loss: 0.1582087129354477 at epoch 89\n",
            "loss: 0.16155841946601868 at epoch 89\n",
            "loss: 0.31475362181663513 at epoch 89\n",
            "loss: 0.37253206968307495 at epoch 89\n",
            "loss: 0.20494885742664337 at epoch 89\n",
            "loss: 0.1180267184972763 at epoch 89\n",
            "loss: 0.07777508348226547 at epoch 89\n",
            "loss: 0.22943449020385742 at epoch 89\n",
            "loss: 0.2732984721660614 at epoch 89\n",
            "loss: 0.26148009300231934 at epoch 89\n",
            "loss: 0.14510184526443481 at epoch 89\n",
            "loss: 0.07016412168741226 at epoch 89\n",
            "loss: 0.19207358360290527 at epoch 89\n",
            "loss: 0.41897937655448914 at epoch 89\n",
            "loss: 0.2823840379714966 at epoch 89\n",
            "loss: 0.09607692807912827 at epoch 89\n",
            "loss: 0.0764414593577385 at epoch 89\n",
            "loss: 0.2664347290992737 at epoch 89\n",
            "loss: 0.36920589208602905 at epoch 89\n",
            "loss: 0.26420363783836365 at epoch 89\n",
            "loss: 0.12924757599830627 at epoch 89\n",
            "loss: 0.01761728711426258 at epoch 89\n",
            "loss: 0.07313469052314758 at epoch 89\n",
            "loss: 0.07405667752027512 at epoch 89\n",
            "loss: 0.06983448565006256 at epoch 89\n",
            "loss: 0.03306572511792183 at epoch 89\n",
            "loss: 0.031159911304712296 at epoch 89\n",
            "loss: 0.019531236961483955 at epoch 89\n",
            "loss: 0.06217877194285393 at epoch 89\n",
            "loss: 0.04847092926502228 at epoch 89\n",
            "loss: 0.02542771026492119 at epoch 89\n",
            "loss: 0.04304009675979614 at epoch 89\n",
            "loss: 0.027147121727466583 at epoch 89\n",
            "loss: 0.06538206338882446 at epoch 89\n",
            "loss: 0.027693912386894226 at epoch 89\n",
            "loss: 0.023077741265296936 at epoch 89\n",
            "loss: 0.040932778269052505 at epoch 89\n",
            "loss: 0.13165518641471863 at epoch 89\n",
            "loss: 0.22987152636051178 at epoch 89\n",
            "loss: 0.1961541771888733 at epoch 89\n",
            "loss: 0.10128186643123627 at epoch 89\n",
            "loss: 0.03112015873193741 at epoch 89\n",
            "loss: 0.06820569932460785 at epoch 89\n",
            "loss: 0.13751676678657532 at epoch 89\n",
            "loss: 0.06493111699819565 at epoch 89\n",
            "loss: 0.048611246049404144 at epoch 89\n",
            "loss: 0.07326426357030869 at epoch 89\n",
            "loss: 0.19224250316619873 at epoch 89\n",
            "loss: 0.21679669618606567 at epoch 89\n",
            "loss: 0.27734971046447754 at epoch 89\n",
            "loss: 0.2708476185798645 at epoch 89\n",
            "loss: 0.07665584236383438 at epoch 89\n",
            "loss: 0.133503258228302 at epoch 89\n",
            "loss: 0.04641173034906387 at epoch 89\n",
            "loss: 0.10032524168491364 at epoch 89\n",
            "loss: 0.1083146333694458 at epoch 89\n",
            "loss: 0.22752751410007477 at epoch 89\n",
            "loss: 0.16570930182933807 at epoch 89\n",
            "loss: 0.27426981925964355 at epoch 89\n",
            "loss: 0.09999845921993256 at epoch 89\n",
            "loss: 0.15926241874694824 at epoch 89\n",
            "loss: 0.15754422545433044 at epoch 89\n",
            "loss: 0.3880411386489868 at epoch 89\n",
            "loss: 0.3474957048892975 at epoch 89\n",
            "loss: 0.10206969082355499 at epoch 89\n",
            "loss: 0.19228258728981018 at epoch 89\n",
            "loss: 0.4064895808696747 at epoch 89\n",
            "loss: 0.6175106167793274 at epoch 89\n",
            "loss: 0.5887346267700195 at epoch 89\n",
            "loss: 0.3998681604862213 at epoch 89\n",
            "loss: 0.33520251512527466 at epoch 89\n",
            "loss: 0.9343335628509521 at epoch 89\n",
            "loss: 1.2949857711791992 at epoch 89\n",
            "loss: 0.8729496598243713 at epoch 89\n",
            "loss: 0.454495370388031 at epoch 89\n",
            "loss: 0.3296123147010803 at epoch 89\n",
            "loss: 0.4574722647666931 at epoch 89\n",
            "loss: 0.47079360485076904 at epoch 89\n",
            "loss: 0.2627793550491333 at epoch 89\n",
            "loss: 0.4808811545372009 at epoch 90\n",
            "loss: 1.4579651355743408 at epoch 90\n",
            "loss: 2.602281093597412 at epoch 90\n",
            "loss: 2.236347198486328 at epoch 90\n",
            "loss: 0.7274782061576843 at epoch 90\n",
            "loss: 1.548017978668213 at epoch 90\n",
            "loss: 3.475367546081543 at epoch 90\n",
            "loss: 1.846971869468689 at epoch 90\n",
            "loss: 0.9845698475837708 at epoch 90\n",
            "loss: 1.5174925327301025 at epoch 90\n",
            "loss: 3.8858349323272705 at epoch 90\n",
            "loss: 2.6210055351257324 at epoch 90\n",
            "loss: 1.4054795503616333 at epoch 90\n",
            "loss: 4.464706897735596 at epoch 90\n",
            "loss: 7.06559944152832 at epoch 90\n",
            "loss: 1.627963662147522 at epoch 90\n",
            "loss: 1.867679476737976 at epoch 90\n",
            "loss: 5.4598565101623535 at epoch 90\n",
            "loss: 2.72719669342041 at epoch 90\n",
            "loss: 2.219015121459961 at epoch 90\n",
            "loss: 4.982263088226318 at epoch 90\n",
            "loss: 1.7812312841415405 at epoch 90\n",
            "loss: 2.1521716117858887 at epoch 90\n",
            "loss: 3.5367112159729004 at epoch 90\n",
            "loss: 1.5980384349822998 at epoch 90\n",
            "loss: 1.8902288675308228 at epoch 90\n",
            "loss: 4.18813943862915 at epoch 90\n",
            "loss: 1.8712668418884277 at epoch 90\n",
            "loss: 1.1523473262786865 at epoch 90\n",
            "loss: 2.2095463275909424 at epoch 90\n",
            "loss: 0.5381686687469482 at epoch 90\n",
            "loss: 0.8812857866287231 at epoch 90\n",
            "loss: 1.7007910013198853 at epoch 90\n",
            "loss: 0.26065415143966675 at epoch 90\n",
            "loss: 0.9074315428733826 at epoch 90\n",
            "loss: 1.000897765159607 at epoch 90\n",
            "loss: 0.16239990293979645 at epoch 90\n",
            "loss: 0.9766997694969177 at epoch 90\n",
            "loss: 0.45459333062171936 at epoch 90\n",
            "loss: 0.43624675273895264 at epoch 90\n",
            "loss: 1.0527657270431519 at epoch 90\n",
            "loss: 0.3729945123195648 at epoch 90\n",
            "loss: 0.43442440032958984 at epoch 90\n",
            "loss: 0.6137920022010803 at epoch 90\n",
            "loss: 0.19107931852340698 at epoch 90\n",
            "loss: 0.462069034576416 at epoch 90\n",
            "loss: 0.4377351999282837 at epoch 90\n",
            "loss: 0.16729973256587982 at epoch 90\n",
            "loss: 0.42644447088241577 at epoch 90\n",
            "loss: 0.29106825590133667 at epoch 90\n",
            "loss: 0.2831687331199646 at epoch 90\n",
            "loss: 0.31396645307540894 at epoch 90\n",
            "loss: 0.09612544625997543 at epoch 90\n",
            "loss: 0.5055675506591797 at epoch 90\n",
            "loss: 0.09845565259456635 at epoch 90\n",
            "loss: 0.24043625593185425 at epoch 90\n",
            "loss: 0.48231253027915955 at epoch 90\n",
            "loss: 0.4514985978603363 at epoch 90\n",
            "loss: 0.43424102663993835 at epoch 90\n",
            "loss: 0.33962228894233704 at epoch 90\n",
            "loss: 0.37756532430648804 at epoch 90\n",
            "loss: 0.2788453698158264 at epoch 90\n",
            "loss: 0.38527071475982666 at epoch 90\n",
            "loss: 0.24798309803009033 at epoch 90\n",
            "loss: 0.55231112241745 at epoch 90\n",
            "loss: 0.2649863362312317 at epoch 90\n",
            "loss: 0.21439245343208313 at epoch 90\n",
            "loss: 0.31912893056869507 at epoch 90\n",
            "loss: 0.21349932253360748 at epoch 90\n",
            "loss: 0.20276115834712982 at epoch 90\n",
            "loss: 0.102267786860466 at epoch 90\n",
            "loss: 0.24192270636558533 at epoch 90\n",
            "loss: 0.15357202291488647 at epoch 90\n",
            "loss: 0.10458835959434509 at epoch 90\n",
            "loss: 0.2427089810371399 at epoch 90\n",
            "loss: 0.19676418602466583 at epoch 90\n",
            "loss: 0.13848435878753662 at epoch 90\n",
            "loss: 0.20269009470939636 at epoch 90\n",
            "loss: 0.0489969477057457 at epoch 90\n",
            "loss: 0.2506791353225708 at epoch 91\n",
            "loss: 0.28595805168151855 at epoch 91\n",
            "loss: 0.09575687348842621 at epoch 91\n",
            "loss: 0.29379019141197205 at epoch 91\n",
            "loss: 0.3259858787059784 at epoch 91\n",
            "loss: 0.04469054937362671 at epoch 91\n",
            "loss: 0.3055896461009979 at epoch 91\n",
            "loss: 0.19677120447158813 at epoch 91\n",
            "loss: 0.1281871348619461 at epoch 91\n",
            "loss: 0.24413195252418518 at epoch 91\n",
            "loss: 0.10500561445951462 at epoch 91\n",
            "loss: 0.13410446047782898 at epoch 91\n",
            "loss: 0.16759516298770905 at epoch 91\n",
            "loss: 0.040349654853343964 at epoch 91\n",
            "loss: 0.15101048350334167 at epoch 91\n",
            "loss: 0.0515829361975193 at epoch 91\n",
            "loss: 0.09870889037847519 at epoch 91\n",
            "loss: 0.07702746987342834 at epoch 91\n",
            "loss: 0.03409653156995773 at epoch 91\n",
            "loss: 0.11841678619384766 at epoch 91\n",
            "loss: 0.07995262742042542 at epoch 91\n",
            "loss: 0.038755662739276886 at epoch 91\n",
            "loss: 0.04563366249203682 at epoch 91\n",
            "loss: 0.024228310212492943 at epoch 91\n",
            "loss: 0.03429945930838585 at epoch 91\n",
            "loss: 0.04527711868286133 at epoch 91\n",
            "loss: 0.022880936041474342 at epoch 91\n",
            "loss: 0.035867128521203995 at epoch 91\n",
            "loss: 0.04730962961912155 at epoch 91\n",
            "loss: 0.02081545628607273 at epoch 91\n",
            "loss: 0.04257678613066673 at epoch 91\n",
            "loss: 0.02350500226020813 at epoch 91\n",
            "loss: 0.025315716862678528 at epoch 91\n",
            "loss: 0.028609925881028175 at epoch 91\n",
            "loss: 0.026774367317557335 at epoch 91\n",
            "loss: 0.014047563076019287 at epoch 91\n",
            "loss: 0.02722839266061783 at epoch 91\n",
            "loss: 0.0205964557826519 at epoch 91\n",
            "loss: 0.008836738765239716 at epoch 91\n",
            "loss: 0.015790415927767754 at epoch 91\n",
            "loss: 0.012397214770317078 at epoch 91\n",
            "loss: 0.00908959936350584 at epoch 91\n",
            "loss: 0.009183887392282486 at epoch 91\n",
            "loss: 0.005482370965182781 at epoch 91\n",
            "loss: 0.0040216813795268536 at epoch 91\n",
            "loss: 0.01555223111063242 at epoch 91\n",
            "loss: 0.00723078940063715 at epoch 91\n",
            "loss: 0.006340645253658295 at epoch 91\n",
            "loss: 0.00705873966217041 at epoch 91\n",
            "loss: 0.005760303232818842 at epoch 91\n",
            "loss: 0.00851220078766346 at epoch 91\n",
            "loss: 0.005887578707188368 at epoch 91\n",
            "loss: 0.008334928192198277 at epoch 91\n",
            "loss: 0.006405369378626347 at epoch 91\n",
            "loss: 0.007130279205739498 at epoch 91\n",
            "loss: 0.006953321397304535 at epoch 91\n",
            "loss: 0.004541055765002966 at epoch 91\n",
            "loss: 0.007536627352237701 at epoch 91\n",
            "loss: 0.006191752385348082 at epoch 91\n",
            "loss: 0.005176024977117777 at epoch 91\n",
            "loss: 0.005327377002686262 at epoch 91\n",
            "loss: 0.0025052025448530912 at epoch 91\n",
            "loss: 0.002481721341609955 at epoch 91\n",
            "loss: 0.003954893909394741 at epoch 91\n",
            "loss: 0.002697041491046548 at epoch 91\n",
            "loss: 0.005460384767502546 at epoch 91\n",
            "loss: 0.002223235322162509 at epoch 91\n",
            "loss: 0.0024268694687634706 at epoch 91\n",
            "loss: 0.007183518260717392 at epoch 91\n",
            "loss: 0.0025159709621220827 at epoch 91\n",
            "loss: 0.00323220225982368 at epoch 91\n",
            "loss: 0.0029559957329183817 at epoch 91\n",
            "loss: 0.0018147564260289073 at epoch 91\n",
            "loss: 0.00191362458281219 at epoch 91\n",
            "loss: 0.0017660303274169564 at epoch 91\n",
            "loss: 0.001305040204897523 at epoch 91\n",
            "loss: 0.011294540017843246 at epoch 91\n",
            "loss: 0.003956709057092667 at epoch 91\n",
            "loss: 0.001002614269964397 at epoch 91\n",
            "loss: 0.0014214575057849288 at epoch 92\n",
            "loss: 0.01384564209729433 at epoch 92\n",
            "loss: 0.004837695509195328 at epoch 92\n",
            "loss: 0.0026816886384040117 at epoch 92\n",
            "loss: 0.0007374325068667531 at epoch 92\n",
            "loss: 0.0011520044645294547 at epoch 92\n",
            "loss: 0.002571186749264598 at epoch 92\n",
            "loss: 0.00268839905038476 at epoch 92\n",
            "loss: 0.004456905648112297 at epoch 92\n",
            "loss: 0.002401885809376836 at epoch 92\n",
            "loss: 0.0012080669403076172 at epoch 92\n",
            "loss: 0.001107487129047513 at epoch 92\n",
            "loss: 0.0018866488244384527 at epoch 92\n",
            "loss: 0.0018315163906663656 at epoch 92\n",
            "loss: 0.001300601288676262 at epoch 92\n",
            "loss: 0.0007508151466026902 at epoch 92\n",
            "loss: 0.0031361388973891735 at epoch 92\n",
            "loss: 0.0034867594949901104 at epoch 92\n",
            "loss: 0.001624040654860437 at epoch 92\n",
            "loss: 0.00047285351320169866 at epoch 92\n",
            "loss: 0.0011200326262041926 at epoch 92\n",
            "loss: 0.0005378905334509909 at epoch 92\n",
            "loss: 0.010989692062139511 at epoch 92\n",
            "loss: 0.0013996397610753775 at epoch 92\n",
            "loss: 0.0006951281102374196 at epoch 92\n",
            "loss: 0.0028780195862054825 at epoch 92\n",
            "loss: 0.0016272139037027955 at epoch 92\n",
            "loss: 0.0006422127480618656 at epoch 92\n",
            "loss: 0.0011013592593371868 at epoch 92\n",
            "loss: 0.0021325917914509773 at epoch 92\n",
            "loss: 0.0019845024216920137 at epoch 92\n",
            "loss: 0.006674039177596569 at epoch 92\n",
            "loss: 0.007908591069281101 at epoch 92\n",
            "loss: 0.006123929750174284 at epoch 92\n",
            "loss: 0.009100092574954033 at epoch 92\n",
            "loss: 0.0012629324337467551 at epoch 92\n",
            "loss: 0.016648009419441223 at epoch 92\n",
            "loss: 0.0061200885102152824 at epoch 92\n",
            "loss: 0.005328877829015255 at epoch 92\n",
            "loss: 0.010221038945019245 at epoch 92\n",
            "loss: 0.0040649049915373325 at epoch 92\n",
            "loss: 0.004388341214507818 at epoch 92\n",
            "loss: 0.005621773190796375 at epoch 92\n",
            "loss: 0.0018712854944169521 at epoch 92\n",
            "loss: 0.005226638168096542 at epoch 92\n",
            "loss: 0.00469306530430913 at epoch 92\n",
            "loss: 0.002693153452128172 at epoch 92\n",
            "loss: 0.0008397741476073861 at epoch 92\n",
            "loss: 0.0032997969537973404 at epoch 92\n",
            "loss: 0.004119609482586384 at epoch 92\n",
            "loss: 0.0018773702904582024 at epoch 92\n",
            "loss: 0.0013474783627316356 at epoch 92\n",
            "loss: 0.003159894375130534 at epoch 92\n",
            "loss: 0.00971895083785057 at epoch 92\n",
            "loss: 0.002455383539199829 at epoch 92\n",
            "loss: 0.0025104572996497154 at epoch 92\n",
            "loss: 0.0014359245542436838 at epoch 92\n",
            "loss: 0.002116985386237502 at epoch 92\n",
            "loss: 0.0021664267405867577 at epoch 92\n",
            "loss: 0.0013263041619211435 at epoch 92\n",
            "loss: 0.0015275237383320928 at epoch 92\n",
            "loss: 0.0019462991040199995 at epoch 92\n",
            "loss: 0.00034651256282813847 at epoch 92\n",
            "loss: 0.0032627778127789497 at epoch 92\n",
            "loss: 0.0005931409541517496 at epoch 92\n",
            "loss: 0.0029197614639997482 at epoch 92\n",
            "loss: 0.0027206288650631905 at epoch 92\n",
            "loss: 0.010224091820418835 at epoch 92\n",
            "loss: 0.0033855047076940536 at epoch 92\n",
            "loss: 0.000809837132692337 at epoch 92\n",
            "loss: 0.003458411665633321 at epoch 92\n",
            "loss: 0.002454896690323949 at epoch 92\n",
            "loss: 0.003197584068402648 at epoch 92\n",
            "loss: 0.0016919372137635946 at epoch 92\n",
            "loss: 0.004472044762223959 at epoch 92\n",
            "loss: 0.004071646835654974 at epoch 92\n",
            "loss: 0.005382180213928223 at epoch 92\n",
            "loss: 0.001316240057349205 at epoch 92\n",
            "loss: 0.007432044018059969 at epoch 92\n",
            "loss: 0.005384273827075958 at epoch 93\n",
            "loss: 0.0038298883009701967 at epoch 93\n",
            "loss: 0.005080459173768759 at epoch 93\n",
            "loss: 0.0036490329075604677 at epoch 93\n",
            "loss: 0.003688257886096835 at epoch 93\n",
            "loss: 0.0054154773242771626 at epoch 93\n",
            "loss: 0.004996221046894789 at epoch 93\n",
            "loss: 0.002024129033088684 at epoch 93\n",
            "loss: 0.0017892300384119153 at epoch 93\n",
            "loss: 0.011361217126250267 at epoch 93\n",
            "loss: 0.002844437025487423 at epoch 93\n",
            "loss: 0.0005517732352018356 at epoch 93\n",
            "loss: 0.004958480596542358 at epoch 93\n",
            "loss: 0.0014294130960479379 at epoch 93\n",
            "loss: 0.003533492563292384 at epoch 93\n",
            "loss: 0.0030324719846248627 at epoch 93\n",
            "loss: 0.002451217034831643 at epoch 93\n",
            "loss: 0.0021829227916896343 at epoch 93\n",
            "loss: 0.0009452057420276105 at epoch 93\n",
            "loss: 0.0006361617706716061 at epoch 93\n",
            "loss: 0.00259218318387866 at epoch 93\n",
            "loss: 0.001273961621336639 at epoch 93\n",
            "loss: 0.0014754219446331263 at epoch 93\n",
            "loss: 0.0028702793642878532 at epoch 93\n",
            "loss: 0.0017627893248572946 at epoch 93\n",
            "loss: 0.0024666935205459595 at epoch 93\n",
            "loss: 0.0006794192595407367 at epoch 93\n",
            "loss: 0.0019892845302820206 at epoch 93\n",
            "loss: 0.0012554517015814781 at epoch 93\n",
            "loss: 0.0016556150512769818 at epoch 93\n",
            "loss: 0.0019479005131870508 at epoch 93\n",
            "loss: 0.0012902210000902414 at epoch 93\n",
            "loss: 0.0010391856776550412 at epoch 93\n",
            "loss: 0.0012310376623645425 at epoch 93\n",
            "loss: 0.0020330268889665604 at epoch 93\n",
            "loss: 0.0019044217187911272 at epoch 93\n",
            "loss: 0.001418030122295022 at epoch 93\n",
            "loss: 0.002597588347271085 at epoch 93\n",
            "loss: 0.002362077124416828 at epoch 93\n",
            "loss: 0.002997934352606535 at epoch 93\n",
            "loss: 0.0005938501562923193 at epoch 93\n",
            "loss: 0.0007059334311634302 at epoch 93\n",
            "loss: 0.00037255013012327254 at epoch 93\n",
            "loss: 0.00024750750162638724 at epoch 93\n",
            "loss: 0.00048633309779688716 at epoch 93\n",
            "loss: 0.003517326433211565 at epoch 93\n",
            "loss: 0.0012164056533947587 at epoch 93\n",
            "loss: 0.0008094956283457577 at epoch 93\n",
            "loss: 0.004783751908689737 at epoch 93\n",
            "loss: 0.0012838535476475954 at epoch 93\n",
            "loss: 0.0014232300454750657 at epoch 93\n",
            "loss: 0.003568049753084779 at epoch 93\n",
            "loss: 0.004073544405400753 at epoch 93\n",
            "loss: 0.0029400354251265526 at epoch 93\n",
            "loss: 0.0009793059434741735 at epoch 93\n",
            "loss: 0.005727049894630909 at epoch 93\n",
            "loss: 0.005695478059351444 at epoch 93\n",
            "loss: 0.0020683249458670616 at epoch 93\n",
            "loss: 0.0026506762951612473 at epoch 93\n",
            "loss: 0.0028815388213843107 at epoch 93\n",
            "loss: 0.00955401360988617 at epoch 93\n",
            "loss: 0.004395846277475357 at epoch 93\n",
            "loss: 0.008828986436128616 at epoch 93\n",
            "loss: 0.0072805918753147125 at epoch 93\n",
            "loss: 0.0086393216624856 at epoch 93\n",
            "loss: 0.010324887000024319 at epoch 93\n",
            "loss: 0.012165037915110588 at epoch 93\n",
            "loss: 0.0041053639724850655 at epoch 93\n",
            "loss: 0.006328720133751631 at epoch 93\n",
            "loss: 0.010305343195796013 at epoch 93\n",
            "loss: 0.011721363291144371 at epoch 93\n",
            "loss: 0.006077242083847523 at epoch 93\n",
            "loss: 0.007939738221466541 at epoch 93\n",
            "loss: 0.005586201325058937 at epoch 93\n",
            "loss: 0.004563681315630674 at epoch 93\n",
            "loss: 0.007498659193515778 at epoch 93\n",
            "loss: 0.0025925543159246445 at epoch 93\n",
            "loss: 0.0017922628903761506 at epoch 93\n",
            "loss: 0.005064573138952255 at epoch 93\n",
            "loss: 0.007938049733638763 at epoch 94\n",
            "loss: 0.004184993449598551 at epoch 94\n",
            "loss: 0.002566397422924638 at epoch 94\n",
            "loss: 0.011370914988219738 at epoch 94\n",
            "loss: 0.013355153612792492 at epoch 94\n",
            "loss: 0.0038659842684865 at epoch 94\n",
            "loss: 0.007581433281302452 at epoch 94\n",
            "loss: 0.013378012925386429 at epoch 94\n",
            "loss: 0.00356406532227993 at epoch 94\n",
            "loss: 0.004986400716006756 at epoch 94\n",
            "loss: 0.008819790557026863 at epoch 94\n",
            "loss: 0.013964942656457424 at epoch 94\n",
            "loss: 0.0023406941909343004 at epoch 94\n",
            "loss: 0.006059377454221249 at epoch 94\n",
            "loss: 0.007170370779931545 at epoch 94\n",
            "loss: 0.01336414460092783 at epoch 94\n",
            "loss: 0.004955783952027559 at epoch 94\n",
            "loss: 0.007632283493876457 at epoch 94\n",
            "loss: 0.004718232899904251 at epoch 94\n",
            "loss: 0.006080984137952328 at epoch 94\n",
            "loss: 0.004120479803532362 at epoch 94\n",
            "loss: 0.003525570733472705 at epoch 94\n",
            "loss: 0.006429397966712713 at epoch 94\n",
            "loss: 0.002759679453447461 at epoch 94\n",
            "loss: 0.0072860135696828365 at epoch 94\n",
            "loss: 0.0034406795166432858 at epoch 94\n",
            "loss: 0.006956436671316624 at epoch 94\n",
            "loss: 0.00409663887694478 at epoch 94\n",
            "loss: 0.0037624193355441093 at epoch 94\n",
            "loss: 0.009807939641177654 at epoch 94\n",
            "loss: 0.008762077428400517 at epoch 94\n",
            "loss: 0.0074224816635251045 at epoch 94\n",
            "loss: 0.008408788591623306 at epoch 94\n",
            "loss: 0.009785488247871399 at epoch 94\n",
            "loss: 0.005888378247618675 at epoch 94\n",
            "loss: 0.003217714838683605 at epoch 94\n",
            "loss: 0.006618578918278217 at epoch 94\n",
            "loss: 0.004222758114337921 at epoch 94\n",
            "loss: 0.0011368000414222479 at epoch 94\n",
            "loss: 0.003129523014649749 at epoch 94\n",
            "loss: 0.0034868833608925343 at epoch 94\n",
            "loss: 0.0021021515130996704 at epoch 94\n",
            "loss: 0.0013518031919375062 at epoch 94\n",
            "loss: 0.00380686460994184 at epoch 94\n",
            "loss: 0.0036855083890259266 at epoch 94\n",
            "loss: 0.003500161226838827 at epoch 94\n",
            "loss: 0.0004152543842792511 at epoch 94\n",
            "loss: 0.003101420123130083 at epoch 94\n",
            "loss: 0.00042505760211497545 at epoch 94\n",
            "loss: 0.001868517487309873 at epoch 94\n",
            "loss: 0.0018690578872337937 at epoch 94\n",
            "loss: 0.0017259134911000729 at epoch 94\n",
            "loss: 0.001059753936715424 at epoch 94\n",
            "loss: 0.003717019222676754 at epoch 94\n",
            "loss: 0.0019832865800708532 at epoch 94\n",
            "loss: 0.0037980019114911556 at epoch 94\n",
            "loss: 0.0031351596117019653 at epoch 94\n",
            "loss: 0.002621057443320751 at epoch 94\n",
            "loss: 0.0014785324456170201 at epoch 94\n",
            "loss: 0.0023290228564292192 at epoch 94\n",
            "loss: 0.0018671852303668857 at epoch 94\n",
            "loss: 0.0016942535294219851 at epoch 94\n",
            "loss: 0.003938330337405205 at epoch 94\n",
            "loss: 0.0015115456189960241 at epoch 94\n",
            "loss: 0.0010315916733816266 at epoch 94\n",
            "loss: 0.0019593210890889168 at epoch 94\n",
            "loss: 0.0010165818966925144 at epoch 94\n",
            "loss: 0.002374502830207348 at epoch 94\n",
            "loss: 0.000875221099704504 at epoch 94\n",
            "loss: 0.0017316689481958747 at epoch 94\n",
            "loss: 0.0008111459319479764 at epoch 94\n",
            "loss: 0.003373686457052827 at epoch 94\n",
            "loss: 0.0004040064522996545 at epoch 94\n",
            "loss: 0.0008231677347794175 at epoch 94\n",
            "loss: 0.0004888731054961681 at epoch 94\n",
            "loss: 0.001720963977277279 at epoch 94\n",
            "loss: 0.0007828020607121289 at epoch 94\n",
            "loss: 0.0013582148822024465 at epoch 94\n",
            "loss: 0.0004601424152497202 at epoch 94\n",
            "loss: 0.0014100790722295642 at epoch 95\n",
            "loss: 0.001001693424768746 at epoch 95\n",
            "loss: 0.001371184946037829 at epoch 95\n",
            "loss: 0.001110657467506826 at epoch 95\n",
            "loss: 0.00039613968692719936 at epoch 95\n",
            "loss: 0.0009881119476631284 at epoch 95\n",
            "loss: 0.00031589006539434195 at epoch 95\n",
            "loss: 0.0010165103012695909 at epoch 95\n",
            "loss: 0.0011211782693862915 at epoch 95\n",
            "loss: 0.0007528204005211592 at epoch 95\n",
            "loss: 0.003385891206562519 at epoch 95\n",
            "loss: 0.0026883489917963743 at epoch 95\n",
            "loss: 0.0021761241368949413 at epoch 95\n",
            "loss: 0.0059439740143716335 at epoch 95\n",
            "loss: 0.0029911184683442116 at epoch 95\n",
            "loss: 0.002783597446978092 at epoch 95\n",
            "loss: 0.002793228719383478 at epoch 95\n",
            "loss: 0.0010281603317707777 at epoch 95\n",
            "loss: 0.0016870609251782298 at epoch 95\n",
            "loss: 0.0038701738230884075 at epoch 95\n",
            "loss: 0.0035661787260323763 at epoch 95\n",
            "loss: 0.0009198680054396391 at epoch 95\n",
            "loss: 0.0040452806279063225 at epoch 95\n",
            "loss: 0.001925177755765617 at epoch 95\n",
            "loss: 0.002743004821240902 at epoch 95\n",
            "loss: 0.0032249095384031534 at epoch 95\n",
            "loss: 0.008134707808494568 at epoch 95\n",
            "loss: 0.002419502940028906 at epoch 95\n",
            "loss: 0.002563909161835909 at epoch 95\n",
            "loss: 0.0015814261278137565 at epoch 95\n",
            "loss: 0.0020976171363145113 at epoch 95\n",
            "loss: 0.0003418367123231292 at epoch 95\n",
            "loss: 0.0012128211092203856 at epoch 95\n",
            "loss: 0.0018428110051900148 at epoch 95\n",
            "loss: 0.0017927329754456878 at epoch 95\n",
            "loss: 0.0017969085602089763 at epoch 95\n",
            "loss: 0.0035071608144789934 at epoch 95\n",
            "loss: 0.0023652866948395967 at epoch 95\n",
            "loss: 0.003090605605393648 at epoch 95\n",
            "loss: 0.003686056239530444 at epoch 95\n",
            "loss: 0.005177126731723547 at epoch 95\n",
            "loss: 0.0014543465804308653 at epoch 95\n",
            "loss: 0.008956197649240494 at epoch 95\n",
            "loss: 0.0017986203311011195 at epoch 95\n",
            "loss: 0.002991674467921257 at epoch 95\n",
            "loss: 0.0016609521117061377 at epoch 95\n",
            "loss: 0.0017118477262556553 at epoch 95\n",
            "loss: 0.002028167247772217 at epoch 95\n",
            "loss: 0.0011756211752071977 at epoch 95\n",
            "loss: 0.007534458301961422 at epoch 95\n",
            "loss: 0.004141128621995449 at epoch 95\n",
            "loss: 0.0068009644746780396 at epoch 95\n",
            "loss: 0.007093367166817188 at epoch 95\n",
            "loss: 0.0043924651108682156 at epoch 95\n",
            "loss: 0.0019387553911656141 at epoch 95\n",
            "loss: 0.0016288389451801777 at epoch 95\n",
            "loss: 0.0014975983649492264 at epoch 95\n",
            "loss: 0.0015062297461554408 at epoch 95\n",
            "loss: 0.0007629859610460699 at epoch 95\n",
            "loss: 0.002286455361172557 at epoch 95\n",
            "loss: 0.0014358500484377146 at epoch 95\n",
            "loss: 0.002225436270236969 at epoch 95\n",
            "loss: 0.0005235312273725867 at epoch 95\n",
            "loss: 0.0022252658382058144 at epoch 95\n",
            "loss: 0.004815994296222925 at epoch 95\n",
            "loss: 0.0023521746043115854 at epoch 95\n",
            "loss: 0.0010877714958041906 at epoch 95\n",
            "loss: 0.0015769678866490722 at epoch 95\n",
            "loss: 0.0012483884347602725 at epoch 95\n",
            "loss: 0.0014329872792586684 at epoch 95\n",
            "loss: 0.0017947121523320675 at epoch 95\n",
            "loss: 0.0013777618296444416 at epoch 95\n",
            "loss: 0.0009416751563549042 at epoch 95\n",
            "loss: 0.0012170730624347925 at epoch 95\n",
            "loss: 0.0008393460884690285 at epoch 95\n",
            "loss: 0.0044793360866606236 at epoch 95\n",
            "loss: 0.003404803341254592 at epoch 95\n",
            "loss: 0.0009640713687986135 at epoch 95\n",
            "loss: 0.002598723629489541 at epoch 95\n",
            "loss: 0.009048374369740486 at epoch 96\n",
            "loss: 0.010609239339828491 at epoch 96\n",
            "loss: 0.0064557995647192 at epoch 96\n",
            "loss: 0.0025095785968005657 at epoch 96\n",
            "loss: 0.01373295672237873 at epoch 96\n",
            "loss: 0.012982266955077648 at epoch 96\n",
            "loss: 0.011019475758075714 at epoch 96\n",
            "loss: 0.003548149950802326 at epoch 96\n",
            "loss: 0.01148217637091875 at epoch 96\n",
            "loss: 0.015923425555229187 at epoch 96\n",
            "loss: 0.011454205960035324 at epoch 96\n",
            "loss: 0.004274269565939903 at epoch 96\n",
            "loss: 0.007178892847150564 at epoch 96\n",
            "loss: 0.011272932402789593 at epoch 96\n",
            "loss: 0.01630520075559616 at epoch 96\n",
            "loss: 0.0074425009079277515 at epoch 96\n",
            "loss: 0.0032935533672571182 at epoch 96\n",
            "loss: 0.005186143796890974 at epoch 96\n",
            "loss: 0.012923945672810078 at epoch 96\n",
            "loss: 0.0019012789707630873 at epoch 96\n",
            "loss: 0.009412653744220734 at epoch 96\n",
            "loss: 0.010285889729857445 at epoch 96\n",
            "loss: 0.013697024434804916 at epoch 96\n",
            "loss: 0.006977192126214504 at epoch 96\n",
            "loss: 0.005488633178174496 at epoch 96\n",
            "loss: 0.010318750515580177 at epoch 96\n",
            "loss: 0.019691981375217438 at epoch 96\n",
            "loss: 0.017217956483364105 at epoch 96\n",
            "loss: 0.007341404445469379 at epoch 96\n",
            "loss: 0.005764897912740707 at epoch 96\n",
            "loss: 0.02511359378695488 at epoch 96\n",
            "loss: 0.034396588802337646 at epoch 96\n",
            "loss: 0.013940464705228806 at epoch 96\n",
            "loss: 0.014080950990319252 at epoch 96\n",
            "loss: 0.019679570570588112 at epoch 96\n",
            "loss: 0.037168532609939575 at epoch 96\n",
            "loss: 0.021598801016807556 at epoch 96\n",
            "loss: 0.010386019013822079 at epoch 96\n",
            "loss: 0.0056412601843476295 at epoch 96\n",
            "loss: 0.011733523570001125 at epoch 96\n",
            "loss: 0.003300000447779894 at epoch 96\n",
            "loss: 0.008673823438584805 at epoch 96\n",
            "loss: 0.007252204231917858 at epoch 96\n",
            "loss: 0.005948895122855902 at epoch 96\n",
            "loss: 0.004883758723735809 at epoch 96\n",
            "loss: 0.008687020279467106 at epoch 96\n",
            "loss: 0.004251545295119286 at epoch 96\n",
            "loss: 0.006198185030370951 at epoch 96\n",
            "loss: 0.005847287364304066 at epoch 96\n",
            "loss: 0.021062085404992104 at epoch 96\n",
            "loss: 0.009114759042859077 at epoch 96\n",
            "loss: 0.004007483366876841 at epoch 96\n",
            "loss: 0.011376874521374702 at epoch 96\n",
            "loss: 0.026156213134527206 at epoch 96\n",
            "loss: 0.04333721846342087 at epoch 96\n",
            "loss: 0.027183223515748978 at epoch 96\n",
            "loss: 0.009521130472421646 at epoch 96\n",
            "loss: 0.00814393162727356 at epoch 96\n",
            "loss: 0.0073579768650233746 at epoch 96\n",
            "loss: 0.01129838079214096 at epoch 96\n",
            "loss: 0.0066097211092710495 at epoch 96\n",
            "loss: 0.003864044789224863 at epoch 96\n",
            "loss: 0.014106232672929764 at epoch 96\n",
            "loss: 0.023456580936908722 at epoch 96\n",
            "loss: 0.01697937399148941 at epoch 96\n",
            "loss: 0.0024725408293306828 at epoch 96\n",
            "loss: 0.019068516790866852 at epoch 96\n",
            "loss: 0.05848689749836922 at epoch 96\n",
            "loss: 0.0634554922580719 at epoch 96\n",
            "loss: 0.04472570866346359 at epoch 96\n",
            "loss: 0.005510448478162289 at epoch 96\n",
            "loss: 0.024609794840216637 at epoch 96\n",
            "loss: 0.06211228296160698 at epoch 96\n",
            "loss: 0.06210268288850784 at epoch 96\n",
            "loss: 0.02671847492456436 at epoch 96\n",
            "loss: 0.013965290039777756 at epoch 96\n",
            "loss: 0.013091472908854485 at epoch 96\n",
            "loss: 0.03201824799180031 at epoch 96\n",
            "loss: 0.04778236895799637 at epoch 96\n",
            "loss: 0.11118447035551071 at epoch 97\n",
            "loss: 0.09028255194425583 at epoch 97\n",
            "loss: 0.07450740039348602 at epoch 97\n",
            "loss: 0.020425518974661827 at epoch 97\n",
            "loss: 0.028771452605724335 at epoch 97\n",
            "loss: 0.05173815041780472 at epoch 97\n",
            "loss: 0.06099216639995575 at epoch 97\n",
            "loss: 0.04120877757668495 at epoch 97\n",
            "loss: 0.028856493532657623 at epoch 97\n",
            "loss: 0.06599847972393036 at epoch 97\n",
            "loss: 0.08147432655096054 at epoch 97\n",
            "loss: 0.02279561385512352 at epoch 97\n",
            "loss: 0.026392005383968353 at epoch 97\n",
            "loss: 0.07114129513502121 at epoch 97\n",
            "loss: 0.07821232080459595 at epoch 97\n",
            "loss: 0.0344601534307003 at epoch 97\n",
            "loss: 0.011693721637129784 at epoch 97\n",
            "loss: 0.0936066210269928 at epoch 97\n",
            "loss: 0.24354799091815948 at epoch 97\n",
            "loss: 0.29816150665283203 at epoch 97\n",
            "loss: 0.15965163707733154 at epoch 97\n",
            "loss: 0.051384337246418 at epoch 97\n",
            "loss: 0.24150438606739044 at epoch 97\n",
            "loss: 0.2765771746635437 at epoch 97\n",
            "loss: 0.12146253883838654 at epoch 97\n",
            "loss: 0.009102719835937023 at epoch 97\n",
            "loss: 0.08461557328701019 at epoch 97\n",
            "loss: 0.15111473202705383 at epoch 97\n",
            "loss: 0.0802137479186058 at epoch 97\n",
            "loss: 0.02017839066684246 at epoch 97\n",
            "loss: 0.04524368792772293 at epoch 97\n",
            "loss: 0.08649511635303497 at epoch 97\n",
            "loss: 0.13425563275814056 at epoch 97\n",
            "loss: 0.0434650182723999 at epoch 97\n",
            "loss: 0.0695948600769043 at epoch 97\n",
            "loss: 0.05062536895275116 at epoch 97\n",
            "loss: 0.09455230087041855 at epoch 97\n",
            "loss: 0.1459922343492508 at epoch 97\n",
            "loss: 0.10990239679813385 at epoch 97\n",
            "loss: 0.14338381588459015 at epoch 97\n",
            "loss: 0.03058868646621704 at epoch 97\n",
            "loss: 0.12636449933052063 at epoch 97\n",
            "loss: 0.09702138602733612 at epoch 97\n",
            "loss: 0.08280960470438004 at epoch 97\n",
            "loss: 0.02177117019891739 at epoch 97\n",
            "loss: 0.10686026513576508 at epoch 97\n",
            "loss: 0.18235932290554047 at epoch 97\n",
            "loss: 0.11839959770441055 at epoch 97\n",
            "loss: 0.0602077879011631 at epoch 97\n",
            "loss: 0.021078241989016533 at epoch 97\n",
            "loss: 0.03680284321308136 at epoch 97\n",
            "loss: 0.049511607736349106 at epoch 97\n",
            "loss: 0.024830428883433342 at epoch 97\n",
            "loss: 0.01928800530731678 at epoch 97\n",
            "loss: 0.03155713155865669 at epoch 97\n",
            "loss: 0.06260811537504196 at epoch 97\n",
            "loss: 0.032888587564229965 at epoch 97\n",
            "loss: 0.036985911428928375 at epoch 97\n",
            "loss: 0.012484133243560791 at epoch 97\n",
            "loss: 0.03164924308657646 at epoch 97\n",
            "loss: 0.027336977422237396 at epoch 97\n",
            "loss: 0.029557595029473305 at epoch 97\n",
            "loss: 0.01814325712621212 at epoch 97\n",
            "loss: 0.022471318021416664 at epoch 97\n",
            "loss: 0.02252848446369171 at epoch 97\n",
            "loss: 0.04345988109707832 at epoch 97\n",
            "loss: 0.016298385336995125 at epoch 97\n",
            "loss: 0.04125438258051872 at epoch 97\n",
            "loss: 0.03397391363978386 at epoch 97\n",
            "loss: 0.06965405493974686 at epoch 97\n",
            "loss: 0.05806136503815651 at epoch 97\n",
            "loss: 0.07562443614006042 at epoch 97\n",
            "loss: 0.03769645094871521 at epoch 97\n",
            "loss: 0.04728247597813606 at epoch 97\n",
            "loss: 0.1095600426197052 at epoch 97\n",
            "loss: 0.13374951481819153 at epoch 97\n",
            "loss: 0.12106389552354813 at epoch 97\n",
            "loss: 0.03313696011900902 at epoch 97\n",
            "loss: 0.11846573650836945 at epoch 97\n",
            "loss: 0.31650012731552124 at epoch 98\n",
            "loss: 0.27475425601005554 at epoch 98\n",
            "loss: 0.2885240316390991 at epoch 98\n",
            "loss: 0.04151639714837074 at epoch 98\n",
            "loss: 0.20404359698295593 at epoch 98\n",
            "loss: 0.14576900005340576 at epoch 98\n",
            "loss: 0.12790462374687195 at epoch 98\n",
            "loss: 0.2510877251625061 at epoch 98\n",
            "loss: 0.18709281086921692 at epoch 98\n",
            "loss: 0.20456251502037048 at epoch 98\n",
            "loss: 0.046561673283576965 at epoch 98\n",
            "loss: 0.2452540099620819 at epoch 98\n",
            "loss: 0.3091813027858734 at epoch 98\n",
            "loss: 0.32952481508255005 at epoch 98\n",
            "loss: 0.15434126555919647 at epoch 98\n",
            "loss: 0.11303368210792542 at epoch 98\n",
            "loss: 0.18767838180065155 at epoch 98\n",
            "loss: 0.3977373242378235 at epoch 98\n",
            "loss: 0.3717624545097351 at epoch 98\n",
            "loss: 0.14447571337223053 at epoch 98\n",
            "loss: 0.23479975759983063 at epoch 98\n",
            "loss: 0.38359031081199646 at epoch 98\n",
            "loss: 0.3717484176158905 at epoch 98\n",
            "loss: 0.09754077345132828 at epoch 98\n",
            "loss: 0.22334013879299164 at epoch 98\n",
            "loss: 0.1280331015586853 at epoch 98\n",
            "loss: 0.21705926954746246 at epoch 98\n",
            "loss: 0.08543995767831802 at epoch 98\n",
            "loss: 0.24073970317840576 at epoch 98\n",
            "loss: 0.07780081778764725 at epoch 98\n",
            "loss: 0.138352632522583 at epoch 98\n",
            "loss: 0.1448066085577011 at epoch 98\n",
            "loss: 0.16741494834423065 at epoch 98\n",
            "loss: 0.1276569366455078 at epoch 98\n",
            "loss: 0.1974898725748062 at epoch 98\n",
            "loss: 0.16884055733680725 at epoch 98\n",
            "loss: 0.4422574043273926 at epoch 98\n",
            "loss: 0.03866684064269066 at epoch 98\n",
            "loss: 0.3213540315628052 at epoch 98\n",
            "loss: 0.08430250734090805 at epoch 98\n",
            "loss: 0.2680809497833252 at epoch 98\n",
            "loss: 0.12292084842920303 at epoch 98\n",
            "loss: 0.2414095103740692 at epoch 98\n",
            "loss: 0.0406455434858799 at epoch 98\n",
            "loss: 0.21810467541217804 at epoch 98\n",
            "loss: 0.39979857206344604 at epoch 98\n",
            "loss: 0.19644887745380402 at epoch 98\n",
            "loss: 0.1345193088054657 at epoch 98\n",
            "loss: 0.23414182662963867 at epoch 98\n",
            "loss: 0.8971958160400391 at epoch 98\n",
            "loss: 1.454139232635498 at epoch 98\n",
            "loss: 1.0085409879684448 at epoch 98\n",
            "loss: 0.5218418836593628 at epoch 98\n",
            "loss: 0.07057007402181625 at epoch 98\n",
            "loss: 0.4378197491168976 at epoch 98\n",
            "loss: 0.31156685948371887 at epoch 98\n",
            "loss: 0.280211478471756 at epoch 98\n",
            "loss: 0.2077607959508896 at epoch 98\n",
            "loss: 0.31230223178863525 at epoch 98\n",
            "loss: 0.40196722745895386 at epoch 98\n",
            "loss: 0.23619318008422852 at epoch 98\n",
            "loss: 0.13455574214458466 at epoch 98\n",
            "loss: 0.05862843245267868 at epoch 98\n",
            "loss: 0.2065684050321579 at epoch 98\n",
            "loss: 0.08991104364395142 at epoch 98\n",
            "loss: 0.07060101628303528 at epoch 98\n",
            "loss: 0.08362539112567902 at epoch 98\n",
            "loss: 0.14664186537265778 at epoch 98\n",
            "loss: 0.07297472655773163 at epoch 98\n",
            "loss: 0.056323520839214325 at epoch 98\n",
            "loss: 0.04255791753530502 at epoch 98\n",
            "loss: 0.06427465379238129 at epoch 98\n",
            "loss: 0.03481072187423706 at epoch 98\n",
            "loss: 0.05069185048341751 at epoch 98\n",
            "loss: 0.042052678763866425 at epoch 98\n",
            "loss: 0.13185209035873413 at epoch 98\n",
            "loss: 0.047313667833805084 at epoch 98\n",
            "loss: 0.0866265520453453 at epoch 98\n",
            "loss: 0.013285519555211067 at epoch 98\n",
            "loss: 0.03615538775920868 at epoch 99\n",
            "loss: 0.013026364147663116 at epoch 99\n",
            "loss: 0.03631247207522392 at epoch 99\n",
            "loss: 0.013602816499769688 at epoch 99\n",
            "loss: 0.017235970124602318 at epoch 99\n",
            "loss: 0.015465761534869671 at epoch 99\n",
            "loss: 0.023103393614292145 at epoch 99\n",
            "loss: 0.022689638659358025 at epoch 99\n",
            "loss: 0.014527441002428532 at epoch 99\n",
            "loss: 0.011042820289731026 at epoch 99\n",
            "loss: 0.01776953972876072 at epoch 99\n",
            "loss: 0.01570615917444229 at epoch 99\n",
            "loss: 0.02603481151163578 at epoch 99\n",
            "loss: 0.007861313410103321 at epoch 99\n",
            "loss: 0.014414445497095585 at epoch 99\n",
            "loss: 0.018699930980801582 at epoch 99\n",
            "loss: 0.017461594194173813 at epoch 99\n",
            "loss: 0.020697107538580894 at epoch 99\n",
            "loss: 0.01588353142142296 at epoch 99\n",
            "loss: 0.025381125509738922 at epoch 99\n",
            "loss: 0.035997532308101654 at epoch 99\n",
            "loss: 0.04993603006005287 at epoch 99\n",
            "loss: 0.042262107133865356 at epoch 99\n",
            "loss: 0.009891706518828869 at epoch 99\n",
            "loss: 0.02759508602321148 at epoch 99\n",
            "loss: 0.058692336082458496 at epoch 99\n",
            "loss: 0.09021860361099243 at epoch 99\n",
            "loss: 0.0425458699464798 at epoch 99\n",
            "loss: 0.014042124152183533 at epoch 99\n",
            "loss: 0.03146782144904137 at epoch 99\n",
            "loss: 0.0659036934375763 at epoch 99\n",
            "loss: 0.06314964592456818 at epoch 99\n",
            "loss: 0.017778605222702026 at epoch 99\n",
            "loss: 0.029736299067735672 at epoch 99\n",
            "loss: 0.06741113215684891 at epoch 99\n",
            "loss: 0.06268763542175293 at epoch 99\n",
            "loss: 0.04269920289516449 at epoch 99\n",
            "loss: 0.034332524985075 at epoch 99\n",
            "loss: 0.011896305717527866 at epoch 99\n",
            "loss: 0.019453272223472595 at epoch 99\n",
            "loss: 0.011113634333014488 at epoch 99\n",
            "loss: 0.03249572589993477 at epoch 99\n",
            "loss: 0.007748034782707691 at epoch 99\n",
            "loss: 0.01904287189245224 at epoch 99\n",
            "loss: 0.017169712111353874 at epoch 99\n",
            "loss: 0.016594476997852325 at epoch 99\n",
            "loss: 0.010225027799606323 at epoch 99\n",
            "loss: 0.006717259995639324 at epoch 99\n",
            "loss: 0.01905440166592598 at epoch 99\n",
            "loss: 0.018712785094976425 at epoch 99\n",
            "loss: 0.04147513210773468 at epoch 99\n",
            "loss: 0.027477623894810677 at epoch 99\n",
            "loss: 0.03161884844303131 at epoch 99\n",
            "loss: 0.017520476132631302 at epoch 99\n",
            "loss: 0.05042976886034012 at epoch 99\n",
            "loss: 0.05727877467870712 at epoch 99\n",
            "loss: 0.059416674077510834 at epoch 99\n",
            "loss: 0.04576930031180382 at epoch 99\n",
            "loss: 0.013723189011216164 at epoch 99\n",
            "loss: 0.03845669701695442 at epoch 99\n",
            "loss: 0.0415182001888752 at epoch 99\n",
            "loss: 0.03792762756347656 at epoch 99\n",
            "loss: 0.0067184437066316605 at epoch 99\n",
            "loss: 0.027156956493854523 at epoch 99\n",
            "loss: 0.04034130275249481 at epoch 99\n",
            "loss: 0.04566860944032669 at epoch 99\n",
            "loss: 0.010114308446645737 at epoch 99\n",
            "loss: 0.022977957502007484 at epoch 99\n",
            "loss: 0.04414253681898117 at epoch 99\n",
            "loss: 0.010478959418833256 at epoch 99\n",
            "loss: 0.03938199207186699 at epoch 99\n",
            "loss: 0.011599669232964516 at epoch 99\n",
            "loss: 0.03663913905620575 at epoch 99\n",
            "loss: 0.006021005101501942 at epoch 99\n",
            "loss: 0.02914147451519966 at epoch 99\n",
            "loss: 0.044363174587488174 at epoch 99\n",
            "loss: 0.06609746068716049 at epoch 99\n",
            "loss: 0.06665714085102081 at epoch 99\n",
            "loss: 0.009252525866031647 at epoch 99\n",
            "\n",
            "=== Iteration 0: Pure Assignment ===\n",
            "faculty_vectors: [[0.48668794 0.14417712 0.14849461 0.13841142 0.08222891]\n",
            " [0.04525466 0.13832002 0.64810297 0.05331816 0.11500418]\n",
            " [0.04525466 0.13832002 0.64810297 0.05331816 0.11500418]\n",
            " ...\n",
            " [0.04525466 0.13832002 0.64810297 0.05331816 0.11500418]\n",
            " [0.11833262 0.0696857  0.09949829 0.09886809 0.6136153 ]\n",
            " [0.11833262 0.0696857  0.09949829 0.09886809 0.6136153 ]]\n",
            "student_features: [[89.88519482 41.54846766 74.22694197 59.58964194 45.58562975]\n",
            " [83.02115884 83.11317284 92.85893252 57.17176261 74.44206429]\n",
            " [73.59603054 64.81041949 99.97542431 48.96063771 89.61899054]\n",
            " ...\n",
            " [93.75800406 97.92221681 96.90743693 62.75254926 79.06923064]\n",
            " [77.69460062 56.3177577  46.91899653 91.0126447  95.41576181]\n",
            " [55.34297034 57.68921809 61.11884162 90.67474645 79.22306035]]\n",
            "\n",
            "=== Iteration 1: Student Learning ===\n",
            "loss: 1.759731 at epoch 0 at applicants training\n",
            "loss: 1.734079 at epoch 1 at applicants training\n",
            "loss: 1.709279 at epoch 2 at applicants training\n",
            "loss: 1.687867 at epoch 3 at applicants training\n",
            "loss: 1.670068 at epoch 4 at applicants training\n",
            "loss: 1.655210 at epoch 5 at applicants training\n",
            "loss: 1.644576 at epoch 6 at applicants training\n",
            "loss: 1.637944 at epoch 7 at applicants training\n",
            "loss: 1.633761 at epoch 8 at applicants training\n",
            "loss: 1.630973 at epoch 9 at applicants training\n",
            "loss: 1.629176 at epoch 10 at applicants training\n",
            "loss: 1.628123 at epoch 11 at applicants training\n",
            "loss: 1.627502 at epoch 12 at applicants training\n",
            "loss: 1.627088 at epoch 13 at applicants training\n",
            "loss: 1.626752 at epoch 14 at applicants training\n",
            "loss: 1.626416 at epoch 15 at applicants training\n",
            "loss: 1.626039 at epoch 16 at applicants training\n",
            "loss: 1.625615 at epoch 17 at applicants training\n",
            "loss: 1.625156 at epoch 18 at applicants training\n",
            "loss: 1.624694 at epoch 19 at applicants training\n",
            "loss: 1.624296 at epoch 20 at applicants training\n",
            "loss: 1.624042 at epoch 21 at applicants training\n",
            "loss: 1.623966 at epoch 22 at applicants training\n",
            "loss: 1.624025 at epoch 23 at applicants training\n",
            "loss: 1.624116 at epoch 24 at applicants training\n",
            "loss: 1.624119 at epoch 25 at applicants training\n",
            "loss: 1.623960 at epoch 26 at applicants training\n",
            "loss: 1.623654 at epoch 27 at applicants training\n",
            "loss: 1.623284 at epoch 28 at applicants training\n",
            "loss: 1.622935 at epoch 29 at applicants training\n",
            "loss: 1.622653 at epoch 30 at applicants training\n",
            "loss: 1.622431 at epoch 31 at applicants training\n",
            "loss: 1.622235 at epoch 32 at applicants training\n",
            "loss: 1.622025 at epoch 33 at applicants training\n",
            "loss: 1.621772 at epoch 34 at applicants training\n",
            "loss: 1.621460 at epoch 35 at applicants training\n",
            "loss: 1.621091 at epoch 36 at applicants training\n",
            "loss: 1.620687 at epoch 37 at applicants training\n",
            "loss: 1.620281 at epoch 38 at applicants training\n",
            "loss: 1.619893 at epoch 39 at applicants training\n",
            "loss: 1.619522 at epoch 40 at applicants training\n",
            "loss: 1.619130 at epoch 41 at applicants training\n",
            "loss: 1.618666 at epoch 42 at applicants training\n",
            "loss: 1.618086 at epoch 43 at applicants training\n",
            "loss: 1.617359 at epoch 44 at applicants training\n",
            "loss: 1.616483 at epoch 45 at applicants training\n",
            "loss: 1.615458 at epoch 46 at applicants training\n",
            "loss: 1.614301 at epoch 47 at applicants training\n",
            "loss: 1.613006 at epoch 48 at applicants training\n",
            "loss: 1.611604 at epoch 49 at applicants training\n",
            "loss: 1.610143 at epoch 50 at applicants training\n",
            "loss: 1.608645 at epoch 51 at applicants training\n",
            "loss: 1.607125 at epoch 52 at applicants training\n",
            "loss: 1.605619 at epoch 53 at applicants training\n",
            "loss: 1.604173 at epoch 54 at applicants training\n",
            "loss: 1.602820 at epoch 55 at applicants training\n",
            "loss: 1.601568 at epoch 56 at applicants training\n",
            "loss: 1.600428 at epoch 57 at applicants training\n",
            "loss: 1.599389 at epoch 58 at applicants training\n",
            "loss: 1.598422 at epoch 59 at applicants training\n",
            "loss: 1.597509 at epoch 60 at applicants training\n",
            "loss: 1.596656 at epoch 61 at applicants training\n",
            "loss: 1.595852 at epoch 62 at applicants training\n",
            "loss: 1.595085 at epoch 63 at applicants training\n",
            "loss: 1.594334 at epoch 64 at applicants training\n",
            "loss: 1.593609 at epoch 65 at applicants training\n",
            "loss: 1.592906 at epoch 66 at applicants training\n",
            "loss: 1.592234 at epoch 67 at applicants training\n",
            "loss: 1.591604 at epoch 68 at applicants training\n",
            "loss: 1.591016 at epoch 69 at applicants training\n",
            "loss: 1.590474 at epoch 70 at applicants training\n",
            "loss: 1.589987 at epoch 71 at applicants training\n",
            "loss: 1.589552 at epoch 72 at applicants training\n",
            "loss: 1.589164 at epoch 73 at applicants training\n",
            "loss: 1.588835 at epoch 74 at applicants training\n",
            "loss: 1.588558 at epoch 75 at applicants training\n",
            "loss: 1.588336 at epoch 76 at applicants training\n",
            "loss: 1.588156 at epoch 77 at applicants training\n",
            "loss: 1.588004 at epoch 78 at applicants training\n",
            "loss: 1.587875 at epoch 79 at applicants training\n",
            "loss: 1.587759 at epoch 80 at applicants training\n",
            "loss: 1.587664 at epoch 81 at applicants training\n",
            "loss: 1.587581 at epoch 82 at applicants training\n",
            "loss: 1.587503 at epoch 83 at applicants training\n",
            "loss: 1.587428 at epoch 84 at applicants training\n",
            "loss: 1.587355 at epoch 85 at applicants training\n",
            "loss: 1.587282 at epoch 86 at applicants training\n",
            "loss: 1.587208 at epoch 87 at applicants training\n",
            "loss: 1.587132 at epoch 88 at applicants training\n",
            "loss: 1.587054 at epoch 89 at applicants training\n",
            "loss: 1.586972 at epoch 90 at applicants training\n",
            "loss: 1.586888 at epoch 91 at applicants training\n",
            "loss: 1.586799 at epoch 92 at applicants training\n",
            "loss: 1.586705 at epoch 93 at applicants training\n",
            "loss: 1.586605 at epoch 94 at applicants training\n",
            "loss: 1.586498 at epoch 95 at applicants training\n",
            "loss: 1.586382 at epoch 96 at applicants training\n",
            "loss: 1.586258 at epoch 97 at applicants training\n",
            "loss: 1.586131 at epoch 98 at applicants training\n",
            "loss: 1.586004 at epoch 99 at applicants training\n",
            "loss: 1.585869 at epoch 100 at applicants training\n",
            "loss: 1.585728 at epoch 101 at applicants training\n",
            "loss: 1.585582 at epoch 102 at applicants training\n",
            "loss: 1.585434 at epoch 103 at applicants training\n",
            "loss: 1.585291 at epoch 104 at applicants training\n",
            "loss: 1.585165 at epoch 105 at applicants training\n",
            "loss: 1.585051 at epoch 106 at applicants training\n",
            "loss: 1.584947 at epoch 107 at applicants training\n",
            "loss: 1.584866 at epoch 108 at applicants training\n",
            "loss: 1.584809 at epoch 109 at applicants training\n",
            "loss: 1.584767 at epoch 110 at applicants training\n",
            "loss: 1.584746 at epoch 111 at applicants training\n",
            "loss: 1.584730 at epoch 112 at applicants training\n",
            "loss: 1.584714 at epoch 113 at applicants training\n",
            "loss: 1.584696 at epoch 114 at applicants training\n",
            "loss: 1.584673 at epoch 115 at applicants training\n",
            "loss: 1.584644 at epoch 116 at applicants training\n",
            "loss: 1.584610 at epoch 117 at applicants training\n",
            "loss: 1.584573 at epoch 118 at applicants training\n",
            "loss: 1.584534 at epoch 119 at applicants training\n",
            "loss: 1.584495 at epoch 120 at applicants training\n",
            "loss: 1.584457 at epoch 121 at applicants training\n",
            "loss: 1.584426 at epoch 122 at applicants training\n",
            "loss: 1.584399 at epoch 123 at applicants training\n",
            "loss: 1.584372 at epoch 124 at applicants training\n",
            "loss: 1.584347 at epoch 125 at applicants training\n",
            "loss: 1.584330 at epoch 126 at applicants training\n",
            "loss: 1.584316 at epoch 127 at applicants training\n",
            "loss: 1.584304 at epoch 128 at applicants training\n",
            "loss: 1.584289 at epoch 129 at applicants training\n",
            "loss: 1.584273 at epoch 130 at applicants training\n",
            "loss: 1.584256 at epoch 131 at applicants training\n",
            "loss: 1.584238 at epoch 132 at applicants training\n",
            "loss: 1.584220 at epoch 133 at applicants training\n",
            "loss: 1.584201 at epoch 134 at applicants training\n",
            "loss: 1.584184 at epoch 135 at applicants training\n",
            "loss: 1.584167 at epoch 136 at applicants training\n",
            "loss: 1.584153 at epoch 137 at applicants training\n",
            "loss: 1.584139 at epoch 138 at applicants training\n",
            "loss: 1.584127 at epoch 139 at applicants training\n",
            "loss: 1.584116 at epoch 140 at applicants training\n",
            "loss: 1.584106 at epoch 141 at applicants training\n",
            "loss: 1.584096 at epoch 142 at applicants training\n",
            "loss: 1.584087 at epoch 143 at applicants training\n",
            "loss: 1.584077 at epoch 144 at applicants training\n",
            "loss: 1.584067 at epoch 145 at applicants training\n",
            "loss: 1.584056 at epoch 146 at applicants training\n",
            "loss: 1.584044 at epoch 147 at applicants training\n",
            "loss: 1.584032 at epoch 148 at applicants training\n",
            "loss: 1.584020 at epoch 149 at applicants training\n",
            "loss: 1.584008 at epoch 150 at applicants training\n",
            "loss: 1.583997 at epoch 151 at applicants training\n",
            "loss: 1.583986 at epoch 152 at applicants training\n",
            "loss: 1.583975 at epoch 153 at applicants training\n",
            "loss: 1.583964 at epoch 154 at applicants training\n",
            "loss: 1.583954 at epoch 155 at applicants training\n",
            "loss: 1.583943 at epoch 156 at applicants training\n",
            "loss: 1.583933 at epoch 157 at applicants training\n",
            "loss: 1.583923 at epoch 158 at applicants training\n",
            "loss: 1.583912 at epoch 159 at applicants training\n",
            "loss: 1.583900 at epoch 160 at applicants training\n",
            "loss: 1.583889 at epoch 161 at applicants training\n",
            "loss: 1.583877 at epoch 162 at applicants training\n",
            "loss: 1.583865 at epoch 163 at applicants training\n",
            "loss: 1.583853 at epoch 164 at applicants training\n",
            "loss: 1.583841 at epoch 165 at applicants training\n",
            "loss: 1.583829 at epoch 166 at applicants training\n",
            "loss: 1.583818 at epoch 167 at applicants training\n",
            "loss: 1.583806 at epoch 168 at applicants training\n",
            "loss: 1.583794 at epoch 169 at applicants training\n",
            "loss: 1.583782 at epoch 170 at applicants training\n",
            "loss: 1.583770 at epoch 171 at applicants training\n",
            "loss: 1.583757 at epoch 172 at applicants training\n",
            "loss: 1.583744 at epoch 173 at applicants training\n",
            "loss: 1.583730 at epoch 174 at applicants training\n",
            "loss: 1.583717 at epoch 175 at applicants training\n",
            "loss: 1.583702 at epoch 176 at applicants training\n",
            "loss: 1.583686 at epoch 177 at applicants training\n",
            "loss: 1.583670 at epoch 178 at applicants training\n",
            "loss: 1.583654 at epoch 179 at applicants training\n",
            "loss: 1.583639 at epoch 180 at applicants training\n",
            "loss: 1.583624 at epoch 181 at applicants training\n",
            "loss: 1.583610 at epoch 182 at applicants training\n",
            "loss: 1.583595 at epoch 183 at applicants training\n",
            "loss: 1.583579 at epoch 184 at applicants training\n",
            "loss: 1.583563 at epoch 185 at applicants training\n",
            "loss: 1.583546 at epoch 186 at applicants training\n",
            "loss: 1.583529 at epoch 187 at applicants training\n",
            "loss: 1.583511 at epoch 188 at applicants training\n",
            "loss: 1.583493 at epoch 189 at applicants training\n",
            "loss: 1.583475 at epoch 190 at applicants training\n",
            "loss: 1.583457 at epoch 191 at applicants training\n",
            "loss: 1.583439 at epoch 192 at applicants training\n",
            "loss: 1.583420 at epoch 193 at applicants training\n",
            "loss: 1.583401 at epoch 194 at applicants training\n",
            "loss: 1.583382 at epoch 195 at applicants training\n",
            "loss: 1.583362 at epoch 196 at applicants training\n",
            "loss: 1.583341 at epoch 197 at applicants training\n",
            "loss: 1.583320 at epoch 198 at applicants training\n",
            "loss: 1.583299 at epoch 199 at applicants training\n",
            "loss: 1.583277 at epoch 200 at applicants training\n",
            "loss: 1.583254 at epoch 201 at applicants training\n",
            "loss: 1.583231 at epoch 202 at applicants training\n",
            "loss: 1.583207 at epoch 203 at applicants training\n",
            "loss: 1.583183 at epoch 204 at applicants training\n",
            "loss: 1.583158 at epoch 205 at applicants training\n",
            "loss: 1.583132 at epoch 206 at applicants training\n",
            "loss: 1.583106 at epoch 207 at applicants training\n",
            "loss: 1.583079 at epoch 208 at applicants training\n",
            "loss: 1.583051 at epoch 209 at applicants training\n",
            "loss: 1.583023 at epoch 210 at applicants training\n",
            "loss: 1.582994 at epoch 211 at applicants training\n",
            "loss: 1.582964 at epoch 212 at applicants training\n",
            "loss: 1.582933 at epoch 213 at applicants training\n",
            "loss: 1.582901 at epoch 214 at applicants training\n",
            "loss: 1.582869 at epoch 215 at applicants training\n",
            "loss: 1.582836 at epoch 216 at applicants training\n",
            "loss: 1.582802 at epoch 217 at applicants training\n",
            "loss: 1.582767 at epoch 218 at applicants training\n",
            "loss: 1.582730 at epoch 219 at applicants training\n",
            "loss: 1.582693 at epoch 220 at applicants training\n",
            "loss: 1.582654 at epoch 221 at applicants training\n",
            "loss: 1.582615 at epoch 222 at applicants training\n",
            "loss: 1.582574 at epoch 223 at applicants training\n",
            "loss: 1.582532 at epoch 224 at applicants training\n",
            "loss: 1.582489 at epoch 225 at applicants training\n",
            "loss: 1.582445 at epoch 226 at applicants training\n",
            "loss: 1.582400 at epoch 227 at applicants training\n",
            "loss: 1.582353 at epoch 228 at applicants training\n",
            "loss: 1.582304 at epoch 229 at applicants training\n",
            "loss: 1.582254 at epoch 230 at applicants training\n",
            "loss: 1.582202 at epoch 231 at applicants training\n",
            "loss: 1.582149 at epoch 232 at applicants training\n",
            "loss: 1.582094 at epoch 233 at applicants training\n",
            "loss: 1.582037 at epoch 234 at applicants training\n",
            "loss: 1.581977 at epoch 235 at applicants training\n",
            "loss: 1.581914 at epoch 236 at applicants training\n",
            "loss: 1.581847 at epoch 237 at applicants training\n",
            "loss: 1.581774 at epoch 238 at applicants training\n",
            "loss: 1.581692 at epoch 239 at applicants training\n",
            "loss: 1.581596 at epoch 240 at applicants training\n",
            "loss: 1.581478 at epoch 241 at applicants training\n",
            "loss: 1.581329 at epoch 242 at applicants training\n",
            "loss: 1.581140 at epoch 243 at applicants training\n",
            "loss: 1.580901 at epoch 244 at applicants training\n",
            "loss: 1.580606 at epoch 245 at applicants training\n",
            "loss: 1.580261 at epoch 246 at applicants training\n",
            "loss: 1.579877 at epoch 247 at applicants training\n",
            "loss: 1.579480 at epoch 248 at applicants training\n",
            "loss: 1.579106 at epoch 249 at applicants training\n",
            "loss: 1.578778 at epoch 250 at applicants training\n",
            "loss: 1.578482 at epoch 251 at applicants training\n",
            "loss: 1.578025 at epoch 252 at applicants training\n",
            "loss: 1.576814 at epoch 253 at applicants training\n",
            "loss: 1.573128 at epoch 254 at applicants training\n",
            "loss: 1.560521 at epoch 255 at applicants training\n",
            "loss: 1.519346 at epoch 256 at applicants training\n",
            "loss: 1.451575 at epoch 257 at applicants training\n",
            "loss: 1.464426 at epoch 258 at applicants training\n",
            "loss: 1.465025 at epoch 259 at applicants training\n",
            "loss: 1.420878 at epoch 260 at applicants training\n",
            "loss: 1.401430 at epoch 261 at applicants training\n",
            "loss: 1.411267 at epoch 262 at applicants training\n",
            "loss: 1.395214 at epoch 263 at applicants training\n",
            "loss: 1.364245 at epoch 264 at applicants training\n",
            "loss: 1.356019 at epoch 265 at applicants training\n",
            "loss: 1.363981 at epoch 266 at applicants training\n",
            "loss: 1.350926 at epoch 267 at applicants training\n",
            "loss: 1.332700 at epoch 268 at applicants training\n",
            "loss: 1.333238 at epoch 269 at applicants training\n",
            "loss: 1.337069 at epoch 270 at applicants training\n",
            "loss: 1.330007 at epoch 271 at applicants training\n",
            "loss: 1.320199 at epoch 272 at applicants training\n",
            "loss: 1.321586 at epoch 273 at applicants training\n",
            "loss: 1.325458 at epoch 274 at applicants training\n",
            "loss: 1.321547 at epoch 275 at applicants training\n",
            "loss: 1.316176 at epoch 276 at applicants training\n",
            "loss: 1.317582 at epoch 277 at applicants training\n",
            "loss: 1.320360 at epoch 278 at applicants training\n",
            "loss: 1.318310 at epoch 279 at applicants training\n",
            "loss: 1.314847 at epoch 280 at applicants training\n",
            "loss: 1.315053 at epoch 281 at applicants training\n",
            "loss: 1.316804 at epoch 282 at applicants training\n",
            "loss: 1.316074 at epoch 283 at applicants training\n",
            "loss: 1.313658 at epoch 284 at applicants training\n",
            "loss: 1.312892 at epoch 285 at applicants training\n",
            "loss: 1.313951 at epoch 286 at applicants training\n",
            "loss: 1.313980 at epoch 287 at applicants training\n",
            "loss: 1.312426 at epoch 288 at applicants training\n",
            "loss: 1.311460 at epoch 289 at applicants training\n",
            "loss: 1.311922 at epoch 290 at applicants training\n",
            "loss: 1.312232 at epoch 291 at applicants training\n",
            "loss: 1.311445 at epoch 292 at applicants training\n",
            "loss: 1.310559 at epoch 293 at applicants training\n",
            "loss: 1.310608 at epoch 294 at applicants training\n",
            "loss: 1.310965 at epoch 295 at applicants training\n",
            "loss: 1.310657 at epoch 296 at applicants training\n",
            "loss: 1.310007 at epoch 297 at applicants training\n",
            "loss: 1.309854 at epoch 298 at applicants training\n",
            "loss: 1.310097 at epoch 299 at applicants training\n",
            "loss: 1.310045 at epoch 300 at applicants training\n",
            "loss: 1.309628 at epoch 301 at applicants training\n",
            "loss: 1.309378 at epoch 302 at applicants training\n",
            "loss: 1.309476 at epoch 303 at applicants training\n",
            "loss: 1.309518 at epoch 304 at applicants training\n",
            "loss: 1.309275 at epoch 305 at applicants training\n",
            "loss: 1.309022 at epoch 306 at applicants training\n",
            "loss: 1.309003 at epoch 307 at applicants training\n",
            "loss: 1.309045 at epoch 308 at applicants training\n",
            "loss: 1.308916 at epoch 309 at applicants training\n",
            "loss: 1.308703 at epoch 310 at applicants training\n",
            "loss: 1.308615 at epoch 311 at applicants training\n",
            "loss: 1.308626 at epoch 312 at applicants training\n",
            "loss: 1.308560 at epoch 313 at applicants training\n",
            "loss: 1.308401 at epoch 314 at applicants training\n",
            "loss: 1.308290 at epoch 315 at applicants training\n",
            "loss: 1.308268 at epoch 316 at applicants training\n",
            "loss: 1.308227 at epoch 317 at applicants training\n",
            "loss: 1.308115 at epoch 318 at applicants training\n",
            "loss: 1.308007 at epoch 319 at applicants training\n",
            "loss: 1.307961 at epoch 320 at applicants training\n",
            "loss: 1.307929 at epoch 321 at applicants training\n",
            "loss: 1.307851 at epoch 322 at applicants training\n",
            "loss: 1.307757 at epoch 323 at applicants training\n",
            "loss: 1.307700 at epoch 324 at applicants training\n",
            "loss: 1.307665 at epoch 325 at applicants training\n",
            "loss: 1.307607 at epoch 326 at applicants training\n",
            "loss: 1.307528 at epoch 327 at applicants training\n",
            "loss: 1.307466 at epoch 328 at applicants training\n",
            "loss: 1.307426 at epoch 329 at applicants training\n",
            "loss: 1.307379 at epoch 330 at applicants training\n",
            "loss: 1.307314 at epoch 331 at applicants training\n",
            "loss: 1.307253 at epoch 332 at applicants training\n",
            "loss: 1.307210 at epoch 333 at applicants training\n",
            "loss: 1.307165 at epoch 334 at applicants training\n",
            "loss: 1.307109 at epoch 335 at applicants training\n",
            "loss: 1.307052 at epoch 336 at applicants training\n",
            "loss: 1.307006 at epoch 337 at applicants training\n",
            "loss: 1.306964 at epoch 338 at applicants training\n",
            "loss: 1.306916 at epoch 339 at applicants training\n",
            "loss: 1.306865 at epoch 340 at applicants training\n",
            "loss: 1.306820 at epoch 341 at applicants training\n",
            "loss: 1.306780 at epoch 342 at applicants training\n",
            "loss: 1.306737 at epoch 343 at applicants training\n",
            "loss: 1.306691 at epoch 344 at applicants training\n",
            "loss: 1.306649 at epoch 345 at applicants training\n",
            "loss: 1.306612 at epoch 346 at applicants training\n",
            "loss: 1.306573 at epoch 347 at applicants training\n",
            "loss: 1.306532 at epoch 348 at applicants training\n",
            "loss: 1.306493 at epoch 349 at applicants training\n",
            "loss: 1.306456 at epoch 350 at applicants training\n",
            "loss: 1.306419 at epoch 351 at applicants training\n",
            "loss: 1.306381 at epoch 352 at applicants training\n",
            "loss: 1.306344 at epoch 353 at applicants training\n",
            "loss: 1.306308 at epoch 354 at applicants training\n",
            "loss: 1.306274 at epoch 355 at applicants training\n",
            "loss: 1.306239 at epoch 356 at applicants training\n",
            "loss: 1.306204 at epoch 357 at applicants training\n",
            "loss: 1.306171 at epoch 358 at applicants training\n",
            "loss: 1.306138 at epoch 359 at applicants training\n",
            "loss: 1.306105 at epoch 360 at applicants training\n",
            "loss: 1.306073 at epoch 361 at applicants training\n",
            "loss: 1.306041 at epoch 362 at applicants training\n",
            "loss: 1.306009 at epoch 363 at applicants training\n",
            "loss: 1.305978 at epoch 364 at applicants training\n",
            "loss: 1.305946 at epoch 365 at applicants training\n",
            "loss: 1.305915 at epoch 366 at applicants training\n",
            "loss: 1.305885 at epoch 367 at applicants training\n",
            "loss: 1.305855 at epoch 368 at applicants training\n",
            "loss: 1.305825 at epoch 369 at applicants training\n",
            "loss: 1.305795 at epoch 370 at applicants training\n",
            "loss: 1.305766 at epoch 371 at applicants training\n",
            "loss: 1.305738 at epoch 372 at applicants training\n",
            "loss: 1.305709 at epoch 373 at applicants training\n",
            "loss: 1.305682 at epoch 374 at applicants training\n",
            "loss: 1.305654 at epoch 375 at applicants training\n",
            "loss: 1.305627 at epoch 376 at applicants training\n",
            "loss: 1.305600 at epoch 377 at applicants training\n",
            "loss: 1.305573 at epoch 378 at applicants training\n",
            "loss: 1.305546 at epoch 379 at applicants training\n",
            "loss: 1.305520 at epoch 380 at applicants training\n",
            "loss: 1.305494 at epoch 381 at applicants training\n",
            "loss: 1.305468 at epoch 382 at applicants training\n",
            "loss: 1.305442 at epoch 383 at applicants training\n",
            "loss: 1.305417 at epoch 384 at applicants training\n",
            "loss: 1.305392 at epoch 385 at applicants training\n",
            "loss: 1.305367 at epoch 386 at applicants training\n",
            "loss: 1.305342 at epoch 387 at applicants training\n",
            "loss: 1.305318 at epoch 388 at applicants training\n",
            "loss: 1.305293 at epoch 389 at applicants training\n",
            "loss: 1.305269 at epoch 390 at applicants training\n",
            "loss: 1.305245 at epoch 391 at applicants training\n",
            "loss: 1.305222 at epoch 392 at applicants training\n",
            "loss: 1.305198 at epoch 393 at applicants training\n",
            "loss: 1.305175 at epoch 394 at applicants training\n",
            "loss: 1.305152 at epoch 395 at applicants training\n",
            "loss: 1.305129 at epoch 396 at applicants training\n",
            "loss: 1.305106 at epoch 397 at applicants training\n",
            "loss: 1.305084 at epoch 398 at applicants training\n",
            "loss: 1.305061 at epoch 399 at applicants training\n",
            "loss: 1.305039 at epoch 400 at applicants training\n",
            "loss: 1.305016 at epoch 401 at applicants training\n",
            "loss: 1.304994 at epoch 402 at applicants training\n",
            "loss: 1.304972 at epoch 403 at applicants training\n",
            "loss: 1.304950 at epoch 404 at applicants training\n",
            "loss: 1.304929 at epoch 405 at applicants training\n",
            "loss: 1.304907 at epoch 406 at applicants training\n",
            "loss: 1.304886 at epoch 407 at applicants training\n",
            "loss: 1.304864 at epoch 408 at applicants training\n",
            "loss: 1.304843 at epoch 409 at applicants training\n",
            "loss: 1.304822 at epoch 410 at applicants training\n",
            "loss: 1.304801 at epoch 411 at applicants training\n",
            "loss: 1.304781 at epoch 412 at applicants training\n",
            "loss: 1.304760 at epoch 413 at applicants training\n",
            "loss: 1.304740 at epoch 414 at applicants training\n",
            "loss: 1.304719 at epoch 415 at applicants training\n",
            "loss: 1.304699 at epoch 416 at applicants training\n",
            "loss: 1.304679 at epoch 417 at applicants training\n",
            "loss: 1.304659 at epoch 418 at applicants training\n",
            "loss: 1.304639 at epoch 419 at applicants training\n",
            "loss: 1.304620 at epoch 420 at applicants training\n",
            "loss: 1.304600 at epoch 421 at applicants training\n",
            "loss: 1.304580 at epoch 422 at applicants training\n",
            "loss: 1.304561 at epoch 423 at applicants training\n",
            "loss: 1.304542 at epoch 424 at applicants training\n",
            "loss: 1.304523 at epoch 425 at applicants training\n",
            "loss: 1.304504 at epoch 426 at applicants training\n",
            "loss: 1.304485 at epoch 427 at applicants training\n",
            "loss: 1.304466 at epoch 428 at applicants training\n",
            "loss: 1.304447 at epoch 429 at applicants training\n",
            "loss: 1.304428 at epoch 430 at applicants training\n",
            "loss: 1.304410 at epoch 431 at applicants training\n",
            "loss: 1.304391 at epoch 432 at applicants training\n",
            "loss: 1.304373 at epoch 433 at applicants training\n",
            "loss: 1.304355 at epoch 434 at applicants training\n",
            "loss: 1.304337 at epoch 435 at applicants training\n",
            "loss: 1.304319 at epoch 436 at applicants training\n",
            "loss: 1.304301 at epoch 437 at applicants training\n",
            "loss: 1.304283 at epoch 438 at applicants training\n",
            "loss: 1.304265 at epoch 439 at applicants training\n",
            "loss: 1.304247 at epoch 440 at applicants training\n",
            "loss: 1.304230 at epoch 441 at applicants training\n",
            "loss: 1.304212 at epoch 442 at applicants training\n",
            "loss: 1.304195 at epoch 443 at applicants training\n",
            "loss: 1.304178 at epoch 444 at applicants training\n",
            "loss: 1.304160 at epoch 445 at applicants training\n",
            "loss: 1.304143 at epoch 446 at applicants training\n",
            "loss: 1.304126 at epoch 447 at applicants training\n",
            "loss: 1.304109 at epoch 448 at applicants training\n",
            "loss: 1.304092 at epoch 449 at applicants training\n",
            "loss: 1.304075 at epoch 450 at applicants training\n",
            "loss: 1.304059 at epoch 451 at applicants training\n",
            "loss: 1.304042 at epoch 452 at applicants training\n",
            "loss: 1.304026 at epoch 453 at applicants training\n",
            "loss: 1.304009 at epoch 454 at applicants training\n",
            "loss: 1.303993 at epoch 455 at applicants training\n",
            "loss: 1.303976 at epoch 456 at applicants training\n",
            "loss: 1.303960 at epoch 457 at applicants training\n",
            "loss: 1.303944 at epoch 458 at applicants training\n",
            "loss: 1.303928 at epoch 459 at applicants training\n",
            "loss: 1.303912 at epoch 460 at applicants training\n",
            "loss: 1.303896 at epoch 461 at applicants training\n",
            "loss: 1.303881 at epoch 462 at applicants training\n",
            "loss: 1.303865 at epoch 463 at applicants training\n",
            "loss: 1.303849 at epoch 464 at applicants training\n",
            "loss: 1.303833 at epoch 465 at applicants training\n",
            "loss: 1.303818 at epoch 466 at applicants training\n",
            "loss: 1.303802 at epoch 467 at applicants training\n",
            "loss: 1.303787 at epoch 468 at applicants training\n",
            "loss: 1.303772 at epoch 469 at applicants training\n",
            "loss: 1.303756 at epoch 470 at applicants training\n",
            "loss: 1.303741 at epoch 471 at applicants training\n",
            "loss: 1.303726 at epoch 472 at applicants training\n",
            "loss: 1.303711 at epoch 473 at applicants training\n",
            "loss: 1.303696 at epoch 474 at applicants training\n",
            "loss: 1.303681 at epoch 475 at applicants training\n",
            "loss: 1.303667 at epoch 476 at applicants training\n",
            "loss: 1.303652 at epoch 477 at applicants training\n",
            "loss: 1.303637 at epoch 478 at applicants training\n",
            "loss: 1.303622 at epoch 479 at applicants training\n",
            "loss: 1.303608 at epoch 480 at applicants training\n",
            "loss: 1.303593 at epoch 481 at applicants training\n",
            "loss: 1.303579 at epoch 482 at applicants training\n",
            "loss: 1.303564 at epoch 483 at applicants training\n",
            "loss: 1.303550 at epoch 484 at applicants training\n",
            "loss: 1.303536 at epoch 485 at applicants training\n",
            "loss: 1.303521 at epoch 486 at applicants training\n",
            "loss: 1.303507 at epoch 487 at applicants training\n",
            "loss: 1.303493 at epoch 488 at applicants training\n",
            "loss: 1.303479 at epoch 489 at applicants training\n",
            "loss: 1.303465 at epoch 490 at applicants training\n",
            "loss: 1.303451 at epoch 491 at applicants training\n",
            "loss: 1.303437 at epoch 492 at applicants training\n",
            "loss: 1.303423 at epoch 493 at applicants training\n",
            "loss: 1.303409 at epoch 494 at applicants training\n",
            "loss: 1.303396 at epoch 495 at applicants training\n",
            "loss: 1.303382 at epoch 496 at applicants training\n",
            "loss: 1.303368 at epoch 497 at applicants training\n",
            "loss: 1.303355 at epoch 498 at applicants training\n",
            "loss: 1.303341 at epoch 499 at applicants training\n",
            "faculty_vectors: [[0.48668794 0.14417712 0.14849461 0.13841142 0.08222891]\n",
            " [0.04525466 0.13832002 0.64810297 0.05331816 0.11500418]\n",
            " [0.04525466 0.13832002 0.64810297 0.05331816 0.11500418]\n",
            " ...\n",
            " [0.04525466 0.13832002 0.64810297 0.05331816 0.11500418]\n",
            " [0.11833262 0.0696857  0.09949829 0.09886809 0.6136153 ]\n",
            " [0.11833262 0.0696857  0.09949829 0.09886809 0.6136153 ]]\n",
            "student_features: [[89.88519482 41.54846766 74.22694197 59.58964194 45.58562975]\n",
            " [83.02115884 83.11317284 92.85893252 57.17176261 74.44206429]\n",
            " [73.59603054 64.81041949 99.97542431 48.96063771 89.61899054]\n",
            " ...\n",
            " [93.75800406 97.92221681 96.90743693 62.75254926 79.06923064]\n",
            " [77.69460062 56.3177577  46.91899653 91.0126447  95.41576181]\n",
            " [55.34297034 57.68921809 61.11884162 90.67474645 79.22306035]]\n",
            "faculty_vectors: [[0.04525466 0.13832002 0.64810297 0.05331816 0.11500418]\n",
            " [0.11833262 0.0696857  0.09949829 0.09886809 0.6136153 ]\n",
            " [0.11833262 0.0696857  0.09949829 0.09886809 0.6136153 ]\n",
            " ...\n",
            " [0.48668794 0.14417712 0.14849461 0.13841142 0.08222891]\n",
            " [0.04525466 0.13832002 0.64810297 0.05331816 0.11500418]\n",
            " [0.48668794 0.14417712 0.14849461 0.13841142 0.08222891]]\n",
            "student_features: [[89.88519482 41.54846766 74.22694197 59.58964194 45.58562975]\n",
            " [83.02115884 83.11317284 92.85893252 57.17176261 74.44206429]\n",
            " [73.59603054 64.81041949 99.97542431 48.96063771 89.61899054]\n",
            " ...\n",
            " [93.75800406 97.92221681 96.90743693 62.75254926 79.06923064]\n",
            " [77.69460062 56.3177577  46.91899653 91.0126447  95.41576181]\n",
            " [55.34297034 57.68921809 61.11884162 90.67474645 79.22306035]]\n",
            "faculty_vectors: [[0.06859329 0.17042057 0.49292317 0.1489869  0.11907607]\n",
            " [0.11833262 0.0696857  0.09949829 0.09886809 0.6136153 ]\n",
            " [0.11833262 0.0696857  0.09949829 0.09886809 0.6136153 ]\n",
            " ...\n",
            " [0.48668794 0.14417712 0.14849461 0.13841142 0.08222891]\n",
            " [0.04525466 0.13832002 0.64810297 0.05331816 0.11500418]\n",
            " [0.48668794 0.14417712 0.14849461 0.13841142 0.08222891]]\n",
            "student_features: [[89.88519482 41.54846766 74.22694197 59.58964194 45.58562975]\n",
            " [83.02115884 83.11317284 92.85893252 57.17176261 74.44206429]\n",
            " [73.59603054 64.81041949 99.97542431 48.96063771 89.61899054]\n",
            " ...\n",
            " [93.75800406 97.92221681 96.90743693 62.75254926 79.06923064]\n",
            " [77.69460062 56.3177577  46.91899653 91.0126447  95.41576181]\n",
            " [55.34297034 57.68921809 61.11884162 90.67474645 79.22306035]]\n",
            "faculty_vectors: [[0.04525466 0.13832002 0.64810297 0.05331816 0.11500418]\n",
            " [0.11833262 0.0696857  0.09949829 0.09886809 0.6136153 ]\n",
            " [0.11833262 0.0696857  0.09949829 0.09886809 0.6136153 ]\n",
            " ...\n",
            " [0.48668794 0.14417712 0.14849461 0.13841142 0.08222891]\n",
            " [0.48668794 0.14417712 0.14849461 0.13841142 0.08222891]\n",
            " [0.48668794 0.14417712 0.14849461 0.13841142 0.08222891]]\n",
            "student_features: [[89.88519482 41.54846766 74.22694197 59.58964194 45.58562975]\n",
            " [83.02115884 83.11317284 92.85893252 57.17176261 74.44206429]\n",
            " [73.59603054 64.81041949 99.97542431 48.96063771 89.61899054]\n",
            " ...\n",
            " [93.75800406 97.92221681 96.90743693 62.75254926 79.06923064]\n",
            " [77.69460062 56.3177577  46.91899653 91.0126447  95.41576181]\n",
            " [55.34297034 57.68921809 61.11884162 90.67474645 79.22306035]]\n",
            "faculty_vectors: [[0.04525466 0.13832002 0.64810297 0.05331816 0.11500418]\n",
            " [0.11833262 0.0696857  0.09949829 0.09886809 0.6136153 ]\n",
            " [0.11833262 0.0696857  0.09949829 0.09886809 0.6136153 ]\n",
            " ...\n",
            " [0.48668794 0.14417712 0.14849461 0.13841142 0.08222891]\n",
            " [0.04525466 0.13832002 0.64810297 0.05331816 0.11500418]\n",
            " [0.48668794 0.14417712 0.14849461 0.13841142 0.08222891]]\n",
            "student_features: [[ 40.72613144 100.          65.28812408  40.          49.62475586]\n",
            " [ 68.44179535  82.39165497  82.81638336  68.71179962  90.76170349]\n",
            " [ 73.59603054  64.81041949  99.97542431  48.96063771  89.61899054]\n",
            " ...\n",
            " [ 93.75800406  97.92221681  96.90743693  62.75254926  79.06923064]\n",
            " [ 77.69460062  56.3177577   46.91899653  91.0126447   95.41576181]\n",
            " [ 55.34297034  57.68921809  61.11884162  90.67474645  79.22306035]]\n",
            "Mean grade: 77.66\n",
            "Faculty distribution: [3349  462 2745 3014  430]\n",
            "Students who got desired faculty: 2097 (21.0%)\n",
            "Mean grade: 70.24\n",
            "Faculty distribution: [3140  442 3057 2920  441]\n",
            "Students who got desired faculty: 3132 (31.3%)\n",
            "Mean grade: 70.23\n",
            "Faculty distribution: [3375  479 2934 2850  362]\n",
            "Students who got desired faculty: 2082 (20.8%)\n",
            "Mean grade: 70.19\n",
            "Faculty distribution: [3028  459 3010 3041  462]\n",
            "Students who got desired faculty: 2069 (20.7%)\n",
            "_Adding Mechina to the process_\n",
            "Mean grade: 73.65\n",
            "Faculty distribution: [3140  442 3057 2920  441]\n",
            "Students who got desired faculty: 3132 (31.3%)\n"
          ]
        }
      ],
      "source": [
        "iteration1_applicants_df, feature_cols, env, trained_model, original_features = run_multi_iteration_example()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xQAL41P-qEY"
      },
      "outputs": [],
      "source": [
        "env = UniversityEnvironment()\n",
        "env.faculties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDqmmPVZ-qEY"
      },
      "outputs": [],
      "source": [
        "def fully_exposed_example(iteration1_applicants_df, feature_cols, env, trained_model, original_features):\n",
        "    modified_features = []\n",
        "    print(\"Starting\")\n",
        "    for idx in range(len(iteration1_applicants_df)):\n",
        "\n",
        "      student_features = iteration1_applicants_df.iloc[idx][feature_cols].values\n",
        "      desired_faculty = iteration1_applicants_df.iloc[idx]['desired_faculty']\n",
        "\n",
        "      # Now students learn from iteration0 data instead of past_df\n",
        "      supp_id, modified_student_features = env.choose_supplier_for_applicant_fully_exposed(\n",
        "          student_features,\n",
        "          desired_faculty,\n",
        "          trained_model,\n",
        "      )\n",
        "      modified_features.append(modified_student_features)\n",
        "      print(f'student {idx} - choose supplier {supp_id}')\n",
        "\n",
        "\n",
        "    modified_features = np.array(modified_features)\n",
        "\n",
        "    # Get final assignments and grades using modified features\n",
        "    final_faculties_modified = env.assign_applicants_to_faculties(\n",
        "        trained_model,\n",
        "        modified_features\n",
        "    )\n",
        "\n",
        "    final_faculties_original = env.assign_applicants_to_faculties(\n",
        "        trained_model,\n",
        "        original_features\n",
        "    )\n",
        "\n",
        "    # Calculate final grades using original features\n",
        "    final_grades_original = env.recommend(original_features, final_faculties_original)\n",
        "    final_grades_modified = env.recommend(original_features, final_faculties_modified)\n",
        "\n",
        "    desired_faculties = iteration1_applicants_df['desired_faculty'].values\n",
        "\n",
        "    # Calculate stats for both iterations\n",
        "    # iter0_matches, iter0_percentage = calculate_desired_faculty_stats(iteration0_faculties, desired_faculties)\n",
        "    final_matches_original, final_percentage_original = calculate_desired_faculty_stats(final_faculties_original, desired_faculties)\n",
        "    final_matches_modified, final_percentage_modified = calculate_desired_faculty_stats(final_faculties_modified, desired_faculties)\n",
        "\n",
        "\n",
        "    # # Print comparison of results\n",
        "    # print(\"\\nResults Comparison:\")\n",
        "    # print(\"\\nIteration 0 (No Gaming):\")\n",
        "    # print(f\"Mean grade: {np.mean(iteration0_grades):.2f}\")\n",
        "    # print(f\"Faculty distribution: {np.bincount(iteration0_faculties)}\")\n",
        "    # print(f\"Students who got desired faculty: {iter0_matches} ({iter0_percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"Mean grade: {np.mean(final_grades_original):.2f}\")\n",
        "    print(f\"Faculty distribution: {np.bincount(final_faculties_original)}\")\n",
        "    print(f\"Students who got desired faculty: {final_matches_original} ({final_percentage_original:.1f}%)\")\n",
        "    print(f\"Mean grade: {np.mean(final_grades_modified):.2f}\")\n",
        "    print(f\"Faculty distribution: {np.bincount(final_faculties_modified)}\")\n",
        "    print(f\"Students who got desired faculty: {final_matches_modified} ({final_percentage_modified:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHdRILN9-qEY"
      },
      "outputs": [],
      "source": [
        "#fully_exposed_example(iteration1_applicants_df, feature_cols, env, trained_model, original_features)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cs236781-hw2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}