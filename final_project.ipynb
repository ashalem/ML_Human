{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Tuple, Dict\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UniversityMLP(nn.Module):\n",
    "    \"\"\"Simple MLP for university decisions\"\"\"\n",
    "    def __init__(self, n_features: int, n_faculties: int):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # Add one-hot encoded faculty to features\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_faculties)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class ApplicantMLP(nn.Module):\n",
    "    \"\"\"MLP for applicant decisions with softmax output\"\"\"\n",
    "    def __init__(self, n_features: int, n_faculties: int):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_faculties),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FacultyParams:\n",
    "    \"\"\"Parameters for each faculty\"\"\"\n",
    "    name: str\n",
    "    utility_vector: np.ndarray  # Hidden vector that determines student success\n",
    "    capacity: int  # Number of spots available (can be infinite)\n",
    "\n",
    "@dataclass\n",
    "class SupplierParams:\n",
    "    \"\"\"Parameters for each preparation supplier\"\"\"\n",
    "    name: str\n",
    "    diff_vector: np.ndarray  # How this supplier modifies student features\n",
    "\n",
    "class UniversityEnvironment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int = 5,  # Number of student features (e.g., math, english, etc.)\n",
    "        n_faculties: int = 5,  # Number of different faculties\n",
    "        n_suppliers: int = 20,  # Number of preparation suppliers\n",
    "        noise_range: Tuple[float, float] = (0,0)  # Range for uniform noise\n",
    "    ):\n",
    "        self.n_features = n_features\n",
    "        self.n_faculties = n_faculties\n",
    "        self.n_suppliers = n_suppliers\n",
    "        self.noise_range = noise_range\n",
    "        \n",
    "        # Initialize faculties with random utility vectors\n",
    "        # Initialize faculties with normalized random utility vectors\n",
    "        self.faculties = [\n",
    "            FacultyParams(\n",
    "                name=[f\"faculty_{i}\" for i in range(n_features)],  # Using the predefined faculty names\n",
    "                utility_vector=self._create_normalized_vector(n_features),\n",
    "                capacity=np.inf  # As per description, infinite capacity\n",
    "            )\n",
    "            for i in range(n_faculties)\n",
    "        ] \n",
    "        \n",
    "        # Initialize suppliers with random modification vectors\n",
    "        self.suppliers = [\n",
    "          SupplierParams(\n",
    "              name=f\"Supplier_{i}\",\n",
    "              diff_vector=np.array([\n",
    "                  15 if j == idx1 else 5 if j == idx2 else -5 if j == idx3 else 0\n",
    "                  for j in range(n_features)\n",
    "              ]),\n",
    "          )\n",
    "          for i in range(n_suppliers)\n",
    "          for idx1, idx2, idx3 in [np.random.choice(n_features, size=3, replace=False)]\n",
    "        ]\n",
    "        \n",
    "        self.past_applicants_df = None\n",
    "        self.current_applicants_df = None\n",
    "\n",
    "    def _create_normalized_vector(self, size: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create a normalized random vector of given size.\n",
    "        Normalization ensures ||vector|| = 1\n",
    "        \"\"\"\n",
    "        vector = np.random.uniform(0, 1, size)\n",
    "        # Normalize the vector to unit length\n",
    "        return vector / np.linalg.norm(vector, ord=1)\n",
    "    \n",
    "    def _generate_truncated_normal_features(self, n_samples: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate features using truncated normal distribution between 55 and 100.\n",
    "        Uses mean at center of range (77.5) and std that makes the distribution fit well in the range.\n",
    "        \"\"\"\n",
    "        # Generate features with uniform distribution between 40 and 100\n",
    "        features = np.random.uniform(40, 100, (n_samples, self.n_features))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def generate_past_applicants(\n",
    "        self,\n",
    "        n_applicants: int = 1000\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Generate dataset of past applicants with their outcomes\"\"\"\n",
    "        # Generate random feature vectors\n",
    "        features = self._generate_truncated_normal_features(n_applicants)\n",
    "        \n",
    "        # Randomly assign faculty for each applicant\n",
    "        df = pd.DataFrame(features, columns=[f\"feature_{i}\" for i in range(self.n_features)])\n",
    "        df['assigned_faculty'] = np.random.randint(0, self.n_faculties, n_applicants)\n",
    "        \n",
    "        # Calculate grade only for assigned faculty\n",
    "        faculty_vectors = np.array([f.utility_vector for f in self.faculties])\n",
    "        grades = np.zeros(n_applicants)\n",
    "        # Get faculty vectors for each applicant based on their assigned faculty\n",
    "        faculty_vectors_per_applicant = faculty_vectors[df['assigned_faculty']]\n",
    "        \n",
    "        # Calculate base grades using matrix multiplication\n",
    "        base_grades = np.sum(features * faculty_vectors_per_applicant, axis=1)\n",
    "        \n",
    "        # Generate noise for all applicants at once\n",
    "        noise = np.random.uniform(*self.noise_range, size=n_applicants)\n",
    "        \n",
    "        # Calculate final grades\n",
    "        grades = base_grades + noise\n",
    "            \n",
    "        df['final_grade'] = grades\n",
    "        self.past_applicants_df = df\n",
    "        return df\n",
    "\n",
    "    def generate_current_applicants(\n",
    "        self,\n",
    "        n_applicants: int = 100\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Generate dataset of current applicants\"\"\"\n",
    "        # Generate random feature vectors\n",
    "        features = self._generate_truncated_normal_features(n_applicants)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        feature_cols = [f\"feature_{i}\" for i in range(self.n_features)]\n",
    "        df = pd.DataFrame(features, columns=feature_cols)\n",
    "        \n",
    "        # Add desired faculty (random)\n",
    "        df['desired_faculty'] = np.random.randint(0, self.n_faculties, n_applicants)\n",
    "        \n",
    "        self.current_applicants_df = df\n",
    "        return df\n",
    "    \n",
    "    def train_applicant_model(\n",
    "        self,\n",
    "        past_data: pd.DataFrame = None\n",
    "    ) -> ApplicantMLP:\n",
    "        \"\"\"Train applicant model on past data\"\"\"\n",
    "        if past_data is None:\n",
    "            past_data = self.past_applicants_df\n",
    "        \n",
    "        if past_data is None:\n",
    "            raise ValueError(\"No past data available. Generate past applicants first.\")\n",
    "        \n",
    "        # Create and train applicant's MLP model\n",
    "        feature_cols = [f\"feature_{i}\" for i in range(self.n_features)]\n",
    "        X_train = torch.FloatTensor(past_data[feature_cols].values)\n",
    "        y_train = torch.LongTensor(past_data['assigned_faculty'].values)\n",
    "        \n",
    "        model = ApplicantMLP(self.n_features, self.n_faculties)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Reduced learning rate\n",
    "        \n",
    "        # Train the model\n",
    "        model.train()\n",
    "        \n",
    "        for epoch in range(500):  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            print(f'loss: {loss.item():.6f} at epoch {epoch} at applicants training')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def choose_supplier_for_applicant(\n",
    "        self,\n",
    "        applicant_features: np.ndarray,\n",
    "        desired_faculty: int,\n",
    "        applicant_model: ApplicantMLP = None\n",
    "    ) -> Tuple[int, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Choose the best supplier for an applicant based on past data and supplier effects.\n",
    "        \n",
    "        Args:\n",
    "            applicant_features: The current features of the applicant\n",
    "            desired_faculty: The faculty index the applicant wants to get into\n",
    "            past_data: Optional past data to train on. If None, uses self.past_applicants_df\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (chosen_supplier_idx, modified_features)\n",
    "        \"\"\"\n",
    "        # Evaluate each supplier's effect\n",
    "        applicant_model.eval()\n",
    "        best_probability = -1\n",
    "        best_supplier_idx = -1\n",
    "        best_modified_features = None\n",
    "        \n",
    "        original_features = torch.FloatTensor(applicant_features).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Try each supplier\n",
    "            for i, supplier in enumerate(self.suppliers):\n",
    "                # Apply supplier's modification\n",
    "                modified_features_unclipped = original_features + torch.FloatTensor(supplier.diff_vector)\n",
    "                modified_features = np.clip(modified_features_unclipped, 40, 100)\n",
    "                \n",
    "                # Get probability distribution over faculties\n",
    "                probabilities = applicant_model(modified_features)\n",
    "                \n",
    "                # Check probability for desired faculty\n",
    "                prob_desired = probabilities[0, int(desired_faculty)].item()\n",
    "                \n",
    "                if prob_desired > best_probability:\n",
    "                    best_probability = prob_desired\n",
    "                    best_supplier_idx = i\n",
    "                    best_modified_features = modified_features.squeeze(0).numpy()\n",
    "        \n",
    "        if best_supplier_idx == -1:\n",
    "            # If no supplier improves probability, return original features with no supplier\n",
    "            return (-1, applicant_features)\n",
    "        \n",
    "        return (best_supplier_idx, best_modified_features)\n",
    "        \n",
    "    def recommend(\n",
    "        self,\n",
    "        student_features: np.ndarray,\n",
    "        recommended_faculties: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Calculate final grades for students given their features and recommended faculties\n",
    "        \n",
    "        Args:\n",
    "            student_features: Features matrix of shape (n_students, n_features)\n",
    "            recommended_faculties: Array of faculty indices of shape (n_students,)\n",
    "            \n",
    "        Returns:\n",
    "            Array of final grades of shape (n_students,)\n",
    "        \"\"\"\n",
    "        # Get utility vectors for all recommended faculties\n",
    "        faculty_vectors = np.array([self.faculties[f].utility_vector for f in recommended_faculties])\n",
    "        \n",
    "        print(f'faculty_vectors: {faculty_vectors}')\n",
    "        print(f'student_features: {student_features}')\n",
    "        \n",
    "        # Calculate base grades using batch matrix multiplication\n",
    "        base_grades = np.sum(student_features * faculty_vectors, axis=1)\n",
    "        \n",
    "        # Generate noise for all students at once\n",
    "        noise = np.random.uniform(*self.noise_range, size=len(student_features))\n",
    "        \n",
    "        return base_grades + noise\n",
    "\n",
    "    def train_university_model(\n",
    "        self,\n",
    "        past_data: pd.DataFrame = None\n",
    "    ) -> UniversityMLP:\n",
    "        \"\"\"\n",
    "        Train university model on past data.\n",
    "        \n",
    "        Args:\n",
    "            past_data: Optional past data to train on. If None, uses self.past_applicants_df\n",
    "        \n",
    "        Returns:\n",
    "            Trained UniversityMLP model\n",
    "        \"\"\"\n",
    "        if past_data is None:\n",
    "            past_data = self.past_applicants_df\n",
    "        \n",
    "        if past_data is None:\n",
    "            raise ValueError(\"No past data available. Generate past applicants first.\")\n",
    "        \n",
    "        # Prepare training data\n",
    "        feature_cols = [f\"feature_{i}\" for i in range(self.n_features)]\n",
    "        X_train = torch.FloatTensor(past_data[feature_cols].values)\n",
    "        \n",
    "        # Create and train university model\n",
    "        model = UniversityMLP(self.n_features, self.n_faculties)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # Custom loss function that only considers the assigned faculty's grade\n",
    "        def custom_loss(predictions, targets, assigned_faculties):\n",
    "            batch_size = predictions.size(0)\n",
    "            indices = torch.arange(batch_size)\n",
    "            predicted_assigned_grades = predictions[indices, assigned_faculties]\n",
    "            return torch.mean((predicted_assigned_grades - targets) ** 2)\n",
    "        \n",
    "        # Train the model\n",
    "        model.train()\n",
    "        batch_size = 128\n",
    "        n_epochs = 100\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Process in batches\n",
    "            permutation = torch.randperm(len(X_train))\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                indices = permutation[i:i + batch_size]\n",
    "                batch_x = X_train[indices]\n",
    "                batch_y = torch.FloatTensor(past_data['final_grade'].values[indices])\n",
    "                batch_assigned = torch.LongTensor(past_data['assigned_faculty'].values[indices])\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(batch_x)\n",
    "                loss = custom_loss(predictions, batch_y, batch_assigned)\n",
    "                print(f'loss: {loss} at epoch {epoch}')\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def assign_applicants_to_faculties(\n",
    "        self,\n",
    "        model: UniversityMLP,\n",
    "        current_applicants_features: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Use trained model to make faculty recommendations for current applicants.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained UniversityMLP model\n",
    "            current_applicants_features: Modified features of current applicants (n_applicants x n_features)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (chosen_faculties, final_grades, mean_grade)\n",
    "            - chosen_faculties: Array of faculty indices chosen for each applicant\n",
    "            - final_grades: Array of final grades received by each applicant\n",
    "            - mean_grade: Average grade across all applicants\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            current_features = torch.FloatTensor(current_applicants_features)\n",
    "            predicted_grades = model(current_features)\n",
    "            \n",
    "            # Choose best faculty for each applicant based on predicted grades\n",
    "            chosen_faculties = torch.argmax(predicted_grades, dim=1).numpy()\n",
    "        \n",
    "        return chosen_faculties\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example():\n",
    "    # Create environment\n",
    "    env = UniversityEnvironment()\n",
    "    \n",
    "    # Generate past applicants\n",
    "    past_df = env.generate_past_applicants(1000)\n",
    "    print(\"Past applicants shape:\", past_df.shape)\n",
    "    \n",
    "    # Generate current applicants\n",
    "    current_df = env.generate_current_applicants(100)\n",
    "    print(\"Current applicants shape:\", current_df.shape)\n",
    "    print(f'current_df: {current_df}')\n",
    "    \n",
    "    # Get modified features for all current applicants\n",
    "    feature_cols = [f\"feature_{i}\" for i in range(env.n_features)]\n",
    "    modified_features = []\n",
    "    original_features = current_df[feature_cols].values\n",
    "    \n",
    "    for idx in range(len(current_df)):\n",
    "        student_features = current_df.iloc[idx][feature_cols].values\n",
    "        desired_faculty = current_df.iloc[idx]['desired_faculty']\n",
    "        \n",
    "        _, modified_student_features = env.choose_supplier_for_applicant(\n",
    "            student_features,\n",
    "            desired_faculty\n",
    "        )\n",
    "        modified_features.append(modified_student_features)\n",
    "    \n",
    "    modified_features = np.array(modified_features)\n",
    "    \n",
    "    # Train university model\n",
    "    trained_model = env.train_university_model(past_df)\n",
    "    \n",
    "    # Make predictions using trained model\n",
    "    chosen_faculties = env.assign_applicants_to_faculties(\n",
    "        trained_model,\n",
    "        modified_features\n",
    "    )\n",
    "    # Calculate percentage of students accepted into their desired faculty\n",
    "    desired_faculties = current_df['desired_faculty'].values\n",
    "    matches = (chosen_faculties == desired_faculties)\n",
    "    acceptance_rate = (np.sum(matches) / len(desired_faculties)) * 100\n",
    "    \n",
    "    # Calculate final grades using original features\n",
    "    final_grades = env.recommend(original_features, chosen_faculties)\n",
    "    mean_grade = np.mean(final_grades)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Mean grade across all applicants: {mean_grade:.2f}\")\n",
    "    print(f\"\\nPercentage of students accepted to desired faculty: {acceptance_rate:.2f}%\")\n",
    "\n",
    "    \n",
    "    # Print detailed results for first 5 applicants\n",
    "    print(\"\\nDetailed results for first 5 applicants:\")\n",
    "    for i in range(5):\n",
    "        desired_faculty = current_df.iloc[i]['desired_faculty']\n",
    "        print(f\"\\nApplicant {i}:\")\n",
    "        print(f\"Desired faculty: {desired_faculty}\")\n",
    "        print(f\"Assigned faculty: {chosen_faculties[i]}\")\n",
    "        print(f\"Final grade: {final_grades[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_desired_faculty_stats(assigned_faculties, desired_faculties):\n",
    "        total_students = len(desired_faculties)\n",
    "        matches = sum(assigned == desired for assigned, desired in zip(assigned_faculties, desired_faculties))\n",
    "        percentage = (matches / total_students) * 100\n",
    "        return matches, percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_iteration_example():\n",
    "    # Create environment\n",
    "    env = UniversityEnvironment()\n",
    "    feature_cols = [f\"feature_{i}\" for i in range(env.n_features)]\n",
    "    \n",
    "    # Iteration -1: Initial University Training\n",
    "    print(\"\\n=== Iteration -1: Initial University Training ===\")\n",
    "    past_df = env.generate_past_applicants(10000)\n",
    "    trained_model = env.train_university_model(past_df)\n",
    "    \n",
    "    # Generate students that will be used in iterations 0\n",
    "    iteration0_applicants_df = env.generate_current_applicants(10000)\n",
    "    original_features = iteration0_applicants_df[feature_cols].values\n",
    "    \n",
    "    # Iteration 0: Pure Assignment\n",
    "    print(\"\\n=== Iteration 0: Pure Assignment ===\")\n",
    "    # Assign faculties using original features\n",
    "    iteration0_faculties = env.assign_applicants_to_faculties(\n",
    "        trained_model,\n",
    "        original_features\n",
    "    )\n",
    "    \n",
    "    # Get real grades for these assignments\n",
    "    iteration0_grades = env.recommend(original_features, iteration0_faculties)\n",
    "    \n",
    "    # Create training data for students from iteration 0\n",
    "    iteration0_df = pd.DataFrame(original_features, columns=feature_cols)\n",
    "    iteration0_df['assigned_faculty'] = iteration0_faculties\n",
    "    iteration0_df['final_grade'] = iteration0_grades\n",
    "    \n",
    "    # Iteration 1: Student Learning\n",
    "    print(\"\\n=== Iteration 1: Student Learning ===\")\n",
    "    iteration1_applicants_df = env.generate_current_applicants(10000)\n",
    "    modified_features_with_features_knowledge = []\n",
    "    modified_features_without_features_knowledge = []\n",
    "    \n",
    "    applicant_model = env.train_applicant_model(iteration0_df)\n",
    "    \n",
    "    for idx in range(len(iteration1_applicants_df)):\n",
    "        student_features = iteration1_applicants_df.iloc[idx][feature_cols].values\n",
    "        desired_faculty = iteration1_applicants_df.iloc[idx]['desired_faculty']\n",
    "        \n",
    "        # Now students learn from iteration0 data instead of past_df\n",
    "        _, modified_student_features = env.choose_supplier_for_applicant(\n",
    "            student_features,\n",
    "            desired_faculty,\n",
    "            applicant_model\n",
    "        )\n",
    "        modified_features_with_features_knowledge.append(modified_student_features)\n",
    "        _, modified_student_features_without_features_knowledge = env.choose_supplier_for_applicant(\n",
    "            np.zeros_like(student_features),\n",
    "            desired_faculty,\n",
    "            applicant_model\n",
    "        )\n",
    "        modified_student_features_without_features_knowledge = modified_student_features_without_features_knowledge + student_features\n",
    "        modified_features_without_features_knowledge.append(modified_student_features_without_features_knowledge)\n",
    "    \n",
    "    modified_features_with_features_knowledge = np.array(modified_features_with_features_knowledge)\n",
    "    modified_features_without_features_knowledge = np.array(modified_features_without_features_knowledge)\n",
    "    \n",
    "    # Get final assignments and grades using modified features\n",
    "    final_faculties_modified = env.assign_applicants_to_faculties(\n",
    "        trained_model,\n",
    "        modified_features_with_features_knowledge\n",
    "    )\n",
    "    \n",
    "    final_faculties_modified_without_features_knowledge = env.assign_applicants_to_faculties(\n",
    "        trained_model,\n",
    "        modified_features_without_features_knowledge\n",
    "    )\n",
    "    \n",
    "    final_faculties_original = env.assign_applicants_to_faculties(\n",
    "        trained_model,\n",
    "        original_features\n",
    "    )\n",
    "    \n",
    "    # Calculate final grades using original features\n",
    "    final_grades_original = env.recommend(original_features, final_faculties_original)\n",
    "    final_grades_modified = env.recommend(original_features, final_faculties_modified)\n",
    "    final_grades_modified_without_features_knowledge = env.recommend(original_features, final_faculties_modified_without_features_knowledge)\n",
    "    \n",
    "    \n",
    "    # Calculate stats for both iterations\n",
    "    desired_faculties = iteration1_applicants_df['desired_faculty'].values\n",
    "    final_matches_original, final_percentage_original = calculate_desired_faculty_stats(final_faculties_original, desired_faculties)\n",
    "    final_matches_modified, final_percentage_modified = calculate_desired_faculty_stats(final_faculties_modified, desired_faculties)\n",
    "    final_matches_modified_without_features_knowledge, final_percentage_modified_without_features_knowledge = calculate_desired_faculty_stats(final_faculties_modified_without_features_knowledge, desired_faculties)\n",
    "    \n",
    "    # # Print comparison of results\n",
    "    # print(\"\\nResults Comparison:\")\n",
    "    # print(\"\\nIteration 0 (No Gaming):\")\n",
    "    # print(f\"Mean grade: {np.mean(iteration0_grades):.2f}\")\n",
    "    # print(f\"Faculty distribution: {np.bincount(iteration0_faculties)}\")\n",
    "    # print(f\"Students who got desired faculty: {iter0_matches} ({iter0_percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"Mean grade: {np.mean(final_grades_original):.2f}\")\n",
    "    print(f\"Faculty distribution: {np.bincount(final_faculties_original)}\")\n",
    "    print(f\"Students who got desired faculty: {final_matches_original} ({final_percentage_original:.1f}%)\")\n",
    "    print(f\"Mean grade: {np.mean(final_grades_modified):.2f}\")\n",
    "    print(f\"Faculty distribution: {np.bincount(final_faculties_modified)}\")\n",
    "    print(f\"Students who got desired faculty: {final_matches_modified} ({final_percentage_modified:.1f}%)\")\n",
    "    print(f\"Mean grade: {np.mean(final_grades_modified_without_features_knowledge):.2f}\")\n",
    "    print(f\"Faculty distribution: {np.bincount(final_faculties_modified_without_features_knowledge)}\")\n",
    "    print(f\"Students who got desired faculty: {final_matches_modified_without_features_knowledge} ({final_percentage_modified_without_features_knowledge:.1f}%)\")\n",
    "    # Print detailed results for first 5 applicants\n",
    "    # print(\"\\nDetailed results for first 5 applicants:\")\n",
    "    # for i in range(5):\n",
    "    #     desired_faculty = iteration1_applicants_df.iloc[i]['desired_faculty']\n",
    "    #     print(f\"\\nApplicant {i}:\")\n",
    "    #     print(f\"Desired faculty: {desired_faculty}\")\n",
    "    #     print(f\"Iteration 0 faculty: {iteration0_faculties[i]}\")\n",
    "    #     print(f\"Iteration 0 grade: {iteration0_grades[i]:.2f}\")\n",
    "    #     print(f\"Final faculty: {final_faculties[i]}\")\n",
    "    #     print(f\"Final grade: {final_grades[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration -1: Initial University Training ===\n",
      "loss: 7336.11279296875 at epoch 0\n",
      "loss: 6221.15283203125 at epoch 0\n",
      "loss: 5112.3779296875 at epoch 0\n",
      "loss: 4586.01904296875 at epoch 0\n",
      "loss: 3273.04736328125 at epoch 0\n",
      "loss: 2528.109375 at epoch 0\n",
      "loss: 1826.801513671875 at epoch 0\n",
      "loss: 1444.75732421875 at epoch 0\n",
      "loss: 829.1385498046875 at epoch 1\n",
      "loss: 387.0880126953125 at epoch 1\n",
      "loss: 201.38360595703125 at epoch 1\n",
      "loss: 142.64141845703125 at epoch 1\n",
      "loss: 178.75143432617188 at epoch 1\n",
      "loss: 460.32647705078125 at epoch 1\n",
      "loss: 583.68505859375 at epoch 1\n",
      "loss: 645.2244873046875 at epoch 1\n",
      "loss: 521.244873046875 at epoch 2\n",
      "loss: 500.2967529296875 at epoch 2\n",
      "loss: 271.85675048828125 at epoch 2\n",
      "loss: 218.40850830078125 at epoch 2\n",
      "loss: 132.61709594726562 at epoch 2\n",
      "loss: 99.20076751708984 at epoch 2\n",
      "loss: 115.2900619506836 at epoch 2\n",
      "loss: 121.66166687011719 at epoch 2\n",
      "loss: 173.25405883789062 at epoch 3\n",
      "loss: 198.99908447265625 at epoch 3\n",
      "loss: 167.5892333984375 at epoch 3\n",
      "loss: 167.54791259765625 at epoch 3\n",
      "loss: 134.7286376953125 at epoch 3\n",
      "loss: 123.42782592773438 at epoch 3\n",
      "loss: 105.34428405761719 at epoch 3\n",
      "loss: 94.07040405273438 at epoch 3\n",
      "loss: 63.818077087402344 at epoch 4\n",
      "loss: 53.57086181640625 at epoch 4\n",
      "loss: 41.92844009399414 at epoch 4\n",
      "loss: 40.655826568603516 at epoch 4\n",
      "loss: 49.920257568359375 at epoch 4\n",
      "loss: 57.97718811035156 at epoch 4\n",
      "loss: 48.70998001098633 at epoch 4\n",
      "loss: 69.20199584960938 at epoch 4\n",
      "loss: 60.41293716430664 at epoch 5\n",
      "loss: 62.73101806640625 at epoch 5\n",
      "loss: 68.36953735351562 at epoch 5\n",
      "loss: 36.6207275390625 at epoch 5\n",
      "loss: 40.44013595581055 at epoch 5\n",
      "loss: 34.006935119628906 at epoch 5\n",
      "loss: 33.24228286743164 at epoch 5\n",
      "loss: 23.072307586669922 at epoch 5\n",
      "loss: 35.968631744384766 at epoch 6\n",
      "loss: 33.554710388183594 at epoch 6\n",
      "loss: 39.35675048828125 at epoch 6\n",
      "loss: 35.203922271728516 at epoch 6\n",
      "loss: 36.980926513671875 at epoch 6\n",
      "loss: 35.14313888549805 at epoch 6\n",
      "loss: 30.689571380615234 at epoch 6\n",
      "loss: 33.85979461669922 at epoch 6\n",
      "loss: 32.04555892944336 at epoch 7\n",
      "loss: 18.4062557220459 at epoch 7\n",
      "loss: 26.088417053222656 at epoch 7\n",
      "loss: 21.283966064453125 at epoch 7\n",
      "loss: 28.314653396606445 at epoch 7\n",
      "loss: 27.407468795776367 at epoch 7\n",
      "loss: 29.943838119506836 at epoch 7\n",
      "loss: 22.553266525268555 at epoch 7\n",
      "loss: 25.169166564941406 at epoch 8\n",
      "loss: 24.817546844482422 at epoch 8\n",
      "loss: 21.270645141601562 at epoch 8\n",
      "loss: 19.229251861572266 at epoch 8\n",
      "loss: 26.446805953979492 at epoch 8\n",
      "loss: 23.275310516357422 at epoch 8\n",
      "loss: 25.616424560546875 at epoch 8\n",
      "loss: 19.682085037231445 at epoch 8\n",
      "loss: 21.60025405883789 at epoch 9\n",
      "loss: 21.464372634887695 at epoch 9\n",
      "loss: 18.400415420532227 at epoch 9\n",
      "loss: 23.97943687438965 at epoch 9\n",
      "loss: 20.766490936279297 at epoch 9\n",
      "loss: 18.287965774536133 at epoch 9\n",
      "loss: 13.680938720703125 at epoch 9\n",
      "loss: 18.419403076171875 at epoch 9\n",
      "loss: 20.907297134399414 at epoch 10\n",
      "loss: 16.01409912109375 at epoch 10\n",
      "loss: 18.482162475585938 at epoch 10\n",
      "loss: 13.075925827026367 at epoch 10\n",
      "loss: 20.606460571289062 at epoch 10\n",
      "loss: 19.546566009521484 at epoch 10\n",
      "loss: 16.5474853515625 at epoch 10\n",
      "loss: 10.530303955078125 at epoch 10\n",
      "loss: 14.8870210647583 at epoch 11\n",
      "loss: 16.55222511291504 at epoch 11\n",
      "loss: 14.171330451965332 at epoch 11\n",
      "loss: 16.953216552734375 at epoch 11\n",
      "loss: 10.538413047790527 at epoch 11\n",
      "loss: 13.758268356323242 at epoch 11\n",
      "loss: 18.86560821533203 at epoch 11\n",
      "loss: 16.675222396850586 at epoch 11\n",
      "loss: 14.066245079040527 at epoch 12\n",
      "loss: 12.104464530944824 at epoch 12\n",
      "loss: 13.98992919921875 at epoch 12\n",
      "loss: 12.64531421661377 at epoch 12\n",
      "loss: 11.471282958984375 at epoch 12\n",
      "loss: 14.726320266723633 at epoch 12\n",
      "loss: 17.64284896850586 at epoch 12\n",
      "loss: 12.757547378540039 at epoch 12\n",
      "loss: 12.901677131652832 at epoch 13\n",
      "loss: 10.583831787109375 at epoch 13\n",
      "loss: 12.513863563537598 at epoch 13\n",
      "loss: 11.323271751403809 at epoch 13\n",
      "loss: 10.957433700561523 at epoch 13\n",
      "loss: 12.647286415100098 at epoch 13\n",
      "loss: 11.08655071258545 at epoch 13\n",
      "loss: 14.456735610961914 at epoch 13\n",
      "loss: 9.850578308105469 at epoch 14\n",
      "loss: 11.749073028564453 at epoch 14\n",
      "loss: 13.896207809448242 at epoch 14\n",
      "loss: 11.118277549743652 at epoch 14\n",
      "loss: 9.342996597290039 at epoch 14\n",
      "loss: 10.974588394165039 at epoch 14\n",
      "loss: 8.577778816223145 at epoch 14\n",
      "loss: 10.356240272521973 at epoch 14\n",
      "loss: 12.043959617614746 at epoch 15\n",
      "loss: 9.845192909240723 at epoch 15\n",
      "loss: 7.729910373687744 at epoch 15\n",
      "loss: 10.444829940795898 at epoch 15\n",
      "loss: 7.5706281661987305 at epoch 15\n",
      "loss: 9.337926864624023 at epoch 15\n",
      "loss: 8.027745246887207 at epoch 15\n",
      "loss: 10.341680526733398 at epoch 15\n",
      "loss: 8.616083145141602 at epoch 16\n",
      "loss: 7.730099201202393 at epoch 16\n",
      "loss: 6.384974002838135 at epoch 16\n",
      "loss: 5.889437675476074 at epoch 16\n",
      "loss: 9.163529396057129 at epoch 16\n",
      "loss: 9.491278648376465 at epoch 16\n",
      "loss: 9.0388822555542 at epoch 16\n",
      "loss: 8.978188514709473 at epoch 16\n",
      "loss: 7.113812446594238 at epoch 17\n",
      "loss: 8.709775924682617 at epoch 17\n",
      "loss: 6.689793586730957 at epoch 17\n",
      "loss: 7.133058547973633 at epoch 17\n",
      "loss: 4.6655049324035645 at epoch 17\n",
      "loss: 7.283649444580078 at epoch 17\n",
      "loss: 7.397189617156982 at epoch 17\n",
      "loss: 6.0650105476379395 at epoch 17\n",
      "loss: 5.679170608520508 at epoch 18\n",
      "loss: 4.301984786987305 at epoch 18\n",
      "loss: 6.0959906578063965 at epoch 18\n",
      "loss: 6.110962390899658 at epoch 18\n",
      "loss: 6.842527389526367 at epoch 18\n",
      "loss: 6.937325477600098 at epoch 18\n",
      "loss: 5.0511908531188965 at epoch 18\n",
      "loss: 6.315597057342529 at epoch 18\n",
      "loss: 3.8716421127319336 at epoch 19\n",
      "loss: 6.723081111907959 at epoch 19\n",
      "loss: 5.6087141036987305 at epoch 19\n",
      "loss: 4.994544506072998 at epoch 19\n",
      "loss: 3.763213872909546 at epoch 19\n",
      "loss: 3.823529005050659 at epoch 19\n",
      "loss: 5.331212043762207 at epoch 19\n",
      "loss: 5.225887775421143 at epoch 19\n",
      "loss: 3.6003048419952393 at epoch 20\n",
      "loss: 4.889256954193115 at epoch 20\n",
      "loss: 4.26053524017334 at epoch 20\n",
      "loss: 3.779724597930908 at epoch 20\n",
      "loss: 4.063333034515381 at epoch 20\n",
      "loss: 4.192870140075684 at epoch 20\n",
      "loss: 4.261261940002441 at epoch 20\n",
      "loss: 4.011843681335449 at epoch 20\n",
      "loss: 3.93115496635437 at epoch 21\n",
      "loss: 4.867542743682861 at epoch 21\n",
      "loss: 3.39699125289917 at epoch 21\n",
      "loss: 3.9903647899627686 at epoch 21\n",
      "loss: 2.7583260536193848 at epoch 21\n",
      "loss: 2.6697275638580322 at epoch 21\n",
      "loss: 2.8602654933929443 at epoch 21\n",
      "loss: 3.406973123550415 at epoch 21\n",
      "loss: 2.900438070297241 at epoch 22\n",
      "loss: 3.154909610748291 at epoch 22\n",
      "loss: 3.0641417503356934 at epoch 22\n",
      "loss: 2.713440179824829 at epoch 22\n",
      "loss: 2.3707406520843506 at epoch 22\n",
      "loss: 2.8171048164367676 at epoch 22\n",
      "loss: 3.0549793243408203 at epoch 22\n",
      "loss: 3.0123023986816406 at epoch 22\n",
      "loss: 1.766273856163025 at epoch 23\n",
      "loss: 2.5909996032714844 at epoch 23\n",
      "loss: 2.018533945083618 at epoch 23\n",
      "loss: 2.7051639556884766 at epoch 23\n",
      "loss: 2.733555555343628 at epoch 23\n",
      "loss: 2.2545576095581055 at epoch 23\n",
      "loss: 2.9505386352539062 at epoch 23\n",
      "loss: 2.389084815979004 at epoch 23\n",
      "loss: 2.3010025024414062 at epoch 24\n",
      "loss: 2.1070868968963623 at epoch 24\n",
      "loss: 2.1854286193847656 at epoch 24\n",
      "loss: 2.1152327060699463 at epoch 24\n",
      "loss: 2.1583974361419678 at epoch 24\n",
      "loss: 2.3919856548309326 at epoch 24\n",
      "loss: 1.8588389158248901 at epoch 24\n",
      "loss: 1.3023070096969604 at epoch 24\n",
      "loss: 1.5227227210998535 at epoch 25\n",
      "loss: 1.6299669742584229 at epoch 25\n",
      "loss: 1.6677730083465576 at epoch 25\n",
      "loss: 2.056764602661133 at epoch 25\n",
      "loss: 1.8238195180892944 at epoch 25\n",
      "loss: 1.9616165161132812 at epoch 25\n",
      "loss: 1.7800096273422241 at epoch 25\n",
      "loss: 1.445510983467102 at epoch 25\n",
      "loss: 1.1829168796539307 at epoch 26\n",
      "loss: 1.4777024984359741 at epoch 26\n",
      "loss: 1.2654314041137695 at epoch 26\n",
      "loss: 2.044933795928955 at epoch 26\n",
      "loss: 1.482271432876587 at epoch 26\n",
      "loss: 1.5137207508087158 at epoch 26\n",
      "loss: 1.3355815410614014 at epoch 26\n",
      "loss: 1.643951654434204 at epoch 26\n",
      "loss: 1.1834027767181396 at epoch 27\n",
      "loss: 1.4250314235687256 at epoch 27\n",
      "loss: 1.149289608001709 at epoch 27\n",
      "loss: 1.3776863813400269 at epoch 27\n",
      "loss: 1.3570334911346436 at epoch 27\n",
      "loss: 1.248899221420288 at epoch 27\n",
      "loss: 0.9699828028678894 at epoch 27\n",
      "loss: 1.8600436449050903 at epoch 27\n",
      "loss: 1.1616220474243164 at epoch 28\n",
      "loss: 1.2183037996292114 at epoch 28\n",
      "loss: 1.0490937232971191 at epoch 28\n",
      "loss: 1.2185499668121338 at epoch 28\n",
      "loss: 1.1923308372497559 at epoch 28\n",
      "loss: 1.1859618425369263 at epoch 28\n",
      "loss: 0.9977474212646484 at epoch 28\n",
      "loss: 0.9431315064430237 at epoch 28\n",
      "loss: 1.2533007860183716 at epoch 29\n",
      "loss: 0.9324789643287659 at epoch 29\n",
      "loss: 0.963367760181427 at epoch 29\n",
      "loss: 0.8278374671936035 at epoch 29\n",
      "loss: 1.129231333732605 at epoch 29\n",
      "loss: 0.8453802466392517 at epoch 29\n",
      "loss: 0.8371413350105286 at epoch 29\n",
      "loss: 0.9143886566162109 at epoch 29\n",
      "loss: 0.9344778060913086 at epoch 30\n",
      "loss: 0.9164767861366272 at epoch 30\n",
      "loss: 0.724132776260376 at epoch 30\n",
      "loss: 0.682623028755188 at epoch 30\n",
      "loss: 0.780285120010376 at epoch 30\n",
      "loss: 0.7186539173126221 at epoch 30\n",
      "loss: 1.0867888927459717 at epoch 30\n",
      "loss: 0.8014959692955017 at epoch 30\n",
      "loss: 0.7378262877464294 at epoch 31\n",
      "loss: 0.6136764883995056 at epoch 31\n",
      "loss: 0.7379556894302368 at epoch 31\n",
      "loss: 0.7365764379501343 at epoch 31\n",
      "loss: 0.8209943771362305 at epoch 31\n",
      "loss: 0.8827697038650513 at epoch 31\n",
      "loss: 0.6269548535346985 at epoch 31\n",
      "loss: 0.7140302658081055 at epoch 31\n",
      "loss: 0.7433367371559143 at epoch 32\n",
      "loss: 0.6993472576141357 at epoch 32\n",
      "loss: 0.5255910158157349 at epoch 32\n",
      "loss: 0.57892245054245 at epoch 32\n",
      "loss: 0.5929677486419678 at epoch 32\n",
      "loss: 0.5545786023139954 at epoch 32\n",
      "loss: 0.6752219200134277 at epoch 32\n",
      "loss: 0.7858794331550598 at epoch 32\n",
      "loss: 0.7595906257629395 at epoch 33\n",
      "loss: 0.4344114661216736 at epoch 33\n",
      "loss: 0.5812289118766785 at epoch 33\n",
      "loss: 0.6250247955322266 at epoch 33\n",
      "loss: 0.5565365552902222 at epoch 33\n",
      "loss: 0.4869464635848999 at epoch 33\n",
      "loss: 0.5317689180374146 at epoch 33\n",
      "loss: 0.6269839406013489 at epoch 33\n",
      "loss: 0.4694342017173767 at epoch 34\n",
      "loss: 0.5522775650024414 at epoch 34\n",
      "loss: 0.4822157919406891 at epoch 34\n",
      "loss: 0.47684308886528015 at epoch 34\n",
      "loss: 0.5736097693443298 at epoch 34\n",
      "loss: 0.4938046932220459 at epoch 34\n",
      "loss: 0.45738205313682556 at epoch 34\n",
      "loss: 0.7154932022094727 at epoch 34\n",
      "loss: 0.5305928587913513 at epoch 35\n",
      "loss: 0.4275452196598053 at epoch 35\n",
      "loss: 0.45519018173217773 at epoch 35\n",
      "loss: 0.47221270203590393 at epoch 35\n",
      "loss: 0.4050787687301636 at epoch 35\n",
      "loss: 0.45802587270736694 at epoch 35\n",
      "loss: 0.37580060958862305 at epoch 35\n",
      "loss: 0.7347685694694519 at epoch 35\n",
      "loss: 0.44667670130729675 at epoch 36\n",
      "loss: 0.500859260559082 at epoch 36\n",
      "loss: 0.4689158797264099 at epoch 36\n",
      "loss: 0.4229925870895386 at epoch 36\n",
      "loss: 0.3560817241668701 at epoch 36\n",
      "loss: 0.4336743652820587 at epoch 36\n",
      "loss: 0.4776157736778259 at epoch 36\n",
      "loss: 0.3047516942024231 at epoch 36\n",
      "loss: 0.32932233810424805 at epoch 37\n",
      "loss: 0.48537784814834595 at epoch 37\n",
      "loss: 0.42831483483314514 at epoch 37\n",
      "loss: 0.3345291018486023 at epoch 37\n",
      "loss: 0.327809602022171 at epoch 37\n",
      "loss: 0.4631555378437042 at epoch 37\n",
      "loss: 0.323604017496109 at epoch 37\n",
      "loss: 0.45891350507736206 at epoch 37\n",
      "loss: 0.4688109755516052 at epoch 38\n",
      "loss: 0.34083229303359985 at epoch 38\n",
      "loss: 0.37830644845962524 at epoch 38\n",
      "loss: 0.35510921478271484 at epoch 38\n",
      "loss: 0.3664152920246124 at epoch 38\n",
      "loss: 0.27114835381507874 at epoch 38\n",
      "loss: 0.368885338306427 at epoch 38\n",
      "loss: 0.3343227207660675 at epoch 38\n",
      "loss: 0.25331202149391174 at epoch 39\n",
      "loss: 0.45850425958633423 at epoch 39\n",
      "loss: 0.2500717043876648 at epoch 39\n",
      "loss: 0.33253273367881775 at epoch 39\n",
      "loss: 0.28510022163391113 at epoch 39\n",
      "loss: 0.29842981696128845 at epoch 39\n",
      "loss: 0.421524316072464 at epoch 39\n",
      "loss: 0.34024831652641296 at epoch 39\n",
      "loss: 0.2849868834018707 at epoch 40\n",
      "loss: 0.2685386538505554 at epoch 40\n",
      "loss: 0.39697909355163574 at epoch 40\n",
      "loss: 0.3558037281036377 at epoch 40\n",
      "loss: 0.23812571167945862 at epoch 40\n",
      "loss: 0.3431841731071472 at epoch 40\n",
      "loss: 0.2756841480731964 at epoch 40\n",
      "loss: 0.2635372281074524 at epoch 40\n",
      "loss: 0.23643264174461365 at epoch 41\n",
      "loss: 0.25888198614120483 at epoch 41\n",
      "loss: 0.3040199279785156 at epoch 41\n",
      "loss: 0.2371699959039688 at epoch 41\n",
      "loss: 0.2515546381473541 at epoch 41\n",
      "loss: 0.38213467597961426 at epoch 41\n",
      "loss: 0.34401851892471313 at epoch 41\n",
      "loss: 0.25361552834510803 at epoch 41\n",
      "loss: 0.2692275643348694 at epoch 42\n",
      "loss: 0.23446665704250336 at epoch 42\n",
      "loss: 0.20037543773651123 at epoch 42\n",
      "loss: 0.27861180901527405 at epoch 42\n",
      "loss: 0.30205047130584717 at epoch 42\n",
      "loss: 0.36253073811531067 at epoch 42\n",
      "loss: 0.2462248057126999 at epoch 42\n",
      "loss: 0.22308644652366638 at epoch 42\n",
      "loss: 0.25795382261276245 at epoch 43\n",
      "loss: 0.22825276851654053 at epoch 43\n",
      "loss: 0.3198513984680176 at epoch 43\n",
      "loss: 0.35765504837036133 at epoch 43\n",
      "loss: 0.15191936492919922 at epoch 43\n",
      "loss: 0.22464270889759064 at epoch 43\n",
      "loss: 0.19809365272521973 at epoch 43\n",
      "loss: 0.21161936223506927 at epoch 43\n",
      "loss: 0.3169335424900055 at epoch 44\n",
      "loss: 0.24484547972679138 at epoch 44\n",
      "loss: 0.25398409366607666 at epoch 44\n",
      "loss: 0.17737217247486115 at epoch 44\n",
      "loss: 0.2647498846054077 at epoch 44\n",
      "loss: 0.21371078491210938 at epoch 44\n",
      "loss: 0.2310861200094223 at epoch 44\n",
      "loss: 0.15732629597187042 at epoch 44\n",
      "loss: 0.2178983986377716 at epoch 45\n",
      "loss: 0.16247211396694183 at epoch 45\n",
      "loss: 0.2906745374202728 at epoch 45\n",
      "loss: 0.18581338226795197 at epoch 45\n",
      "loss: 0.1976379156112671 at epoch 45\n",
      "loss: 0.15889610350131989 at epoch 45\n",
      "loss: 0.21548408269882202 at epoch 45\n",
      "loss: 0.3014398515224457 at epoch 45\n",
      "loss: 0.15200918912887573 at epoch 46\n",
      "loss: 0.2120964378118515 at epoch 46\n",
      "loss: 0.16003361344337463 at epoch 46\n",
      "loss: 0.18573397397994995 at epoch 46\n",
      "loss: 0.2625074088573456 at epoch 46\n",
      "loss: 0.2517584264278412 at epoch 46\n",
      "loss: 0.16924495995044708 at epoch 46\n",
      "loss: 0.20148152112960815 at epoch 46\n",
      "loss: 0.18120141327381134 at epoch 47\n",
      "loss: 0.25249671936035156 at epoch 47\n",
      "loss: 0.12339158356189728 at epoch 47\n",
      "loss: 0.2306344360113144 at epoch 47\n",
      "loss: 0.17669381201267242 at epoch 47\n",
      "loss: 0.17044435441493988 at epoch 47\n",
      "loss: 0.18750475347042084 at epoch 47\n",
      "loss: 0.181085005402565 at epoch 47\n",
      "loss: 0.1704491674900055 at epoch 48\n",
      "loss: 0.1490636169910431 at epoch 48\n",
      "loss: 0.18815135955810547 at epoch 48\n",
      "loss: 0.20295508205890656 at epoch 48\n",
      "loss: 0.1770634949207306 at epoch 48\n",
      "loss: 0.12610864639282227 at epoch 48\n",
      "loss: 0.22362738847732544 at epoch 48\n",
      "loss: 0.17775477468967438 at epoch 48\n",
      "loss: 0.18098144233226776 at epoch 49\n",
      "loss: 0.1449413299560547 at epoch 49\n",
      "loss: 0.11974671483039856 at epoch 49\n",
      "loss: 0.11132998764514923 at epoch 49\n",
      "loss: 0.20752495527267456 at epoch 49\n",
      "loss: 0.23034578561782837 at epoch 49\n",
      "loss: 0.21311871707439423 at epoch 49\n",
      "loss: 0.17697614431381226 at epoch 49\n",
      "loss: 0.15047544240951538 at epoch 50\n",
      "loss: 0.20870725810527802 at epoch 50\n",
      "loss: 0.1825171858072281 at epoch 50\n",
      "loss: 0.12514013051986694 at epoch 50\n",
      "loss: 0.16418609023094177 at epoch 50\n",
      "loss: 0.22845201194286346 at epoch 50\n",
      "loss: 0.11106488108634949 at epoch 50\n",
      "loss: 0.1318999081850052 at epoch 50\n",
      "loss: 0.20422424376010895 at epoch 51\n",
      "loss: 0.14598143100738525 at epoch 51\n",
      "loss: 0.15742221474647522 at epoch 51\n",
      "loss: 0.13082203269004822 at epoch 51\n",
      "loss: 0.13637827336788177 at epoch 51\n",
      "loss: 0.1825856864452362 at epoch 51\n",
      "loss: 0.17728273570537567 at epoch 51\n",
      "loss: 0.10155788064002991 at epoch 51\n",
      "loss: 0.23937676846981049 at epoch 52\n",
      "loss: 0.08993347734212875 at epoch 52\n",
      "loss: 0.1479259729385376 at epoch 52\n",
      "loss: 0.13262391090393066 at epoch 52\n",
      "loss: 0.10234423726797104 at epoch 52\n",
      "loss: 0.13929061591625214 at epoch 52\n",
      "loss: 0.1958627700805664 at epoch 52\n",
      "loss: 0.09510155767202377 at epoch 52\n",
      "loss: 0.11307679116725922 at epoch 53\n",
      "loss: 0.10699871182441711 at epoch 53\n",
      "loss: 0.1309245526790619 at epoch 53\n",
      "loss: 0.15159206092357635 at epoch 53\n",
      "loss: 0.2351156324148178 at epoch 53\n",
      "loss: 0.07993923127651215 at epoch 53\n",
      "loss: 0.10829787701368332 at epoch 53\n",
      "loss: 0.17652608454227448 at epoch 53\n",
      "loss: 0.1527150571346283 at epoch 54\n",
      "loss: 0.17731362581253052 at epoch 54\n",
      "loss: 0.11837445199489594 at epoch 54\n",
      "loss: 0.16256773471832275 at epoch 54\n",
      "loss: 0.1243191510438919 at epoch 54\n",
      "loss: 0.10132262110710144 at epoch 54\n",
      "loss: 0.12163296341896057 at epoch 54\n",
      "loss: 0.09292270243167877 at epoch 54\n",
      "loss: 0.11919897794723511 at epoch 55\n",
      "loss: 0.09534455090761185 at epoch 55\n",
      "loss: 0.1543000489473343 at epoch 55\n",
      "loss: 0.12326942384243011 at epoch 55\n",
      "loss: 0.1246262937784195 at epoch 55\n",
      "loss: 0.12665435671806335 at epoch 55\n",
      "loss: 0.11569920182228088 at epoch 55\n",
      "loss: 0.15721967816352844 at epoch 55\n",
      "loss: 0.13513362407684326 at epoch 56\n",
      "loss: 0.13293488323688507 at epoch 56\n",
      "loss: 0.10426364839076996 at epoch 56\n",
      "loss: 0.13645535707473755 at epoch 56\n",
      "loss: 0.07255833595991135 at epoch 56\n",
      "loss: 0.12781566381454468 at epoch 56\n",
      "loss: 0.14989911019802094 at epoch 56\n",
      "loss: 0.09075760841369629 at epoch 56\n",
      "loss: 0.0837019681930542 at epoch 57\n",
      "loss: 0.12907075881958008 at epoch 57\n",
      "loss: 0.14237551391124725 at epoch 57\n",
      "loss: 0.11528131365776062 at epoch 57\n",
      "loss: 0.08320555090904236 at epoch 57\n",
      "loss: 0.11380571871995926 at epoch 57\n",
      "loss: 0.08245405554771423 at epoch 57\n",
      "loss: 0.1683543473482132 at epoch 57\n",
      "loss: 0.0913577452301979 at epoch 58\n",
      "loss: 0.07527763396501541 at epoch 58\n",
      "loss: 0.055919498205184937 at epoch 58\n",
      "loss: 0.13147172331809998 at epoch 58\n",
      "loss: 0.11603853851556778 at epoch 58\n",
      "loss: 0.10174202173948288 at epoch 58\n",
      "loss: 0.08467144519090652 at epoch 58\n",
      "loss: 0.2164832204580307 at epoch 58\n",
      "loss: 0.15787635743618011 at epoch 59\n",
      "loss: 0.07613446563482285 at epoch 59\n",
      "loss: 0.09997865557670593 at epoch 59\n",
      "loss: 0.1335548758506775 at epoch 59\n",
      "loss: 0.07326097786426544 at epoch 59\n",
      "loss: 0.06574753671884537 at epoch 59\n",
      "loss: 0.09038674831390381 at epoch 59\n",
      "loss: 0.18341152369976044 at epoch 59\n",
      "loss: 0.11502357572317123 at epoch 60\n",
      "loss: 0.11520253121852875 at epoch 60\n",
      "loss: 0.05749744176864624 at epoch 60\n",
      "loss: 0.10587038099765778 at epoch 60\n",
      "loss: 0.11902374029159546 at epoch 60\n",
      "loss: 0.08739234507083893 at epoch 60\n",
      "loss: 0.11430872231721878 at epoch 60\n",
      "loss: 0.10405847430229187 at epoch 60\n",
      "loss: 0.08019675314426422 at epoch 61\n",
      "loss: 0.07832570374011993 at epoch 61\n",
      "loss: 0.06507111340761185 at epoch 61\n",
      "loss: 0.17333318293094635 at epoch 61\n",
      "loss: 0.0628078430891037 at epoch 61\n",
      "loss: 0.14796043932437897 at epoch 61\n",
      "loss: 0.08036115765571594 at epoch 61\n",
      "loss: 0.07865297794342041 at epoch 61\n",
      "loss: 0.11892974376678467 at epoch 62\n",
      "loss: 0.06442244350910187 at epoch 62\n",
      "loss: 0.0837317481637001 at epoch 62\n",
      "loss: 0.059515759348869324 at epoch 62\n",
      "loss: 0.083834707736969 at epoch 62\n",
      "loss: 0.15849536657333374 at epoch 62\n",
      "loss: 0.11557035148143768 at epoch 62\n",
      "loss: 0.06477039307355881 at epoch 62\n",
      "loss: 0.11192530393600464 at epoch 63\n",
      "loss: 0.07638363540172577 at epoch 63\n",
      "loss: 0.08044129610061646 at epoch 63\n",
      "loss: 0.059398286044597626 at epoch 63\n",
      "loss: 0.2112533003091812 at epoch 63\n",
      "loss: 0.07895606011152267 at epoch 63\n",
      "loss: 0.04213628172874451 at epoch 63\n",
      "loss: 0.045582905411720276 at epoch 63\n",
      "loss: 0.09495335072278976 at epoch 64\n",
      "loss: 0.16749504208564758 at epoch 64\n",
      "loss: 0.06981925666332245 at epoch 64\n",
      "loss: 0.06587324291467667 at epoch 64\n",
      "loss: 0.04910318925976753 at epoch 64\n",
      "loss: 0.0488273948431015 at epoch 64\n",
      "loss: 0.08002524822950363 at epoch 64\n",
      "loss: 0.11916458606719971 at epoch 64\n",
      "loss: 0.10251496732234955 at epoch 65\n",
      "loss: 0.04087601229548454 at epoch 65\n",
      "loss: 0.08444889634847641 at epoch 65\n",
      "loss: 0.08869153261184692 at epoch 65\n",
      "loss: 0.12685148417949677 at epoch 65\n",
      "loss: 0.09050486981868744 at epoch 65\n",
      "loss: 0.03400624170899391 at epoch 65\n",
      "loss: 0.05640838295221329 at epoch 65\n",
      "loss: 0.07354770600795746 at epoch 66\n",
      "loss: 0.07895758748054504 at epoch 66\n",
      "loss: 0.0709131509065628 at epoch 66\n",
      "loss: 0.05159600451588631 at epoch 66\n",
      "loss: 0.08962643891572952 at epoch 66\n",
      "loss: 0.061722151935100555 at epoch 66\n",
      "loss: 0.04306209832429886 at epoch 66\n",
      "loss: 0.15958666801452637 at epoch 66\n",
      "loss: 0.12679605185985565 at epoch 67\n",
      "loss: 0.054938867688179016 at epoch 67\n",
      "loss: 0.08364088833332062 at epoch 67\n",
      "loss: 0.04266414791345596 at epoch 67\n",
      "loss: 0.05654304102063179 at epoch 67\n",
      "loss: 0.10129456967115402 at epoch 67\n",
      "loss: 0.05982963368296623 at epoch 67\n",
      "loss: 0.041980572044849396 at epoch 67\n",
      "loss: 0.08784043788909912 at epoch 68\n",
      "loss: 0.06739151477813721 at epoch 68\n",
      "loss: 0.036084242165088654 at epoch 68\n",
      "loss: 0.10034222900867462 at epoch 68\n",
      "loss: 0.04541879519820213 at epoch 68\n",
      "loss: 0.09661497920751572 at epoch 68\n",
      "loss: 0.03260313719511032 at epoch 68\n",
      "loss: 0.07345223426818848 at epoch 68\n",
      "loss: 0.05984862893819809 at epoch 69\n",
      "loss: 0.04986334592103958 at epoch 69\n",
      "loss: 0.12165892869234085 at epoch 69\n",
      "loss: 0.07604143023490906 at epoch 69\n",
      "loss: 0.029354147613048553 at epoch 69\n",
      "loss: 0.08973845839500427 at epoch 69\n",
      "loss: 0.04728430509567261 at epoch 69\n",
      "loss: 0.05556299164891243 at epoch 69\n",
      "loss: 0.06387898325920105 at epoch 70\n",
      "loss: 0.04578990489244461 at epoch 70\n",
      "loss: 0.10955597460269928 at epoch 70\n",
      "loss: 0.062267791479825974 at epoch 70\n",
      "loss: 0.03398016467690468 at epoch 70\n",
      "loss: 0.04825400561094284 at epoch 70\n",
      "loss: 0.054775867611169815 at epoch 70\n",
      "loss: 0.06824365258216858 at epoch 70\n",
      "loss: 0.05600621923804283 at epoch 71\n",
      "loss: 0.07636873424053192 at epoch 71\n",
      "loss: 0.029070528224110603 at epoch 71\n",
      "loss: 0.09874173998832703 at epoch 71\n",
      "loss: 0.08013377338647842 at epoch 71\n",
      "loss: 0.05160799250006676 at epoch 71\n",
      "loss: 0.03861765190958977 at epoch 71\n",
      "loss: 0.02399858459830284 at epoch 71\n",
      "loss: 0.041935600340366364 at epoch 72\n",
      "loss: 0.050254713743925095 at epoch 72\n",
      "loss: 0.08004103600978851 at epoch 72\n",
      "loss: 0.02636648342013359 at epoch 72\n",
      "loss: 0.07524985820055008 at epoch 72\n",
      "loss: 0.040291883051395416 at epoch 72\n",
      "loss: 0.02299756370484829 at epoch 72\n",
      "loss: 0.11307699233293533 at epoch 72\n",
      "loss: 0.05484599620103836 at epoch 73\n",
      "loss: 0.11030342429876328 at epoch 73\n",
      "loss: 0.03206827491521835 at epoch 73\n",
      "loss: 0.042509764432907104 at epoch 73\n",
      "loss: 0.08635430783033371 at epoch 73\n",
      "loss: 0.0330236554145813 at epoch 73\n",
      "loss: 0.03322379291057587 at epoch 73\n",
      "loss: 0.037561483681201935 at epoch 73\n",
      "loss: 0.07765186578035355 at epoch 74\n",
      "loss: 0.04300113394856453 at epoch 74\n",
      "loss: 0.02793307416141033 at epoch 74\n",
      "loss: 0.07126685976982117 at epoch 74\n",
      "loss: 0.05886145308613777 at epoch 74\n",
      "loss: 0.03410527482628822 at epoch 74\n",
      "loss: 0.0665523037314415 at epoch 74\n",
      "loss: 0.03242511302232742 at epoch 74\n",
      "loss: 0.08604186028242111 at epoch 75\n",
      "loss: 0.037143100053071976 at epoch 75\n",
      "loss: 0.060106921941041946 at epoch 75\n",
      "loss: 0.05907062441110611 at epoch 75\n",
      "loss: 0.022599872201681137 at epoch 75\n",
      "loss: 0.07011504471302032 at epoch 75\n",
      "loss: 0.03624517098069191 at epoch 75\n",
      "loss: 0.01811997778713703 at epoch 75\n",
      "loss: 0.06578898429870605 at epoch 76\n",
      "loss: 0.06863882392644882 at epoch 76\n",
      "loss: 0.041847825050354004 at epoch 76\n",
      "loss: 0.02947683446109295 at epoch 76\n",
      "loss: 0.024126751348376274 at epoch 76\n",
      "loss: 0.055375829339027405 at epoch 76\n",
      "loss: 0.025548473000526428 at epoch 76\n",
      "loss: 0.07370831817388535 at epoch 76\n",
      "loss: 0.02418266050517559 at epoch 77\n",
      "loss: 0.09061884880065918 at epoch 77\n",
      "loss: 0.06152474507689476 at epoch 77\n",
      "loss: 0.031651318073272705 at epoch 77\n",
      "loss: 0.05375991389155388 at epoch 77\n",
      "loss: 0.041398290544748306 at epoch 77\n",
      "loss: 0.020013853907585144 at epoch 77\n",
      "loss: 0.03147251531481743 at epoch 77\n",
      "loss: 0.03240905702114105 at epoch 78\n",
      "loss: 0.05211736634373665 at epoch 78\n",
      "loss: 0.021189458668231964 at epoch 78\n",
      "loss: 0.057437408715486526 at epoch 78\n",
      "loss: 0.07444074749946594 at epoch 78\n",
      "loss: 0.06580185890197754 at epoch 78\n",
      "loss: 0.013064177706837654 at epoch 78\n",
      "loss: 0.03197392076253891 at epoch 78\n",
      "loss: 0.05775881186127663 at epoch 79\n",
      "loss: 0.02842460200190544 at epoch 79\n",
      "loss: 0.061564479023218155 at epoch 79\n",
      "loss: 0.03597937524318695 at epoch 79\n",
      "loss: 0.04798375442624092 at epoch 79\n",
      "loss: 0.034089069813489914 at epoch 79\n",
      "loss: 0.04449129104614258 at epoch 79\n",
      "loss: 0.018803555518388748 at epoch 79\n",
      "loss: 0.02222573384642601 at epoch 80\n",
      "loss: 0.062093522399663925 at epoch 80\n",
      "loss: 0.10156376659870148 at epoch 80\n",
      "loss: 0.02992377057671547 at epoch 80\n",
      "loss: 0.012774079106748104 at epoch 80\n",
      "loss: 0.0392032153904438 at epoch 80\n",
      "loss: 0.026269471272826195 at epoch 80\n",
      "loss: 0.02275354601442814 at epoch 80\n",
      "loss: 0.05250365287065506 at epoch 81\n",
      "loss: 0.020215237513184547 at epoch 81\n",
      "loss: 0.018951360136270523 at epoch 81\n",
      "loss: 0.036056868731975555 at epoch 81\n",
      "loss: 0.04542507231235504 at epoch 81\n",
      "loss: 0.048389703035354614 at epoch 81\n",
      "loss: 0.04730306565761566 at epoch 81\n",
      "loss: 0.031828854233026505 at epoch 81\n",
      "loss: 0.01806091144680977 at epoch 82\n",
      "loss: 0.04534551128745079 at epoch 82\n",
      "loss: 0.038444261997938156 at epoch 82\n",
      "loss: 0.03424378111958504 at epoch 82\n",
      "loss: 0.028394704684615135 at epoch 82\n",
      "loss: 0.07144090533256531 at epoch 82\n",
      "loss: 0.027474280446767807 at epoch 82\n",
      "loss: 0.02731827087700367 at epoch 82\n",
      "loss: 0.05482904240489006 at epoch 83\n",
      "loss: 0.05675926432013512 at epoch 83\n",
      "loss: 0.02820778079330921 at epoch 83\n",
      "loss: 0.04203789681196213 at epoch 83\n",
      "loss: 0.041670653969049454 at epoch 83\n",
      "loss: 0.017975758761167526 at epoch 83\n",
      "loss: 0.027163539081811905 at epoch 83\n",
      "loss: 0.014726380817592144 at epoch 83\n",
      "loss: 0.015017019584774971 at epoch 84\n",
      "loss: 0.025378316640853882 at epoch 84\n",
      "loss: 0.06638700515031815 at epoch 84\n",
      "loss: 0.050711825489997864 at epoch 84\n",
      "loss: 0.020308762788772583 at epoch 84\n",
      "loss: 0.048980120569467545 at epoch 84\n",
      "loss: 0.022183861583471298 at epoch 84\n",
      "loss: 0.023240987211465836 at epoch 84\n",
      "loss: 0.022734588012099266 at epoch 85\n",
      "loss: 0.02115297131240368 at epoch 85\n",
      "loss: 0.007839782163500786 at epoch 85\n",
      "loss: 0.03144608438014984 at epoch 85\n",
      "loss: 0.04373103380203247 at epoch 85\n",
      "loss: 0.08014077693223953 at epoch 85\n",
      "loss: 0.014222927391529083 at epoch 85\n",
      "loss: 0.04764196649193764 at epoch 85\n",
      "loss: 0.045485902577638626 at epoch 86\n",
      "loss: 0.01641460880637169 at epoch 86\n",
      "loss: 0.03052709996700287 at epoch 86\n",
      "loss: 0.014247732236981392 at epoch 86\n",
      "loss: 0.021926332265138626 at epoch 86\n",
      "loss: 0.06555602699518204 at epoch 86\n",
      "loss: 0.010952330194413662 at epoch 86\n",
      "loss: 0.050184011459350586 at epoch 86\n",
      "loss: 0.023104092106223106 at epoch 87\n",
      "loss: 0.051818545907735825 at epoch 87\n",
      "loss: 0.0471622571349144 at epoch 87\n",
      "loss: 0.016652803868055344 at epoch 87\n",
      "loss: 0.016128459945321083 at epoch 87\n",
      "loss: 0.051227569580078125 at epoch 87\n",
      "loss: 0.024277931079268456 at epoch 87\n",
      "loss: 0.008143075741827488 at epoch 87\n",
      "loss: 0.014777556993067265 at epoch 88\n",
      "loss: 0.016943251714110374 at epoch 88\n",
      "loss: 0.05383177101612091 at epoch 88\n",
      "loss: 0.009854573756456375 at epoch 88\n",
      "loss: 0.02620970457792282 at epoch 88\n",
      "loss: 0.043750960379838943 at epoch 88\n",
      "loss: 0.05707039684057236 at epoch 88\n",
      "loss: 0.015653733164072037 at epoch 88\n",
      "loss: 0.04106127843260765 at epoch 89\n",
      "loss: 0.019994767382740974 at epoch 89\n",
      "loss: 0.03866204619407654 at epoch 89\n",
      "loss: 0.023410893976688385 at epoch 89\n",
      "loss: 0.015792658552527428 at epoch 89\n",
      "loss: 0.05341021344065666 at epoch 89\n",
      "loss: 0.02775590494275093 at epoch 89\n",
      "loss: 0.008377032354474068 at epoch 89\n",
      "loss: 0.022133316844701767 at epoch 90\n",
      "loss: 0.01702401600778103 at epoch 90\n",
      "loss: 0.008741437457501888 at epoch 90\n",
      "loss: 0.04101807251572609 at epoch 90\n",
      "loss: 0.023858770728111267 at epoch 90\n",
      "loss: 0.07075125724077225 at epoch 90\n",
      "loss: 0.021052103489637375 at epoch 90\n",
      "loss: 0.020955385640263557 at epoch 90\n",
      "loss: 0.028442731127142906 at epoch 91\n",
      "loss: 0.052222829312086105 at epoch 91\n",
      "loss: 0.015497617423534393 at epoch 91\n",
      "loss: 0.01512059010565281 at epoch 91\n",
      "loss: 0.05212528631091118 at epoch 91\n",
      "loss: 0.031686194241046906 at epoch 91\n",
      "loss: 0.0107130017131567 at epoch 91\n",
      "loss: 0.010126092471182346 at epoch 91\n",
      "loss: 0.00867537036538124 at epoch 92\n",
      "loss: 0.03597022220492363 at epoch 92\n",
      "loss: 0.02274586632847786 at epoch 92\n",
      "loss: 0.04658404365181923 at epoch 92\n",
      "loss: 0.011902032420039177 at epoch 92\n",
      "loss: 0.019649412482976913 at epoch 92\n",
      "loss: 0.040899794548749924 at epoch 92\n",
      "loss: 0.02596643567085266 at epoch 92\n",
      "loss: 0.015819329768419266 at epoch 93\n",
      "loss: 0.012529886327683926 at epoch 93\n",
      "loss: 0.020391613245010376 at epoch 93\n",
      "loss: 0.01850341074168682 at epoch 93\n",
      "loss: 0.03219987824559212 at epoch 93\n",
      "loss: 0.019222822040319443 at epoch 93\n",
      "loss: 0.0667671486735344 at epoch 93\n",
      "loss: 0.012265561148524284 at epoch 93\n",
      "loss: 0.04041142016649246 at epoch 94\n",
      "loss: 0.017649134621024132 at epoch 94\n",
      "loss: 0.00927102193236351 at epoch 94\n",
      "loss: 0.015065440908074379 at epoch 94\n",
      "loss: 0.03213111683726311 at epoch 94\n",
      "loss: 0.03540108725428581 at epoch 94\n",
      "loss: 0.025466691702604294 at epoch 94\n",
      "loss: 0.018778089433908463 at epoch 94\n",
      "loss: 0.02487683668732643 at epoch 95\n",
      "loss: 0.03349700942635536 at epoch 95\n",
      "loss: 0.014480321668088436 at epoch 95\n",
      "loss: 0.02372295968234539 at epoch 95\n",
      "loss: 0.01373091246932745 at epoch 95\n",
      "loss: 0.01533547230064869 at epoch 95\n",
      "loss: 0.054905153810977936 at epoch 95\n",
      "loss: 0.009747679345309734 at epoch 95\n",
      "loss: 0.03946967050433159 at epoch 96\n",
      "loss: 0.009591101668775082 at epoch 96\n",
      "loss: 0.03965418413281441 at epoch 96\n",
      "loss: 0.015070291236042976 at epoch 96\n",
      "loss: 0.019172152504324913 at epoch 96\n",
      "loss: 0.035402730107307434 at epoch 96\n",
      "loss: 0.03020343743264675 at epoch 96\n",
      "loss: 0.010117318481206894 at epoch 96\n",
      "loss: 0.01079503819346428 at epoch 97\n",
      "loss: 0.032190125435590744 at epoch 97\n",
      "loss: 0.018914684653282166 at epoch 97\n",
      "loss: 0.04193693771958351 at epoch 97\n",
      "loss: 0.022102802991867065 at epoch 97\n",
      "loss: 0.02073238417506218 at epoch 97\n",
      "loss: 0.03442411124706268 at epoch 97\n",
      "loss: 0.007750296965241432 at epoch 97\n",
      "loss: 0.04214312881231308 at epoch 98\n",
      "loss: 0.01505918987095356 at epoch 98\n",
      "loss: 0.015528546646237373 at epoch 98\n",
      "loss: 0.014757556840777397 at epoch 98\n",
      "loss: 0.011867621913552284 at epoch 98\n",
      "loss: 0.020969312638044357 at epoch 98\n",
      "loss: 0.025460314005613327 at epoch 98\n",
      "loss: 0.04114023596048355 at epoch 98\n",
      "loss: 0.02025085687637329 at epoch 99\n",
      "loss: 0.01166999340057373 at epoch 99\n",
      "loss: 0.03822500631213188 at epoch 99\n",
      "loss: 0.04819339141249657 at epoch 99\n",
      "loss: 0.015009259805083275 at epoch 99\n",
      "loss: 0.011793362908065319 at epoch 99\n",
      "loss: 0.02029642090201378 at epoch 99\n",
      "loss: 0.009484498761594296 at epoch 99\n",
      "\n",
      "=== Iteration 0: Pure Assignment ===\n",
      "faculty_vectors: [[0.08362218 0.17375544 0.16631222 0.54604909 0.03026108]\n",
      " [0.02326233 0.07179004 0.31720936 0.21441055 0.37332771]\n",
      " [0.19568395 0.26757348 0.2104309  0.23123746 0.09507421]\n",
      " ...\n",
      " [0.08362218 0.17375544 0.16631222 0.54604909 0.03026108]\n",
      " [0.2344319  0.08078156 0.31845629 0.24955716 0.11677309]\n",
      " [0.02326233 0.07179004 0.31720936 0.21441055 0.37332771]]\n",
      "student_features: [[41.37740283 55.54499531 63.2445039  90.99988334 68.24683762]\n",
      " [57.67969614 53.27236941 74.96803362 75.59032365 88.58033461]\n",
      " [74.98544741 87.48145353 50.41128859 53.43208998 41.4084824 ]\n",
      " ...\n",
      " [53.69226362 71.14954648 53.9785624  87.69581789 77.87375094]\n",
      " [96.81713608 61.26772906 53.84190687 63.89713764 56.43906141]\n",
      " [58.22900084 43.17256947 84.19873199 51.70050632 96.21627865]]\n",
      "\n",
      "=== Iteration 1: Student Learning ===\n",
      "loss: 1.901905 at epoch 0 at applicants training\n",
      "loss: 1.898167 at epoch 1 at applicants training\n",
      "loss: 1.891237 at epoch 2 at applicants training\n",
      "loss: 1.880817 at epoch 3 at applicants training\n",
      "loss: 1.866385 at epoch 4 at applicants training\n",
      "loss: 1.845443 at epoch 5 at applicants training\n",
      "loss: 1.817933 at epoch 6 at applicants training\n",
      "loss: 1.784011 at epoch 7 at applicants training\n",
      "loss: 1.743073 at epoch 8 at applicants training\n",
      "loss: 1.701621 at epoch 9 at applicants training\n",
      "loss: 1.662756 at epoch 10 at applicants training\n",
      "loss: 1.625429 at epoch 11 at applicants training\n",
      "loss: 1.596609 at epoch 12 at applicants training\n",
      "loss: 1.583228 at epoch 13 at applicants training\n",
      "loss: 1.579615 at epoch 14 at applicants training\n",
      "loss: 1.578504 at epoch 15 at applicants training\n",
      "loss: 1.577978 at epoch 16 at applicants training\n",
      "loss: 1.577645 at epoch 17 at applicants training\n",
      "loss: 1.577461 at epoch 18 at applicants training\n",
      "loss: 1.577384 at epoch 19 at applicants training\n",
      "loss: 1.577359 at epoch 20 at applicants training\n",
      "loss: 1.577366 at epoch 21 at applicants training\n",
      "loss: 1.577369 at epoch 22 at applicants training\n",
      "loss: 1.577364 at epoch 23 at applicants training\n",
      "loss: 1.577339 at epoch 24 at applicants training\n",
      "loss: 1.577291 at epoch 25 at applicants training\n",
      "loss: 1.577217 at epoch 26 at applicants training\n",
      "loss: 1.577111 at epoch 27 at applicants training\n",
      "loss: 1.576958 at epoch 28 at applicants training\n",
      "loss: 1.576744 at epoch 29 at applicants training\n",
      "loss: 1.576443 at epoch 30 at applicants training\n",
      "loss: 1.576024 at epoch 31 at applicants training\n",
      "loss: 1.575442 at epoch 32 at applicants training\n",
      "loss: 1.574654 at epoch 33 at applicants training\n",
      "loss: 1.573614 at epoch 34 at applicants training\n",
      "loss: 1.572273 at epoch 35 at applicants training\n",
      "loss: 1.570532 at epoch 36 at applicants training\n",
      "loss: 1.568212 at epoch 37 at applicants training\n",
      "loss: 1.565187 at epoch 38 at applicants training\n",
      "loss: 1.561627 at epoch 39 at applicants training\n",
      "loss: 1.558071 at epoch 40 at applicants training\n",
      "loss: 1.554853 at epoch 41 at applicants training\n",
      "loss: 1.551334 at epoch 42 at applicants training\n",
      "loss: 1.546353 at epoch 43 at applicants training\n",
      "loss: 1.539742 at epoch 44 at applicants training\n",
      "loss: 1.533033 at epoch 45 at applicants training\n",
      "loss: 1.526674 at epoch 46 at applicants training\n",
      "loss: 1.519851 at epoch 47 at applicants training\n",
      "loss: 1.513420 at epoch 48 at applicants training\n",
      "loss: 1.507777 at epoch 49 at applicants training\n",
      "loss: 1.504447 at epoch 50 at applicants training\n",
      "loss: 1.503174 at epoch 51 at applicants training\n",
      "loss: 1.501088 at epoch 52 at applicants training\n",
      "loss: 1.496504 at epoch 53 at applicants training\n",
      "loss: 1.489459 at epoch 54 at applicants training\n",
      "loss: 1.481180 at epoch 55 at applicants training\n",
      "loss: 1.473884 at epoch 56 at applicants training\n",
      "loss: 1.468982 at epoch 57 at applicants training\n",
      "loss: 1.465491 at epoch 58 at applicants training\n",
      "loss: 1.461982 at epoch 59 at applicants training\n",
      "loss: 1.457852 at epoch 60 at applicants training\n",
      "loss: 1.452571 at epoch 61 at applicants training\n",
      "loss: 1.446430 at epoch 62 at applicants training\n",
      "loss: 1.440132 at epoch 63 at applicants training\n",
      "loss: 1.434531 at epoch 64 at applicants training\n",
      "loss: 1.430132 at epoch 65 at applicants training\n",
      "loss: 1.426436 at epoch 66 at applicants training\n",
      "loss: 1.422353 at epoch 67 at applicants training\n",
      "loss: 1.417342 at epoch 68 at applicants training\n",
      "loss: 1.411793 at epoch 69 at applicants training\n",
      "loss: 1.406637 at epoch 70 at applicants training\n",
      "loss: 1.402290 at epoch 71 at applicants training\n",
      "loss: 1.398322 at epoch 72 at applicants training\n",
      "loss: 1.393910 at epoch 73 at applicants training\n",
      "loss: 1.388717 at epoch 74 at applicants training\n",
      "loss: 1.383185 at epoch 75 at applicants training\n",
      "loss: 1.377835 at epoch 76 at applicants training\n",
      "loss: 1.372552 at epoch 77 at applicants training\n",
      "loss: 1.366584 at epoch 78 at applicants training\n",
      "loss: 1.359680 at epoch 79 at applicants training\n",
      "loss: 1.352361 at epoch 80 at applicants training\n",
      "loss: 1.345176 at epoch 81 at applicants training\n",
      "loss: 1.338072 at epoch 82 at applicants training\n",
      "loss: 1.330733 at epoch 83 at applicants training\n",
      "loss: 1.323107 at epoch 84 at applicants training\n",
      "loss: 1.315325 at epoch 85 at applicants training\n",
      "loss: 1.307467 at epoch 86 at applicants training\n",
      "loss: 1.299375 at epoch 87 at applicants training\n",
      "loss: 1.291113 at epoch 88 at applicants training\n",
      "loss: 1.282733 at epoch 89 at applicants training\n",
      "loss: 1.274328 at epoch 90 at applicants training\n",
      "loss: 1.266199 at epoch 91 at applicants training\n",
      "loss: 1.258672 at epoch 92 at applicants training\n",
      "loss: 1.251283 at epoch 93 at applicants training\n",
      "loss: 1.244136 at epoch 94 at applicants training\n",
      "loss: 1.237617 at epoch 95 at applicants training\n",
      "loss: 1.231568 at epoch 96 at applicants training\n",
      "loss: 1.225986 at epoch 97 at applicants training\n",
      "loss: 1.220902 at epoch 98 at applicants training\n",
      "loss: 1.216029 at epoch 99 at applicants training\n",
      "loss: 1.211464 at epoch 100 at applicants training\n",
      "loss: 1.207370 at epoch 101 at applicants training\n",
      "loss: 1.203522 at epoch 102 at applicants training\n",
      "loss: 1.199843 at epoch 103 at applicants training\n",
      "loss: 1.196368 at epoch 104 at applicants training\n",
      "loss: 1.193057 at epoch 105 at applicants training\n",
      "loss: 1.189982 at epoch 106 at applicants training\n",
      "loss: 1.187021 at epoch 107 at applicants training\n",
      "loss: 1.184084 at epoch 108 at applicants training\n",
      "loss: 1.181314 at epoch 109 at applicants training\n",
      "loss: 1.178683 at epoch 110 at applicants training\n",
      "loss: 1.176118 at epoch 111 at applicants training\n",
      "loss: 1.173675 at epoch 112 at applicants training\n",
      "loss: 1.171319 at epoch 113 at applicants training\n",
      "loss: 1.169076 at epoch 114 at applicants training\n",
      "loss: 1.166940 at epoch 115 at applicants training\n",
      "loss: 1.164865 at epoch 116 at applicants training\n",
      "loss: 1.162906 at epoch 117 at applicants training\n",
      "loss: 1.161070 at epoch 118 at applicants training\n",
      "loss: 1.159297 at epoch 119 at applicants training\n",
      "loss: 1.157611 at epoch 120 at applicants training\n",
      "loss: 1.156046 at epoch 121 at applicants training\n",
      "loss: 1.154569 at epoch 122 at applicants training\n",
      "loss: 1.153174 at epoch 123 at applicants training\n",
      "loss: 1.151883 at epoch 124 at applicants training\n",
      "loss: 1.150686 at epoch 125 at applicants training\n",
      "loss: 1.149574 at epoch 126 at applicants training\n",
      "loss: 1.148540 at epoch 127 at applicants training\n",
      "loss: 1.147592 at epoch 128 at applicants training\n",
      "loss: 1.146722 at epoch 129 at applicants training\n",
      "loss: 1.145909 at epoch 130 at applicants training\n",
      "loss: 1.145160 at epoch 131 at applicants training\n",
      "loss: 1.144471 at epoch 132 at applicants training\n",
      "loss: 1.143819 at epoch 133 at applicants training\n",
      "loss: 1.143210 at epoch 134 at applicants training\n",
      "loss: 1.142640 at epoch 135 at applicants training\n",
      "loss: 1.142096 at epoch 136 at applicants training\n",
      "loss: 1.141576 at epoch 137 at applicants training\n",
      "loss: 1.141078 at epoch 138 at applicants training\n",
      "loss: 1.140597 at epoch 139 at applicants training\n",
      "loss: 1.140130 at epoch 140 at applicants training\n",
      "loss: 1.139675 at epoch 141 at applicants training\n",
      "loss: 1.139236 at epoch 142 at applicants training\n",
      "loss: 1.138806 at epoch 143 at applicants training\n",
      "loss: 1.138386 at epoch 144 at applicants training\n",
      "loss: 1.137980 at epoch 145 at applicants training\n",
      "loss: 1.137585 at epoch 146 at applicants training\n",
      "loss: 1.137199 at epoch 147 at applicants training\n",
      "loss: 1.136826 at epoch 148 at applicants training\n",
      "loss: 1.136463 at epoch 149 at applicants training\n",
      "loss: 1.136109 at epoch 150 at applicants training\n",
      "loss: 1.135764 at epoch 151 at applicants training\n",
      "loss: 1.135427 at epoch 152 at applicants training\n",
      "loss: 1.135097 at epoch 153 at applicants training\n",
      "loss: 1.134772 at epoch 154 at applicants training\n",
      "loss: 1.134452 at epoch 155 at applicants training\n",
      "loss: 1.134138 at epoch 156 at applicants training\n",
      "loss: 1.133828 at epoch 157 at applicants training\n",
      "loss: 1.133523 at epoch 158 at applicants training\n",
      "loss: 1.133220 at epoch 159 at applicants training\n",
      "loss: 1.132922 at epoch 160 at applicants training\n",
      "loss: 1.132629 at epoch 161 at applicants training\n",
      "loss: 1.132339 at epoch 162 at applicants training\n",
      "loss: 1.132053 at epoch 163 at applicants training\n",
      "loss: 1.131769 at epoch 164 at applicants training\n",
      "loss: 1.131483 at epoch 165 at applicants training\n",
      "loss: 1.131201 at epoch 166 at applicants training\n",
      "loss: 1.130923 at epoch 167 at applicants training\n",
      "loss: 1.130648 at epoch 168 at applicants training\n",
      "loss: 1.130378 at epoch 169 at applicants training\n",
      "loss: 1.130114 at epoch 170 at applicants training\n",
      "loss: 1.129856 at epoch 171 at applicants training\n",
      "loss: 1.129602 at epoch 172 at applicants training\n",
      "loss: 1.129351 at epoch 173 at applicants training\n",
      "loss: 1.129102 at epoch 174 at applicants training\n",
      "loss: 1.128857 at epoch 175 at applicants training\n",
      "loss: 1.128619 at epoch 176 at applicants training\n",
      "loss: 1.128384 at epoch 177 at applicants training\n",
      "loss: 1.128152 at epoch 178 at applicants training\n",
      "loss: 1.127923 at epoch 179 at applicants training\n",
      "loss: 1.127697 at epoch 180 at applicants training\n",
      "loss: 1.127474 at epoch 181 at applicants training\n",
      "loss: 1.127255 at epoch 182 at applicants training\n",
      "loss: 1.127039 at epoch 183 at applicants training\n",
      "loss: 1.126826 at epoch 184 at applicants training\n",
      "loss: 1.126615 at epoch 185 at applicants training\n",
      "loss: 1.126408 at epoch 186 at applicants training\n",
      "loss: 1.126204 at epoch 187 at applicants training\n",
      "loss: 1.126002 at epoch 188 at applicants training\n",
      "loss: 1.125803 at epoch 189 at applicants training\n",
      "loss: 1.125607 at epoch 190 at applicants training\n",
      "loss: 1.125414 at epoch 191 at applicants training\n",
      "loss: 1.125223 at epoch 192 at applicants training\n",
      "loss: 1.125034 at epoch 193 at applicants training\n",
      "loss: 1.124845 at epoch 194 at applicants training\n",
      "loss: 1.124659 at epoch 195 at applicants training\n",
      "loss: 1.124475 at epoch 196 at applicants training\n",
      "loss: 1.124293 at epoch 197 at applicants training\n",
      "loss: 1.124113 at epoch 198 at applicants training\n",
      "loss: 1.123935 at epoch 199 at applicants training\n",
      "loss: 1.123758 at epoch 200 at applicants training\n",
      "loss: 1.123582 at epoch 201 at applicants training\n",
      "loss: 1.123407 at epoch 202 at applicants training\n",
      "loss: 1.123232 at epoch 203 at applicants training\n",
      "loss: 1.123058 at epoch 204 at applicants training\n",
      "loss: 1.122885 at epoch 205 at applicants training\n",
      "loss: 1.122714 at epoch 206 at applicants training\n",
      "loss: 1.122547 at epoch 207 at applicants training\n",
      "loss: 1.122381 at epoch 208 at applicants training\n",
      "loss: 1.122216 at epoch 209 at applicants training\n",
      "loss: 1.122051 at epoch 210 at applicants training\n",
      "loss: 1.121888 at epoch 211 at applicants training\n",
      "loss: 1.121725 at epoch 212 at applicants training\n",
      "loss: 1.121564 at epoch 213 at applicants training\n",
      "loss: 1.121405 at epoch 214 at applicants training\n",
      "loss: 1.121248 at epoch 215 at applicants training\n",
      "loss: 1.121096 at epoch 216 at applicants training\n",
      "loss: 1.120945 at epoch 217 at applicants training\n",
      "loss: 1.120794 at epoch 218 at applicants training\n",
      "loss: 1.120645 at epoch 219 at applicants training\n",
      "loss: 1.120495 at epoch 220 at applicants training\n",
      "loss: 1.120345 at epoch 221 at applicants training\n",
      "loss: 1.120196 at epoch 222 at applicants training\n",
      "loss: 1.120047 at epoch 223 at applicants training\n",
      "loss: 1.119900 at epoch 224 at applicants training\n",
      "loss: 1.119750 at epoch 225 at applicants training\n",
      "loss: 1.119596 at epoch 226 at applicants training\n",
      "loss: 1.119447 at epoch 227 at applicants training\n",
      "loss: 1.119301 at epoch 228 at applicants training\n",
      "loss: 1.119154 at epoch 229 at applicants training\n",
      "loss: 1.119009 at epoch 230 at applicants training\n",
      "loss: 1.118867 at epoch 231 at applicants training\n",
      "loss: 1.118725 at epoch 232 at applicants training\n",
      "loss: 1.118582 at epoch 233 at applicants training\n",
      "loss: 1.118440 at epoch 234 at applicants training\n",
      "loss: 1.118298 at epoch 235 at applicants training\n",
      "loss: 1.118157 at epoch 236 at applicants training\n",
      "loss: 1.118019 at epoch 237 at applicants training\n",
      "loss: 1.117884 at epoch 238 at applicants training\n",
      "loss: 1.117751 at epoch 239 at applicants training\n",
      "loss: 1.117620 at epoch 240 at applicants training\n",
      "loss: 1.117490 at epoch 241 at applicants training\n",
      "loss: 1.117360 at epoch 242 at applicants training\n",
      "loss: 1.117229 at epoch 243 at applicants training\n",
      "loss: 1.117096 at epoch 244 at applicants training\n",
      "loss: 1.116960 at epoch 245 at applicants training\n",
      "loss: 1.116821 at epoch 246 at applicants training\n",
      "loss: 1.116689 at epoch 247 at applicants training\n",
      "loss: 1.116561 at epoch 248 at applicants training\n",
      "loss: 1.116437 at epoch 249 at applicants training\n",
      "loss: 1.116310 at epoch 250 at applicants training\n",
      "loss: 1.116188 at epoch 251 at applicants training\n",
      "loss: 1.116061 at epoch 252 at applicants training\n",
      "loss: 1.115932 at epoch 253 at applicants training\n",
      "loss: 1.115793 at epoch 254 at applicants training\n",
      "loss: 1.115670 at epoch 255 at applicants training\n",
      "loss: 1.115555 at epoch 256 at applicants training\n",
      "loss: 1.115432 at epoch 257 at applicants training\n",
      "loss: 1.115301 at epoch 258 at applicants training\n",
      "loss: 1.115180 at epoch 259 at applicants training\n",
      "loss: 1.115069 at epoch 260 at applicants training\n",
      "loss: 1.114954 at epoch 261 at applicants training\n",
      "loss: 1.114835 at epoch 262 at applicants training\n",
      "loss: 1.114717 at epoch 263 at applicants training\n",
      "loss: 1.114610 at epoch 264 at applicants training\n",
      "loss: 1.114506 at epoch 265 at applicants training\n",
      "loss: 1.114394 at epoch 266 at applicants training\n",
      "loss: 1.114286 at epoch 267 at applicants training\n",
      "loss: 1.114180 at epoch 268 at applicants training\n",
      "loss: 1.114075 at epoch 269 at applicants training\n",
      "loss: 1.113971 at epoch 270 at applicants training\n",
      "loss: 1.113865 at epoch 271 at applicants training\n",
      "loss: 1.113762 at epoch 272 at applicants training\n",
      "loss: 1.113658 at epoch 273 at applicants training\n",
      "loss: 1.113556 at epoch 274 at applicants training\n",
      "loss: 1.113454 at epoch 275 at applicants training\n",
      "loss: 1.113353 at epoch 276 at applicants training\n",
      "loss: 1.113256 at epoch 277 at applicants training\n",
      "loss: 1.113159 at epoch 278 at applicants training\n",
      "loss: 1.113058 at epoch 279 at applicants training\n",
      "loss: 1.112958 at epoch 280 at applicants training\n",
      "loss: 1.112862 at epoch 281 at applicants training\n",
      "loss: 1.112767 at epoch 282 at applicants training\n",
      "loss: 1.112670 at epoch 283 at applicants training\n",
      "loss: 1.112580 at epoch 284 at applicants training\n",
      "loss: 1.112485 at epoch 285 at applicants training\n",
      "loss: 1.112393 at epoch 286 at applicants training\n",
      "loss: 1.112301 at epoch 287 at applicants training\n",
      "loss: 1.112206 at epoch 288 at applicants training\n",
      "loss: 1.112125 at epoch 289 at applicants training\n",
      "loss: 1.112029 at epoch 290 at applicants training\n",
      "loss: 1.111941 at epoch 291 at applicants training\n",
      "loss: 1.111855 at epoch 292 at applicants training\n",
      "loss: 1.111767 at epoch 293 at applicants training\n",
      "loss: 1.111677 at epoch 294 at applicants training\n",
      "loss: 1.111586 at epoch 295 at applicants training\n",
      "loss: 1.111501 at epoch 296 at applicants training\n",
      "loss: 1.111419 at epoch 297 at applicants training\n",
      "loss: 1.111327 at epoch 298 at applicants training\n",
      "loss: 1.111243 at epoch 299 at applicants training\n",
      "loss: 1.111161 at epoch 300 at applicants training\n",
      "loss: 1.111077 at epoch 301 at applicants training\n",
      "loss: 1.110988 at epoch 302 at applicants training\n",
      "loss: 1.110910 at epoch 303 at applicants training\n",
      "loss: 1.110832 at epoch 304 at applicants training\n",
      "loss: 1.110741 at epoch 305 at applicants training\n",
      "loss: 1.110667 at epoch 306 at applicants training\n",
      "loss: 1.110592 at epoch 307 at applicants training\n",
      "loss: 1.110515 at epoch 308 at applicants training\n",
      "loss: 1.110434 at epoch 309 at applicants training\n",
      "loss: 1.110351 at epoch 310 at applicants training\n",
      "loss: 1.110267 at epoch 311 at applicants training\n",
      "loss: 1.110183 at epoch 312 at applicants training\n",
      "loss: 1.110110 at epoch 313 at applicants training\n",
      "loss: 1.110030 at epoch 314 at applicants training\n",
      "loss: 1.109951 at epoch 315 at applicants training\n",
      "loss: 1.109876 at epoch 316 at applicants training\n",
      "loss: 1.109798 at epoch 317 at applicants training\n",
      "loss: 1.109719 at epoch 318 at applicants training\n",
      "loss: 1.109644 at epoch 319 at applicants training\n",
      "loss: 1.109571 at epoch 320 at applicants training\n",
      "loss: 1.109494 at epoch 321 at applicants training\n",
      "loss: 1.109420 at epoch 322 at applicants training\n",
      "loss: 1.109346 at epoch 323 at applicants training\n",
      "loss: 1.109272 at epoch 324 at applicants training\n",
      "loss: 1.109198 at epoch 325 at applicants training\n",
      "loss: 1.109126 at epoch 326 at applicants training\n",
      "loss: 1.109052 at epoch 327 at applicants training\n",
      "loss: 1.108980 at epoch 328 at applicants training\n",
      "loss: 1.108911 at epoch 329 at applicants training\n",
      "loss: 1.108837 at epoch 330 at applicants training\n",
      "loss: 1.108765 at epoch 331 at applicants training\n",
      "loss: 1.108697 at epoch 332 at applicants training\n",
      "loss: 1.108625 at epoch 333 at applicants training\n",
      "loss: 1.108555 at epoch 334 at applicants training\n",
      "loss: 1.108486 at epoch 335 at applicants training\n",
      "loss: 1.108416 at epoch 336 at applicants training\n",
      "loss: 1.108347 at epoch 337 at applicants training\n",
      "loss: 1.108280 at epoch 338 at applicants training\n",
      "loss: 1.108209 at epoch 339 at applicants training\n",
      "loss: 1.108142 at epoch 340 at applicants training\n",
      "loss: 1.108077 at epoch 341 at applicants training\n",
      "loss: 1.108010 at epoch 342 at applicants training\n",
      "loss: 1.107945 at epoch 343 at applicants training\n",
      "loss: 1.107878 at epoch 344 at applicants training\n",
      "loss: 1.107810 at epoch 345 at applicants training\n",
      "loss: 1.107743 at epoch 346 at applicants training\n",
      "loss: 1.107677 at epoch 347 at applicants training\n",
      "loss: 1.107612 at epoch 348 at applicants training\n",
      "loss: 1.107548 at epoch 349 at applicants training\n",
      "loss: 1.107483 at epoch 350 at applicants training\n",
      "loss: 1.107418 at epoch 351 at applicants training\n",
      "loss: 1.107353 at epoch 352 at applicants training\n",
      "loss: 1.107290 at epoch 353 at applicants training\n",
      "loss: 1.107228 at epoch 354 at applicants training\n",
      "loss: 1.107165 at epoch 355 at applicants training\n",
      "loss: 1.107101 at epoch 356 at applicants training\n",
      "loss: 1.107039 at epoch 357 at applicants training\n",
      "loss: 1.106978 at epoch 358 at applicants training\n",
      "loss: 1.106917 at epoch 359 at applicants training\n",
      "loss: 1.106856 at epoch 360 at applicants training\n",
      "loss: 1.106793 at epoch 361 at applicants training\n",
      "loss: 1.106730 at epoch 362 at applicants training\n",
      "loss: 1.106679 at epoch 363 at applicants training\n",
      "loss: 1.106608 at epoch 364 at applicants training\n",
      "loss: 1.106552 at epoch 365 at applicants training\n",
      "loss: 1.106495 at epoch 366 at applicants training\n",
      "loss: 1.106436 at epoch 367 at applicants training\n",
      "loss: 1.106376 at epoch 368 at applicants training\n",
      "loss: 1.106314 at epoch 369 at applicants training\n",
      "loss: 1.106251 at epoch 370 at applicants training\n",
      "loss: 1.106189 at epoch 371 at applicants training\n",
      "loss: 1.106146 at epoch 372 at applicants training\n",
      "loss: 1.106079 at epoch 373 at applicants training\n",
      "loss: 1.106016 at epoch 374 at applicants training\n",
      "loss: 1.105963 at epoch 375 at applicants training\n",
      "loss: 1.105910 at epoch 376 at applicants training\n",
      "loss: 1.105854 at epoch 377 at applicants training\n",
      "loss: 1.105796 at epoch 378 at applicants training\n",
      "loss: 1.105735 at epoch 379 at applicants training\n",
      "loss: 1.105675 at epoch 380 at applicants training\n",
      "loss: 1.105614 at epoch 381 at applicants training\n",
      "loss: 1.105566 at epoch 382 at applicants training\n",
      "loss: 1.105504 at epoch 383 at applicants training\n",
      "loss: 1.105448 at epoch 384 at applicants training\n",
      "loss: 1.105398 at epoch 385 at applicants training\n",
      "loss: 1.105346 at epoch 386 at applicants training\n",
      "loss: 1.105293 at epoch 387 at applicants training\n",
      "loss: 1.105237 at epoch 388 at applicants training\n",
      "loss: 1.105180 at epoch 389 at applicants training\n",
      "loss: 1.105122 at epoch 390 at applicants training\n",
      "loss: 1.105065 at epoch 391 at applicants training\n",
      "loss: 1.105015 at epoch 392 at applicants training\n",
      "loss: 1.104957 at epoch 393 at applicants training\n",
      "loss: 1.104904 at epoch 394 at applicants training\n",
      "loss: 1.104853 at epoch 395 at applicants training\n",
      "loss: 1.104801 at epoch 396 at applicants training\n",
      "loss: 1.104748 at epoch 397 at applicants training\n",
      "loss: 1.104694 at epoch 398 at applicants training\n",
      "loss: 1.104640 at epoch 399 at applicants training\n",
      "loss: 1.104594 at epoch 400 at applicants training\n",
      "loss: 1.104538 at epoch 401 at applicants training\n",
      "loss: 1.104488 at epoch 402 at applicants training\n",
      "loss: 1.104438 at epoch 403 at applicants training\n",
      "loss: 1.104387 at epoch 404 at applicants training\n",
      "loss: 1.104334 at epoch 405 at applicants training\n",
      "loss: 1.104282 at epoch 406 at applicants training\n",
      "loss: 1.104232 at epoch 407 at applicants training\n",
      "loss: 1.104182 at epoch 408 at applicants training\n",
      "loss: 1.104133 at epoch 409 at applicants training\n",
      "loss: 1.104083 at epoch 410 at applicants training\n",
      "loss: 1.104033 at epoch 411 at applicants training\n",
      "loss: 1.103991 at epoch 412 at applicants training\n",
      "loss: 1.103937 at epoch 413 at applicants training\n",
      "loss: 1.103891 at epoch 414 at applicants training\n",
      "loss: 1.103844 at epoch 415 at applicants training\n",
      "loss: 1.103795 at epoch 416 at applicants training\n",
      "loss: 1.103745 at epoch 417 at applicants training\n",
      "loss: 1.103695 at epoch 418 at applicants training\n",
      "loss: 1.103659 at epoch 419 at applicants training\n",
      "loss: 1.103601 at epoch 420 at applicants training\n",
      "loss: 1.103555 at epoch 421 at applicants training\n",
      "loss: 1.103509 at epoch 422 at applicants training\n",
      "loss: 1.103461 at epoch 423 at applicants training\n",
      "loss: 1.103414 at epoch 424 at applicants training\n",
      "loss: 1.103368 at epoch 425 at applicants training\n",
      "loss: 1.103321 at epoch 426 at applicants training\n",
      "loss: 1.103275 at epoch 427 at applicants training\n",
      "loss: 1.103230 at epoch 428 at applicants training\n",
      "loss: 1.103184 at epoch 429 at applicants training\n",
      "loss: 1.103142 at epoch 430 at applicants training\n",
      "loss: 1.103095 at epoch 431 at applicants training\n",
      "loss: 1.103053 at epoch 432 at applicants training\n",
      "loss: 1.103010 at epoch 433 at applicants training\n",
      "loss: 1.102964 at epoch 434 at applicants training\n",
      "loss: 1.102918 at epoch 435 at applicants training\n",
      "loss: 1.102871 at epoch 436 at applicants training\n",
      "loss: 1.102829 at epoch 437 at applicants training\n",
      "loss: 1.102782 at epoch 438 at applicants training\n",
      "loss: 1.102740 at epoch 439 at applicants training\n",
      "loss: 1.102697 at epoch 440 at applicants training\n",
      "loss: 1.102653 at epoch 441 at applicants training\n",
      "loss: 1.102609 at epoch 442 at applicants training\n",
      "loss: 1.102565 at epoch 443 at applicants training\n",
      "loss: 1.102528 at epoch 444 at applicants training\n",
      "loss: 1.102480 at epoch 445 at applicants training\n",
      "loss: 1.102439 at epoch 446 at applicants training\n",
      "loss: 1.102398 at epoch 447 at applicants training\n",
      "loss: 1.102355 at epoch 448 at applicants training\n",
      "loss: 1.102312 at epoch 449 at applicants training\n",
      "loss: 1.102269 at epoch 450 at applicants training\n",
      "loss: 1.102226 at epoch 451 at applicants training\n",
      "loss: 1.102185 at epoch 452 at applicants training\n",
      "loss: 1.102143 at epoch 453 at applicants training\n",
      "loss: 1.102101 at epoch 454 at applicants training\n",
      "loss: 1.102060 at epoch 455 at applicants training\n",
      "loss: 1.102018 at epoch 456 at applicants training\n",
      "loss: 1.101977 at epoch 457 at applicants training\n",
      "loss: 1.101937 at epoch 458 at applicants training\n",
      "loss: 1.101895 at epoch 459 at applicants training\n",
      "loss: 1.101859 at epoch 460 at applicants training\n",
      "loss: 1.101817 at epoch 461 at applicants training\n",
      "loss: 1.101779 at epoch 462 at applicants training\n",
      "loss: 1.101740 at epoch 463 at applicants training\n",
      "loss: 1.101700 at epoch 464 at applicants training\n",
      "loss: 1.101659 at epoch 465 at applicants training\n",
      "loss: 1.101617 at epoch 466 at applicants training\n",
      "loss: 1.101576 at epoch 467 at applicants training\n",
      "loss: 1.101540 at epoch 468 at applicants training\n",
      "loss: 1.101500 at epoch 469 at applicants training\n",
      "loss: 1.101463 at epoch 470 at applicants training\n",
      "loss: 1.101426 at epoch 471 at applicants training\n",
      "loss: 1.101387 at epoch 472 at applicants training\n",
      "loss: 1.101347 at epoch 473 at applicants training\n",
      "loss: 1.101306 at epoch 474 at applicants training\n",
      "loss: 1.101265 at epoch 475 at applicants training\n",
      "loss: 1.101241 at epoch 476 at applicants training\n",
      "loss: 1.101190 at epoch 477 at applicants training\n",
      "loss: 1.101153 at epoch 478 at applicants training\n",
      "loss: 1.101118 at epoch 479 at applicants training\n",
      "loss: 1.101081 at epoch 480 at applicants training\n",
      "loss: 1.101044 at epoch 481 at applicants training\n",
      "loss: 1.101005 at epoch 482 at applicants training\n",
      "loss: 1.100965 at epoch 483 at applicants training\n",
      "loss: 1.100926 at epoch 484 at applicants training\n",
      "loss: 1.100893 at epoch 485 at applicants training\n",
      "loss: 1.100851 at epoch 486 at applicants training\n",
      "loss: 1.100814 at epoch 487 at applicants training\n",
      "loss: 1.100778 at epoch 488 at applicants training\n",
      "loss: 1.100741 at epoch 489 at applicants training\n",
      "loss: 1.100703 at epoch 490 at applicants training\n",
      "loss: 1.100670 at epoch 491 at applicants training\n",
      "loss: 1.100631 at epoch 492 at applicants training\n",
      "loss: 1.100596 at epoch 493 at applicants training\n",
      "loss: 1.100561 at epoch 494 at applicants training\n",
      "loss: 1.100525 at epoch 495 at applicants training\n",
      "loss: 1.100487 at epoch 496 at applicants training\n",
      "loss: 1.100450 at epoch 497 at applicants training\n",
      "loss: 1.100416 at epoch 498 at applicants training\n",
      "loss: 1.100382 at epoch 499 at applicants training\n",
      "faculty_vectors: [[0.08362218 0.17375544 0.16631222 0.54604909 0.03026108]\n",
      " [0.02326233 0.07179004 0.31720936 0.21441055 0.37332771]\n",
      " [0.19568395 0.26757348 0.2104309  0.23123746 0.09507421]\n",
      " ...\n",
      " [0.08362218 0.17375544 0.16631222 0.54604909 0.03026108]\n",
      " [0.2344319  0.08078156 0.31845629 0.24955716 0.11677309]\n",
      " [0.02326233 0.07179004 0.31720936 0.21441055 0.37332771]]\n",
      "student_features: [[41.37740283 55.54499531 63.2445039  90.99988334 68.24683762]\n",
      " [57.67969614 53.27236941 74.96803362 75.59032365 88.58033461]\n",
      " [74.98544741 87.48145353 50.41128859 53.43208998 41.4084824 ]\n",
      " ...\n",
      " [53.69226362 71.14954648 53.9785624  87.69581789 77.87375094]\n",
      " [96.81713608 61.26772906 53.84190687 63.89713764 56.43906141]\n",
      " [58.22900084 43.17256947 84.19873199 51.70050632 96.21627865]]\n",
      "faculty_vectors: [[0.02326233 0.07179004 0.31720936 0.21441055 0.37332771]\n",
      " [0.08362218 0.17375544 0.16631222 0.54604909 0.03026108]\n",
      " [0.2344319  0.08078156 0.31845629 0.24955716 0.11677309]\n",
      " ...\n",
      " [0.02326233 0.07179004 0.31720936 0.21441055 0.37332771]\n",
      " [0.02326233 0.07179004 0.31720936 0.21441055 0.37332771]\n",
      " [0.08362218 0.17375544 0.16631222 0.54604909 0.03026108]]\n",
      "student_features: [[41.37740283 55.54499531 63.2445039  90.99988334 68.24683762]\n",
      " [57.67969614 53.27236941 74.96803362 75.59032365 88.58033461]\n",
      " [74.98544741 87.48145353 50.41128859 53.43208998 41.4084824 ]\n",
      " ...\n",
      " [53.69226362 71.14954648 53.9785624  87.69581789 77.87375094]\n",
      " [96.81713608 61.26772906 53.84190687 63.89713764 56.43906141]\n",
      " [58.22900084 43.17256947 84.19873199 51.70050632 96.21627865]]\n",
      "faculty_vectors: [[0.02326233 0.07179004 0.31720936 0.21441055 0.37332771]\n",
      " [0.08362218 0.17375544 0.16631222 0.54604909 0.03026108]\n",
      " [0.2344319  0.08078156 0.31845629 0.24955716 0.11677309]\n",
      " ...\n",
      " [0.02326233 0.07179004 0.31720936 0.21441055 0.37332771]\n",
      " [0.02326233 0.07179004 0.31720936 0.21441055 0.37332771]\n",
      " [0.08362218 0.17375544 0.16631222 0.54604909 0.03026108]]\n",
      "student_features: [[41.37740283 55.54499531 63.2445039  90.99988334 68.24683762]\n",
      " [57.67969614 53.27236941 74.96803362 75.59032365 88.58033461]\n",
      " [74.98544741 87.48145353 50.41128859 53.43208998 41.4084824 ]\n",
      " ...\n",
      " [53.69226362 71.14954648 53.9785624  87.69581789 77.87375094]\n",
      " [96.81713608 61.26772906 53.84190687 63.89713764 56.43906141]\n",
      " [58.22900084 43.17256947 84.19873199 51.70050632 96.21627865]]\n",
      "Mean grade: 74.62\n",
      "Faculty distribution: [175 316   0 182 327]\n",
      "Students who got desired faculty: 216 (21.6%)\n",
      "Mean grade: 69.91\n",
      "Faculty distribution: [169 293   3 198 337]\n",
      "Students who got desired faculty: 381 (38.1%)\n",
      "Mean grade: 69.76\n",
      "Faculty distribution: [142 313   3 193 349]\n",
      "Students who got desired faculty: 199 (19.9%)\n"
     ]
    }
   ],
   "source": [
    "run_multi_iteration_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs236781-hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
