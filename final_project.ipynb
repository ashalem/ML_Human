{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Tuple, Dict\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants:\n",
    "FACULTY_NAMES = ['Computer Science', 'Economics', 'Psychology', 'Law', 'Art']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UniversityMLP(nn.Module):\n",
    "    \"\"\"Simple MLP for university decisions\"\"\"\n",
    "    def __init__(self, n_features: int, n_faculties: int):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # Add one-hot encoded faculty to features\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_faculties)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class ApplicantMLP(nn.Module):\n",
    "    \"\"\"MLP for applicant decisions with softmax output\"\"\"\n",
    "    def __init__(self, n_features: int, n_faculties: int):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_faculties),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FacultyParams:\n",
    "    \"\"\"Parameters for each faculty\"\"\"\n",
    "    name: str\n",
    "    utility_vector: np.ndarray  # Hidden vector that determines student success\n",
    "    capacity: int  # Number of spots available (can be infinite)\n",
    "\n",
    "@dataclass\n",
    "class SupplierParams:\n",
    "    \"\"\"Parameters for each preparation supplier\"\"\"\n",
    "    name: str\n",
    "    diff_vector: np.ndarray  # How this supplier modifies student features\n",
    "\n",
    "class UniversityEnvironment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int = 10,  # Number of student features (e.g., math, english, etc.)\n",
    "        n_faculties: int = 5,  # Number of different faculties\n",
    "        n_suppliers: int = 7,  # Number of preparation suppliers\n",
    "        noise_range: Tuple[float, float] = (-5, 5)  # Range for uniform noise\n",
    "    ):\n",
    "        self.n_features = n_features\n",
    "        self.n_faculties = n_faculties\n",
    "        self.n_suppliers = n_suppliers\n",
    "        self.noise_range = noise_range\n",
    "        \n",
    "        # Initialize faculties with random utility vectors\n",
    "        # Initialize faculties with normalized random utility vectors\n",
    "        self.faculties = [\n",
    "            FacultyParams(\n",
    "                name=FACULTY_NAMES[i],  # Using the predefined faculty names\n",
    "                utility_vector=self._create_normalized_vector(n_features),\n",
    "                capacity=np.inf  # As per description, infinite capacity\n",
    "            )\n",
    "            for i in range(n_faculties)\n",
    "        ] \n",
    "        \n",
    "        # Initialize suppliers with random modification vectors\n",
    "        self.suppliers = [\n",
    "          SupplierParams(\n",
    "              name=f\"Supplier_{i}\",\n",
    "              diff_vector=np.array([\n",
    "                  20 if j == idx1 else 0 if j == idx2 else 0\n",
    "                  for j in range(n_features)\n",
    "              ]),\n",
    "          )\n",
    "          for i in range(n_suppliers)\n",
    "          for idx1, idx2 in [np.random.choice(n_features, size=2, replace=False)]\n",
    "        ]\n",
    "        \n",
    "        self.past_applicants_df = None\n",
    "        self.current_applicants_df = None\n",
    "\n",
    "    def _create_normalized_vector(self, size: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create a normalized random vector of given size.\n",
    "        Normalization ensures ||vector|| = 1\n",
    "        \"\"\"\n",
    "        vector = np.random.uniform(0, 1, size)\n",
    "        # Normalize the vector to unit length\n",
    "        return vector / np.linalg.norm(vector, ord=1)\n",
    "    \n",
    "    def _generate_truncated_normal_features(self, n_samples: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate features using truncated normal distribution between 55 and 100.\n",
    "        Uses mean at center of range (77.5) and std that makes the distribution fit well in the range.\n",
    "        \"\"\"\n",
    "        mean = 70\n",
    "        std = 25 # This ensures ~99.7% of values fall within range before truncation\n",
    "        \n",
    "        features = np.random.normal(mean, std, (n_samples, self.n_features))\n",
    "        \n",
    "        # Truncate values to be within [40, 100]\n",
    "        features = np.clip(features, 40, 100)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def generate_past_applicants(\n",
    "        self,\n",
    "        n_applicants: int = 1000\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Generate dataset of past applicants with their outcomes\"\"\"\n",
    "        # Generate random feature vectors\n",
    "        features = self._generate_truncated_normal_features(n_applicants)\n",
    "        \n",
    "        # Randomly assign faculty for each applicant\n",
    "        df = pd.DataFrame(features, columns=[f\"feature_{i}\" for i in range(self.n_features)])\n",
    "        df['assigned_faculty'] = np.random.randint(0, self.n_faculties, n_applicants)\n",
    "        \n",
    "        # Calculate grade only for assigned faculty\n",
    "        faculty_vectors = np.array([f.utility_vector for f in self.faculties])\n",
    "        grades = np.zeros(n_applicants)\n",
    "        # Get faculty vectors for each applicant based on their assigned faculty\n",
    "        faculty_vectors_per_applicant = faculty_vectors[df['assigned_faculty']]\n",
    "        \n",
    "        # Calculate base grades using matrix multiplication\n",
    "        base_grades = np.sum(features * faculty_vectors_per_applicant, axis=1)\n",
    "        \n",
    "        # Generate noise for all applicants at once\n",
    "        noise = np.random.uniform(*self.noise_range, size=n_applicants)\n",
    "        \n",
    "        # Calculate final grades\n",
    "        grades = base_grades + noise\n",
    "            \n",
    "        df['final_grade'] = grades\n",
    "        self.past_applicants_df = df\n",
    "        return df\n",
    "\n",
    "    def generate_current_applicants(\n",
    "        self,\n",
    "        n_applicants: int = 100\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Generate dataset of current applicants\"\"\"\n",
    "        # Generate random feature vectors\n",
    "        features = self._generate_truncated_normal_features(n_applicants)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        feature_cols = [f\"feature_{i}\" for i in range(self.n_features)]\n",
    "        df = pd.DataFrame(features, columns=feature_cols)\n",
    "        \n",
    "        # Add desired faculty (random)\n",
    "        df['desired_faculty'] = np.random.randint(0, self.n_faculties, n_applicants)\n",
    "        \n",
    "        self.current_applicants_df = df\n",
    "        return df\n",
    "    \n",
    "    def train_applicant_model(\n",
    "        self,\n",
    "        past_data: pd.DataFrame = None\n",
    "    ) -> ApplicantMLP:\n",
    "        \"\"\"Train applicant model on past data\"\"\"\n",
    "        if past_data is None:\n",
    "            past_data = self.past_applicants_df\n",
    "        \n",
    "        if past_data is None:\n",
    "            raise ValueError(\"No past data available. Generate past applicants first.\")\n",
    "        \n",
    "        # Create and train applicant's MLP model\n",
    "        feature_cols = [f\"feature_{i}\" for i in range(self.n_features)]\n",
    "        X_train = torch.FloatTensor(past_data[feature_cols].values)\n",
    "        y_train = torch.LongTensor(past_data['assigned_faculty'].values)\n",
    "        \n",
    "        model = ApplicantMLP(self.n_features, self.n_faculties)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        \n",
    "        # Train the model\n",
    "        model.train()\n",
    "        for epoch in range(100):  # Quick training, adjust epochs as needed\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            print(f'loss: {loss} at epoch {epoch} at applicants training')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def choose_supplier_for_applicant(\n",
    "        self,\n",
    "        applicant_features: np.ndarray,\n",
    "        desired_faculty: int,\n",
    "        applicant_model: ApplicantMLP = None\n",
    "    ) -> Tuple[int, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Choose the best supplier for an applicant based on past data and supplier effects.\n",
    "        \n",
    "        Args:\n",
    "            applicant_features: The current features of the applicant\n",
    "            desired_faculty: The faculty index the applicant wants to get into\n",
    "            past_data: Optional past data to train on. If None, uses self.past_applicants_df\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (chosen_supplier_idx, modified_features)\n",
    "        \"\"\"\n",
    "        # Evaluate each supplier's effect\n",
    "        applicant_model.eval()\n",
    "        best_probability = -1\n",
    "        best_supplier_idx = -1\n",
    "        best_modified_features = None\n",
    "        \n",
    "        original_features = torch.FloatTensor(applicant_features).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Try each supplier\n",
    "            for i, supplier in enumerate(self.suppliers):\n",
    "                # Apply supplier's modification\n",
    "                modified_features_unclipped = original_features + torch.FloatTensor(supplier.diff_vector)\n",
    "                modified_features = np.clip(modified_features_unclipped, 40, 100)\n",
    "                \n",
    "                # Get probability distribution over faculties\n",
    "                probabilities = applicant_model(modified_features)\n",
    "                \n",
    "                # Check probability for desired faculty\n",
    "                prob_desired = probabilities[0, int(desired_faculty)].item()\n",
    "                \n",
    "                if prob_desired > best_probability:\n",
    "                    best_probability = prob_desired\n",
    "                    best_supplier_idx = i\n",
    "                    best_modified_features = modified_features.squeeze(0).numpy()\n",
    "        \n",
    "        if best_supplier_idx == -1:\n",
    "            # If no supplier improves probability, return original features with no supplier\n",
    "            return (-1, applicant_features)\n",
    "        \n",
    "        return (best_supplier_idx, best_modified_features)\n",
    "        \n",
    "    def recommend(\n",
    "        self,\n",
    "        student_features: np.ndarray,\n",
    "        recommended_faculties: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Calculate final grades for students given their features and recommended faculties\n",
    "        \n",
    "        Args:\n",
    "            student_features: Features matrix of shape (n_students, n_features)\n",
    "            recommended_faculties: Array of faculty indices of shape (n_students,)\n",
    "            \n",
    "        Returns:\n",
    "            Array of final grades of shape (n_students,)\n",
    "        \"\"\"\n",
    "        # Get utility vectors for all recommended faculties\n",
    "        faculty_vectors = np.array([self.faculties[f].utility_vector for f in recommended_faculties])\n",
    "        \n",
    "        print(f'faculty_vectors: {faculty_vectors}')\n",
    "        print(f'student_features: {student_features}')\n",
    "        \n",
    "        # Calculate base grades using batch matrix multiplication\n",
    "        base_grades = np.sum(student_features * faculty_vectors, axis=1)\n",
    "        \n",
    "        # Generate noise for all students at once\n",
    "        noise = np.random.uniform(*self.noise_range, size=len(student_features))\n",
    "        \n",
    "        return base_grades + noise\n",
    "\n",
    "    def train_university_model(\n",
    "        self,\n",
    "        past_data: pd.DataFrame = None\n",
    "    ) -> UniversityMLP:\n",
    "        \"\"\"\n",
    "        Train university model on past data.\n",
    "        \n",
    "        Args:\n",
    "            past_data: Optional past data to train on. If None, uses self.past_applicants_df\n",
    "        \n",
    "        Returns:\n",
    "            Trained UniversityMLP model\n",
    "        \"\"\"\n",
    "        if past_data is None:\n",
    "            past_data = self.past_applicants_df\n",
    "        \n",
    "        if past_data is None:\n",
    "            raise ValueError(\"No past data available. Generate past applicants first.\")\n",
    "        \n",
    "        # Prepare training data\n",
    "        feature_cols = [f\"feature_{i}\" for i in range(self.n_features)]\n",
    "        X_train = torch.FloatTensor(past_data[feature_cols].values)\n",
    "        \n",
    "        # Create and train university model\n",
    "        model = UniversityMLP(self.n_features, self.n_faculties)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # Custom loss function that only considers the assigned faculty's grade\n",
    "        def custom_loss(predictions, targets, assigned_faculties):\n",
    "            batch_size = predictions.size(0)\n",
    "            indices = torch.arange(batch_size)\n",
    "            predicted_assigned_grades = predictions[indices, assigned_faculties]\n",
    "            return torch.mean((predicted_assigned_grades - targets) ** 2)\n",
    "        \n",
    "        # Train the model\n",
    "        model.train()\n",
    "        batch_size = 128\n",
    "        n_epochs = 200\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Process in batches\n",
    "            permutation = torch.randperm(len(X_train))\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                indices = permutation[i:i + batch_size]\n",
    "                batch_x = X_train[indices]\n",
    "                batch_y = torch.FloatTensor(past_data['final_grade'].values[indices])\n",
    "                batch_assigned = torch.LongTensor(past_data['assigned_faculty'].values[indices])\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(batch_x)\n",
    "                loss = custom_loss(predictions, batch_y, batch_assigned)\n",
    "                print(f'loss: {loss} at epoch {epoch}')\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def assign_applicants_to_faculties(\n",
    "        self,\n",
    "        model: UniversityMLP,\n",
    "        current_applicants_features: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Use trained model to make faculty recommendations for current applicants.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained UniversityMLP model\n",
    "            current_applicants_features: Modified features of current applicants (n_applicants x n_features)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (chosen_faculties, final_grades, mean_grade)\n",
    "            - chosen_faculties: Array of faculty indices chosen for each applicant\n",
    "            - final_grades: Array of final grades received by each applicant\n",
    "            - mean_grade: Average grade across all applicants\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            current_features = torch.FloatTensor(current_applicants_features)\n",
    "            predicted_grades = model(current_features)\n",
    "            \n",
    "            # Choose best faculty for each applicant based on predicted grades\n",
    "            chosen_faculties = torch.argmax(predicted_grades, dim=1).numpy()\n",
    "        \n",
    "        return chosen_faculties\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example():\n",
    "    # Create environment\n",
    "    env = UniversityEnvironment()\n",
    "    \n",
    "    # Generate past applicants\n",
    "    past_df = env.generate_past_applicants(1000)\n",
    "    print(\"Past applicants shape:\", past_df.shape)\n",
    "    \n",
    "    # Generate current applicants\n",
    "    current_df = env.generate_current_applicants(100)\n",
    "    print(\"Current applicants shape:\", current_df.shape)\n",
    "    print(f'current_df: {current_df}')\n",
    "    \n",
    "    # Get modified features for all current applicants\n",
    "    feature_cols = [f\"feature_{i}\" for i in range(env.n_features)]\n",
    "    modified_features = []\n",
    "    original_features = current_df[feature_cols].values\n",
    "    \n",
    "    for idx in range(len(current_df)):\n",
    "        student_features = current_df.iloc[idx][feature_cols].values\n",
    "        desired_faculty = current_df.iloc[idx]['desired_faculty']\n",
    "        \n",
    "        _, modified_student_features = env.choose_supplier_for_applicant(\n",
    "            student_features,\n",
    "            desired_faculty\n",
    "        )\n",
    "        modified_features.append(modified_student_features)\n",
    "    \n",
    "    modified_features = np.array(modified_features)\n",
    "    \n",
    "    # Train university model\n",
    "    trained_model = env.train_university_model(past_df)\n",
    "    \n",
    "    # Make predictions using trained model\n",
    "    chosen_faculties = env.assign_applicants_to_faculties(\n",
    "        trained_model,\n",
    "        modified_features\n",
    "    )\n",
    "    # Calculate percentage of students accepted into their desired faculty\n",
    "    desired_faculties = current_df['desired_faculty'].values\n",
    "    matches = (chosen_faculties == desired_faculties)\n",
    "    acceptance_rate = (np.sum(matches) / len(desired_faculties)) * 100\n",
    "    \n",
    "    # Calculate final grades using original features\n",
    "    final_grades = env.recommend(original_features, chosen_faculties)\n",
    "    mean_grade = np.mean(final_grades)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Mean grade across all applicants: {mean_grade:.2f}\")\n",
    "    print(f\"\\nPercentage of students accepted to desired faculty: {acceptance_rate:.2f}%\")\n",
    "\n",
    "    \n",
    "    # Print detailed results for first 5 applicants\n",
    "    print(\"\\nDetailed results for first 5 applicants:\")\n",
    "    for i in range(5):\n",
    "        desired_faculty = current_df.iloc[i]['desired_faculty']\n",
    "        print(f\"\\nApplicant {i}:\")\n",
    "        print(f\"Desired faculty: {desired_faculty}\")\n",
    "        print(f\"Assigned faculty: {chosen_faculties[i]}\")\n",
    "        print(f\"Final grade: {final_grades[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past applicants shape: (1000, 12)\n",
      "Current applicants shape: (100, 11)\n",
      "current_df:      feature_0   feature_1  feature_2   feature_3   feature_4   feature_5  \\\n",
      "0    42.130331   82.896435  87.526611   40.000000   84.548278   69.425058   \n",
      "1    70.621951   65.401288  61.827161   98.653011   51.783437   84.269128   \n",
      "2   100.000000   89.435155  40.000000   55.111455  100.000000   52.495976   \n",
      "3    53.715615   76.190377  65.781734   88.337941   50.871614   45.016538   \n",
      "4    40.000000  100.000000  57.220875   89.083780   82.577927   49.563704   \n",
      "..         ...         ...        ...         ...         ...         ...   \n",
      "95   40.000000   69.810899  96.368410  100.000000   61.915276   93.987424   \n",
      "96   40.000000   40.000000  49.496010   45.257607   80.233597   57.479468   \n",
      "97   72.680221   89.568123  61.476285   80.225664  100.000000  100.000000   \n",
      "98   69.804964  100.000000  51.712177   63.932529   40.000000   54.230594   \n",
      "99   85.132963   40.000000  62.213696   40.000000   89.345818   40.000000   \n",
      "\n",
      "     feature_6  feature_7   feature_8   feature_9  desired_faculty  \n",
      "0    90.160019  56.994210   74.831718   58.605317                1  \n",
      "1    61.017454  42.226289   69.587579   40.000000                1  \n",
      "2    40.000000  40.000000   68.345609   74.985640                0  \n",
      "3   100.000000  91.907698   40.000000  100.000000                2  \n",
      "4    90.962405  74.563555   46.106713   40.000000                4  \n",
      "..         ...        ...         ...         ...              ...  \n",
      "95   40.000000  51.214592   92.124190   52.696932                3  \n",
      "96   40.000000  81.076145   60.987944  100.000000                1  \n",
      "97   73.866510  58.086164   55.093469   40.000000                2  \n",
      "98   65.142637  96.316417   77.821295  100.000000                3  \n",
      "99   65.363850  41.113154  100.000000   86.777724                2  \n",
      "\n",
      "[100 rows x 11 columns]\n",
      "loss: 1.6799148321151733 at epoch 0 at applicants training\n",
      "loss: 1.6896953582763672 at epoch 1 at applicants training\n",
      "loss: 1.7096242904663086 at epoch 2 at applicants training\n",
      "loss: 1.7076715230941772 at epoch 3 at applicants training\n",
      "loss: 1.6941031217575073 at epoch 4 at applicants training\n",
      "loss: 1.7049561738967896 at epoch 5 at applicants training\n",
      "loss: 1.7079946994781494 at epoch 6 at applicants training\n",
      "loss: 1.700674295425415 at epoch 7 at applicants training\n",
      "loss: 1.6925435066223145 at epoch 8 at applicants training\n",
      "loss: 1.691165804862976 at epoch 9 at applicants training\n",
      "loss: 1.6951806545257568 at epoch 10 at applicants training\n",
      "loss: 1.6947048902511597 at epoch 11 at applicants training\n",
      "loss: 1.691954255104065 at epoch 12 at applicants training\n",
      "loss: 1.6917643547058105 at epoch 13 at applicants training\n",
      "loss: 1.690833330154419 at epoch 14 at applicants training\n",
      "loss: 1.6876813173294067 at epoch 15 at applicants training\n",
      "loss: 1.684720516204834 at epoch 16 at applicants training\n",
      "loss: 1.6801012754440308 at epoch 17 at applicants training\n",
      "loss: 1.6783406734466553 at epoch 18 at applicants training\n",
      "loss: 1.676337480545044 at epoch 19 at applicants training\n",
      "loss: 1.6737614870071411 at epoch 20 at applicants training\n",
      "loss: 1.674784541130066 at epoch 21 at applicants training\n",
      "loss: 1.6737432479858398 at epoch 22 at applicants training\n",
      "loss: 1.6705492734909058 at epoch 23 at applicants training\n",
      "loss: 1.669635534286499 at epoch 24 at applicants training\n",
      "loss: 1.6671017408370972 at epoch 25 at applicants training\n",
      "loss: 1.6646370887756348 at epoch 26 at applicants training\n",
      "loss: 1.662835717201233 at epoch 27 at applicants training\n",
      "loss: 1.660183310508728 at epoch 28 at applicants training\n",
      "loss: 1.6544170379638672 at epoch 29 at applicants training\n",
      "loss: 1.6511303186416626 at epoch 30 at applicants training\n",
      "loss: 1.66322660446167 at epoch 31 at applicants training\n",
      "loss: 1.6561576128005981 at epoch 32 at applicants training\n",
      "loss: 1.6471880674362183 at epoch 33 at applicants training\n",
      "loss: 1.647573471069336 at epoch 34 at applicants training\n",
      "loss: 1.653396725654602 at epoch 35 at applicants training\n",
      "loss: 1.640002727508545 at epoch 36 at applicants training\n",
      "loss: 1.6380481719970703 at epoch 37 at applicants training\n",
      "loss: 1.6439152956008911 at epoch 38 at applicants training\n",
      "loss: 1.6358520984649658 at epoch 39 at applicants training\n",
      "loss: 1.6311488151550293 at epoch 40 at applicants training\n",
      "loss: 1.6352976560592651 at epoch 41 at applicants training\n",
      "loss: 1.6238946914672852 at epoch 42 at applicants training\n",
      "loss: 1.6268136501312256 at epoch 43 at applicants training\n",
      "loss: 1.6252366304397583 at epoch 44 at applicants training\n",
      "loss: 1.6181672811508179 at epoch 45 at applicants training\n",
      "loss: 1.6211860179901123 at epoch 46 at applicants training\n",
      "loss: 1.613718867301941 at epoch 47 at applicants training\n",
      "loss: 1.619093656539917 at epoch 48 at applicants training\n",
      "loss: 1.6120704412460327 at epoch 49 at applicants training\n",
      "loss: 1.614123821258545 at epoch 50 at applicants training\n",
      "loss: 1.6072068214416504 at epoch 51 at applicants training\n",
      "loss: 1.6090145111083984 at epoch 52 at applicants training\n",
      "loss: 1.6069637537002563 at epoch 53 at applicants training\n",
      "loss: 1.6047877073287964 at epoch 54 at applicants training\n",
      "loss: 1.6050928831100464 at epoch 55 at applicants training\n",
      "loss: 1.6022039651870728 at epoch 56 at applicants training\n",
      "loss: 1.6029335260391235 at epoch 57 at applicants training\n",
      "loss: 1.598319411277771 at epoch 58 at applicants training\n",
      "loss: 1.598586082458496 at epoch 59 at applicants training\n",
      "loss: 1.598515510559082 at epoch 60 at applicants training\n",
      "loss: 1.6066747903823853 at epoch 61 at applicants training\n",
      "loss: 1.6004750728607178 at epoch 62 at applicants training\n",
      "loss: 1.595097541809082 at epoch 63 at applicants training\n",
      "loss: 1.593448519706726 at epoch 64 at applicants training\n",
      "loss: 1.5940351486206055 at epoch 65 at applicants training\n",
      "loss: 1.5938010215759277 at epoch 66 at applicants training\n",
      "loss: 1.5923616886138916 at epoch 67 at applicants training\n",
      "loss: 1.5904452800750732 at epoch 68 at applicants training\n",
      "loss: 1.5911884307861328 at epoch 69 at applicants training\n",
      "loss: 1.5903074741363525 at epoch 70 at applicants training\n",
      "loss: 1.5899382829666138 at epoch 71 at applicants training\n",
      "loss: 1.587754249572754 at epoch 72 at applicants training\n",
      "loss: 1.5879427194595337 at epoch 73 at applicants training\n",
      "loss: 1.5873465538024902 at epoch 74 at applicants training\n",
      "loss: 1.5862524509429932 at epoch 75 at applicants training\n",
      "loss: 1.5850234031677246 at epoch 76 at applicants training\n",
      "loss: 1.5852564573287964 at epoch 77 at applicants training\n",
      "loss: 1.5852354764938354 at epoch 78 at applicants training\n",
      "loss: 1.5837887525558472 at epoch 79 at applicants training\n",
      "loss: 1.5826205015182495 at epoch 80 at applicants training\n",
      "loss: 1.582202672958374 at epoch 81 at applicants training\n",
      "loss: 1.5831166505813599 at epoch 82 at applicants training\n",
      "loss: 1.5812700986862183 at epoch 83 at applicants training\n",
      "loss: 1.5800362825393677 at epoch 84 at applicants training\n",
      "loss: 1.5790019035339355 at epoch 85 at applicants training\n",
      "loss: 1.5788782835006714 at epoch 86 at applicants training\n",
      "loss: 1.579399824142456 at epoch 87 at applicants training\n",
      "loss: 1.5790812969207764 at epoch 88 at applicants training\n",
      "loss: 1.5792242288589478 at epoch 89 at applicants training\n",
      "loss: 1.5777709484100342 at epoch 90 at applicants training\n",
      "loss: 1.5759941339492798 at epoch 91 at applicants training\n",
      "loss: 1.575243353843689 at epoch 92 at applicants training\n",
      "loss: 1.574208378791809 at epoch 93 at applicants training\n",
      "loss: 1.5744166374206543 at epoch 94 at applicants training\n",
      "loss: 1.5748932361602783 at epoch 95 at applicants training\n",
      "loss: 1.5753642320632935 at epoch 96 at applicants training\n",
      "loss: 1.5747617483139038 at epoch 97 at applicants training\n",
      "loss: 1.5726146697998047 at epoch 98 at applicants training\n",
      "loss: 1.5711679458618164 at epoch 99 at applicants training\n",
      "loss: 1.6648261547088623 at epoch 0 at applicants training\n",
      "loss: 1.712375521659851 at epoch 1 at applicants training\n",
      "loss: 1.706990361213684 at epoch 2 at applicants training\n",
      "loss: 1.7169978618621826 at epoch 3 at applicants training\n",
      "loss: 1.7178261280059814 at epoch 4 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 5 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 6 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 7 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 8 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 9 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 10 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 11 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 12 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 13 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 14 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 15 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 16 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 17 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 18 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 19 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 20 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 21 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 22 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 23 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 24 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 25 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 26 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 27 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 28 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 29 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 30 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 31 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 32 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 33 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 34 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 35 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 36 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 37 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 38 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 39 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 40 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 41 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 42 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 43 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 44 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 45 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 46 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 47 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 48 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 49 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 50 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 51 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 52 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 53 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 54 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 55 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 56 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 57 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 58 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 59 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 60 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 61 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 62 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 63 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 64 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 65 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 66 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 67 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 68 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 69 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 70 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 71 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 72 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 73 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 74 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 75 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 76 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 77 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 78 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 79 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 80 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 81 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 82 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 83 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 84 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 85 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 86 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 87 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 88 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 89 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 90 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 91 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 92 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 93 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 94 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 95 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 96 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 97 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 98 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 99 at applicants training\n",
      "loss: 1.6987980604171753 at epoch 0 at applicants training\n",
      "loss: 1.6997356414794922 at epoch 1 at applicants training\n",
      "loss: 1.7144291400909424 at epoch 2 at applicants training\n",
      "loss: 1.7176874876022339 at epoch 3 at applicants training\n",
      "loss: 1.717829942703247 at epoch 4 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 5 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 6 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 7 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 8 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 9 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 10 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 11 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 12 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 13 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 14 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 15 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 16 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 17 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 18 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 19 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 20 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 21 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 22 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 23 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 24 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 25 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 26 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 27 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 28 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 29 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 30 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 31 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 32 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 33 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 34 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 35 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 36 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 37 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 38 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 39 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 40 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 41 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 42 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 43 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 44 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 45 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 46 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 47 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 48 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 49 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 50 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 51 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 52 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 53 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 54 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 55 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 56 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 57 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 58 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 59 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 60 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 61 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 62 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 63 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 64 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 65 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 66 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 67 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 68 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 69 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 70 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 71 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 72 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 73 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 74 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 75 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 76 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 77 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 78 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 79 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 80 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 81 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 82 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 83 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 84 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 85 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 86 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 87 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 88 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 89 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 90 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 91 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 92 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 93 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 94 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 95 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 96 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 97 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 98 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 99 at applicants training\n",
      "loss: 1.7162665128707886 at epoch 0 at applicants training\n",
      "loss: 1.6855193376541138 at epoch 1 at applicants training\n",
      "loss: 1.685379147529602 at epoch 2 at applicants training\n",
      "loss: 1.678366780281067 at epoch 3 at applicants training\n",
      "loss: 1.6782793998718262 at epoch 4 at applicants training\n",
      "loss: 1.686369776725769 at epoch 5 at applicants training\n",
      "loss: 1.6791452169418335 at epoch 6 at applicants training\n",
      "loss: 1.6801728010177612 at epoch 7 at applicants training\n",
      "loss: 1.6793339252471924 at epoch 8 at applicants training\n",
      "loss: 1.6774479150772095 at epoch 9 at applicants training\n",
      "loss: 1.67164146900177 at epoch 10 at applicants training\n",
      "loss: 1.6693508625030518 at epoch 11 at applicants training\n",
      "loss: 1.6673001050949097 at epoch 12 at applicants training\n",
      "loss: 1.666778326034546 at epoch 13 at applicants training\n",
      "loss: 1.6700166463851929 at epoch 14 at applicants training\n",
      "loss: 1.662716269493103 at epoch 15 at applicants training\n",
      "loss: 1.6703901290893555 at epoch 16 at applicants training\n",
      "loss: 1.664287805557251 at epoch 17 at applicants training\n",
      "loss: 1.663664698600769 at epoch 18 at applicants training\n",
      "loss: 1.663611888885498 at epoch 19 at applicants training\n",
      "loss: 1.6610703468322754 at epoch 20 at applicants training\n",
      "loss: 1.6642705202102661 at epoch 21 at applicants training\n",
      "loss: 1.6593348979949951 at epoch 22 at applicants training\n",
      "loss: 1.6614819765090942 at epoch 23 at applicants training\n",
      "loss: 1.6579432487487793 at epoch 24 at applicants training\n",
      "loss: 1.659116268157959 at epoch 25 at applicants training\n",
      "loss: 1.659256935119629 at epoch 26 at applicants training\n",
      "loss: 1.6554960012435913 at epoch 27 at applicants training\n",
      "loss: 1.658778190612793 at epoch 28 at applicants training\n",
      "loss: 1.6544735431671143 at epoch 29 at applicants training\n",
      "loss: 1.6571800708770752 at epoch 30 at applicants training\n",
      "loss: 1.6532715559005737 at epoch 31 at applicants training\n",
      "loss: 1.6547819375991821 at epoch 32 at applicants training\n",
      "loss: 1.6524548530578613 at epoch 33 at applicants training\n",
      "loss: 1.6513158082962036 at epoch 34 at applicants training\n",
      "loss: 1.6527128219604492 at epoch 35 at applicants training\n",
      "loss: 1.649522066116333 at epoch 36 at applicants training\n",
      "loss: 1.6513222455978394 at epoch 37 at applicants training\n",
      "loss: 1.6502412557601929 at epoch 38 at applicants training\n",
      "loss: 1.6472941637039185 at epoch 39 at applicants training\n",
      "loss: 1.6497055292129517 at epoch 40 at applicants training\n",
      "loss: 1.6483207941055298 at epoch 41 at applicants training\n",
      "loss: 1.6435643434524536 at epoch 42 at applicants training\n",
      "loss: 1.648428201675415 at epoch 43 at applicants training\n",
      "loss: 1.6472890377044678 at epoch 44 at applicants training\n",
      "loss: 1.639171838760376 at epoch 45 at applicants training\n",
      "loss: 1.6446000337600708 at epoch 46 at applicants training\n",
      "loss: 1.6480128765106201 at epoch 47 at applicants training\n",
      "loss: 1.6360771656036377 at epoch 48 at applicants training\n",
      "loss: 1.6385507583618164 at epoch 49 at applicants training\n",
      "loss: 1.646080732345581 at epoch 50 at applicants training\n",
      "loss: 1.6341822147369385 at epoch 51 at applicants training\n",
      "loss: 1.6335581541061401 at epoch 52 at applicants training\n",
      "loss: 1.6416634321212769 at epoch 53 at applicants training\n",
      "loss: 1.63428795337677 at epoch 54 at applicants training\n",
      "loss: 1.6286108493804932 at epoch 55 at applicants training\n",
      "loss: 1.634290337562561 at epoch 56 at applicants training\n",
      "loss: 1.6392805576324463 at epoch 57 at applicants training\n",
      "loss: 1.6339088678359985 at epoch 58 at applicants training\n",
      "loss: 1.6265387535095215 at epoch 59 at applicants training\n",
      "loss: 1.6293176412582397 at epoch 60 at applicants training\n",
      "loss: 1.634509563446045 at epoch 61 at applicants training\n",
      "loss: 1.6299997568130493 at epoch 62 at applicants training\n",
      "loss: 1.6244146823883057 at epoch 63 at applicants training\n",
      "loss: 1.6250156164169312 at epoch 64 at applicants training\n",
      "loss: 1.6288225650787354 at epoch 65 at applicants training\n",
      "loss: 1.6250543594360352 at epoch 66 at applicants training\n",
      "loss: 1.6284626722335815 at epoch 67 at applicants training\n",
      "loss: 1.6161366701126099 at epoch 68 at applicants training\n",
      "loss: 1.6197707653045654 at epoch 69 at applicants training\n",
      "loss: 1.620664119720459 at epoch 70 at applicants training\n",
      "loss: 1.6184775829315186 at epoch 71 at applicants training\n",
      "loss: 1.6125528812408447 at epoch 72 at applicants training\n",
      "loss: 1.6141796112060547 at epoch 73 at applicants training\n",
      "loss: 1.612094521522522 at epoch 74 at applicants training\n",
      "loss: 1.6064945459365845 at epoch 75 at applicants training\n",
      "loss: 1.609618902206421 at epoch 76 at applicants training\n",
      "loss: 1.603972315788269 at epoch 77 at applicants training\n",
      "loss: 1.604248046875 at epoch 78 at applicants training\n",
      "loss: 1.6034777164459229 at epoch 79 at applicants training\n",
      "loss: 1.5988961458206177 at epoch 80 at applicants training\n",
      "loss: 1.5991681814193726 at epoch 81 at applicants training\n",
      "loss: 1.5992822647094727 at epoch 82 at applicants training\n",
      "loss: 1.6030375957489014 at epoch 83 at applicants training\n",
      "loss: 1.5956789255142212 at epoch 84 at applicants training\n",
      "loss: 1.5970169305801392 at epoch 85 at applicants training\n",
      "loss: 1.5970890522003174 at epoch 86 at applicants training\n",
      "loss: 1.5977308750152588 at epoch 87 at applicants training\n",
      "loss: 1.5953119993209839 at epoch 88 at applicants training\n",
      "loss: 1.5931698083877563 at epoch 89 at applicants training\n",
      "loss: 1.5961947441101074 at epoch 90 at applicants training\n",
      "loss: 1.5953243970870972 at epoch 91 at applicants training\n",
      "loss: 1.591102957725525 at epoch 92 at applicants training\n",
      "loss: 1.5911197662353516 at epoch 93 at applicants training\n",
      "loss: 1.5893207788467407 at epoch 94 at applicants training\n",
      "loss: 1.5917302370071411 at epoch 95 at applicants training\n",
      "loss: 1.5922489166259766 at epoch 96 at applicants training\n",
      "loss: 1.590209722518921 at epoch 97 at applicants training\n",
      "loss: 1.5860507488250732 at epoch 98 at applicants training\n",
      "loss: 1.5870366096496582 at epoch 99 at applicants training\n",
      "loss: 1.699745535850525 at epoch 0 at applicants training\n",
      "loss: 1.6925314664840698 at epoch 1 at applicants training\n",
      "loss: 1.6938402652740479 at epoch 2 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 3 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 4 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 5 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 6 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 7 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 8 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 9 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 10 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 11 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 12 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 13 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 14 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 15 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 16 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 17 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 18 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 19 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 20 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 21 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 22 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 23 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 24 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 25 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 26 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 27 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 28 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 29 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 30 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 31 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 32 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 33 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 34 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 35 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 36 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 37 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 38 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 39 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 40 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 41 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 42 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 43 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 44 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 45 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 46 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 47 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 48 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 49 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 50 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 51 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 52 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 53 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 54 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 55 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 56 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 57 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 58 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 59 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 60 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 61 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 62 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 63 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 64 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 65 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 66 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 67 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 68 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 69 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 70 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 71 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 72 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 73 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 74 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 75 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 76 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 77 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 78 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 79 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 80 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 81 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 82 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 83 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 84 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 85 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 86 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 87 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 88 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 89 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 90 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 91 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 92 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 93 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 94 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 95 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 96 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 97 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 98 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 99 at applicants training\n",
      "loss: 1.7073942422866821 at epoch 0 at applicants training\n",
      "loss: 1.6729905605316162 at epoch 1 at applicants training\n",
      "loss: 1.6610740423202515 at epoch 2 at applicants training\n",
      "loss: 1.6708720922470093 at epoch 3 at applicants training\n",
      "loss: 1.6762173175811768 at epoch 4 at applicants training\n",
      "loss: 1.6770398616790771 at epoch 5 at applicants training\n",
      "loss: 1.6759254932403564 at epoch 6 at applicants training\n",
      "loss: 1.6706913709640503 at epoch 7 at applicants training\n",
      "loss: 1.6625585556030273 at epoch 8 at applicants training\n",
      "loss: 1.6585326194763184 at epoch 9 at applicants training\n",
      "loss: 1.6621745824813843 at epoch 10 at applicants training\n",
      "loss: 1.6557317972183228 at epoch 11 at applicants training\n",
      "loss: 1.6533105373382568 at epoch 12 at applicants training\n",
      "loss: 1.6552541255950928 at epoch 13 at applicants training\n",
      "loss: 1.6493812799453735 at epoch 14 at applicants training\n",
      "loss: 1.6490652561187744 at epoch 15 at applicants training\n",
      "loss: 1.6492878198623657 at epoch 16 at applicants training\n",
      "loss: 1.645530104637146 at epoch 17 at applicants training\n",
      "loss: 1.644932508468628 at epoch 18 at applicants training\n",
      "loss: 1.6436960697174072 at epoch 19 at applicants training\n",
      "loss: 1.6416598558425903 at epoch 20 at applicants training\n",
      "loss: 1.6400506496429443 at epoch 21 at applicants training\n",
      "loss: 1.6410261392593384 at epoch 22 at applicants training\n",
      "loss: 1.6386111974716187 at epoch 23 at applicants training\n",
      "loss: 1.6415324211120605 at epoch 24 at applicants training\n",
      "loss: 1.6381350755691528 at epoch 25 at applicants training\n",
      "loss: 1.6363428831100464 at epoch 26 at applicants training\n",
      "loss: 1.6376392841339111 at epoch 27 at applicants training\n",
      "loss: 1.6337738037109375 at epoch 28 at applicants training\n",
      "loss: 1.6347318887710571 at epoch 29 at applicants training\n",
      "loss: 1.6326299905776978 at epoch 30 at applicants training\n",
      "loss: 1.6327335834503174 at epoch 31 at applicants training\n",
      "loss: 1.6325273513793945 at epoch 32 at applicants training\n",
      "loss: 1.6302776336669922 at epoch 33 at applicants training\n",
      "loss: 1.6304421424865723 at epoch 34 at applicants training\n",
      "loss: 1.6286821365356445 at epoch 35 at applicants training\n",
      "loss: 1.6260793209075928 at epoch 36 at applicants training\n",
      "loss: 1.6249194145202637 at epoch 37 at applicants training\n",
      "loss: 1.6260154247283936 at epoch 38 at applicants training\n",
      "loss: 1.629601001739502 at epoch 39 at applicants training\n",
      "loss: 1.624725580215454 at epoch 40 at applicants training\n",
      "loss: 1.6215908527374268 at epoch 41 at applicants training\n",
      "loss: 1.6208442449569702 at epoch 42 at applicants training\n",
      "loss: 1.6226214170455933 at epoch 43 at applicants training\n",
      "loss: 1.6259628534317017 at epoch 44 at applicants training\n",
      "loss: 1.6234642267227173 at epoch 45 at applicants training\n",
      "loss: 1.6189831495285034 at epoch 46 at applicants training\n",
      "loss: 1.6194730997085571 at epoch 47 at applicants training\n",
      "loss: 1.6226136684417725 at epoch 48 at applicants training\n",
      "loss: 1.6202658414840698 at epoch 49 at applicants training\n",
      "loss: 1.6164212226867676 at epoch 50 at applicants training\n",
      "loss: 1.6165647506713867 at epoch 51 at applicants training\n",
      "loss: 1.61874258518219 at epoch 52 at applicants training\n",
      "loss: 1.6190497875213623 at epoch 53 at applicants training\n",
      "loss: 1.615172266960144 at epoch 54 at applicants training\n",
      "loss: 1.6135574579238892 at epoch 55 at applicants training\n",
      "loss: 1.615805745124817 at epoch 56 at applicants training\n",
      "loss: 1.6176568269729614 at epoch 57 at applicants training\n",
      "loss: 1.613621473312378 at epoch 58 at applicants training\n",
      "loss: 1.6119884252548218 at epoch 59 at applicants training\n",
      "loss: 1.6124690771102905 at epoch 60 at applicants training\n",
      "loss: 1.614007592201233 at epoch 61 at applicants training\n",
      "loss: 1.6132404804229736 at epoch 62 at applicants training\n",
      "loss: 1.6108121871948242 at epoch 63 at applicants training\n",
      "loss: 1.6100231409072876 at epoch 64 at applicants training\n",
      "loss: 1.6107556819915771 at epoch 65 at applicants training\n",
      "loss: 1.6107722520828247 at epoch 66 at applicants training\n",
      "loss: 1.6106191873550415 at epoch 67 at applicants training\n",
      "loss: 1.6091965436935425 at epoch 68 at applicants training\n",
      "loss: 1.607833743095398 at epoch 69 at applicants training\n",
      "loss: 1.6067965030670166 at epoch 70 at applicants training\n",
      "loss: 1.6062631607055664 at epoch 71 at applicants training\n",
      "loss: 1.6059399843215942 at epoch 72 at applicants training\n",
      "loss: 1.6060490608215332 at epoch 73 at applicants training\n",
      "loss: 1.6070767641067505 at epoch 74 at applicants training\n",
      "loss: 1.6084600687026978 at epoch 75 at applicants training\n",
      "loss: 1.6106586456298828 at epoch 76 at applicants training\n",
      "loss: 1.6105314493179321 at epoch 77 at applicants training\n",
      "loss: 1.6087958812713623 at epoch 78 at applicants training\n",
      "loss: 1.605005145072937 at epoch 79 at applicants training\n",
      "loss: 1.6037107706069946 at epoch 80 at applicants training\n",
      "loss: 1.6047495603561401 at epoch 81 at applicants training\n",
      "loss: 1.6068902015686035 at epoch 82 at applicants training\n",
      "loss: 1.6081719398498535 at epoch 83 at applicants training\n",
      "loss: 1.6051266193389893 at epoch 84 at applicants training\n",
      "loss: 1.6035104990005493 at epoch 85 at applicants training\n",
      "loss: 1.6027624607086182 at epoch 86 at applicants training\n",
      "loss: 1.6033697128295898 at epoch 87 at applicants training\n",
      "loss: 1.6040465831756592 at epoch 88 at applicants training\n",
      "loss: 1.603591799736023 at epoch 89 at applicants training\n",
      "loss: 1.6023833751678467 at epoch 90 at applicants training\n",
      "loss: 1.6013792753219604 at epoch 91 at applicants training\n",
      "loss: 1.6016861200332642 at epoch 92 at applicants training\n",
      "loss: 1.602046251296997 at epoch 93 at applicants training\n",
      "loss: 1.6017770767211914 at epoch 94 at applicants training\n",
      "loss: 1.6014177799224854 at epoch 95 at applicants training\n",
      "loss: 1.6009514331817627 at epoch 96 at applicants training\n",
      "loss: 1.6003336906433105 at epoch 97 at applicants training\n",
      "loss: 1.600090742111206 at epoch 98 at applicants training\n",
      "loss: 1.6000200510025024 at epoch 99 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 0 at applicants training\n",
      "loss: 1.6965906620025635 at epoch 1 at applicants training\n",
      "loss: 1.7208034992218018 at epoch 2 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 3 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 4 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 5 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 6 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 7 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 8 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 9 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 10 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 11 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 12 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 13 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 14 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 15 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 16 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 17 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 18 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 19 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 20 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 21 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 22 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 23 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 24 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 25 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 26 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 27 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 28 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 29 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 30 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 31 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 32 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 33 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 34 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 35 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 36 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 37 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 38 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 39 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 40 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 41 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 42 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 43 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 44 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 45 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 46 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 47 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 48 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 49 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 50 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 51 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 52 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 53 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 54 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 55 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 56 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 57 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 58 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 59 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 60 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 61 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 62 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 63 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 64 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 65 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 66 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 67 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 68 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 69 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 70 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 71 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 72 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 73 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 74 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 75 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 76 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 77 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 78 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 79 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 80 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 81 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 82 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 83 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 84 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 85 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 86 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 87 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 88 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 89 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 90 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 91 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 92 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 93 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 94 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 95 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 96 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 97 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 98 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 99 at applicants training\n",
      "loss: 1.672891616821289 at epoch 0 at applicants training\n",
      "loss: 1.6784769296646118 at epoch 1 at applicants training\n",
      "loss: 1.6788209676742554 at epoch 2 at applicants training\n",
      "loss: 1.6788314580917358 at epoch 3 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 4 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.7025868892669678 at epoch 0 at applicants training\n",
      "loss: 1.679504156112671 at epoch 1 at applicants training\n",
      "loss: 1.678064227104187 at epoch 2 at applicants training\n",
      "loss: 1.678895115852356 at epoch 3 at applicants training\n",
      "loss: 1.678832769393921 at epoch 4 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.67207670211792 at epoch 0 at applicants training\n",
      "loss: 1.693828821182251 at epoch 1 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 2 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 3 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 4 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 5 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 6 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 7 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 8 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 9 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 10 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 11 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 12 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 13 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 14 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 15 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 16 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 17 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 18 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 19 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 20 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 21 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 22 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 23 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 24 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 25 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 26 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 27 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 28 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 29 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 30 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 31 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 32 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 33 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 34 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 35 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 36 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 37 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 38 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 39 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 40 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 41 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 42 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 43 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 44 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 45 at applicants training\n",
      "loss: 1.693832278251648 at epoch 46 at applicants training\n",
      "loss: 1.6938183307647705 at epoch 47 at applicants training\n",
      "loss: 1.6947017908096313 at epoch 48 at applicants training\n",
      "loss: 1.6938108205795288 at epoch 49 at applicants training\n",
      "loss: 1.6938320398330688 at epoch 50 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 51 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 52 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 53 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 54 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 55 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 56 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 57 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 58 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 59 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 60 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 61 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 62 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 63 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 64 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 65 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 66 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 67 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 68 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 69 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 70 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 71 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 72 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 73 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 74 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 75 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 76 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 77 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 78 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 79 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 80 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 81 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 82 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 83 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 84 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 85 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 86 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 87 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 88 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 89 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 90 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 91 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 92 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 93 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 94 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 95 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 96 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 97 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 98 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 99 at applicants training\n",
      "loss: 1.684539556503296 at epoch 0 at applicants training\n",
      "loss: 1.67710280418396 at epoch 1 at applicants training\n",
      "loss: 1.6787654161453247 at epoch 2 at applicants training\n",
      "loss: 1.6788322925567627 at epoch 3 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 4 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.6930596828460693 at epoch 0 at applicants training\n",
      "loss: 1.7202837467193604 at epoch 1 at applicants training\n",
      "loss: 1.7113412618637085 at epoch 2 at applicants training\n",
      "loss: 1.7013895511627197 at epoch 3 at applicants training\n",
      "loss: 1.7171969413757324 at epoch 4 at applicants training\n",
      "loss: 1.7178324460983276 at epoch 5 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 6 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 7 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 8 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 9 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 10 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 11 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 12 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 13 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 14 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 15 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 16 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 17 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 18 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 19 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 20 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 21 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 22 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 23 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 24 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 25 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 26 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 27 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 28 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 29 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 30 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 31 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 32 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 33 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 34 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 35 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 36 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 37 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 38 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 39 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 40 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 41 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 42 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 43 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 44 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 45 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 46 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 47 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 48 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 49 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 50 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 51 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 52 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 53 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 54 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 55 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 56 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 57 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 58 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 59 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 60 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 61 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 62 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 63 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 64 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 65 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 66 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 67 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 68 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 69 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 70 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 71 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 72 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 73 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 74 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 75 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 76 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 77 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 78 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 79 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 80 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 81 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 82 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 83 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 84 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 85 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 86 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 87 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 88 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 89 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 90 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 91 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 92 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 93 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 94 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 95 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 96 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 97 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 98 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 99 at applicants training\n",
      "loss: 1.6623461246490479 at epoch 0 at applicants training\n",
      "loss: 1.659172773361206 at epoch 1 at applicants training\n",
      "loss: 1.6757946014404297 at epoch 2 at applicants training\n",
      "loss: 1.6619278192520142 at epoch 3 at applicants training\n",
      "loss: 1.6750401258468628 at epoch 4 at applicants training\n",
      "loss: 1.6804105043411255 at epoch 5 at applicants training\n",
      "loss: 1.6775236129760742 at epoch 6 at applicants training\n",
      "loss: 1.6657214164733887 at epoch 7 at applicants training\n",
      "loss: 1.6513259410858154 at epoch 8 at applicants training\n",
      "loss: 1.6495503187179565 at epoch 9 at applicants training\n",
      "loss: 1.652350902557373 at epoch 10 at applicants training\n",
      "loss: 1.6519039869308472 at epoch 11 at applicants training\n",
      "loss: 1.64608895778656 at epoch 12 at applicants training\n",
      "loss: 1.6427710056304932 at epoch 13 at applicants training\n",
      "loss: 1.644978642463684 at epoch 14 at applicants training\n",
      "loss: 1.6387519836425781 at epoch 15 at applicants training\n",
      "loss: 1.6413203477859497 at epoch 16 at applicants training\n",
      "loss: 1.6406930685043335 at epoch 17 at applicants training\n",
      "loss: 1.6368682384490967 at epoch 18 at applicants training\n",
      "loss: 1.639744520187378 at epoch 19 at applicants training\n",
      "loss: 1.6386853456497192 at epoch 20 at applicants training\n",
      "loss: 1.6345481872558594 at epoch 21 at applicants training\n",
      "loss: 1.6286507844924927 at epoch 22 at applicants training\n",
      "loss: 1.629400372505188 at epoch 23 at applicants training\n",
      "loss: 1.6285473108291626 at epoch 24 at applicants training\n",
      "loss: 1.6250420808792114 at epoch 25 at applicants training\n",
      "loss: 1.6236534118652344 at epoch 26 at applicants training\n",
      "loss: 1.6251767873764038 at epoch 27 at applicants training\n",
      "loss: 1.6195855140686035 at epoch 28 at applicants training\n",
      "loss: 1.6186460256576538 at epoch 29 at applicants training\n",
      "loss: 1.6200149059295654 at epoch 30 at applicants training\n",
      "loss: 1.617255449295044 at epoch 31 at applicants training\n",
      "loss: 1.616737961769104 at epoch 32 at applicants training\n",
      "loss: 1.6178975105285645 at epoch 33 at applicants training\n",
      "loss: 1.6144893169403076 at epoch 34 at applicants training\n",
      "loss: 1.6138057708740234 at epoch 35 at applicants training\n",
      "loss: 1.613419532775879 at epoch 36 at applicants training\n",
      "loss: 1.6120808124542236 at epoch 37 at applicants training\n",
      "loss: 1.610417127609253 at epoch 38 at applicants training\n",
      "loss: 1.6087270975112915 at epoch 39 at applicants training\n",
      "loss: 1.6130375862121582 at epoch 40 at applicants training\n",
      "loss: 1.6066590547561646 at epoch 41 at applicants training\n",
      "loss: 1.6142346858978271 at epoch 42 at applicants training\n",
      "loss: 1.6125997304916382 at epoch 43 at applicants training\n",
      "loss: 1.6123816967010498 at epoch 44 at applicants training\n",
      "loss: 1.6075360774993896 at epoch 45 at applicants training\n",
      "loss: 1.6116492748260498 at epoch 46 at applicants training\n",
      "loss: 1.6135436296463013 at epoch 47 at applicants training\n",
      "loss: 1.6101197004318237 at epoch 48 at applicants training\n",
      "loss: 1.606749415397644 at epoch 49 at applicants training\n",
      "loss: 1.6156542301177979 at epoch 50 at applicants training\n",
      "loss: 1.6011989116668701 at epoch 51 at applicants training\n",
      "loss: 1.6085505485534668 at epoch 52 at applicants training\n",
      "loss: 1.6002066135406494 at epoch 53 at applicants training\n",
      "loss: 1.6068460941314697 at epoch 54 at applicants training\n",
      "loss: 1.6000752449035645 at epoch 55 at applicants training\n",
      "loss: 1.6016322374343872 at epoch 56 at applicants training\n",
      "loss: 1.6007229089736938 at epoch 57 at applicants training\n",
      "loss: 1.5982438325881958 at epoch 58 at applicants training\n",
      "loss: 1.5979628562927246 at epoch 59 at applicants training\n",
      "loss: 1.5964205265045166 at epoch 60 at applicants training\n",
      "loss: 1.5954790115356445 at epoch 61 at applicants training\n",
      "loss: 1.5964099168777466 at epoch 62 at applicants training\n",
      "loss: 1.5986214876174927 at epoch 63 at applicants training\n",
      "loss: 1.59307861328125 at epoch 64 at applicants training\n",
      "loss: 1.6056923866271973 at epoch 65 at applicants training\n",
      "loss: 1.6041338443756104 at epoch 66 at applicants training\n",
      "loss: 1.6048403978347778 at epoch 67 at applicants training\n",
      "loss: 1.6045624017715454 at epoch 68 at applicants training\n",
      "loss: 1.5914427042007446 at epoch 69 at applicants training\n",
      "loss: 1.6022130250930786 at epoch 70 at applicants training\n",
      "loss: 1.6062754392623901 at epoch 71 at applicants training\n",
      "loss: 1.595716118812561 at epoch 72 at applicants training\n",
      "loss: 1.6026594638824463 at epoch 73 at applicants training\n",
      "loss: 1.5963345766067505 at epoch 74 at applicants training\n",
      "loss: 1.5935834646224976 at epoch 75 at applicants training\n",
      "loss: 1.5952404737472534 at epoch 76 at applicants training\n",
      "loss: 1.5923678874969482 at epoch 77 at applicants training\n",
      "loss: 1.5963054895401 at epoch 78 at applicants training\n",
      "loss: 1.5961991548538208 at epoch 79 at applicants training\n",
      "loss: 1.5859664678573608 at epoch 80 at applicants training\n",
      "loss: 1.5951825380325317 at epoch 81 at applicants training\n",
      "loss: 1.5903981924057007 at epoch 82 at applicants training\n",
      "loss: 1.5925532579421997 at epoch 83 at applicants training\n",
      "loss: 1.5916136503219604 at epoch 84 at applicants training\n",
      "loss: 1.5861880779266357 at epoch 85 at applicants training\n",
      "loss: 1.58475923538208 at epoch 86 at applicants training\n",
      "loss: 1.586873173713684 at epoch 87 at applicants training\n",
      "loss: 1.5829758644104004 at epoch 88 at applicants training\n",
      "loss: 1.588549017906189 at epoch 89 at applicants training\n",
      "loss: 1.5790979862213135 at epoch 90 at applicants training\n",
      "loss: 1.5877182483673096 at epoch 91 at applicants training\n",
      "loss: 1.5800563097000122 at epoch 92 at applicants training\n",
      "loss: 1.5816036462783813 at epoch 93 at applicants training\n",
      "loss: 1.5835117101669312 at epoch 94 at applicants training\n",
      "loss: 1.5776690244674683 at epoch 95 at applicants training\n",
      "loss: 1.580383062362671 at epoch 96 at applicants training\n",
      "loss: 1.5763678550720215 at epoch 97 at applicants training\n",
      "loss: 1.578499674797058 at epoch 98 at applicants training\n",
      "loss: 1.5741753578186035 at epoch 99 at applicants training\n",
      "loss: 1.7079548835754395 at epoch 0 at applicants training\n",
      "loss: 1.6700767278671265 at epoch 1 at applicants training\n",
      "loss: 1.7197072505950928 at epoch 2 at applicants training\n",
      "loss: 1.7194502353668213 at epoch 3 at applicants training\n",
      "loss: 1.6867824792861938 at epoch 4 at applicants training\n",
      "loss: 1.6916100978851318 at epoch 5 at applicants training\n",
      "loss: 1.693876028060913 at epoch 6 at applicants training\n",
      "loss: 1.6938337087631226 at epoch 7 at applicants training\n",
      "loss: 1.6938328742980957 at epoch 8 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 9 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 10 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 11 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 12 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 13 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 14 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 15 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 16 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 17 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 18 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 19 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 20 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 21 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 22 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 23 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 24 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 25 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 26 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 27 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 28 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 29 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 30 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 31 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 32 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 33 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 34 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 35 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 36 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 37 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 38 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 39 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 40 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 41 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 42 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 43 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 44 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 45 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 46 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 47 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 48 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 49 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 50 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 51 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 52 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 53 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 54 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 55 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 56 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 57 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 58 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 59 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 60 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 61 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 62 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 63 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 64 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 65 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 66 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 67 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 68 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 69 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 70 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 71 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 72 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 73 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 74 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 75 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 76 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 77 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 78 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 79 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 80 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 81 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 82 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 83 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 84 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 85 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 86 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 87 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 88 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 89 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 90 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 91 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 92 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 93 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 94 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 95 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 96 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 97 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 98 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 99 at applicants training\n",
      "loss: 1.6952433586120605 at epoch 0 at applicants training\n",
      "loss: 1.6820029020309448 at epoch 1 at applicants training\n",
      "loss: 1.6822526454925537 at epoch 2 at applicants training\n",
      "loss: 1.677222728729248 at epoch 3 at applicants training\n",
      "loss: 1.6697574853897095 at epoch 4 at applicants training\n",
      "loss: 1.6749480962753296 at epoch 5 at applicants training\n",
      "loss: 1.6675810813903809 at epoch 6 at applicants training\n",
      "loss: 1.671184778213501 at epoch 7 at applicants training\n",
      "loss: 1.6635798215866089 at epoch 8 at applicants training\n",
      "loss: 1.665639877319336 at epoch 9 at applicants training\n",
      "loss: 1.658598780632019 at epoch 10 at applicants training\n",
      "loss: 1.6580641269683838 at epoch 11 at applicants training\n",
      "loss: 1.6566996574401855 at epoch 12 at applicants training\n",
      "loss: 1.6497039794921875 at epoch 13 at applicants training\n",
      "loss: 1.644429087638855 at epoch 14 at applicants training\n",
      "loss: 1.6393437385559082 at epoch 15 at applicants training\n",
      "loss: 1.6251479387283325 at epoch 16 at applicants training\n",
      "loss: 1.6379321813583374 at epoch 17 at applicants training\n",
      "loss: 1.6356561183929443 at epoch 18 at applicants training\n",
      "loss: 1.6251156330108643 at epoch 19 at applicants training\n",
      "loss: 1.6304668188095093 at epoch 20 at applicants training\n",
      "loss: 1.6231147050857544 at epoch 21 at applicants training\n",
      "loss: 1.6221940517425537 at epoch 22 at applicants training\n",
      "loss: 1.6186703443527222 at epoch 23 at applicants training\n",
      "loss: 1.6176255941390991 at epoch 24 at applicants training\n",
      "loss: 1.6158989667892456 at epoch 25 at applicants training\n",
      "loss: 1.6150953769683838 at epoch 26 at applicants training\n",
      "loss: 1.6138399839401245 at epoch 27 at applicants training\n",
      "loss: 1.6110906600952148 at epoch 28 at applicants training\n",
      "loss: 1.6089222431182861 at epoch 29 at applicants training\n",
      "loss: 1.6080706119537354 at epoch 30 at applicants training\n",
      "loss: 1.6057038307189941 at epoch 31 at applicants training\n",
      "loss: 1.607710361480713 at epoch 32 at applicants training\n",
      "loss: 1.6039292812347412 at epoch 33 at applicants training\n",
      "loss: 1.6033583879470825 at epoch 34 at applicants training\n",
      "loss: 1.60024893283844 at epoch 35 at applicants training\n",
      "loss: 1.5989364385604858 at epoch 36 at applicants training\n",
      "loss: 1.5969935655593872 at epoch 37 at applicants training\n",
      "loss: 1.5959038734436035 at epoch 38 at applicants training\n",
      "loss: 1.596290946006775 at epoch 39 at applicants training\n",
      "loss: 1.5955924987792969 at epoch 40 at applicants training\n",
      "loss: 1.5939695835113525 at epoch 41 at applicants training\n",
      "loss: 1.594644546508789 at epoch 42 at applicants training\n",
      "loss: 1.5934727191925049 at epoch 43 at applicants training\n",
      "loss: 1.5907257795333862 at epoch 44 at applicants training\n",
      "loss: 1.5906622409820557 at epoch 45 at applicants training\n",
      "loss: 1.590584397315979 at epoch 46 at applicants training\n",
      "loss: 1.5889732837677002 at epoch 47 at applicants training\n",
      "loss: 1.5875332355499268 at epoch 48 at applicants training\n",
      "loss: 1.5876554250717163 at epoch 49 at applicants training\n",
      "loss: 1.5868620872497559 at epoch 50 at applicants training\n",
      "loss: 1.5860828161239624 at epoch 51 at applicants training\n",
      "loss: 1.5850697755813599 at epoch 52 at applicants training\n",
      "loss: 1.5848045349121094 at epoch 53 at applicants training\n",
      "loss: 1.5843913555145264 at epoch 54 at applicants training\n",
      "loss: 1.5837364196777344 at epoch 55 at applicants training\n",
      "loss: 1.5833626985549927 at epoch 56 at applicants training\n",
      "loss: 1.5826774835586548 at epoch 57 at applicants training\n",
      "loss: 1.5822330713272095 at epoch 58 at applicants training\n",
      "loss: 1.5816060304641724 at epoch 59 at applicants training\n",
      "loss: 1.5815154314041138 at epoch 60 at applicants training\n",
      "loss: 1.5825791358947754 at epoch 61 at applicants training\n",
      "loss: 1.586378812789917 at epoch 62 at applicants training\n",
      "loss: 1.5880128145217896 at epoch 63 at applicants training\n",
      "loss: 1.5808601379394531 at epoch 64 at applicants training\n",
      "loss: 1.5784635543823242 at epoch 65 at applicants training\n",
      "loss: 1.582068920135498 at epoch 66 at applicants training\n",
      "loss: 1.5812174081802368 at epoch 67 at applicants training\n",
      "loss: 1.5769803524017334 at epoch 68 at applicants training\n",
      "loss: 1.578492283821106 at epoch 69 at applicants training\n",
      "loss: 1.5803827047348022 at epoch 70 at applicants training\n",
      "loss: 1.5785666704177856 at epoch 71 at applicants training\n",
      "loss: 1.5765163898468018 at epoch 72 at applicants training\n",
      "loss: 1.5772080421447754 at epoch 73 at applicants training\n",
      "loss: 1.5807766914367676 at epoch 74 at applicants training\n",
      "loss: 1.5737535953521729 at epoch 75 at applicants training\n",
      "loss: 1.5770319700241089 at epoch 76 at applicants training\n",
      "loss: 1.5784047842025757 at epoch 77 at applicants training\n",
      "loss: 1.5735278129577637 at epoch 78 at applicants training\n",
      "loss: 1.5755566358566284 at epoch 79 at applicants training\n",
      "loss: 1.5775535106658936 at epoch 80 at applicants training\n",
      "loss: 1.5750501155853271 at epoch 81 at applicants training\n",
      "loss: 1.573301911354065 at epoch 82 at applicants training\n",
      "loss: 1.5792202949523926 at epoch 83 at applicants training\n",
      "loss: 1.5730170011520386 at epoch 84 at applicants training\n",
      "loss: 1.5725409984588623 at epoch 85 at applicants training\n",
      "loss: 1.5764189958572388 at epoch 86 at applicants training\n",
      "loss: 1.5697829723358154 at epoch 87 at applicants training\n",
      "loss: 1.5727918148040771 at epoch 88 at applicants training\n",
      "loss: 1.5703346729278564 at epoch 89 at applicants training\n",
      "loss: 1.5694042444229126 at epoch 90 at applicants training\n",
      "loss: 1.5711182355880737 at epoch 91 at applicants training\n",
      "loss: 1.5700870752334595 at epoch 92 at applicants training\n",
      "loss: 1.5690935850143433 at epoch 93 at applicants training\n",
      "loss: 1.5704256296157837 at epoch 94 at applicants training\n",
      "loss: 1.5692574977874756 at epoch 95 at applicants training\n",
      "loss: 1.5679506063461304 at epoch 96 at applicants training\n",
      "loss: 1.570200800895691 at epoch 97 at applicants training\n",
      "loss: 1.56755793094635 at epoch 98 at applicants training\n",
      "loss: 1.5683727264404297 at epoch 99 at applicants training\n",
      "loss: 1.691294550895691 at epoch 0 at applicants training\n",
      "loss: 1.7206984758377075 at epoch 1 at applicants training\n",
      "loss: 1.6903274059295654 at epoch 2 at applicants training\n",
      "loss: 1.7081166505813599 at epoch 3 at applicants training\n",
      "loss: 1.7040908336639404 at epoch 4 at applicants training\n",
      "loss: 1.6930336952209473 at epoch 5 at applicants training\n",
      "loss: 1.694008469581604 at epoch 6 at applicants training\n",
      "loss: 1.689926028251648 at epoch 7 at applicants training\n",
      "loss: 1.6846258640289307 at epoch 8 at applicants training\n",
      "loss: 1.6831417083740234 at epoch 9 at applicants training\n",
      "loss: 1.6792241334915161 at epoch 10 at applicants training\n",
      "loss: 1.6718840599060059 at epoch 11 at applicants training\n",
      "loss: 1.6755104064941406 at epoch 12 at applicants training\n",
      "loss: 1.6797163486480713 at epoch 13 at applicants training\n",
      "loss: 1.6623833179473877 at epoch 14 at applicants training\n",
      "loss: 1.6852147579193115 at epoch 15 at applicants training\n",
      "loss: 1.669350266456604 at epoch 16 at applicants training\n",
      "loss: 1.675103783607483 at epoch 17 at applicants training\n",
      "loss: 1.668232798576355 at epoch 18 at applicants training\n",
      "loss: 1.675665259361267 at epoch 19 at applicants training\n",
      "loss: 1.6660577058792114 at epoch 20 at applicants training\n",
      "loss: 1.6707971096038818 at epoch 21 at applicants training\n",
      "loss: 1.6648122072219849 at epoch 22 at applicants training\n",
      "loss: 1.6655313968658447 at epoch 23 at applicants training\n",
      "loss: 1.664239525794983 at epoch 24 at applicants training\n",
      "loss: 1.6633986234664917 at epoch 25 at applicants training\n",
      "loss: 1.664688229560852 at epoch 26 at applicants training\n",
      "loss: 1.661345362663269 at epoch 27 at applicants training\n",
      "loss: 1.6626466512680054 at epoch 28 at applicants training\n",
      "loss: 1.6609177589416504 at epoch 29 at applicants training\n",
      "loss: 1.658837080001831 at epoch 30 at applicants training\n",
      "loss: 1.660224199295044 at epoch 31 at applicants training\n",
      "loss: 1.6651865243911743 at epoch 32 at applicants training\n",
      "loss: 1.6694810390472412 at epoch 33 at applicants training\n",
      "loss: 1.6547973155975342 at epoch 34 at applicants training\n",
      "loss: 1.6748822927474976 at epoch 35 at applicants training\n",
      "loss: 1.6614617109298706 at epoch 36 at applicants training\n",
      "loss: 1.6569169759750366 at epoch 37 at applicants training\n",
      "loss: 1.6697425842285156 at epoch 38 at applicants training\n",
      "loss: 1.6535152196884155 at epoch 39 at applicants training\n",
      "loss: 1.6667778491973877 at epoch 40 at applicants training\n",
      "loss: 1.6520451307296753 at epoch 41 at applicants training\n",
      "loss: 1.666152834892273 at epoch 42 at applicants training\n",
      "loss: 1.6522849798202515 at epoch 43 at applicants training\n",
      "loss: 1.6598540544509888 at epoch 44 at applicants training\n",
      "loss: 1.65654718875885 at epoch 45 at applicants training\n",
      "loss: 1.6568045616149902 at epoch 46 at applicants training\n",
      "loss: 1.6547349691390991 at epoch 47 at applicants training\n",
      "loss: 1.651745080947876 at epoch 48 at applicants training\n",
      "loss: 1.6555331945419312 at epoch 49 at applicants training\n",
      "loss: 1.6490404605865479 at epoch 50 at applicants training\n",
      "loss: 1.6525754928588867 at epoch 51 at applicants training\n",
      "loss: 1.6491330862045288 at epoch 52 at applicants training\n",
      "loss: 1.6463336944580078 at epoch 53 at applicants training\n",
      "loss: 1.6500787734985352 at epoch 54 at applicants training\n",
      "loss: 1.6492959260940552 at epoch 55 at applicants training\n",
      "loss: 1.6454739570617676 at epoch 56 at applicants training\n",
      "loss: 1.643149733543396 at epoch 57 at applicants training\n",
      "loss: 1.6429710388183594 at epoch 58 at applicants training\n",
      "loss: 1.6461498737335205 at epoch 59 at applicants training\n",
      "loss: 1.6623542308807373 at epoch 60 at applicants training\n",
      "loss: 1.6695992946624756 at epoch 61 at applicants training\n",
      "loss: 1.6429790258407593 at epoch 62 at applicants training\n",
      "loss: 1.688607096672058 at epoch 63 at applicants training\n",
      "loss: 1.653947114944458 at epoch 64 at applicants training\n",
      "loss: 1.6962597370147705 at epoch 65 at applicants training\n",
      "loss: 1.709065318107605 at epoch 66 at applicants training\n",
      "loss: 1.7112233638763428 at epoch 67 at applicants training\n",
      "loss: 1.7119734287261963 at epoch 68 at applicants training\n",
      "loss: 1.7122743129730225 at epoch 69 at applicants training\n",
      "loss: 1.7123390436172485 at epoch 70 at applicants training\n",
      "loss: 1.712209939956665 at epoch 71 at applicants training\n",
      "loss: 1.7118343114852905 at epoch 72 at applicants training\n",
      "loss: 1.7115024328231812 at epoch 73 at applicants training\n",
      "loss: 1.7114818096160889 at epoch 74 at applicants training\n",
      "loss: 1.7110775709152222 at epoch 75 at applicants training\n",
      "loss: 1.709754467010498 at epoch 76 at applicants training\n",
      "loss: 1.7066264152526855 at epoch 77 at applicants training\n",
      "loss: 1.7028791904449463 at epoch 78 at applicants training\n",
      "loss: 1.7009491920471191 at epoch 79 at applicants training\n",
      "loss: 1.7058453559875488 at epoch 80 at applicants training\n",
      "loss: 1.7031067609786987 at epoch 81 at applicants training\n",
      "loss: 1.6937339305877686 at epoch 82 at applicants training\n",
      "loss: 1.6978830099105835 at epoch 83 at applicants training\n",
      "loss: 1.6945198774337769 at epoch 84 at applicants training\n",
      "loss: 1.6922765970230103 at epoch 85 at applicants training\n",
      "loss: 1.6931108236312866 at epoch 86 at applicants training\n",
      "loss: 1.689394235610962 at epoch 87 at applicants training\n",
      "loss: 1.6903525590896606 at epoch 88 at applicants training\n",
      "loss: 1.6901557445526123 at epoch 89 at applicants training\n",
      "loss: 1.6883134841918945 at epoch 90 at applicants training\n",
      "loss: 1.6864055395126343 at epoch 91 at applicants training\n",
      "loss: 1.687358021736145 at epoch 92 at applicants training\n",
      "loss: 1.683929204940796 at epoch 93 at applicants training\n",
      "loss: 1.684474229812622 at epoch 94 at applicants training\n",
      "loss: 1.6820961236953735 at epoch 95 at applicants training\n",
      "loss: 1.679848551750183 at epoch 96 at applicants training\n",
      "loss: 1.6784753799438477 at epoch 97 at applicants training\n",
      "loss: 1.6750537157058716 at epoch 98 at applicants training\n",
      "loss: 1.6738744974136353 at epoch 99 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 0 at applicants training\n",
      "loss: 1.7120473384857178 at epoch 1 at applicants training\n",
      "loss: 1.7054917812347412 at epoch 2 at applicants training\n",
      "loss: 1.666646122932434 at epoch 3 at applicants training\n",
      "loss: 1.6602803468704224 at epoch 4 at applicants training\n",
      "loss: 1.6667160987854004 at epoch 5 at applicants training\n",
      "loss: 1.654047966003418 at epoch 6 at applicants training\n",
      "loss: 1.7118206024169922 at epoch 7 at applicants training\n",
      "loss: 1.715138554573059 at epoch 8 at applicants training\n",
      "loss: 1.7102479934692383 at epoch 9 at applicants training\n",
      "loss: 1.6582493782043457 at epoch 10 at applicants training\n",
      "loss: 1.6764965057373047 at epoch 11 at applicants training\n",
      "loss: 1.6788311004638672 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.715730905532837 at epoch 0 at applicants training\n",
      "loss: 1.7138645648956299 at epoch 1 at applicants training\n",
      "loss: 1.7115368843078613 at epoch 2 at applicants training\n",
      "loss: 1.6914116144180298 at epoch 3 at applicants training\n",
      "loss: 1.687992811203003 at epoch 4 at applicants training\n",
      "loss: 1.6849939823150635 at epoch 5 at applicants training\n",
      "loss: 1.6879069805145264 at epoch 6 at applicants training\n",
      "loss: 1.6873308420181274 at epoch 7 at applicants training\n",
      "loss: 1.6880877017974854 at epoch 8 at applicants training\n",
      "loss: 1.6860007047653198 at epoch 9 at applicants training\n",
      "loss: 1.6796512603759766 at epoch 10 at applicants training\n",
      "loss: 1.667519211769104 at epoch 11 at applicants training\n",
      "loss: 1.6660161018371582 at epoch 12 at applicants training\n",
      "loss: 1.671120047569275 at epoch 13 at applicants training\n",
      "loss: 1.6709071397781372 at epoch 14 at applicants training\n",
      "loss: 1.6680124998092651 at epoch 15 at applicants training\n",
      "loss: 1.6609395742416382 at epoch 16 at applicants training\n",
      "loss: 1.649865984916687 at epoch 17 at applicants training\n",
      "loss: 1.6537086963653564 at epoch 18 at applicants training\n",
      "loss: 1.6490105390548706 at epoch 19 at applicants training\n",
      "loss: 1.6460881233215332 at epoch 20 at applicants training\n",
      "loss: 1.6486765146255493 at epoch 21 at applicants training\n",
      "loss: 1.6419183015823364 at epoch 22 at applicants training\n",
      "loss: 1.6454591751098633 at epoch 23 at applicants training\n",
      "loss: 1.643320918083191 at epoch 24 at applicants training\n",
      "loss: 1.6402544975280762 at epoch 25 at applicants training\n",
      "loss: 1.640303611755371 at epoch 26 at applicants training\n",
      "loss: 1.6381019353866577 at epoch 27 at applicants training\n",
      "loss: 1.6387791633605957 at epoch 28 at applicants training\n",
      "loss: 1.6352057456970215 at epoch 29 at applicants training\n",
      "loss: 1.6359138488769531 at epoch 30 at applicants training\n",
      "loss: 1.6338226795196533 at epoch 31 at applicants training\n",
      "loss: 1.6333311796188354 at epoch 32 at applicants training\n",
      "loss: 1.631343126296997 at epoch 33 at applicants training\n",
      "loss: 1.6322228908538818 at epoch 34 at applicants training\n",
      "loss: 1.63021981716156 at epoch 35 at applicants training\n",
      "loss: 1.6300321817398071 at epoch 36 at applicants training\n",
      "loss: 1.6293673515319824 at epoch 37 at applicants training\n",
      "loss: 1.6284500360488892 at epoch 38 at applicants training\n",
      "loss: 1.627474069595337 at epoch 39 at applicants training\n",
      "loss: 1.6270160675048828 at epoch 40 at applicants training\n",
      "loss: 1.6262037754058838 at epoch 41 at applicants training\n",
      "loss: 1.6243890523910522 at epoch 42 at applicants training\n",
      "loss: 1.6235727071762085 at epoch 43 at applicants training\n",
      "loss: 1.6217337846755981 at epoch 44 at applicants training\n",
      "loss: 1.621100664138794 at epoch 45 at applicants training\n",
      "loss: 1.6209841966629028 at epoch 46 at applicants training\n",
      "loss: 1.6195112466812134 at epoch 47 at applicants training\n",
      "loss: 1.6175400018692017 at epoch 48 at applicants training\n",
      "loss: 1.6172065734863281 at epoch 49 at applicants training\n",
      "loss: 1.6186789274215698 at epoch 50 at applicants training\n",
      "loss: 1.6197290420532227 at epoch 51 at applicants training\n",
      "loss: 1.6163334846496582 at epoch 52 at applicants training\n",
      "loss: 1.6152255535125732 at epoch 53 at applicants training\n",
      "loss: 1.6160519123077393 at epoch 54 at applicants training\n",
      "loss: 1.613892674446106 at epoch 55 at applicants training\n",
      "loss: 1.612708568572998 at epoch 56 at applicants training\n",
      "loss: 1.6148446798324585 at epoch 57 at applicants training\n",
      "loss: 1.615674614906311 at epoch 58 at applicants training\n",
      "loss: 1.6119942665100098 at epoch 59 at applicants training\n",
      "loss: 1.6108591556549072 at epoch 60 at applicants training\n",
      "loss: 1.6115447282791138 at epoch 61 at applicants training\n",
      "loss: 1.6108577251434326 at epoch 62 at applicants training\n",
      "loss: 1.6092321872711182 at epoch 63 at applicants training\n",
      "loss: 1.6094822883605957 at epoch 64 at applicants training\n",
      "loss: 1.6111438274383545 at epoch 65 at applicants training\n",
      "loss: 1.61024010181427 at epoch 66 at applicants training\n",
      "loss: 1.6075235605239868 at epoch 67 at applicants training\n",
      "loss: 1.606290340423584 at epoch 68 at applicants training\n",
      "loss: 1.60651433467865 at epoch 69 at applicants training\n",
      "loss: 1.607035517692566 at epoch 70 at applicants training\n",
      "loss: 1.6071749925613403 at epoch 71 at applicants training\n",
      "loss: 1.6064790487289429 at epoch 72 at applicants training\n",
      "loss: 1.60530424118042 at epoch 73 at applicants training\n",
      "loss: 1.6042373180389404 at epoch 74 at applicants training\n",
      "loss: 1.6039893627166748 at epoch 75 at applicants training\n",
      "loss: 1.6035442352294922 at epoch 76 at applicants training\n",
      "loss: 1.6036643981933594 at epoch 77 at applicants training\n",
      "loss: 1.6031041145324707 at epoch 78 at applicants training\n",
      "loss: 1.6034613847732544 at epoch 79 at applicants training\n",
      "loss: 1.6025232076644897 at epoch 80 at applicants training\n",
      "loss: 1.602189302444458 at epoch 81 at applicants training\n",
      "loss: 1.6018861532211304 at epoch 82 at applicants training\n",
      "loss: 1.6016948223114014 at epoch 83 at applicants training\n",
      "loss: 1.6019320487976074 at epoch 84 at applicants training\n",
      "loss: 1.6012616157531738 at epoch 85 at applicants training\n",
      "loss: 1.6009759902954102 at epoch 86 at applicants training\n",
      "loss: 1.6004092693328857 at epoch 87 at applicants training\n",
      "loss: 1.6000981330871582 at epoch 88 at applicants training\n",
      "loss: 1.599557638168335 at epoch 89 at applicants training\n",
      "loss: 1.5993916988372803 at epoch 90 at applicants training\n",
      "loss: 1.5992273092269897 at epoch 91 at applicants training\n",
      "loss: 1.5995041131973267 at epoch 92 at applicants training\n",
      "loss: 1.601267695426941 at epoch 93 at applicants training\n",
      "loss: 1.6018221378326416 at epoch 94 at applicants training\n",
      "loss: 1.6010246276855469 at epoch 95 at applicants training\n",
      "loss: 1.5981398820877075 at epoch 96 at applicants training\n",
      "loss: 1.5972166061401367 at epoch 97 at applicants training\n",
      "loss: 1.59745454788208 at epoch 98 at applicants training\n",
      "loss: 1.599344253540039 at epoch 99 at applicants training\n",
      "loss: 1.6724684238433838 at epoch 0 at applicants training\n",
      "loss: 1.7208302021026611 at epoch 1 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 2 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 3 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 4 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 5 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 6 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 7 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 8 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 9 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 10 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 11 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 12 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 13 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 14 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 15 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 16 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 17 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 18 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 19 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 20 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 21 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 22 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 23 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 24 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 25 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 26 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 27 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 28 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 29 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 30 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 31 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 32 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 33 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 34 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 35 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 36 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 37 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 38 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 39 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 40 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 41 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 42 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 43 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 44 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 45 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 46 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 47 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 48 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 49 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 50 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 51 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 52 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 53 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 54 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 55 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 56 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 57 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 58 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 59 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 60 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 61 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 62 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 63 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 64 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 65 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 66 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 67 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 68 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 69 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 70 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 71 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 72 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 73 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 74 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 75 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 76 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 77 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 78 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 79 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 80 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 81 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 82 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 83 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 84 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 85 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 86 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 87 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 88 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 89 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 90 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 91 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 92 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 93 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 94 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 95 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 96 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 97 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 98 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 99 at applicants training\n",
      "loss: 1.6801280975341797 at epoch 0 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 1 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 2 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 3 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 4 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.720821738243103 at epoch 0 at applicants training\n",
      "loss: 1.6972804069519043 at epoch 1 at applicants training\n",
      "loss: 1.6874953508377075 at epoch 2 at applicants training\n",
      "loss: 1.7102773189544678 at epoch 3 at applicants training\n",
      "loss: 1.6835029125213623 at epoch 4 at applicants training\n",
      "loss: 1.6925214529037476 at epoch 5 at applicants training\n",
      "loss: 1.692073941230774 at epoch 6 at applicants training\n",
      "loss: 1.6895872354507446 at epoch 7 at applicants training\n",
      "loss: 1.6907824277877808 at epoch 8 at applicants training\n",
      "loss: 1.688321828842163 at epoch 9 at applicants training\n",
      "loss: 1.683927059173584 at epoch 10 at applicants training\n",
      "loss: 1.681084394454956 at epoch 11 at applicants training\n",
      "loss: 1.6796255111694336 at epoch 12 at applicants training\n",
      "loss: 1.6782214641571045 at epoch 13 at applicants training\n",
      "loss: 1.6772898435592651 at epoch 14 at applicants training\n",
      "loss: 1.6755443811416626 at epoch 15 at applicants training\n",
      "loss: 1.6731079816818237 at epoch 16 at applicants training\n",
      "loss: 1.6735490560531616 at epoch 17 at applicants training\n",
      "loss: 1.6826720237731934 at epoch 18 at applicants training\n",
      "loss: 1.6833794116973877 at epoch 19 at applicants training\n",
      "loss: 1.6706005334854126 at epoch 20 at applicants training\n",
      "loss: 1.6919043064117432 at epoch 21 at applicants training\n",
      "loss: 1.6983379125595093 at epoch 22 at applicants training\n",
      "loss: 1.690355658531189 at epoch 23 at applicants training\n",
      "loss: 1.6704238653182983 at epoch 24 at applicants training\n",
      "loss: 1.6869200468063354 at epoch 25 at applicants training\n",
      "loss: 1.6889288425445557 at epoch 26 at applicants training\n",
      "loss: 1.6715726852416992 at epoch 27 at applicants training\n",
      "loss: 1.6811935901641846 at epoch 28 at applicants training\n",
      "loss: 1.6855429410934448 at epoch 29 at applicants training\n",
      "loss: 1.6802799701690674 at epoch 30 at applicants training\n",
      "loss: 1.6694914102554321 at epoch 31 at applicants training\n",
      "loss: 1.677520751953125 at epoch 32 at applicants training\n",
      "loss: 1.6756545305252075 at epoch 33 at applicants training\n",
      "loss: 1.6680001020431519 at epoch 34 at applicants training\n",
      "loss: 1.6722713708877563 at epoch 35 at applicants training\n",
      "loss: 1.6660271883010864 at epoch 36 at applicants training\n",
      "loss: 1.6578036546707153 at epoch 37 at applicants training\n",
      "loss: 1.6611403226852417 at epoch 38 at applicants training\n",
      "loss: 1.6583963632583618 at epoch 39 at applicants training\n",
      "loss: 1.6602920293807983 at epoch 40 at applicants training\n",
      "loss: 1.6581182479858398 at epoch 41 at applicants training\n",
      "loss: 1.6540733575820923 at epoch 42 at applicants training\n",
      "loss: 1.6548247337341309 at epoch 43 at applicants training\n",
      "loss: 1.6434156894683838 at epoch 44 at applicants training\n",
      "loss: 1.6447007656097412 at epoch 45 at applicants training\n",
      "loss: 1.6434024572372437 at epoch 46 at applicants training\n",
      "loss: 1.6429449319839478 at epoch 47 at applicants training\n",
      "loss: 1.6393001079559326 at epoch 48 at applicants training\n",
      "loss: 1.632038950920105 at epoch 49 at applicants training\n",
      "loss: 1.6366513967514038 at epoch 50 at applicants training\n",
      "loss: 1.629489779472351 at epoch 51 at applicants training\n",
      "loss: 1.6322681903839111 at epoch 52 at applicants training\n",
      "loss: 1.6303073167800903 at epoch 53 at applicants training\n",
      "loss: 1.6289674043655396 at epoch 54 at applicants training\n",
      "loss: 1.6278657913208008 at epoch 55 at applicants training\n",
      "loss: 1.627243161201477 at epoch 56 at applicants training\n",
      "loss: 1.6245237588882446 at epoch 57 at applicants training\n",
      "loss: 1.6246508359909058 at epoch 58 at applicants training\n",
      "loss: 1.6231393814086914 at epoch 59 at applicants training\n",
      "loss: 1.6208778619766235 at epoch 60 at applicants training\n",
      "loss: 1.6220403909683228 at epoch 61 at applicants training\n",
      "loss: 1.6188534498214722 at epoch 62 at applicants training\n",
      "loss: 1.6193698644638062 at epoch 63 at applicants training\n",
      "loss: 1.6173235177993774 at epoch 64 at applicants training\n",
      "loss: 1.6158283948898315 at epoch 65 at applicants training\n",
      "loss: 1.6155567169189453 at epoch 66 at applicants training\n",
      "loss: 1.6149570941925049 at epoch 67 at applicants training\n",
      "loss: 1.6147618293762207 at epoch 68 at applicants training\n",
      "loss: 1.613128423690796 at epoch 69 at applicants training\n",
      "loss: 1.6124862432479858 at epoch 70 at applicants training\n",
      "loss: 1.610648512840271 at epoch 71 at applicants training\n",
      "loss: 1.6097948551177979 at epoch 72 at applicants training\n",
      "loss: 1.607364535331726 at epoch 73 at applicants training\n",
      "loss: 1.6070560216903687 at epoch 74 at applicants training\n",
      "loss: 1.6055419445037842 at epoch 75 at applicants training\n",
      "loss: 1.6051788330078125 at epoch 76 at applicants training\n",
      "loss: 1.6033809185028076 at epoch 77 at applicants training\n",
      "loss: 1.6024870872497559 at epoch 78 at applicants training\n",
      "loss: 1.6008353233337402 at epoch 79 at applicants training\n",
      "loss: 1.599680781364441 at epoch 80 at applicants training\n",
      "loss: 1.5989234447479248 at epoch 81 at applicants training\n",
      "loss: 1.597853660583496 at epoch 82 at applicants training\n",
      "loss: 1.5996894836425781 at epoch 83 at applicants training\n",
      "loss: 1.6099499464035034 at epoch 84 at applicants training\n",
      "loss: 1.6124050617218018 at epoch 85 at applicants training\n",
      "loss: 1.5960620641708374 at epoch 86 at applicants training\n",
      "loss: 1.605847954750061 at epoch 87 at applicants training\n",
      "loss: 1.6077320575714111 at epoch 88 at applicants training\n",
      "loss: 1.5962556600570679 at epoch 89 at applicants training\n",
      "loss: 1.6104791164398193 at epoch 90 at applicants training\n",
      "loss: 1.5935375690460205 at epoch 91 at applicants training\n",
      "loss: 1.6077313423156738 at epoch 92 at applicants training\n",
      "loss: 1.5928688049316406 at epoch 93 at applicants training\n",
      "loss: 1.6040147542953491 at epoch 94 at applicants training\n",
      "loss: 1.5926508903503418 at epoch 95 at applicants training\n",
      "loss: 1.5971790552139282 at epoch 96 at applicants training\n",
      "loss: 1.5940802097320557 at epoch 97 at applicants training\n",
      "loss: 1.5929445028305054 at epoch 98 at applicants training\n",
      "loss: 1.595887303352356 at epoch 99 at applicants training\n",
      "loss: 1.714076042175293 at epoch 0 at applicants training\n",
      "loss: 1.6842849254608154 at epoch 1 at applicants training\n",
      "loss: 1.6863141059875488 at epoch 2 at applicants training\n",
      "loss: 1.684097170829773 at epoch 3 at applicants training\n",
      "loss: 1.6850653886795044 at epoch 4 at applicants training\n",
      "loss: 1.6756579875946045 at epoch 5 at applicants training\n",
      "loss: 1.6746826171875 at epoch 6 at applicants training\n",
      "loss: 1.6830557584762573 at epoch 7 at applicants training\n",
      "loss: 1.6795237064361572 at epoch 8 at applicants training\n",
      "loss: 1.6676160097122192 at epoch 9 at applicants training\n",
      "loss: 1.6661763191223145 at epoch 10 at applicants training\n",
      "loss: 1.6717393398284912 at epoch 11 at applicants training\n",
      "loss: 1.6634674072265625 at epoch 12 at applicants training\n",
      "loss: 1.6767449378967285 at epoch 13 at applicants training\n",
      "loss: 1.6618973016738892 at epoch 14 at applicants training\n",
      "loss: 1.670710802078247 at epoch 15 at applicants training\n",
      "loss: 1.6727656126022339 at epoch 16 at applicants training\n",
      "loss: 1.6603831052780151 at epoch 17 at applicants training\n",
      "loss: 1.6699916124343872 at epoch 18 at applicants training\n",
      "loss: 1.6623071432113647 at epoch 19 at applicants training\n",
      "loss: 1.6610547304153442 at epoch 20 at applicants training\n",
      "loss: 1.6669816970825195 at epoch 21 at applicants training\n",
      "loss: 1.6582459211349487 at epoch 22 at applicants training\n",
      "loss: 1.6582026481628418 at epoch 23 at applicants training\n",
      "loss: 1.6609753370285034 at epoch 24 at applicants training\n",
      "loss: 1.6518974304199219 at epoch 25 at applicants training\n",
      "loss: 1.6587482690811157 at epoch 26 at applicants training\n",
      "loss: 1.6499457359313965 at epoch 27 at applicants training\n",
      "loss: 1.6554616689682007 at epoch 28 at applicants training\n",
      "loss: 1.6483895778656006 at epoch 29 at applicants training\n",
      "loss: 1.6543090343475342 at epoch 30 at applicants training\n",
      "loss: 1.6461820602416992 at epoch 31 at applicants training\n",
      "loss: 1.6539024114608765 at epoch 32 at applicants training\n",
      "loss: 1.6434192657470703 at epoch 33 at applicants training\n",
      "loss: 1.6512562036514282 at epoch 34 at applicants training\n",
      "loss: 1.6418166160583496 at epoch 35 at applicants training\n",
      "loss: 1.65067720413208 at epoch 36 at applicants training\n",
      "loss: 1.6405917406082153 at epoch 37 at applicants training\n",
      "loss: 1.6478997468948364 at epoch 38 at applicants training\n",
      "loss: 1.6392501592636108 at epoch 39 at applicants training\n",
      "loss: 1.6466542482376099 at epoch 40 at applicants training\n",
      "loss: 1.6385854482650757 at epoch 41 at applicants training\n",
      "loss: 1.6431703567504883 at epoch 42 at applicants training\n",
      "loss: 1.6381527185440063 at epoch 43 at applicants training\n",
      "loss: 1.6389704942703247 at epoch 44 at applicants training\n",
      "loss: 1.6391335725784302 at epoch 45 at applicants training\n",
      "loss: 1.6354173421859741 at epoch 46 at applicants training\n",
      "loss: 1.6394028663635254 at epoch 47 at applicants training\n",
      "loss: 1.6344354152679443 at epoch 48 at applicants training\n",
      "loss: 1.6349726915359497 at epoch 49 at applicants training\n",
      "loss: 1.6358308792114258 at epoch 50 at applicants training\n",
      "loss: 1.6320512294769287 at epoch 51 at applicants training\n",
      "loss: 1.632794976234436 at epoch 52 at applicants training\n",
      "loss: 1.6340075731277466 at epoch 53 at applicants training\n",
      "loss: 1.6313939094543457 at epoch 54 at applicants training\n",
      "loss: 1.6303694248199463 at epoch 55 at applicants training\n",
      "loss: 1.6318291425704956 at epoch 56 at applicants training\n",
      "loss: 1.6311771869659424 at epoch 57 at applicants training\n",
      "loss: 1.6285017728805542 at epoch 58 at applicants training\n",
      "loss: 1.6292740106582642 at epoch 59 at applicants training\n",
      "loss: 1.630726933479309 at epoch 60 at applicants training\n",
      "loss: 1.6280428171157837 at epoch 61 at applicants training\n",
      "loss: 1.6264986991882324 at epoch 62 at applicants training\n",
      "loss: 1.6273945569992065 at epoch 63 at applicants training\n",
      "loss: 1.6285881996154785 at epoch 64 at applicants training\n",
      "loss: 1.6284024715423584 at epoch 65 at applicants training\n",
      "loss: 1.6254974603652954 at epoch 66 at applicants training\n",
      "loss: 1.6242716312408447 at epoch 67 at applicants training\n",
      "loss: 1.625070571899414 at epoch 68 at applicants training\n",
      "loss: 1.6265569925308228 at epoch 69 at applicants training\n",
      "loss: 1.6269097328186035 at epoch 70 at applicants training\n",
      "loss: 1.6240135431289673 at epoch 71 at applicants training\n",
      "loss: 1.6225225925445557 at epoch 72 at applicants training\n",
      "loss: 1.6227924823760986 at epoch 73 at applicants training\n",
      "loss: 1.6238003969192505 at epoch 74 at applicants training\n",
      "loss: 1.624477744102478 at epoch 75 at applicants training\n",
      "loss: 1.6226317882537842 at epoch 76 at applicants training\n",
      "loss: 1.6210217475891113 at epoch 77 at applicants training\n",
      "loss: 1.620177149772644 at epoch 78 at applicants training\n",
      "loss: 1.6200886964797974 at epoch 79 at applicants training\n",
      "loss: 1.620970606803894 at epoch 80 at applicants training\n",
      "loss: 1.6218916177749634 at epoch 81 at applicants training\n",
      "loss: 1.6241016387939453 at epoch 82 at applicants training\n",
      "loss: 1.6218279600143433 at epoch 83 at applicants training\n",
      "loss: 1.6197373867034912 at epoch 84 at applicants training\n",
      "loss: 1.6179089546203613 at epoch 85 at applicants training\n",
      "loss: 1.6176373958587646 at epoch 86 at applicants training\n",
      "loss: 1.6185250282287598 at epoch 87 at applicants training\n",
      "loss: 1.61963951587677 at epoch 88 at applicants training\n",
      "loss: 1.6216707229614258 at epoch 89 at applicants training\n",
      "loss: 1.6194077730178833 at epoch 90 at applicants training\n",
      "loss: 1.6175140142440796 at epoch 91 at applicants training\n",
      "loss: 1.6159638166427612 at epoch 92 at applicants training\n",
      "loss: 1.6153873205184937 at epoch 93 at applicants training\n",
      "loss: 1.6154221296310425 at epoch 94 at applicants training\n",
      "loss: 1.6158722639083862 at epoch 95 at applicants training\n",
      "loss: 1.6176080703735352 at epoch 96 at applicants training\n",
      "loss: 1.619413137435913 at epoch 97 at applicants training\n",
      "loss: 1.6218148469924927 at epoch 98 at applicants training\n",
      "loss: 1.616448998451233 at epoch 99 at applicants training\n",
      "loss: 1.6893264055252075 at epoch 0 at applicants training\n",
      "loss: 1.709590196609497 at epoch 1 at applicants training\n",
      "loss: 1.7034517526626587 at epoch 2 at applicants training\n",
      "loss: 1.6979409456253052 at epoch 3 at applicants training\n",
      "loss: 1.692755103111267 at epoch 4 at applicants training\n",
      "loss: 1.696195363998413 at epoch 5 at applicants training\n",
      "loss: 1.6966854333877563 at epoch 6 at applicants training\n",
      "loss: 1.6876325607299805 at epoch 7 at applicants training\n",
      "loss: 1.6940456628799438 at epoch 8 at applicants training\n",
      "loss: 1.6940834522247314 at epoch 9 at applicants training\n",
      "loss: 1.693130612373352 at epoch 10 at applicants training\n",
      "loss: 1.69303297996521 at epoch 11 at applicants training\n",
      "loss: 1.693251371383667 at epoch 12 at applicants training\n",
      "loss: 1.6932587623596191 at epoch 13 at applicants training\n",
      "loss: 1.6929901838302612 at epoch 14 at applicants training\n",
      "loss: 1.6924105882644653 at epoch 15 at applicants training\n",
      "loss: 1.691567063331604 at epoch 16 at applicants training\n",
      "loss: 1.6901440620422363 at epoch 17 at applicants training\n",
      "loss: 1.6861108541488647 at epoch 18 at applicants training\n",
      "loss: 1.6680349111557007 at epoch 19 at applicants training\n",
      "loss: 1.6669011116027832 at epoch 20 at applicants training\n",
      "loss: 1.6741100549697876 at epoch 21 at applicants training\n",
      "loss: 1.6740261316299438 at epoch 22 at applicants training\n",
      "loss: 1.6731492280960083 at epoch 23 at applicants training\n",
      "loss: 1.672365427017212 at epoch 24 at applicants training\n",
      "loss: 1.6712168455123901 at epoch 25 at applicants training\n",
      "loss: 1.669429898262024 at epoch 26 at applicants training\n",
      "loss: 1.6682499647140503 at epoch 27 at applicants training\n",
      "loss: 1.6676188707351685 at epoch 28 at applicants training\n",
      "loss: 1.6677488088607788 at epoch 29 at applicants training\n",
      "loss: 1.6694999933242798 at epoch 30 at applicants training\n",
      "loss: 1.6647635698318481 at epoch 31 at applicants training\n",
      "loss: 1.6587800979614258 at epoch 32 at applicants training\n",
      "loss: 1.6563154458999634 at epoch 33 at applicants training\n",
      "loss: 1.6461962461471558 at epoch 34 at applicants training\n",
      "loss: 1.6542367935180664 at epoch 35 at applicants training\n",
      "loss: 1.6458499431610107 at epoch 36 at applicants training\n",
      "loss: 1.6517260074615479 at epoch 37 at applicants training\n",
      "loss: 1.6442384719848633 at epoch 38 at applicants training\n",
      "loss: 1.6442532539367676 at epoch 39 at applicants training\n",
      "loss: 1.6435264348983765 at epoch 40 at applicants training\n",
      "loss: 1.6343470811843872 at epoch 41 at applicants training\n",
      "loss: 1.6414707899093628 at epoch 42 at applicants training\n",
      "loss: 1.6359789371490479 at epoch 43 at applicants training\n",
      "loss: 1.64047372341156 at epoch 44 at applicants training\n",
      "loss: 1.6311581134796143 at epoch 45 at applicants training\n",
      "loss: 1.6431864500045776 at epoch 46 at applicants training\n",
      "loss: 1.6270309686660767 at epoch 47 at applicants training\n",
      "loss: 1.6328333616256714 at epoch 48 at applicants training\n",
      "loss: 1.633256196975708 at epoch 49 at applicants training\n",
      "loss: 1.630366563796997 at epoch 50 at applicants training\n",
      "loss: 1.628989577293396 at epoch 51 at applicants training\n",
      "loss: 1.6213592290878296 at epoch 52 at applicants training\n",
      "loss: 1.6265019178390503 at epoch 53 at applicants training\n",
      "loss: 1.6220723390579224 at epoch 54 at applicants training\n",
      "loss: 1.6217738389968872 at epoch 55 at applicants training\n",
      "loss: 1.6183987855911255 at epoch 56 at applicants training\n",
      "loss: 1.6225080490112305 at epoch 57 at applicants training\n",
      "loss: 1.617351770401001 at epoch 58 at applicants training\n",
      "loss: 1.6197319030761719 at epoch 59 at applicants training\n",
      "loss: 1.619922161102295 at epoch 60 at applicants training\n",
      "loss: 1.6215442419052124 at epoch 61 at applicants training\n",
      "loss: 1.6168009042739868 at epoch 62 at applicants training\n",
      "loss: 1.617112636566162 at epoch 63 at applicants training\n",
      "loss: 1.6173832416534424 at epoch 64 at applicants training\n",
      "loss: 1.6204904317855835 at epoch 65 at applicants training\n",
      "loss: 1.6153298616409302 at epoch 66 at applicants training\n",
      "loss: 1.6187459230422974 at epoch 67 at applicants training\n",
      "loss: 1.6250959634780884 at epoch 68 at applicants training\n",
      "loss: 1.6128491163253784 at epoch 69 at applicants training\n",
      "loss: 1.6266711950302124 at epoch 70 at applicants training\n",
      "loss: 1.61526358127594 at epoch 71 at applicants training\n",
      "loss: 1.6165544986724854 at epoch 72 at applicants training\n",
      "loss: 1.6222717761993408 at epoch 73 at applicants training\n",
      "loss: 1.6120601892471313 at epoch 74 at applicants training\n",
      "loss: 1.6164194345474243 at epoch 75 at applicants training\n",
      "loss: 1.6114717721939087 at epoch 76 at applicants training\n",
      "loss: 1.6121766567230225 at epoch 77 at applicants training\n",
      "loss: 1.6139206886291504 at epoch 78 at applicants training\n",
      "loss: 1.6111037731170654 at epoch 79 at applicants training\n",
      "loss: 1.6108020544052124 at epoch 80 at applicants training\n",
      "loss: 1.6124553680419922 at epoch 81 at applicants training\n",
      "loss: 1.6083903312683105 at epoch 82 at applicants training\n",
      "loss: 1.6115856170654297 at epoch 83 at applicants training\n",
      "loss: 1.6097773313522339 at epoch 84 at applicants training\n",
      "loss: 1.607823133468628 at epoch 85 at applicants training\n",
      "loss: 1.6090149879455566 at epoch 86 at applicants training\n",
      "loss: 1.6097551584243774 at epoch 87 at applicants training\n",
      "loss: 1.6061509847640991 at epoch 88 at applicants training\n",
      "loss: 1.6084132194519043 at epoch 89 at applicants training\n",
      "loss: 1.6090903282165527 at epoch 90 at applicants training\n",
      "loss: 1.6064915657043457 at epoch 91 at applicants training\n",
      "loss: 1.6061102151870728 at epoch 92 at applicants training\n",
      "loss: 1.6088166236877441 at epoch 93 at applicants training\n",
      "loss: 1.6059759855270386 at epoch 94 at applicants training\n",
      "loss: 1.603980541229248 at epoch 95 at applicants training\n",
      "loss: 1.6070020198822021 at epoch 96 at applicants training\n",
      "loss: 1.6066880226135254 at epoch 97 at applicants training\n",
      "loss: 1.6047585010528564 at epoch 98 at applicants training\n",
      "loss: 1.6024142503738403 at epoch 99 at applicants training\n",
      "loss: 1.7148849964141846 at epoch 0 at applicants training\n",
      "loss: 1.7149205207824707 at epoch 1 at applicants training\n",
      "loss: 1.695102334022522 at epoch 2 at applicants training\n",
      "loss: 1.6764347553253174 at epoch 3 at applicants training\n",
      "loss: 1.6786510944366455 at epoch 4 at applicants training\n",
      "loss: 1.6788259744644165 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.716417670249939 at epoch 0 at applicants training\n",
      "loss: 1.711695909500122 at epoch 1 at applicants training\n",
      "loss: 1.6836199760437012 at epoch 2 at applicants training\n",
      "loss: 1.6833899021148682 at epoch 3 at applicants training\n",
      "loss: 1.6747777462005615 at epoch 4 at applicants training\n",
      "loss: 1.6733812093734741 at epoch 5 at applicants training\n",
      "loss: 1.6791329383850098 at epoch 6 at applicants training\n",
      "loss: 1.6753358840942383 at epoch 7 at applicants training\n",
      "loss: 1.6750695705413818 at epoch 8 at applicants training\n",
      "loss: 1.6750197410583496 at epoch 9 at applicants training\n",
      "loss: 1.674851417541504 at epoch 10 at applicants training\n",
      "loss: 1.674676537513733 at epoch 11 at applicants training\n",
      "loss: 1.6745452880859375 at epoch 12 at applicants training\n",
      "loss: 1.6745506525039673 at epoch 13 at applicants training\n",
      "loss: 1.6741522550582886 at epoch 14 at applicants training\n",
      "loss: 1.6739436388015747 at epoch 15 at applicants training\n",
      "loss: 1.673858642578125 at epoch 16 at applicants training\n",
      "loss: 1.6735376119613647 at epoch 17 at applicants training\n",
      "loss: 1.6732748746871948 at epoch 18 at applicants training\n",
      "loss: 1.6731926202774048 at epoch 19 at applicants training\n",
      "loss: 1.6726830005645752 at epoch 20 at applicants training\n",
      "loss: 1.6719105243682861 at epoch 21 at applicants training\n",
      "loss: 1.6724858283996582 at epoch 22 at applicants training\n",
      "loss: 1.6717146635055542 at epoch 23 at applicants training\n",
      "loss: 1.6715857982635498 at epoch 24 at applicants training\n",
      "loss: 1.6724168062210083 at epoch 25 at applicants training\n",
      "loss: 1.6707006692886353 at epoch 26 at applicants training\n",
      "loss: 1.66843581199646 at epoch 27 at applicants training\n",
      "loss: 1.6660312414169312 at epoch 28 at applicants training\n",
      "loss: 1.664242148399353 at epoch 29 at applicants training\n",
      "loss: 1.6636494398117065 at epoch 30 at applicants training\n",
      "loss: 1.6689702272415161 at epoch 31 at applicants training\n",
      "loss: 1.6614563465118408 at epoch 32 at applicants training\n",
      "loss: 1.6606028079986572 at epoch 33 at applicants training\n",
      "loss: 1.660547137260437 at epoch 34 at applicants training\n",
      "loss: 1.6607438325881958 at epoch 35 at applicants training\n",
      "loss: 1.6634981632232666 at epoch 36 at applicants training\n",
      "loss: 1.6643744707107544 at epoch 37 at applicants training\n",
      "loss: 1.6593618392944336 at epoch 38 at applicants training\n",
      "loss: 1.6610515117645264 at epoch 39 at applicants training\n",
      "loss: 1.6604117155075073 at epoch 40 at applicants training\n",
      "loss: 1.6580495834350586 at epoch 41 at applicants training\n",
      "loss: 1.6598446369171143 at epoch 42 at applicants training\n",
      "loss: 1.6572946310043335 at epoch 43 at applicants training\n",
      "loss: 1.6587566137313843 at epoch 44 at applicants training\n",
      "loss: 1.6583445072174072 at epoch 45 at applicants training\n",
      "loss: 1.6564258337020874 at epoch 46 at applicants training\n",
      "loss: 1.6577236652374268 at epoch 47 at applicants training\n",
      "loss: 1.6564266681671143 at epoch 48 at applicants training\n",
      "loss: 1.6553645133972168 at epoch 49 at applicants training\n",
      "loss: 1.655466914176941 at epoch 50 at applicants training\n",
      "loss: 1.655712604522705 at epoch 51 at applicants training\n",
      "loss: 1.6554934978485107 at epoch 52 at applicants training\n",
      "loss: 1.654523491859436 at epoch 53 at applicants training\n",
      "loss: 1.6539274454116821 at epoch 54 at applicants training\n",
      "loss: 1.6538267135620117 at epoch 55 at applicants training\n",
      "loss: 1.6545771360397339 at epoch 56 at applicants training\n",
      "loss: 1.656726360321045 at epoch 57 at applicants training\n",
      "loss: 1.6579787731170654 at epoch 58 at applicants training\n",
      "loss: 1.6525890827178955 at epoch 59 at applicants training\n",
      "loss: 1.6574549674987793 at epoch 60 at applicants training\n",
      "loss: 1.65576171875 at epoch 61 at applicants training\n",
      "loss: 1.652626633644104 at epoch 62 at applicants training\n",
      "loss: 1.6575151681900024 at epoch 63 at applicants training\n",
      "loss: 1.651692271232605 at epoch 64 at applicants training\n",
      "loss: 1.655808925628662 at epoch 65 at applicants training\n",
      "loss: 1.6551059484481812 at epoch 66 at applicants training\n",
      "loss: 1.6515021324157715 at epoch 67 at applicants training\n",
      "loss: 1.659072995185852 at epoch 68 at applicants training\n",
      "loss: 1.6509851217269897 at epoch 69 at applicants training\n",
      "loss: 1.6608877182006836 at epoch 70 at applicants training\n",
      "loss: 1.6545485258102417 at epoch 71 at applicants training\n",
      "loss: 1.6599310636520386 at epoch 72 at applicants training\n",
      "loss: 1.6613988876342773 at epoch 73 at applicants training\n",
      "loss: 1.653673529624939 at epoch 74 at applicants training\n",
      "loss: 1.6579480171203613 at epoch 75 at applicants training\n",
      "loss: 1.6553634405136108 at epoch 76 at applicants training\n",
      "loss: 1.6541426181793213 at epoch 77 at applicants training\n",
      "loss: 1.6582800149917603 at epoch 78 at applicants training\n",
      "loss: 1.6570056676864624 at epoch 79 at applicants training\n",
      "loss: 1.651809811592102 at epoch 80 at applicants training\n",
      "loss: 1.6541318893432617 at epoch 81 at applicants training\n",
      "loss: 1.650174856185913 at epoch 82 at applicants training\n",
      "loss: 1.6497790813446045 at epoch 83 at applicants training\n",
      "loss: 1.6508668661117554 at epoch 84 at applicants training\n",
      "loss: 1.6479613780975342 at epoch 85 at applicants training\n",
      "loss: 1.6456416845321655 at epoch 86 at applicants training\n",
      "loss: 1.650651454925537 at epoch 87 at applicants training\n",
      "loss: 1.6442264318466187 at epoch 88 at applicants training\n",
      "loss: 1.646095871925354 at epoch 89 at applicants training\n",
      "loss: 1.6499807834625244 at epoch 90 at applicants training\n",
      "loss: 1.6429575681686401 at epoch 91 at applicants training\n",
      "loss: 1.6444294452667236 at epoch 92 at applicants training\n",
      "loss: 1.6459721326828003 at epoch 93 at applicants training\n",
      "loss: 1.6412070989608765 at epoch 94 at applicants training\n",
      "loss: 1.6430847644805908 at epoch 95 at applicants training\n",
      "loss: 1.6433024406433105 at epoch 96 at applicants training\n",
      "loss: 1.6396445035934448 at epoch 97 at applicants training\n",
      "loss: 1.6395456790924072 at epoch 98 at applicants training\n",
      "loss: 1.6421332359313965 at epoch 99 at applicants training\n",
      "loss: 1.710808277130127 at epoch 0 at applicants training\n",
      "loss: 1.7148467302322388 at epoch 1 at applicants training\n",
      "loss: 1.7095600366592407 at epoch 2 at applicants training\n",
      "loss: 1.7152084112167358 at epoch 3 at applicants training\n",
      "loss: 1.7151472568511963 at epoch 4 at applicants training\n",
      "loss: 1.7155325412750244 at epoch 5 at applicants training\n",
      "loss: 1.7154737710952759 at epoch 6 at applicants training\n",
      "loss: 1.7148094177246094 at epoch 7 at applicants training\n",
      "loss: 1.7147939205169678 at epoch 8 at applicants training\n",
      "loss: 1.7133980989456177 at epoch 9 at applicants training\n",
      "loss: 1.7093729972839355 at epoch 10 at applicants training\n",
      "loss: 1.6965614557266235 at epoch 11 at applicants training\n",
      "loss: 1.676738977432251 at epoch 12 at applicants training\n",
      "loss: 1.6687475442886353 at epoch 13 at applicants training\n",
      "loss: 1.6747217178344727 at epoch 14 at applicants training\n",
      "loss: 1.6746647357940674 at epoch 15 at applicants training\n",
      "loss: 1.6717047691345215 at epoch 16 at applicants training\n",
      "loss: 1.6635671854019165 at epoch 17 at applicants training\n",
      "loss: 1.6643282175064087 at epoch 18 at applicants training\n",
      "loss: 1.6676630973815918 at epoch 19 at applicants training\n",
      "loss: 1.6588674783706665 at epoch 20 at applicants training\n",
      "loss: 1.6656701564788818 at epoch 21 at applicants training\n",
      "loss: 1.6675680875778198 at epoch 22 at applicants training\n",
      "loss: 1.6645725965499878 at epoch 23 at applicants training\n",
      "loss: 1.6592761278152466 at epoch 24 at applicants training\n",
      "loss: 1.6597943305969238 at epoch 25 at applicants training\n",
      "loss: 1.6638416051864624 at epoch 26 at applicants training\n",
      "loss: 1.658689022064209 at epoch 27 at applicants training\n",
      "loss: 1.6592833995819092 at epoch 28 at applicants training\n",
      "loss: 1.6587883234024048 at epoch 29 at applicants training\n",
      "loss: 1.6586567163467407 at epoch 30 at applicants training\n",
      "loss: 1.659074068069458 at epoch 31 at applicants training\n",
      "loss: 1.6587841510772705 at epoch 32 at applicants training\n",
      "loss: 1.6580660343170166 at epoch 33 at applicants training\n",
      "loss: 1.6573405265808105 at epoch 34 at applicants training\n",
      "loss: 1.6557323932647705 at epoch 35 at applicants training\n",
      "loss: 1.6584502458572388 at epoch 36 at applicants training\n",
      "loss: 1.6548271179199219 at epoch 37 at applicants training\n",
      "loss: 1.6554303169250488 at epoch 38 at applicants training\n",
      "loss: 1.6557581424713135 at epoch 39 at applicants training\n",
      "loss: 1.6546543836593628 at epoch 40 at applicants training\n",
      "loss: 1.6530917882919312 at epoch 41 at applicants training\n",
      "loss: 1.6541638374328613 at epoch 42 at applicants training\n",
      "loss: 1.6517106294631958 at epoch 43 at applicants training\n",
      "loss: 1.653029203414917 at epoch 44 at applicants training\n",
      "loss: 1.650451898574829 at epoch 45 at applicants training\n",
      "loss: 1.6523300409317017 at epoch 46 at applicants training\n",
      "loss: 1.649561882019043 at epoch 47 at applicants training\n",
      "loss: 1.6512326002120972 at epoch 48 at applicants training\n",
      "loss: 1.651148796081543 at epoch 49 at applicants training\n",
      "loss: 1.6485496759414673 at epoch 50 at applicants training\n",
      "loss: 1.6504111289978027 at epoch 51 at applicants training\n",
      "loss: 1.6499606370925903 at epoch 52 at applicants training\n",
      "loss: 1.6479642391204834 at epoch 53 at applicants training\n",
      "loss: 1.6475579738616943 at epoch 54 at applicants training\n",
      "loss: 1.648767352104187 at epoch 55 at applicants training\n",
      "loss: 1.6498855352401733 at epoch 56 at applicants training\n",
      "loss: 1.6464717388153076 at epoch 57 at applicants training\n",
      "loss: 1.6488293409347534 at epoch 58 at applicants training\n",
      "loss: 1.6508923768997192 at epoch 59 at applicants training\n",
      "loss: 1.6466478109359741 at epoch 60 at applicants training\n",
      "loss: 1.6547584533691406 at epoch 61 at applicants training\n",
      "loss: 1.6462221145629883 at epoch 62 at applicants training\n",
      "loss: 1.6498503684997559 at epoch 63 at applicants training\n",
      "loss: 1.6457104682922363 at epoch 64 at applicants training\n",
      "loss: 1.647216796875 at epoch 65 at applicants training\n",
      "loss: 1.647562026977539 at epoch 66 at applicants training\n",
      "loss: 1.646054983139038 at epoch 67 at applicants training\n",
      "loss: 1.648391604423523 at epoch 68 at applicants training\n",
      "loss: 1.6445101499557495 at epoch 69 at applicants training\n",
      "loss: 1.6465197801589966 at epoch 70 at applicants training\n",
      "loss: 1.6449004411697388 at epoch 71 at applicants training\n",
      "loss: 1.6452168226242065 at epoch 72 at applicants training\n",
      "loss: 1.645084023475647 at epoch 73 at applicants training\n",
      "loss: 1.6439255475997925 at epoch 74 at applicants training\n",
      "loss: 1.6448407173156738 at epoch 75 at applicants training\n",
      "loss: 1.643716812133789 at epoch 76 at applicants training\n",
      "loss: 1.6437597274780273 at epoch 77 at applicants training\n",
      "loss: 1.6438883543014526 at epoch 78 at applicants training\n",
      "loss: 1.643040657043457 at epoch 79 at applicants training\n",
      "loss: 1.643252968788147 at epoch 80 at applicants training\n",
      "loss: 1.6430801153182983 at epoch 81 at applicants training\n",
      "loss: 1.6424615383148193 at epoch 82 at applicants training\n",
      "loss: 1.642672061920166 at epoch 83 at applicants training\n",
      "loss: 1.642566204071045 at epoch 84 at applicants training\n",
      "loss: 1.64204740524292 at epoch 85 at applicants training\n",
      "loss: 1.641778826713562 at epoch 86 at applicants training\n",
      "loss: 1.6418304443359375 at epoch 87 at applicants training\n",
      "loss: 1.6417949199676514 at epoch 88 at applicants training\n",
      "loss: 1.641544222831726 at epoch 89 at applicants training\n",
      "loss: 1.6412451267242432 at epoch 90 at applicants training\n",
      "loss: 1.6410608291625977 at epoch 91 at applicants training\n",
      "loss: 1.6408878564834595 at epoch 92 at applicants training\n",
      "loss: 1.640672206878662 at epoch 93 at applicants training\n",
      "loss: 1.6406795978546143 at epoch 94 at applicants training\n",
      "loss: 1.6409893035888672 at epoch 95 at applicants training\n",
      "loss: 1.6428751945495605 at epoch 96 at applicants training\n",
      "loss: 1.6455364227294922 at epoch 97 at applicants training\n",
      "loss: 1.6446272134780884 at epoch 98 at applicants training\n",
      "loss: 1.6404448747634888 at epoch 99 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 0 at applicants training\n",
      "loss: 1.6774083375930786 at epoch 1 at applicants training\n",
      "loss: 1.6452598571777344 at epoch 2 at applicants training\n",
      "loss: 1.6493191719055176 at epoch 3 at applicants training\n",
      "loss: 1.6759536266326904 at epoch 4 at applicants training\n",
      "loss: 1.6787663698196411 at epoch 5 at applicants training\n",
      "loss: 1.678831696510315 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.671907663345337 at epoch 0 at applicants training\n",
      "loss: 1.6948727369308472 at epoch 1 at applicants training\n",
      "loss: 1.7132952213287354 at epoch 2 at applicants training\n",
      "loss: 1.7128328084945679 at epoch 3 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 4 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 5 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 6 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 7 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 8 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 9 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 10 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 11 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 12 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 13 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 14 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 15 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 16 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 17 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 18 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 19 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 20 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 21 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 22 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 23 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 24 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 25 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 26 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 27 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 28 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 29 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 30 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 31 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 32 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 33 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 34 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 35 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 36 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 37 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 38 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 39 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 40 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 41 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 42 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 43 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 44 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 45 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 46 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 47 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 48 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 49 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 50 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 51 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 52 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 53 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 54 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 55 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 56 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 57 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 58 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 59 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 60 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 61 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 62 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 63 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 64 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 65 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 66 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 67 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 68 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 69 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 70 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 71 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 72 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 73 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 74 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 75 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 76 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 77 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 78 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 79 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 80 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 81 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 82 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 83 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 84 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 85 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 86 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 87 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 88 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 89 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 90 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 91 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 92 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 93 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 94 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 95 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 96 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 97 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 98 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 99 at applicants training\n",
      "loss: 1.6877729892730713 at epoch 0 at applicants training\n",
      "loss: 1.6908776760101318 at epoch 1 at applicants training\n",
      "loss: 1.67683744430542 at epoch 2 at applicants training\n",
      "loss: 1.669466257095337 at epoch 3 at applicants training\n",
      "loss: 1.678896188735962 at epoch 4 at applicants training\n",
      "loss: 1.6788586378097534 at epoch 5 at applicants training\n",
      "loss: 1.6788337230682373 at epoch 6 at applicants training\n",
      "loss: 1.678832769393921 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.6989858150482178 at epoch 0 at applicants training\n",
      "loss: 1.7128325700759888 at epoch 1 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 2 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 3 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 4 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 5 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 6 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 7 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 8 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 9 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 10 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 11 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 12 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 13 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 14 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 15 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 16 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 17 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 18 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 19 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 20 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 21 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 22 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 23 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 24 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 25 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 26 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 27 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 28 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 29 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 30 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 31 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 32 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 33 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 34 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 35 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 36 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 37 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 38 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 39 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 40 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 41 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 42 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 43 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 44 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 45 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 46 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 47 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 48 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 49 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 50 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 51 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 52 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 53 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 54 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 55 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 56 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 57 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 58 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 59 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 60 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 61 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 62 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 63 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 64 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 65 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 66 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 67 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 68 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 69 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 70 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 71 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 72 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 73 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 74 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 75 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 76 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 77 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 78 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 79 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 80 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 81 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 82 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 83 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 84 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 85 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 86 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 87 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 88 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 89 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 90 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 91 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 92 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 93 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 94 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 95 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 96 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 97 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 98 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 99 at applicants training\n",
      "loss: 1.7208997011184692 at epoch 0 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 1 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 2 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 3 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 4 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 5 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 6 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 7 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 8 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 9 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 10 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 11 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 12 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 13 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 14 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 15 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 16 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 17 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 18 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 19 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 20 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 21 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 22 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 23 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 24 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 25 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 26 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 27 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 28 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 29 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 30 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 31 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 32 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 33 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 34 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 35 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 36 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 37 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 38 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 39 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 40 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 41 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 42 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 43 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 44 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 45 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 46 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 47 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 48 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 49 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 50 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 51 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 52 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 53 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 54 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 55 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 56 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 57 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 58 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 59 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 60 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 61 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 62 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 63 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 64 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 65 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 66 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 67 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 68 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 69 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 70 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 71 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 72 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 73 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 74 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 75 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 76 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 77 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 78 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 79 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 80 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 81 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 82 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 83 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 84 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 85 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 86 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 87 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 88 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 89 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 90 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 91 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 92 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 93 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 94 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 95 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 96 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 97 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 98 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 99 at applicants training\n",
      "loss: 1.7155671119689941 at epoch 0 at applicants training\n",
      "loss: 1.680469274520874 at epoch 1 at applicants training\n",
      "loss: 1.6937153339385986 at epoch 2 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 3 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 4 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 5 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 6 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 7 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 8 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 9 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 10 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 11 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 12 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 13 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 14 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 15 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 16 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 17 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 18 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 19 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 20 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 21 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 22 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 23 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 24 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 25 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 26 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 27 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 28 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 29 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 30 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 31 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 32 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 33 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 34 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 35 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 36 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 37 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 38 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 39 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 40 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 41 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 42 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 43 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 44 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 45 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 46 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 47 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 48 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 49 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 50 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 51 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 52 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 53 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 54 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 55 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 56 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 57 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 58 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 59 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 60 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 61 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 62 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 63 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 64 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 65 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 66 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 67 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 68 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 69 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 70 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 71 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 72 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 73 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 74 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 75 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 76 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 77 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 78 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 79 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 80 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 81 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 82 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 83 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 84 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 85 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 86 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 87 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 88 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 89 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 90 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 91 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 92 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 93 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 94 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 95 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 96 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 97 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 98 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 99 at applicants training\n",
      "loss: 1.7130638360977173 at epoch 0 at applicants training\n",
      "loss: 1.7127814292907715 at epoch 1 at applicants training\n",
      "loss: 1.7115509510040283 at epoch 2 at applicants training\n",
      "loss: 1.7044425010681152 at epoch 3 at applicants training\n",
      "loss: 1.7151955366134644 at epoch 4 at applicants training\n",
      "loss: 1.716436743736267 at epoch 5 at applicants training\n",
      "loss: 1.7088463306427002 at epoch 6 at applicants training\n",
      "loss: 1.6996270418167114 at epoch 7 at applicants training\n",
      "loss: 1.7085859775543213 at epoch 8 at applicants training\n",
      "loss: 1.708908200263977 at epoch 9 at applicants training\n",
      "loss: 1.7039427757263184 at epoch 10 at applicants training\n",
      "loss: 1.703063726425171 at epoch 11 at applicants training\n",
      "loss: 1.708221435546875 at epoch 12 at applicants training\n",
      "loss: 1.7063599824905396 at epoch 13 at applicants training\n",
      "loss: 1.7127145528793335 at epoch 14 at applicants training\n",
      "loss: 1.710746169090271 at epoch 15 at applicants training\n",
      "loss: 1.7043087482452393 at epoch 16 at applicants training\n",
      "loss: 1.7023385763168335 at epoch 17 at applicants training\n",
      "loss: 1.698564887046814 at epoch 18 at applicants training\n",
      "loss: 1.7012122869491577 at epoch 19 at applicants training\n",
      "loss: 1.6992154121398926 at epoch 20 at applicants training\n",
      "loss: 1.6932835578918457 at epoch 21 at applicants training\n",
      "loss: 1.6951063871383667 at epoch 22 at applicants training\n",
      "loss: 1.6900631189346313 at epoch 23 at applicants training\n",
      "loss: 1.68989098072052 at epoch 24 at applicants training\n",
      "loss: 1.687994360923767 at epoch 25 at applicants training\n",
      "loss: 1.6836042404174805 at epoch 26 at applicants training\n",
      "loss: 1.6827528476715088 at epoch 27 at applicants training\n",
      "loss: 1.6796989440917969 at epoch 28 at applicants training\n",
      "loss: 1.6781365871429443 at epoch 29 at applicants training\n",
      "loss: 1.6747843027114868 at epoch 30 at applicants training\n",
      "loss: 1.672100305557251 at epoch 31 at applicants training\n",
      "loss: 1.6708333492279053 at epoch 32 at applicants training\n",
      "loss: 1.6715757846832275 at epoch 33 at applicants training\n",
      "loss: 1.6785571575164795 at epoch 34 at applicants training\n",
      "loss: 1.6720389127731323 at epoch 35 at applicants training\n",
      "loss: 1.6891294717788696 at epoch 36 at applicants training\n",
      "loss: 1.6769791841506958 at epoch 37 at applicants training\n",
      "loss: 1.6830151081085205 at epoch 38 at applicants training\n",
      "loss: 1.6922013759613037 at epoch 39 at applicants training\n",
      "loss: 1.6889790296554565 at epoch 40 at applicants training\n",
      "loss: 1.6778149604797363 at epoch 41 at applicants training\n",
      "loss: 1.6790002584457397 at epoch 42 at applicants training\n",
      "loss: 1.6818656921386719 at epoch 43 at applicants training\n",
      "loss: 1.6734275817871094 at epoch 44 at applicants training\n",
      "loss: 1.6791514158248901 at epoch 45 at applicants training\n",
      "loss: 1.68002450466156 at epoch 46 at applicants training\n",
      "loss: 1.6754169464111328 at epoch 47 at applicants training\n",
      "loss: 1.6730073690414429 at epoch 48 at applicants training\n",
      "loss: 1.6799440383911133 at epoch 49 at applicants training\n",
      "loss: 1.6749876737594604 at epoch 50 at applicants training\n",
      "loss: 1.6727619171142578 at epoch 51 at applicants training\n",
      "loss: 1.677348017692566 at epoch 52 at applicants training\n",
      "loss: 1.6771355867385864 at epoch 53 at applicants training\n",
      "loss: 1.6715487241744995 at epoch 54 at applicants training\n",
      "loss: 1.6730148792266846 at epoch 55 at applicants training\n",
      "loss: 1.6738039255142212 at epoch 56 at applicants training\n",
      "loss: 1.6690572500228882 at epoch 57 at applicants training\n",
      "loss: 1.6718835830688477 at epoch 58 at applicants training\n",
      "loss: 1.6696070432662964 at epoch 59 at applicants training\n",
      "loss: 1.6690762042999268 at epoch 60 at applicants training\n",
      "loss: 1.668412208557129 at epoch 61 at applicants training\n",
      "loss: 1.6681102514266968 at epoch 62 at applicants training\n",
      "loss: 1.6672192811965942 at epoch 63 at applicants training\n",
      "loss: 1.667757511138916 at epoch 64 at applicants training\n",
      "loss: 1.6652148962020874 at epoch 65 at applicants training\n",
      "loss: 1.6675612926483154 at epoch 66 at applicants training\n",
      "loss: 1.6643882989883423 at epoch 67 at applicants training\n",
      "loss: 1.666947603225708 at epoch 68 at applicants training\n",
      "loss: 1.6641709804534912 at epoch 69 at applicants training\n",
      "loss: 1.6662193536758423 at epoch 70 at applicants training\n",
      "loss: 1.663852334022522 at epoch 71 at applicants training\n",
      "loss: 1.665170431137085 at epoch 72 at applicants training\n",
      "loss: 1.6642712354660034 at epoch 73 at applicants training\n",
      "loss: 1.6635545492172241 at epoch 74 at applicants training\n",
      "loss: 1.6647061109542847 at epoch 75 at applicants training\n",
      "loss: 1.6631569862365723 at epoch 76 at applicants training\n",
      "loss: 1.6637016534805298 at epoch 77 at applicants training\n",
      "loss: 1.6631786823272705 at epoch 78 at applicants training\n",
      "loss: 1.6624095439910889 at epoch 79 at applicants training\n",
      "loss: 1.66312575340271 at epoch 80 at applicants training\n",
      "loss: 1.6621755361557007 at epoch 81 at applicants training\n",
      "loss: 1.6621745824813843 at epoch 82 at applicants training\n",
      "loss: 1.6626471281051636 at epoch 83 at applicants training\n",
      "loss: 1.661813735961914 at epoch 84 at applicants training\n",
      "loss: 1.6615195274353027 at epoch 85 at applicants training\n",
      "loss: 1.6619081497192383 at epoch 86 at applicants training\n",
      "loss: 1.6612937450408936 at epoch 87 at applicants training\n",
      "loss: 1.66110098361969 at epoch 88 at applicants training\n",
      "loss: 1.6612263917922974 at epoch 89 at applicants training\n",
      "loss: 1.6607470512390137 at epoch 90 at applicants training\n",
      "loss: 1.6604341268539429 at epoch 91 at applicants training\n",
      "loss: 1.6605380773544312 at epoch 92 at applicants training\n",
      "loss: 1.6602671146392822 at epoch 93 at applicants training\n",
      "loss: 1.6598753929138184 at epoch 94 at applicants training\n",
      "loss: 1.659919023513794 at epoch 95 at applicants training\n",
      "loss: 1.6599777936935425 at epoch 96 at applicants training\n",
      "loss: 1.659630537033081 at epoch 97 at applicants training\n",
      "loss: 1.6592016220092773 at epoch 98 at applicants training\n",
      "loss: 1.6589640378952026 at epoch 99 at applicants training\n",
      "loss: 1.6922258138656616 at epoch 0 at applicants training\n",
      "loss: 1.6900826692581177 at epoch 1 at applicants training\n",
      "loss: 1.687530517578125 at epoch 2 at applicants training\n",
      "loss: 1.691774606704712 at epoch 3 at applicants training\n",
      "loss: 1.693286418914795 at epoch 4 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 5 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 6 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 7 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 8 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 9 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 10 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 11 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 12 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 13 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 14 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 15 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 16 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 17 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 18 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 19 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 20 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 21 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 22 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 23 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 24 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 25 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 26 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 27 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 28 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 29 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 30 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 31 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 32 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 33 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 34 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 35 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 36 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 37 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 38 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 39 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 40 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 41 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 42 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 43 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 44 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 45 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 46 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 47 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 48 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 49 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 50 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 51 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 52 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 53 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 54 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 55 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 56 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 57 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 58 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 59 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 60 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 61 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 62 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 63 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 64 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 65 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 66 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 67 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 68 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 69 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 70 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 71 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 72 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 73 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 74 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 75 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 76 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 77 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 78 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 79 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 80 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 81 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 82 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 83 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 84 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 85 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 86 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 87 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 88 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 89 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 90 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 91 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 92 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 93 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 94 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 95 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 96 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 97 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 98 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 99 at applicants training\n",
      "loss: 1.7088422775268555 at epoch 0 at applicants training\n",
      "loss: 1.7124829292297363 at epoch 1 at applicants training\n",
      "loss: 1.710705280303955 at epoch 2 at applicants training\n",
      "loss: 1.7140393257141113 at epoch 3 at applicants training\n",
      "loss: 1.7093585729599 at epoch 4 at applicants training\n",
      "loss: 1.7039488554000854 at epoch 5 at applicants training\n",
      "loss: 1.6945667266845703 at epoch 6 at applicants training\n",
      "loss: 1.6820849180221558 at epoch 7 at applicants training\n",
      "loss: 1.67958664894104 at epoch 8 at applicants training\n",
      "loss: 1.6786655187606812 at epoch 9 at applicants training\n",
      "loss: 1.678485631942749 at epoch 10 at applicants training\n",
      "loss: 1.6755578517913818 at epoch 11 at applicants training\n",
      "loss: 1.6707950830459595 at epoch 12 at applicants training\n",
      "loss: 1.6776354312896729 at epoch 13 at applicants training\n",
      "loss: 1.6816599369049072 at epoch 14 at applicants training\n",
      "loss: 1.6672751903533936 at epoch 15 at applicants training\n",
      "loss: 1.6917853355407715 at epoch 16 at applicants training\n",
      "loss: 1.668605923652649 at epoch 17 at applicants training\n",
      "loss: 1.6892180442810059 at epoch 18 at applicants training\n",
      "loss: 1.6946207284927368 at epoch 19 at applicants training\n",
      "loss: 1.6756904125213623 at epoch 20 at applicants training\n",
      "loss: 1.68048894405365 at epoch 21 at applicants training\n",
      "loss: 1.689883828163147 at epoch 22 at applicants training\n",
      "loss: 1.6709308624267578 at epoch 23 at applicants training\n",
      "loss: 1.6767514944076538 at epoch 24 at applicants training\n",
      "loss: 1.6826175451278687 at epoch 25 at applicants training\n",
      "loss: 1.6661227941513062 at epoch 26 at applicants training\n",
      "loss: 1.6749653816223145 at epoch 27 at applicants training\n",
      "loss: 1.6740689277648926 at epoch 28 at applicants training\n",
      "loss: 1.6633814573287964 at epoch 29 at applicants training\n",
      "loss: 1.6742967367172241 at epoch 30 at applicants training\n",
      "loss: 1.6669496297836304 at epoch 31 at applicants training\n",
      "loss: 1.6646264791488647 at epoch 32 at applicants training\n",
      "loss: 1.6706783771514893 at epoch 33 at applicants training\n",
      "loss: 1.6613314151763916 at epoch 34 at applicants training\n",
      "loss: 1.6665122509002686 at epoch 35 at applicants training\n",
      "loss: 1.665159821510315 at epoch 36 at applicants training\n",
      "loss: 1.659796118736267 at epoch 37 at applicants training\n",
      "loss: 1.6646000146865845 at epoch 38 at applicants training\n",
      "loss: 1.6580537557601929 at epoch 39 at applicants training\n",
      "loss: 1.661131501197815 at epoch 40 at applicants training\n",
      "loss: 1.6573398113250732 at epoch 41 at applicants training\n",
      "loss: 1.6584054231643677 at epoch 42 at applicants training\n",
      "loss: 1.6530110836029053 at epoch 43 at applicants training\n",
      "loss: 1.657812476158142 at epoch 44 at applicants training\n",
      "loss: 1.6505024433135986 at epoch 45 at applicants training\n",
      "loss: 1.6571258306503296 at epoch 46 at applicants training\n",
      "loss: 1.6516417264938354 at epoch 47 at applicants training\n",
      "loss: 1.649351954460144 at epoch 48 at applicants training\n",
      "loss: 1.653419017791748 at epoch 49 at applicants training\n",
      "loss: 1.646987795829773 at epoch 50 at applicants training\n",
      "loss: 1.6460899114608765 at epoch 51 at applicants training\n",
      "loss: 1.648550271987915 at epoch 52 at applicants training\n",
      "loss: 1.644022822380066 at epoch 53 at applicants training\n",
      "loss: 1.6417748928070068 at epoch 54 at applicants training\n",
      "loss: 1.644909143447876 at epoch 55 at applicants training\n",
      "loss: 1.6456305980682373 at epoch 56 at applicants training\n",
      "loss: 1.6398282051086426 at epoch 57 at applicants training\n",
      "loss: 1.6367191076278687 at epoch 58 at applicants training\n",
      "loss: 1.635784387588501 at epoch 59 at applicants training\n",
      "loss: 1.6402438879013062 at epoch 60 at applicants training\n",
      "loss: 1.6573683023452759 at epoch 61 at applicants training\n",
      "loss: 1.6405478715896606 at epoch 62 at applicants training\n",
      "loss: 1.6337469816207886 at epoch 63 at applicants training\n",
      "loss: 1.6322041749954224 at epoch 64 at applicants training\n",
      "loss: 1.6364037990570068 at epoch 65 at applicants training\n",
      "loss: 1.6535394191741943 at epoch 66 at applicants training\n",
      "loss: 1.6394727230072021 at epoch 67 at applicants training\n",
      "loss: 1.632196068763733 at epoch 68 at applicants training\n",
      "loss: 1.6299622058868408 at epoch 69 at applicants training\n",
      "loss: 1.6286046504974365 at epoch 70 at applicants training\n",
      "loss: 1.628007411956787 at epoch 71 at applicants training\n",
      "loss: 1.6285101175308228 at epoch 72 at applicants training\n",
      "loss: 1.6315261125564575 at epoch 73 at applicants training\n",
      "loss: 1.6386183500289917 at epoch 74 at applicants training\n",
      "loss: 1.643636703491211 at epoch 75 at applicants training\n",
      "loss: 1.6267123222351074 at epoch 76 at applicants training\n",
      "loss: 1.6277912855148315 at epoch 77 at applicants training\n",
      "loss: 1.6407747268676758 at epoch 78 at applicants training\n",
      "loss: 1.630394458770752 at epoch 79 at applicants training\n",
      "loss: 1.6239596605300903 at epoch 80 at applicants training\n",
      "loss: 1.6251671314239502 at epoch 81 at applicants training\n",
      "loss: 1.6314260959625244 at epoch 82 at applicants training\n",
      "loss: 1.6356459856033325 at epoch 83 at applicants training\n",
      "loss: 1.6246092319488525 at epoch 84 at applicants training\n",
      "loss: 1.6247851848602295 at epoch 85 at applicants training\n",
      "loss: 1.635273814201355 at epoch 86 at applicants training\n",
      "loss: 1.6263508796691895 at epoch 87 at applicants training\n",
      "loss: 1.6215180158615112 at epoch 88 at applicants training\n",
      "loss: 1.626936912536621 at epoch 89 at applicants training\n",
      "loss: 1.6276041269302368 at epoch 90 at applicants training\n",
      "loss: 1.6229747533798218 at epoch 91 at applicants training\n",
      "loss: 1.6208322048187256 at epoch 92 at applicants training\n",
      "loss: 1.62235689163208 at epoch 93 at applicants training\n",
      "loss: 1.6253710985183716 at epoch 94 at applicants training\n",
      "loss: 1.6245352029800415 at epoch 95 at applicants training\n",
      "loss: 1.6212432384490967 at epoch 96 at applicants training\n",
      "loss: 1.619162678718567 at epoch 97 at applicants training\n",
      "loss: 1.6212836503982544 at epoch 98 at applicants training\n",
      "loss: 1.6257144212722778 at epoch 99 at applicants training\n",
      "loss: 1.7112168073654175 at epoch 0 at applicants training\n",
      "loss: 1.6916872262954712 at epoch 1 at applicants training\n",
      "loss: 1.6809334754943848 at epoch 2 at applicants training\n",
      "loss: 1.6919456720352173 at epoch 3 at applicants training\n",
      "loss: 1.6935765743255615 at epoch 4 at applicants training\n",
      "loss: 1.6938090324401855 at epoch 5 at applicants training\n",
      "loss: 1.6938308477401733 at epoch 6 at applicants training\n",
      "loss: 1.693832516670227 at epoch 7 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 8 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 9 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 10 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 11 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 12 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 13 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 14 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 15 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 16 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 17 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 18 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 19 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 20 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 21 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 22 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 23 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 24 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 25 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 26 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 27 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 28 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 29 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 30 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 31 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 32 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 33 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 34 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 35 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 36 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 37 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 38 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 39 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 40 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 41 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 42 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 43 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 44 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 45 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 46 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 47 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 48 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 49 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 50 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 51 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 52 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 53 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 54 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 55 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 56 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 57 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 58 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 59 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 60 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 61 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 62 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 63 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 64 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 65 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 66 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 67 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 68 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 69 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 70 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 71 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 72 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 73 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 74 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 75 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 76 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 77 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 78 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 79 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 80 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 81 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 82 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 83 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 84 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 85 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 86 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 87 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 88 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 89 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 90 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 91 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 92 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 93 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 94 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 95 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 96 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 97 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 98 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 99 at applicants training\n",
      "loss: 1.7160439491271973 at epoch 0 at applicants training\n",
      "loss: 1.7125402688980103 at epoch 1 at applicants training\n",
      "loss: 1.7128242254257202 at epoch 2 at applicants training\n",
      "loss: 1.7127012014389038 at epoch 3 at applicants training\n",
      "loss: 1.7082098722457886 at epoch 4 at applicants training\n",
      "loss: 1.67436945438385 at epoch 5 at applicants training\n",
      "loss: 1.6723185777664185 at epoch 6 at applicants training\n",
      "loss: 1.6632200479507446 at epoch 7 at applicants training\n",
      "loss: 1.67354416847229 at epoch 8 at applicants training\n",
      "loss: 1.6681272983551025 at epoch 9 at applicants training\n",
      "loss: 1.6728589534759521 at epoch 10 at applicants training\n",
      "loss: 1.6704533100128174 at epoch 11 at applicants training\n",
      "loss: 1.660635232925415 at epoch 12 at applicants training\n",
      "loss: 1.6714116334915161 at epoch 13 at applicants training\n",
      "loss: 1.6673057079315186 at epoch 14 at applicants training\n",
      "loss: 1.6577534675598145 at epoch 15 at applicants training\n",
      "loss: 1.6652835607528687 at epoch 16 at applicants training\n",
      "loss: 1.6549406051635742 at epoch 17 at applicants training\n",
      "loss: 1.6599160432815552 at epoch 18 at applicants training\n",
      "loss: 1.6570185422897339 at epoch 19 at applicants training\n",
      "loss: 1.6528372764587402 at epoch 20 at applicants training\n",
      "loss: 1.6566646099090576 at epoch 21 at applicants training\n",
      "loss: 1.6515181064605713 at epoch 22 at applicants training\n",
      "loss: 1.6607706546783447 at epoch 23 at applicants training\n",
      "loss: 1.650346040725708 at epoch 24 at applicants training\n",
      "loss: 1.6531602144241333 at epoch 25 at applicants training\n",
      "loss: 1.6584382057189941 at epoch 26 at applicants training\n",
      "loss: 1.648803949356079 at epoch 27 at applicants training\n",
      "loss: 1.657421350479126 at epoch 28 at applicants training\n",
      "loss: 1.6507591009140015 at epoch 29 at applicants training\n",
      "loss: 1.6492151021957397 at epoch 30 at applicants training\n",
      "loss: 1.6552956104278564 at epoch 31 at applicants training\n",
      "loss: 1.6471959352493286 at epoch 32 at applicants training\n",
      "loss: 1.6544241905212402 at epoch 33 at applicants training\n",
      "loss: 1.6482030153274536 at epoch 34 at applicants training\n",
      "loss: 1.6477406024932861 at epoch 35 at applicants training\n",
      "loss: 1.6478900909423828 at epoch 36 at applicants training\n",
      "loss: 1.6454893350601196 at epoch 37 at applicants training\n",
      "loss: 1.649895191192627 at epoch 38 at applicants training\n",
      "loss: 1.6454620361328125 at epoch 39 at applicants training\n",
      "loss: 1.6454155445098877 at epoch 40 at applicants training\n",
      "loss: 1.648260235786438 at epoch 41 at applicants training\n",
      "loss: 1.6433395147323608 at epoch 42 at applicants training\n",
      "loss: 1.6467515230178833 at epoch 43 at applicants training\n",
      "loss: 1.6446642875671387 at epoch 44 at applicants training\n",
      "loss: 1.6429388523101807 at epoch 45 at applicants training\n",
      "loss: 1.6439409255981445 at epoch 46 at applicants training\n",
      "loss: 1.6426115036010742 at epoch 47 at applicants training\n",
      "loss: 1.643625020980835 at epoch 48 at applicants training\n",
      "loss: 1.6404722929000854 at epoch 49 at applicants training\n",
      "loss: 1.643118977546692 at epoch 50 at applicants training\n",
      "loss: 1.6399846076965332 at epoch 51 at applicants training\n",
      "loss: 1.6416577100753784 at epoch 52 at applicants training\n",
      "loss: 1.6390539407730103 at epoch 53 at applicants training\n",
      "loss: 1.6405974626541138 at epoch 54 at applicants training\n",
      "loss: 1.6391172409057617 at epoch 55 at applicants training\n",
      "loss: 1.6391676664352417 at epoch 56 at applicants training\n",
      "loss: 1.6391527652740479 at epoch 57 at applicants training\n",
      "loss: 1.6375796794891357 at epoch 58 at applicants training\n",
      "loss: 1.6389877796173096 at epoch 59 at applicants training\n",
      "loss: 1.6370259523391724 at epoch 60 at applicants training\n",
      "loss: 1.6371183395385742 at epoch 61 at applicants training\n",
      "loss: 1.6379584074020386 at epoch 62 at applicants training\n",
      "loss: 1.636508584022522 at epoch 63 at applicants training\n",
      "loss: 1.6357494592666626 at epoch 64 at applicants training\n",
      "loss: 1.6367262601852417 at epoch 65 at applicants training\n",
      "loss: 1.637058138847351 at epoch 66 at applicants training\n",
      "loss: 1.6353319883346558 at epoch 67 at applicants training\n",
      "loss: 1.6347471475601196 at epoch 68 at applicants training\n",
      "loss: 1.635455846786499 at epoch 69 at applicants training\n",
      "loss: 1.6351096630096436 at epoch 70 at applicants training\n",
      "loss: 1.6341936588287354 at epoch 71 at applicants training\n",
      "loss: 1.6341571807861328 at epoch 72 at applicants training\n",
      "loss: 1.6345183849334717 at epoch 73 at applicants training\n",
      "loss: 1.6341851949691772 at epoch 74 at applicants training\n",
      "loss: 1.6333683729171753 at epoch 75 at applicants training\n",
      "loss: 1.6331067085266113 at epoch 76 at applicants training\n",
      "loss: 1.6334443092346191 at epoch 77 at applicants training\n",
      "loss: 1.633617639541626 at epoch 78 at applicants training\n",
      "loss: 1.6331722736358643 at epoch 79 at applicants training\n",
      "loss: 1.6323268413543701 at epoch 80 at applicants training\n",
      "loss: 1.6318633556365967 at epoch 81 at applicants training\n",
      "loss: 1.6318930387496948 at epoch 82 at applicants training\n",
      "loss: 1.632319450378418 at epoch 83 at applicants training\n",
      "loss: 1.6332110166549683 at epoch 84 at applicants training\n",
      "loss: 1.6340385675430298 at epoch 85 at applicants training\n",
      "loss: 1.633458137512207 at epoch 86 at applicants training\n",
      "loss: 1.6316087245941162 at epoch 87 at applicants training\n",
      "loss: 1.630927324295044 at epoch 88 at applicants training\n",
      "loss: 1.632117748260498 at epoch 89 at applicants training\n",
      "loss: 1.633831262588501 at epoch 90 at applicants training\n",
      "loss: 1.633735179901123 at epoch 91 at applicants training\n",
      "loss: 1.6316660642623901 at epoch 92 at applicants training\n",
      "loss: 1.630130648612976 at epoch 93 at applicants training\n",
      "loss: 1.6315975189208984 at epoch 94 at applicants training\n",
      "loss: 1.6322239637374878 at epoch 95 at applicants training\n",
      "loss: 1.6300020217895508 at epoch 96 at applicants training\n",
      "loss: 1.629614233970642 at epoch 97 at applicants training\n",
      "loss: 1.6311123371124268 at epoch 98 at applicants training\n",
      "loss: 1.63060462474823 at epoch 99 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 0 at applicants training\n",
      "loss: 1.7095273733139038 at epoch 1 at applicants training\n",
      "loss: 1.671444058418274 at epoch 2 at applicants training\n",
      "loss: 1.687691330909729 at epoch 3 at applicants training\n",
      "loss: 1.6565606594085693 at epoch 4 at applicants training\n",
      "loss: 1.6575350761413574 at epoch 5 at applicants training\n",
      "loss: 1.6672378778457642 at epoch 6 at applicants training\n",
      "loss: 1.6521265506744385 at epoch 7 at applicants training\n",
      "loss: 1.6608248949050903 at epoch 8 at applicants training\n",
      "loss: 1.6519951820373535 at epoch 9 at applicants training\n",
      "loss: 1.6618835926055908 at epoch 10 at applicants training\n",
      "loss: 1.6568400859832764 at epoch 11 at applicants training\n",
      "loss: 1.6546056270599365 at epoch 12 at applicants training\n",
      "loss: 1.6565306186676025 at epoch 13 at applicants training\n",
      "loss: 1.6495660543441772 at epoch 14 at applicants training\n",
      "loss: 1.6550179719924927 at epoch 15 at applicants training\n",
      "loss: 1.647691011428833 at epoch 16 at applicants training\n",
      "loss: 1.652561068534851 at epoch 17 at applicants training\n",
      "loss: 1.6489262580871582 at epoch 18 at applicants training\n",
      "loss: 1.650038480758667 at epoch 19 at applicants training\n",
      "loss: 1.6458560228347778 at epoch 20 at applicants training\n",
      "loss: 1.6482784748077393 at epoch 21 at applicants training\n",
      "loss: 1.6461325883865356 at epoch 22 at applicants training\n",
      "loss: 1.649483323097229 at epoch 23 at applicants training\n",
      "loss: 1.6453357934951782 at epoch 24 at applicants training\n",
      "loss: 1.6453953981399536 at epoch 25 at applicants training\n",
      "loss: 1.6455549001693726 at epoch 26 at applicants training\n",
      "loss: 1.642381191253662 at epoch 27 at applicants training\n",
      "loss: 1.6436887979507446 at epoch 28 at applicants training\n",
      "loss: 1.6417105197906494 at epoch 29 at applicants training\n",
      "loss: 1.6409789323806763 at epoch 30 at applicants training\n",
      "loss: 1.6416739225387573 at epoch 31 at applicants training\n",
      "loss: 1.6393489837646484 at epoch 32 at applicants training\n",
      "loss: 1.6380692720413208 at epoch 33 at applicants training\n",
      "loss: 1.6379655599594116 at epoch 34 at applicants training\n",
      "loss: 1.6383928060531616 at epoch 35 at applicants training\n",
      "loss: 1.641859769821167 at epoch 36 at applicants training\n",
      "loss: 1.6360210180282593 at epoch 37 at applicants training\n",
      "loss: 1.6331758499145508 at epoch 38 at applicants training\n",
      "loss: 1.6346808671951294 at epoch 39 at applicants training\n",
      "loss: 1.636764645576477 at epoch 40 at applicants training\n",
      "loss: 1.6360349655151367 at epoch 41 at applicants training\n",
      "loss: 1.6294076442718506 at epoch 42 at applicants training\n",
      "loss: 1.6255792379379272 at epoch 43 at applicants training\n",
      "loss: 1.6252821683883667 at epoch 44 at applicants training\n",
      "loss: 1.6295851469039917 at epoch 45 at applicants training\n",
      "loss: 1.6457842588424683 at epoch 46 at applicants training\n",
      "loss: 1.627589464187622 at epoch 47 at applicants training\n",
      "loss: 1.6225430965423584 at epoch 48 at applicants training\n",
      "loss: 1.6267979145050049 at epoch 49 at applicants training\n",
      "loss: 1.6366475820541382 at epoch 50 at applicants training\n",
      "loss: 1.6399743556976318 at epoch 51 at applicants training\n",
      "loss: 1.618466854095459 at epoch 52 at applicants training\n",
      "loss: 1.632280707359314 at epoch 53 at applicants training\n",
      "loss: 1.647892713546753 at epoch 54 at applicants training\n",
      "loss: 1.6291087865829468 at epoch 55 at applicants training\n",
      "loss: 1.6704778671264648 at epoch 56 at applicants training\n",
      "loss: 1.6537877321243286 at epoch 57 at applicants training\n",
      "loss: 1.649383783340454 at epoch 58 at applicants training\n",
      "loss: 1.6651153564453125 at epoch 59 at applicants training\n",
      "loss: 1.6522217988967896 at epoch 60 at applicants training\n",
      "loss: 1.6309168338775635 at epoch 61 at applicants training\n",
      "loss: 1.6524065732955933 at epoch 62 at applicants training\n",
      "loss: 1.6350665092468262 at epoch 63 at applicants training\n",
      "loss: 1.6318731307983398 at epoch 64 at applicants training\n",
      "loss: 1.6439485549926758 at epoch 65 at applicants training\n",
      "loss: 1.6239787340164185 at epoch 66 at applicants training\n",
      "loss: 1.642661452293396 at epoch 67 at applicants training\n",
      "loss: 1.6286762952804565 at epoch 68 at applicants training\n",
      "loss: 1.6332818269729614 at epoch 69 at applicants training\n",
      "loss: 1.634784460067749 at epoch 70 at applicants training\n",
      "loss: 1.6191457509994507 at epoch 71 at applicants training\n",
      "loss: 1.6317886114120483 at epoch 72 at applicants training\n",
      "loss: 1.621554970741272 at epoch 73 at applicants training\n",
      "loss: 1.6267746686935425 at epoch 74 at applicants training\n",
      "loss: 1.623624563217163 at epoch 75 at applicants training\n",
      "loss: 1.6197532415390015 at epoch 76 at applicants training\n",
      "loss: 1.6240696907043457 at epoch 77 at applicants training\n",
      "loss: 1.6193194389343262 at epoch 78 at applicants training\n",
      "loss: 1.6227818727493286 at epoch 79 at applicants training\n",
      "loss: 1.61842942237854 at epoch 80 at applicants training\n",
      "loss: 1.6219062805175781 at epoch 81 at applicants training\n",
      "loss: 1.6184711456298828 at epoch 82 at applicants training\n",
      "loss: 1.6200252771377563 at epoch 83 at applicants training\n",
      "loss: 1.6159800291061401 at epoch 84 at applicants training\n",
      "loss: 1.618397831916809 at epoch 85 at applicants training\n",
      "loss: 1.6151853799819946 at epoch 86 at applicants training\n",
      "loss: 1.6160414218902588 at epoch 87 at applicants training\n",
      "loss: 1.6138523817062378 at epoch 88 at applicants training\n",
      "loss: 1.6151103973388672 at epoch 89 at applicants training\n",
      "loss: 1.6131401062011719 at epoch 90 at applicants training\n",
      "loss: 1.6127477884292603 at epoch 91 at applicants training\n",
      "loss: 1.6119285821914673 at epoch 92 at applicants training\n",
      "loss: 1.6124775409698486 at epoch 93 at applicants training\n",
      "loss: 1.6117569208145142 at epoch 94 at applicants training\n",
      "loss: 1.610504388809204 at epoch 95 at applicants training\n",
      "loss: 1.6104872226715088 at epoch 96 at applicants training\n",
      "loss: 1.6103624105453491 at epoch 97 at applicants training\n",
      "loss: 1.6103198528289795 at epoch 98 at applicants training\n",
      "loss: 1.6098988056182861 at epoch 99 at applicants training\n",
      "loss: 1.6978706121444702 at epoch 0 at applicants training\n",
      "loss: 1.6763242483139038 at epoch 1 at applicants training\n",
      "loss: 1.678787112236023 at epoch 2 at applicants training\n",
      "loss: 1.6788206100463867 at epoch 3 at applicants training\n",
      "loss: 1.6788132190704346 at epoch 4 at applicants training\n",
      "loss: 1.6787651777267456 at epoch 5 at applicants training\n",
      "loss: 1.6784604787826538 at epoch 6 at applicants training\n",
      "loss: 1.676376461982727 at epoch 7 at applicants training\n",
      "loss: 1.6730024814605713 at epoch 8 at applicants training\n",
      "loss: 1.6752948760986328 at epoch 9 at applicants training\n",
      "loss: 1.673358678817749 at epoch 10 at applicants training\n",
      "loss: 1.672275185585022 at epoch 11 at applicants training\n",
      "loss: 1.6728888750076294 at epoch 12 at applicants training\n",
      "loss: 1.6721149682998657 at epoch 13 at applicants training\n",
      "loss: 1.6704505681991577 at epoch 14 at applicants training\n",
      "loss: 1.6688786745071411 at epoch 15 at applicants training\n",
      "loss: 1.6667951345443726 at epoch 16 at applicants training\n",
      "loss: 1.6596957445144653 at epoch 17 at applicants training\n",
      "loss: 1.6670626401901245 at epoch 18 at applicants training\n",
      "loss: 1.6617095470428467 at epoch 19 at applicants training\n",
      "loss: 1.6571828126907349 at epoch 20 at applicants training\n",
      "loss: 1.658869981765747 at epoch 21 at applicants training\n",
      "loss: 1.6621181964874268 at epoch 22 at applicants training\n",
      "loss: 1.6565929651260376 at epoch 23 at applicants training\n",
      "loss: 1.654025673866272 at epoch 24 at applicants training\n",
      "loss: 1.65301513671875 at epoch 25 at applicants training\n",
      "loss: 1.654205322265625 at epoch 26 at applicants training\n",
      "loss: 1.6486406326293945 at epoch 27 at applicants training\n",
      "loss: 1.6517399549484253 at epoch 28 at applicants training\n",
      "loss: 1.6466726064682007 at epoch 29 at applicants training\n",
      "loss: 1.6348389387130737 at epoch 30 at applicants training\n",
      "loss: 1.6643017530441284 at epoch 31 at applicants training\n",
      "loss: 1.6436816453933716 at epoch 32 at applicants training\n",
      "loss: 1.6391185522079468 at epoch 33 at applicants training\n",
      "loss: 1.6388230323791504 at epoch 34 at applicants training\n",
      "loss: 1.6474641561508179 at epoch 35 at applicants training\n",
      "loss: 1.6450741291046143 at epoch 36 at applicants training\n",
      "loss: 1.6387673616409302 at epoch 37 at applicants training\n",
      "loss: 1.6470370292663574 at epoch 38 at applicants training\n",
      "loss: 1.6420818567276 at epoch 39 at applicants training\n",
      "loss: 1.6377159357070923 at epoch 40 at applicants training\n",
      "loss: 1.645606517791748 at epoch 41 at applicants training\n",
      "loss: 1.6373686790466309 at epoch 42 at applicants training\n",
      "loss: 1.636806607246399 at epoch 43 at applicants training\n",
      "loss: 1.6416479349136353 at epoch 44 at applicants training\n",
      "loss: 1.6342079639434814 at epoch 45 at applicants training\n",
      "loss: 1.634922981262207 at epoch 46 at applicants training\n",
      "loss: 1.634979248046875 at epoch 47 at applicants training\n",
      "loss: 1.6289108991622925 at epoch 48 at applicants training\n",
      "loss: 1.6341866254806519 at epoch 49 at applicants training\n",
      "loss: 1.627526044845581 at epoch 50 at applicants training\n",
      "loss: 1.6306328773498535 at epoch 51 at applicants training\n",
      "loss: 1.627357006072998 at epoch 52 at applicants training\n",
      "loss: 1.6310747861862183 at epoch 53 at applicants training\n",
      "loss: 1.6274265050888062 at epoch 54 at applicants training\n",
      "loss: 1.6298764944076538 at epoch 55 at applicants training\n",
      "loss: 1.6259419918060303 at epoch 56 at applicants training\n",
      "loss: 1.6278303861618042 at epoch 57 at applicants training\n",
      "loss: 1.6244405508041382 at epoch 58 at applicants training\n",
      "loss: 1.6253246068954468 at epoch 59 at applicants training\n",
      "loss: 1.6217199563980103 at epoch 60 at applicants training\n",
      "loss: 1.6249749660491943 at epoch 61 at applicants training\n",
      "loss: 1.6242706775665283 at epoch 62 at applicants training\n",
      "loss: 1.6211551427841187 at epoch 63 at applicants training\n",
      "loss: 1.6239551305770874 at epoch 64 at applicants training\n",
      "loss: 1.6200127601623535 at epoch 65 at applicants training\n",
      "loss: 1.6195262670516968 at epoch 66 at applicants training\n",
      "loss: 1.6210780143737793 at epoch 67 at applicants training\n",
      "loss: 1.61749267578125 at epoch 68 at applicants training\n",
      "loss: 1.6158154010772705 at epoch 69 at applicants training\n",
      "loss: 1.6182318925857544 at epoch 70 at applicants training\n",
      "loss: 1.6203324794769287 at epoch 71 at applicants training\n",
      "loss: 1.6143020391464233 at epoch 72 at applicants training\n",
      "loss: 1.6149770021438599 at epoch 73 at applicants training\n",
      "loss: 1.619857907295227 at epoch 74 at applicants training\n",
      "loss: 1.613315224647522 at epoch 75 at applicants training\n",
      "loss: 1.612941026687622 at epoch 76 at applicants training\n",
      "loss: 1.6173325777053833 at epoch 77 at applicants training\n",
      "loss: 1.6131346225738525 at epoch 78 at applicants training\n",
      "loss: 1.6111849546432495 at epoch 79 at applicants training\n",
      "loss: 1.613163709640503 at epoch 80 at applicants training\n",
      "loss: 1.6124218702316284 at epoch 81 at applicants training\n",
      "loss: 1.6103253364562988 at epoch 82 at applicants training\n",
      "loss: 1.6096504926681519 at epoch 83 at applicants training\n",
      "loss: 1.610911250114441 at epoch 84 at applicants training\n",
      "loss: 1.6126530170440674 at epoch 85 at applicants training\n",
      "loss: 1.6118628978729248 at epoch 86 at applicants training\n",
      "loss: 1.6095479726791382 at epoch 87 at applicants training\n",
      "loss: 1.607964277267456 at epoch 88 at applicants training\n",
      "loss: 1.609192967414856 at epoch 89 at applicants training\n",
      "loss: 1.6108031272888184 at epoch 90 at applicants training\n",
      "loss: 1.6098897457122803 at epoch 91 at applicants training\n",
      "loss: 1.6075688600540161 at epoch 92 at applicants training\n",
      "loss: 1.6066418886184692 at epoch 93 at applicants training\n",
      "loss: 1.6083344221115112 at epoch 94 at applicants training\n",
      "loss: 1.609220027923584 at epoch 95 at applicants training\n",
      "loss: 1.6078695058822632 at epoch 96 at applicants training\n",
      "loss: 1.6056727170944214 at epoch 97 at applicants training\n",
      "loss: 1.6048696041107178 at epoch 98 at applicants training\n",
      "loss: 1.607253909111023 at epoch 99 at applicants training\n",
      "loss: 1.7053319215774536 at epoch 0 at applicants training\n",
      "loss: 1.6749764680862427 at epoch 1 at applicants training\n",
      "loss: 1.6713643074035645 at epoch 2 at applicants training\n",
      "loss: 1.6678351163864136 at epoch 3 at applicants training\n",
      "loss: 1.6659233570098877 at epoch 4 at applicants training\n",
      "loss: 1.6480669975280762 at epoch 5 at applicants training\n",
      "loss: 1.647528886795044 at epoch 6 at applicants training\n",
      "loss: 1.65779447555542 at epoch 7 at applicants training\n",
      "loss: 1.640349268913269 at epoch 8 at applicants training\n",
      "loss: 1.6461609601974487 at epoch 9 at applicants training\n",
      "loss: 1.6423805952072144 at epoch 10 at applicants training\n",
      "loss: 1.626212477684021 at epoch 11 at applicants training\n",
      "loss: 1.6325432062149048 at epoch 12 at applicants training\n",
      "loss: 1.62349534034729 at epoch 13 at applicants training\n",
      "loss: 1.6252720355987549 at epoch 14 at applicants training\n",
      "loss: 1.6206685304641724 at epoch 15 at applicants training\n",
      "loss: 1.6173063516616821 at epoch 16 at applicants training\n",
      "loss: 1.6151355504989624 at epoch 17 at applicants training\n",
      "loss: 1.6129933595657349 at epoch 18 at applicants training\n",
      "loss: 1.6083506345748901 at epoch 19 at applicants training\n",
      "loss: 1.6089221239089966 at epoch 20 at applicants training\n",
      "loss: 1.6060760021209717 at epoch 21 at applicants training\n",
      "loss: 1.6036134958267212 at epoch 22 at applicants training\n",
      "loss: 1.6043286323547363 at epoch 23 at applicants training\n",
      "loss: 1.6003198623657227 at epoch 24 at applicants training\n",
      "loss: 1.600866675376892 at epoch 25 at applicants training\n",
      "loss: 1.59740149974823 at epoch 26 at applicants training\n",
      "loss: 1.597392201423645 at epoch 27 at applicants training\n",
      "loss: 1.593960165977478 at epoch 28 at applicants training\n",
      "loss: 1.59365975856781 at epoch 29 at applicants training\n",
      "loss: 1.5913796424865723 at epoch 30 at applicants training\n",
      "loss: 1.5908138751983643 at epoch 31 at applicants training\n",
      "loss: 1.586828351020813 at epoch 32 at applicants training\n",
      "loss: 1.5864752531051636 at epoch 33 at applicants training\n",
      "loss: 1.5842788219451904 at epoch 34 at applicants training\n",
      "loss: 1.5842574834823608 at epoch 35 at applicants training\n",
      "loss: 1.5826215744018555 at epoch 36 at applicants training\n",
      "loss: 1.5821712017059326 at epoch 37 at applicants training\n",
      "loss: 1.5798428058624268 at epoch 38 at applicants training\n",
      "loss: 1.5791857242584229 at epoch 39 at applicants training\n",
      "loss: 1.5769230127334595 at epoch 40 at applicants training\n",
      "loss: 1.5757859945297241 at epoch 41 at applicants training\n",
      "loss: 1.5750377178192139 at epoch 42 at applicants training\n",
      "loss: 1.5737502574920654 at epoch 43 at applicants training\n",
      "loss: 1.5735310316085815 at epoch 44 at applicants training\n",
      "loss: 1.5727671384811401 at epoch 45 at applicants training\n",
      "loss: 1.5704535245895386 at epoch 46 at applicants training\n",
      "loss: 1.5696017742156982 at epoch 47 at applicants training\n",
      "loss: 1.5691336393356323 at epoch 48 at applicants training\n",
      "loss: 1.56866455078125 at epoch 49 at applicants training\n",
      "loss: 1.5692689418792725 at epoch 50 at applicants training\n",
      "loss: 1.5679720640182495 at epoch 51 at applicants training\n",
      "loss: 1.569015622138977 at epoch 52 at applicants training\n",
      "loss: 1.5670416355133057 at epoch 53 at applicants training\n",
      "loss: 1.5653102397918701 at epoch 54 at applicants training\n",
      "loss: 1.563369870185852 at epoch 55 at applicants training\n",
      "loss: 1.5627444982528687 at epoch 56 at applicants training\n",
      "loss: 1.563317894935608 at epoch 57 at applicants training\n",
      "loss: 1.5635666847229004 at epoch 58 at applicants training\n",
      "loss: 1.5635427236557007 at epoch 59 at applicants training\n",
      "loss: 1.5624456405639648 at epoch 60 at applicants training\n",
      "loss: 1.5600546598434448 at epoch 61 at applicants training\n",
      "loss: 1.5587424039840698 at epoch 62 at applicants training\n",
      "loss: 1.557873249053955 at epoch 63 at applicants training\n",
      "loss: 1.5573481321334839 at epoch 64 at applicants training\n",
      "loss: 1.5563774108886719 at epoch 65 at applicants training\n",
      "loss: 1.5559329986572266 at epoch 66 at applicants training\n",
      "loss: 1.5562899112701416 at epoch 67 at applicants training\n",
      "loss: 1.5596972703933716 at epoch 68 at applicants training\n",
      "loss: 1.5673881769180298 at epoch 69 at applicants training\n",
      "loss: 1.5618234872817993 at epoch 70 at applicants training\n",
      "loss: 1.5537742376327515 at epoch 71 at applicants training\n",
      "loss: 1.5652790069580078 at epoch 72 at applicants training\n",
      "loss: 1.572243332862854 at epoch 73 at applicants training\n",
      "loss: 1.5677587985992432 at epoch 74 at applicants training\n",
      "loss: 1.5593492984771729 at epoch 75 at applicants training\n",
      "loss: 1.5638337135314941 at epoch 76 at applicants training\n",
      "loss: 1.5531257390975952 at epoch 77 at applicants training\n",
      "loss: 1.5593351125717163 at epoch 78 at applicants training\n",
      "loss: 1.551480770111084 at epoch 79 at applicants training\n",
      "loss: 1.5544549226760864 at epoch 80 at applicants training\n",
      "loss: 1.5520975589752197 at epoch 81 at applicants training\n",
      "loss: 1.5514943599700928 at epoch 82 at applicants training\n",
      "loss: 1.5494135618209839 at epoch 83 at applicants training\n",
      "loss: 1.5530428886413574 at epoch 84 at applicants training\n",
      "loss: 1.5507203340530396 at epoch 85 at applicants training\n",
      "loss: 1.5488934516906738 at epoch 86 at applicants training\n",
      "loss: 1.5489548444747925 at epoch 87 at applicants training\n",
      "loss: 1.5488948822021484 at epoch 88 at applicants training\n",
      "loss: 1.5468868017196655 at epoch 89 at applicants training\n",
      "loss: 1.5449138879776 at epoch 90 at applicants training\n",
      "loss: 1.5459084510803223 at epoch 91 at applicants training\n",
      "loss: 1.544756293296814 at epoch 92 at applicants training\n",
      "loss: 1.544487714767456 at epoch 93 at applicants training\n",
      "loss: 1.542542815208435 at epoch 94 at applicants training\n",
      "loss: 1.5435047149658203 at epoch 95 at applicants training\n",
      "loss: 1.542883276939392 at epoch 96 at applicants training\n",
      "loss: 1.5435280799865723 at epoch 97 at applicants training\n",
      "loss: 1.5395652055740356 at epoch 98 at applicants training\n",
      "loss: 1.5407997369766235 at epoch 99 at applicants training\n",
      "loss: 1.6875417232513428 at epoch 0 at applicants training\n",
      "loss: 1.6752203702926636 at epoch 1 at applicants training\n",
      "loss: 1.7016404867172241 at epoch 2 at applicants training\n",
      "loss: 1.6730937957763672 at epoch 3 at applicants training\n",
      "loss: 1.6900523900985718 at epoch 4 at applicants training\n",
      "loss: 1.686466932296753 at epoch 5 at applicants training\n",
      "loss: 1.670291781425476 at epoch 6 at applicants training\n",
      "loss: 1.670938491821289 at epoch 7 at applicants training\n",
      "loss: 1.6698249578475952 at epoch 8 at applicants training\n",
      "loss: 1.6627503633499146 at epoch 9 at applicants training\n",
      "loss: 1.6826568841934204 at epoch 10 at applicants training\n",
      "loss: 1.6737582683563232 at epoch 11 at applicants training\n",
      "loss: 1.6759392023086548 at epoch 12 at applicants training\n",
      "loss: 1.6762951612472534 at epoch 13 at applicants training\n",
      "loss: 1.6636905670166016 at epoch 14 at applicants training\n",
      "loss: 1.6719646453857422 at epoch 15 at applicants training\n",
      "loss: 1.662866234779358 at epoch 16 at applicants training\n",
      "loss: 1.66926109790802 at epoch 17 at applicants training\n",
      "loss: 1.6665560007095337 at epoch 18 at applicants training\n",
      "loss: 1.6598740816116333 at epoch 19 at applicants training\n",
      "loss: 1.6633551120758057 at epoch 20 at applicants training\n",
      "loss: 1.6557546854019165 at epoch 21 at applicants training\n",
      "loss: 1.6622081995010376 at epoch 22 at applicants training\n",
      "loss: 1.6623637676239014 at epoch 23 at applicants training\n",
      "loss: 1.656830072402954 at epoch 24 at applicants training\n",
      "loss: 1.6562265157699585 at epoch 25 at applicants training\n",
      "loss: 1.655688762664795 at epoch 26 at applicants training\n",
      "loss: 1.6495535373687744 at epoch 27 at applicants training\n",
      "loss: 1.6518666744232178 at epoch 28 at applicants training\n",
      "loss: 1.6499502658843994 at epoch 29 at applicants training\n",
      "loss: 1.653730034828186 at epoch 30 at applicants training\n",
      "loss: 1.6434229612350464 at epoch 31 at applicants training\n",
      "loss: 1.6454901695251465 at epoch 32 at applicants training\n",
      "loss: 1.6405425071716309 at epoch 33 at applicants training\n",
      "loss: 1.641778588294983 at epoch 34 at applicants training\n",
      "loss: 1.63848876953125 at epoch 35 at applicants training\n",
      "loss: 1.636237382888794 at epoch 36 at applicants training\n",
      "loss: 1.6336097717285156 at epoch 37 at applicants training\n",
      "loss: 1.6366291046142578 at epoch 38 at applicants training\n",
      "loss: 1.6359457969665527 at epoch 39 at applicants training\n",
      "loss: 1.6273980140686035 at epoch 40 at applicants training\n",
      "loss: 1.630540132522583 at epoch 41 at applicants training\n",
      "loss: 1.6279356479644775 at epoch 42 at applicants training\n",
      "loss: 1.6246949434280396 at epoch 43 at applicants training\n",
      "loss: 1.6246634721755981 at epoch 44 at applicants training\n",
      "loss: 1.6231993436813354 at epoch 45 at applicants training\n",
      "loss: 1.6214354038238525 at epoch 46 at applicants training\n",
      "loss: 1.6218616962432861 at epoch 47 at applicants training\n",
      "loss: 1.618090033531189 at epoch 48 at applicants training\n",
      "loss: 1.6204255819320679 at epoch 49 at applicants training\n",
      "loss: 1.6158716678619385 at epoch 50 at applicants training\n",
      "loss: 1.614223599433899 at epoch 51 at applicants training\n",
      "loss: 1.6136337518692017 at epoch 52 at applicants training\n",
      "loss: 1.6128745079040527 at epoch 53 at applicants training\n",
      "loss: 1.6173322200775146 at epoch 54 at applicants training\n",
      "loss: 1.6050007343292236 at epoch 55 at applicants training\n",
      "loss: 1.614667534828186 at epoch 56 at applicants training\n",
      "loss: 1.6106852293014526 at epoch 57 at applicants training\n",
      "loss: 1.6094857454299927 at epoch 58 at applicants training\n",
      "loss: 1.6094223260879517 at epoch 59 at applicants training\n",
      "loss: 1.6067768335342407 at epoch 60 at applicants training\n",
      "loss: 1.6059038639068604 at epoch 61 at applicants training\n",
      "loss: 1.6007959842681885 at epoch 62 at applicants training\n",
      "loss: 1.602842092514038 at epoch 63 at applicants training\n",
      "loss: 1.6055271625518799 at epoch 64 at applicants training\n",
      "loss: 1.6030004024505615 at epoch 65 at applicants training\n",
      "loss: 1.6047996282577515 at epoch 66 at applicants training\n",
      "loss: 1.6049442291259766 at epoch 67 at applicants training\n",
      "loss: 1.6024900674819946 at epoch 68 at applicants training\n",
      "loss: 1.6015123128890991 at epoch 69 at applicants training\n",
      "loss: 1.5950614213943481 at epoch 70 at applicants training\n",
      "loss: 1.602189540863037 at epoch 71 at applicants training\n",
      "loss: 1.6010435819625854 at epoch 72 at applicants training\n",
      "loss: 1.6067615747451782 at epoch 73 at applicants training\n",
      "loss: 1.6131106615066528 at epoch 74 at applicants training\n",
      "loss: 1.607173204421997 at epoch 75 at applicants training\n",
      "loss: 1.6107192039489746 at epoch 76 at applicants training\n",
      "loss: 1.6093461513519287 at epoch 77 at applicants training\n",
      "loss: 1.5962191820144653 at epoch 78 at applicants training\n",
      "loss: 1.603163242340088 at epoch 79 at applicants training\n",
      "loss: 1.5938853025436401 at epoch 80 at applicants training\n",
      "loss: 1.5946606397628784 at epoch 81 at applicants training\n",
      "loss: 1.5988606214523315 at epoch 82 at applicants training\n",
      "loss: 1.5944759845733643 at epoch 83 at applicants training\n",
      "loss: 1.5942809581756592 at epoch 84 at applicants training\n",
      "loss: 1.6061261892318726 at epoch 85 at applicants training\n",
      "loss: 1.5972923040390015 at epoch 86 at applicants training\n",
      "loss: 1.5954054594039917 at epoch 87 at applicants training\n",
      "loss: 1.6033916473388672 at epoch 88 at applicants training\n",
      "loss: 1.5909788608551025 at epoch 89 at applicants training\n",
      "loss: 1.593077540397644 at epoch 90 at applicants training\n",
      "loss: 1.5912365913391113 at epoch 91 at applicants training\n",
      "loss: 1.590340256690979 at epoch 92 at applicants training\n",
      "loss: 1.5863544940948486 at epoch 93 at applicants training\n",
      "loss: 1.5911056995391846 at epoch 94 at applicants training\n",
      "loss: 1.5851870775222778 at epoch 95 at applicants training\n",
      "loss: 1.5888136625289917 at epoch 96 at applicants training\n",
      "loss: 1.5875945091247559 at epoch 97 at applicants training\n",
      "loss: 1.5882433652877808 at epoch 98 at applicants training\n",
      "loss: 1.5839673280715942 at epoch 99 at applicants training\n",
      "loss: 1.720831274986267 at epoch 0 at applicants training\n",
      "loss: 1.7128305435180664 at epoch 1 at applicants training\n",
      "loss: 1.6999454498291016 at epoch 2 at applicants training\n",
      "loss: 1.7207900285720825 at epoch 3 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 4 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 5 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 6 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 7 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 8 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 9 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 10 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 11 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 12 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 13 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 14 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 15 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 16 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 17 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 18 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 19 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 20 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 21 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 22 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 23 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 24 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 25 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 26 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 27 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 28 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 29 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 30 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 31 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 32 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 33 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 34 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 35 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 36 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 37 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 38 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 39 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 40 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 41 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 42 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 43 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 44 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 45 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 46 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 47 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 48 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 49 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 50 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 51 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 52 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 53 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 54 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 55 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 56 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 57 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 58 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 59 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 60 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 61 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 62 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 63 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 64 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 65 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 66 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 67 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 68 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 69 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 70 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 71 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 72 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 73 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 74 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 75 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 76 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 77 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 78 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 79 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 80 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 81 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 82 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 83 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 84 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 85 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 86 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 87 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 88 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 89 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 90 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 91 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 92 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 93 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 94 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 95 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 96 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 97 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 98 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 99 at applicants training\n",
      "loss: 1.678167462348938 at epoch 0 at applicants training\n",
      "loss: 1.6813310384750366 at epoch 1 at applicants training\n",
      "loss: 1.6727205514907837 at epoch 2 at applicants training\n",
      "loss: 1.6750766038894653 at epoch 3 at applicants training\n",
      "loss: 1.6639013290405273 at epoch 4 at applicants training\n",
      "loss: 1.6643786430358887 at epoch 5 at applicants training\n",
      "loss: 1.6575480699539185 at epoch 6 at applicants training\n",
      "loss: 1.6597113609313965 at epoch 7 at applicants training\n",
      "loss: 1.657310128211975 at epoch 8 at applicants training\n",
      "loss: 1.6537171602249146 at epoch 9 at applicants training\n",
      "loss: 1.650282859802246 at epoch 10 at applicants training\n",
      "loss: 1.649603247642517 at epoch 11 at applicants training\n",
      "loss: 1.6486845016479492 at epoch 12 at applicants training\n",
      "loss: 1.6477406024932861 at epoch 13 at applicants training\n",
      "loss: 1.6471892595291138 at epoch 14 at applicants training\n",
      "loss: 1.6475410461425781 at epoch 15 at applicants training\n",
      "loss: 1.6462910175323486 at epoch 16 at applicants training\n",
      "loss: 1.6457064151763916 at epoch 17 at applicants training\n",
      "loss: 1.6436251401901245 at epoch 18 at applicants training\n",
      "loss: 1.6423027515411377 at epoch 19 at applicants training\n",
      "loss: 1.6396342515945435 at epoch 20 at applicants training\n",
      "loss: 1.6377263069152832 at epoch 21 at applicants training\n",
      "loss: 1.635998249053955 at epoch 22 at applicants training\n",
      "loss: 1.635276198387146 at epoch 23 at applicants training\n",
      "loss: 1.6381827592849731 at epoch 24 at applicants training\n",
      "loss: 1.633670687675476 at epoch 25 at applicants training\n",
      "loss: 1.6319698095321655 at epoch 26 at applicants training\n",
      "loss: 1.6283210515975952 at epoch 27 at applicants training\n",
      "loss: 1.6272944211959839 at epoch 28 at applicants training\n",
      "loss: 1.6334782838821411 at epoch 29 at applicants training\n",
      "loss: 1.631874442100525 at epoch 30 at applicants training\n",
      "loss: 1.6303205490112305 at epoch 31 at applicants training\n",
      "loss: 1.6222095489501953 at epoch 32 at applicants training\n",
      "loss: 1.625334620475769 at epoch 33 at applicants training\n",
      "loss: 1.6385245323181152 at epoch 34 at applicants training\n",
      "loss: 1.623341679573059 at epoch 35 at applicants training\n",
      "loss: 1.6504576206207275 at epoch 36 at applicants training\n",
      "loss: 1.6305298805236816 at epoch 37 at applicants training\n",
      "loss: 1.6483358144760132 at epoch 38 at applicants training\n",
      "loss: 1.6509852409362793 at epoch 39 at applicants training\n",
      "loss: 1.6230964660644531 at epoch 40 at applicants training\n",
      "loss: 1.6454685926437378 at epoch 41 at applicants training\n",
      "loss: 1.6452045440673828 at epoch 42 at applicants training\n",
      "loss: 1.6235711574554443 at epoch 43 at applicants training\n",
      "loss: 1.6394128799438477 at epoch 44 at applicants training\n",
      "loss: 1.6390646696090698 at epoch 45 at applicants training\n",
      "loss: 1.6218655109405518 at epoch 46 at applicants training\n",
      "loss: 1.6451565027236938 at epoch 47 at applicants training\n",
      "loss: 1.638854742050171 at epoch 48 at applicants training\n",
      "loss: 1.625626564025879 at epoch 49 at applicants training\n",
      "loss: 1.6373350620269775 at epoch 50 at applicants training\n",
      "loss: 1.633180856704712 at epoch 51 at applicants training\n",
      "loss: 1.621492624282837 at epoch 52 at applicants training\n",
      "loss: 1.6335583925247192 at epoch 53 at applicants training\n",
      "loss: 1.6304272413253784 at epoch 54 at applicants training\n",
      "loss: 1.6225098371505737 at epoch 55 at applicants training\n",
      "loss: 1.6289814710617065 at epoch 56 at applicants training\n",
      "loss: 1.6293038129806519 at epoch 57 at applicants training\n",
      "loss: 1.622580647468567 at epoch 58 at applicants training\n",
      "loss: 1.623784065246582 at epoch 59 at applicants training\n",
      "loss: 1.62544846534729 at epoch 60 at applicants training\n",
      "loss: 1.6187094449996948 at epoch 61 at applicants training\n",
      "loss: 1.6231988668441772 at epoch 62 at applicants training\n",
      "loss: 1.6187535524368286 at epoch 63 at applicants training\n",
      "loss: 1.6209375858306885 at epoch 64 at applicants training\n",
      "loss: 1.6191749572753906 at epoch 65 at applicants training\n",
      "loss: 1.6185939311981201 at epoch 66 at applicants training\n",
      "loss: 1.61911940574646 at epoch 67 at applicants training\n",
      "loss: 1.6157704591751099 at epoch 68 at applicants training\n",
      "loss: 1.6188088655471802 at epoch 69 at applicants training\n",
      "loss: 1.6151175498962402 at epoch 70 at applicants training\n",
      "loss: 1.617238163948059 at epoch 71 at applicants training\n",
      "loss: 1.6141791343688965 at epoch 72 at applicants training\n",
      "loss: 1.6165035963058472 at epoch 73 at applicants training\n",
      "loss: 1.613746166229248 at epoch 74 at applicants training\n",
      "loss: 1.6154783964157104 at epoch 75 at applicants training\n",
      "loss: 1.6134045124053955 at epoch 76 at applicants training\n",
      "loss: 1.6144866943359375 at epoch 77 at applicants training\n",
      "loss: 1.612586259841919 at epoch 78 at applicants training\n",
      "loss: 1.6136090755462646 at epoch 79 at applicants training\n",
      "loss: 1.61203932762146 at epoch 80 at applicants training\n",
      "loss: 1.6131565570831299 at epoch 81 at applicants training\n",
      "loss: 1.6113730669021606 at epoch 82 at applicants training\n",
      "loss: 1.6123378276824951 at epoch 83 at applicants training\n",
      "loss: 1.6107369661331177 at epoch 84 at applicants training\n",
      "loss: 1.6113601922988892 at epoch 85 at applicants training\n",
      "loss: 1.610653042793274 at epoch 86 at applicants training\n",
      "loss: 1.610197901725769 at epoch 87 at applicants training\n",
      "loss: 1.6106044054031372 at epoch 88 at applicants training\n",
      "loss: 1.6094049215316772 at epoch 89 at applicants training\n",
      "loss: 1.6097711324691772 at epoch 90 at applicants training\n",
      "loss: 1.609615445137024 at epoch 91 at applicants training\n",
      "loss: 1.608577847480774 at epoch 92 at applicants training\n",
      "loss: 1.6090203523635864 at epoch 93 at applicants training\n",
      "loss: 1.609078049659729 at epoch 94 at applicants training\n",
      "loss: 1.608038306236267 at epoch 95 at applicants training\n",
      "loss: 1.6079298257827759 at epoch 96 at applicants training\n",
      "loss: 1.6083402633666992 at epoch 97 at applicants training\n",
      "loss: 1.6074750423431396 at epoch 98 at applicants training\n",
      "loss: 1.6055082082748413 at epoch 99 at applicants training\n",
      "loss: 1.6938313245773315 at epoch 0 at applicants training\n",
      "loss: 1.7089838981628418 at epoch 1 at applicants training\n",
      "loss: 1.6925878524780273 at epoch 2 at applicants training\n",
      "loss: 1.690643310546875 at epoch 3 at applicants training\n",
      "loss: 1.6890488862991333 at epoch 4 at applicants training\n",
      "loss: 1.6945695877075195 at epoch 5 at applicants training\n",
      "loss: 1.6912968158721924 at epoch 6 at applicants training\n",
      "loss: 1.6938481330871582 at epoch 7 at applicants training\n",
      "loss: 1.6975632905960083 at epoch 8 at applicants training\n",
      "loss: 1.6785579919815063 at epoch 9 at applicants training\n",
      "loss: 1.6608467102050781 at epoch 10 at applicants training\n",
      "loss: 1.6688296794891357 at epoch 11 at applicants training\n",
      "loss: 1.6693686246871948 at epoch 12 at applicants training\n",
      "loss: 1.6676242351531982 at epoch 13 at applicants training\n",
      "loss: 1.6648950576782227 at epoch 14 at applicants training\n",
      "loss: 1.6683710813522339 at epoch 15 at applicants training\n",
      "loss: 1.662541151046753 at epoch 16 at applicants training\n",
      "loss: 1.6621034145355225 at epoch 17 at applicants training\n",
      "loss: 1.6591150760650635 at epoch 18 at applicants training\n",
      "loss: 1.6583317518234253 at epoch 19 at applicants training\n",
      "loss: 1.6583753824234009 at epoch 20 at applicants training\n",
      "loss: 1.6552040576934814 at epoch 21 at applicants training\n",
      "loss: 1.6531376838684082 at epoch 22 at applicants training\n",
      "loss: 1.6533924341201782 at epoch 23 at applicants training\n",
      "loss: 1.6504640579223633 at epoch 24 at applicants training\n",
      "loss: 1.647027611732483 at epoch 25 at applicants training\n",
      "loss: 1.646871566772461 at epoch 26 at applicants training\n",
      "loss: 1.6455326080322266 at epoch 27 at applicants training\n",
      "loss: 1.6437188386917114 at epoch 28 at applicants training\n",
      "loss: 1.6433701515197754 at epoch 29 at applicants training\n",
      "loss: 1.6408618688583374 at epoch 30 at applicants training\n",
      "loss: 1.6393245458602905 at epoch 31 at applicants training\n",
      "loss: 1.6400319337844849 at epoch 32 at applicants training\n",
      "loss: 1.6382218599319458 at epoch 33 at applicants training\n",
      "loss: 1.6417170763015747 at epoch 34 at applicants training\n",
      "loss: 1.637608289718628 at epoch 35 at applicants training\n",
      "loss: 1.6380375623703003 at epoch 36 at applicants training\n",
      "loss: 1.63875150680542 at epoch 37 at applicants training\n",
      "loss: 1.63563072681427 at epoch 38 at applicants training\n",
      "loss: 1.6358810663223267 at epoch 39 at applicants training\n",
      "loss: 1.6353803873062134 at epoch 40 at applicants training\n",
      "loss: 1.634728193283081 at epoch 41 at applicants training\n",
      "loss: 1.6345698833465576 at epoch 42 at applicants training\n",
      "loss: 1.6337578296661377 at epoch 43 at applicants training\n",
      "loss: 1.6337497234344482 at epoch 44 at applicants training\n",
      "loss: 1.6339116096496582 at epoch 45 at applicants training\n",
      "loss: 1.6349672079086304 at epoch 46 at applicants training\n",
      "loss: 1.6414495706558228 at epoch 47 at applicants training\n",
      "loss: 1.6330151557922363 at epoch 48 at applicants training\n",
      "loss: 1.634714961051941 at epoch 49 at applicants training\n",
      "loss: 1.6391916275024414 at epoch 50 at applicants training\n",
      "loss: 1.6326955556869507 at epoch 51 at applicants training\n",
      "loss: 1.6416239738464355 at epoch 52 at applicants training\n",
      "loss: 1.6351428031921387 at epoch 53 at applicants training\n",
      "loss: 1.634250521659851 at epoch 54 at applicants training\n",
      "loss: 1.6364777088165283 at epoch 55 at applicants training\n",
      "loss: 1.6345336437225342 at epoch 56 at applicants training\n",
      "loss: 1.634128451347351 at epoch 57 at applicants training\n",
      "loss: 1.6300959587097168 at epoch 58 at applicants training\n",
      "loss: 1.6327358484268188 at epoch 59 at applicants training\n",
      "loss: 1.6287440061569214 at epoch 60 at applicants training\n",
      "loss: 1.6338635683059692 at epoch 61 at applicants training\n",
      "loss: 1.629388451576233 at epoch 62 at applicants training\n",
      "loss: 1.6292948722839355 at epoch 63 at applicants training\n",
      "loss: 1.6308997869491577 at epoch 64 at applicants training\n",
      "loss: 1.6273531913757324 at epoch 65 at applicants training\n",
      "loss: 1.6265478134155273 at epoch 66 at applicants training\n",
      "loss: 1.6294898986816406 at epoch 67 at applicants training\n",
      "loss: 1.629210114479065 at epoch 68 at applicants training\n",
      "loss: 1.6256275177001953 at epoch 69 at applicants training\n",
      "loss: 1.6262930631637573 at epoch 70 at applicants training\n",
      "loss: 1.6268216371536255 at epoch 71 at applicants training\n",
      "loss: 1.6258348226547241 at epoch 72 at applicants training\n",
      "loss: 1.622883677482605 at epoch 73 at applicants training\n",
      "loss: 1.623947262763977 at epoch 74 at applicants training\n",
      "loss: 1.622915267944336 at epoch 75 at applicants training\n",
      "loss: 1.6220163106918335 at epoch 76 at applicants training\n",
      "loss: 1.62009596824646 at epoch 77 at applicants training\n",
      "loss: 1.618645191192627 at epoch 78 at applicants training\n",
      "loss: 1.6179412603378296 at epoch 79 at applicants training\n",
      "loss: 1.6166776418685913 at epoch 80 at applicants training\n",
      "loss: 1.6166104078292847 at epoch 81 at applicants training\n",
      "loss: 1.6239380836486816 at epoch 82 at applicants training\n",
      "loss: 1.6358736753463745 at epoch 83 at applicants training\n",
      "loss: 1.6194701194763184 at epoch 84 at applicants training\n",
      "loss: 1.6592118740081787 at epoch 85 at applicants training\n",
      "loss: 1.6617361307144165 at epoch 86 at applicants training\n",
      "loss: 1.657015085220337 at epoch 87 at applicants training\n",
      "loss: 1.661689043045044 at epoch 88 at applicants training\n",
      "loss: 1.6547532081604004 at epoch 89 at applicants training\n",
      "loss: 1.6268129348754883 at epoch 90 at applicants training\n",
      "loss: 1.6364716291427612 at epoch 91 at applicants training\n",
      "loss: 1.6283955574035645 at epoch 92 at applicants training\n",
      "loss: 1.6509400606155396 at epoch 93 at applicants training\n",
      "loss: 1.6332423686981201 at epoch 94 at applicants training\n",
      "loss: 1.64949369430542 at epoch 95 at applicants training\n",
      "loss: 1.6353923082351685 at epoch 96 at applicants training\n",
      "loss: 1.622503399848938 at epoch 97 at applicants training\n",
      "loss: 1.6325675249099731 at epoch 98 at applicants training\n",
      "loss: 1.6295998096466064 at epoch 99 at applicants training\n",
      "loss: 1.7105581760406494 at epoch 0 at applicants training\n",
      "loss: 1.6812087297439575 at epoch 1 at applicants training\n",
      "loss: 1.678821325302124 at epoch 2 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 3 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 4 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.702706217765808 at epoch 0 at applicants training\n",
      "loss: 1.7011641263961792 at epoch 1 at applicants training\n",
      "loss: 1.717918038368225 at epoch 2 at applicants training\n",
      "loss: 1.7139309644699097 at epoch 3 at applicants training\n",
      "loss: 1.6926852464675903 at epoch 4 at applicants training\n",
      "loss: 1.6820399761199951 at epoch 5 at applicants training\n",
      "loss: 1.6801083087921143 at epoch 6 at applicants training\n",
      "loss: 1.6783134937286377 at epoch 7 at applicants training\n",
      "loss: 1.678823709487915 at epoch 8 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.691801905632019 at epoch 0 at applicants training\n",
      "loss: 1.6972911357879639 at epoch 1 at applicants training\n",
      "loss: 1.6921560764312744 at epoch 2 at applicants training\n",
      "loss: 1.6938321590423584 at epoch 3 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 4 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 5 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 6 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 7 at applicants training\n",
      "loss: 1.693832516670227 at epoch 8 at applicants training\n",
      "loss: 1.6938303709030151 at epoch 9 at applicants training\n",
      "loss: 1.693811058998108 at epoch 10 at applicants training\n",
      "loss: 1.693524956703186 at epoch 11 at applicants training\n",
      "loss: 1.6892492771148682 at epoch 12 at applicants training\n",
      "loss: 1.671709418296814 at epoch 13 at applicants training\n",
      "loss: 1.677254319190979 at epoch 14 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.7182914018630981 at epoch 0 at applicants training\n",
      "loss: 1.6999682188034058 at epoch 1 at applicants training\n",
      "loss: 1.692824363708496 at epoch 2 at applicants training\n",
      "loss: 1.6871768236160278 at epoch 3 at applicants training\n",
      "loss: 1.689078688621521 at epoch 4 at applicants training\n",
      "loss: 1.6908562183380127 at epoch 5 at applicants training\n",
      "loss: 1.690708041191101 at epoch 6 at applicants training\n",
      "loss: 1.6840860843658447 at epoch 7 at applicants training\n",
      "loss: 1.6625396013259888 at epoch 8 at applicants training\n",
      "loss: 1.6684831380844116 at epoch 9 at applicants training\n",
      "loss: 1.6672372817993164 at epoch 10 at applicants training\n",
      "loss: 1.674513816833496 at epoch 11 at applicants training\n",
      "loss: 1.6740541458129883 at epoch 12 at applicants training\n",
      "loss: 1.6714861392974854 at epoch 13 at applicants training\n",
      "loss: 1.6714102029800415 at epoch 14 at applicants training\n",
      "loss: 1.668251395225525 at epoch 15 at applicants training\n",
      "loss: 1.6588010787963867 at epoch 16 at applicants training\n",
      "loss: 1.6507399082183838 at epoch 17 at applicants training\n",
      "loss: 1.6509402990341187 at epoch 18 at applicants training\n",
      "loss: 1.6534785032272339 at epoch 19 at applicants training\n",
      "loss: 1.6418672800064087 at epoch 20 at applicants training\n",
      "loss: 1.6459311246871948 at epoch 21 at applicants training\n",
      "loss: 1.6535701751708984 at epoch 22 at applicants training\n",
      "loss: 1.6401338577270508 at epoch 23 at applicants training\n",
      "loss: 1.6370571851730347 at epoch 24 at applicants training\n",
      "loss: 1.6467206478118896 at epoch 25 at applicants training\n",
      "loss: 1.6384245157241821 at epoch 26 at applicants training\n",
      "loss: 1.6355156898498535 at epoch 27 at applicants training\n",
      "loss: 1.6432685852050781 at epoch 28 at applicants training\n",
      "loss: 1.6375761032104492 at epoch 29 at applicants training\n",
      "loss: 1.635253667831421 at epoch 30 at applicants training\n",
      "loss: 1.6396679878234863 at epoch 31 at applicants training\n",
      "loss: 1.637211561203003 at epoch 32 at applicants training\n",
      "loss: 1.6322897672653198 at epoch 33 at applicants training\n",
      "loss: 1.6373813152313232 at epoch 34 at applicants training\n",
      "loss: 1.6348336935043335 at epoch 35 at applicants training\n",
      "loss: 1.6323446035385132 at epoch 36 at applicants training\n",
      "loss: 1.635453462600708 at epoch 37 at applicants training\n",
      "loss: 1.6336584091186523 at epoch 38 at applicants training\n",
      "loss: 1.6313180923461914 at epoch 39 at applicants training\n",
      "loss: 1.6332197189331055 at epoch 40 at applicants training\n",
      "loss: 1.6320983171463013 at epoch 41 at applicants training\n",
      "loss: 1.6297179460525513 at epoch 42 at applicants training\n",
      "loss: 1.6314698457717896 at epoch 43 at applicants training\n",
      "loss: 1.628912329673767 at epoch 44 at applicants training\n",
      "loss: 1.6285983324050903 at epoch 45 at applicants training\n",
      "loss: 1.6283199787139893 at epoch 46 at applicants training\n",
      "loss: 1.6257067918777466 at epoch 47 at applicants training\n",
      "loss: 1.6267006397247314 at epoch 48 at applicants training\n",
      "loss: 1.6242256164550781 at epoch 49 at applicants training\n",
      "loss: 1.6250554323196411 at epoch 50 at applicants training\n",
      "loss: 1.6221445798873901 at epoch 51 at applicants training\n",
      "loss: 1.6229544878005981 at epoch 52 at applicants training\n",
      "loss: 1.6209977865219116 at epoch 53 at applicants training\n",
      "loss: 1.6216295957565308 at epoch 54 at applicants training\n",
      "loss: 1.6204359531402588 at epoch 55 at applicants training\n",
      "loss: 1.619493007659912 at epoch 56 at applicants training\n",
      "loss: 1.6192762851715088 at epoch 57 at applicants training\n",
      "loss: 1.617636799812317 at epoch 58 at applicants training\n",
      "loss: 1.6180180311203003 at epoch 59 at applicants training\n",
      "loss: 1.6167654991149902 at epoch 60 at applicants training\n",
      "loss: 1.6157923936843872 at epoch 61 at applicants training\n",
      "loss: 1.6154788732528687 at epoch 62 at applicants training\n",
      "loss: 1.6139905452728271 at epoch 63 at applicants training\n",
      "loss: 1.6127253770828247 at epoch 64 at applicants training\n",
      "loss: 1.6120827198028564 at epoch 65 at applicants training\n",
      "loss: 1.6107823848724365 at epoch 66 at applicants training\n",
      "loss: 1.609982967376709 at epoch 67 at applicants training\n",
      "loss: 1.610005497932434 at epoch 68 at applicants training\n",
      "loss: 1.6091372966766357 at epoch 69 at applicants training\n",
      "loss: 1.608162760734558 at epoch 70 at applicants training\n",
      "loss: 1.6076215505599976 at epoch 71 at applicants training\n",
      "loss: 1.607061743736267 at epoch 72 at applicants training\n",
      "loss: 1.606315016746521 at epoch 73 at applicants training\n",
      "loss: 1.6057287454605103 at epoch 74 at applicants training\n",
      "loss: 1.6051756143569946 at epoch 75 at applicants training\n",
      "loss: 1.604580044746399 at epoch 76 at applicants training\n",
      "loss: 1.6039084196090698 at epoch 77 at applicants training\n",
      "loss: 1.6033294200897217 at epoch 78 at applicants training\n",
      "loss: 1.6029632091522217 at epoch 79 at applicants training\n",
      "loss: 1.602526068687439 at epoch 80 at applicants training\n",
      "loss: 1.602243185043335 at epoch 81 at applicants training\n",
      "loss: 1.6016908884048462 at epoch 82 at applicants training\n",
      "loss: 1.6013401746749878 at epoch 83 at applicants training\n",
      "loss: 1.6003139019012451 at epoch 84 at applicants training\n",
      "loss: 1.6001639366149902 at epoch 85 at applicants training\n",
      "loss: 1.5992937088012695 at epoch 86 at applicants training\n",
      "loss: 1.598836898803711 at epoch 87 at applicants training\n",
      "loss: 1.5983307361602783 at epoch 88 at applicants training\n",
      "loss: 1.5979557037353516 at epoch 89 at applicants training\n",
      "loss: 1.5977745056152344 at epoch 90 at applicants training\n",
      "loss: 1.5974092483520508 at epoch 91 at applicants training\n",
      "loss: 1.597458004951477 at epoch 92 at applicants training\n",
      "loss: 1.5972903966903687 at epoch 93 at applicants training\n",
      "loss: 1.5977387428283691 at epoch 94 at applicants training\n",
      "loss: 1.598831295967102 at epoch 95 at applicants training\n",
      "loss: 1.598793864250183 at epoch 96 at applicants training\n",
      "loss: 1.5977864265441895 at epoch 97 at applicants training\n",
      "loss: 1.5954136848449707 at epoch 98 at applicants training\n",
      "loss: 1.5938609838485718 at epoch 99 at applicants training\n",
      "loss: 1.7144036293029785 at epoch 0 at applicants training\n",
      "loss: 1.7128324508666992 at epoch 1 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 2 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 3 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 4 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 5 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 6 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 7 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 8 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 9 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 10 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 11 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 12 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 13 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 14 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 15 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 16 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 17 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 18 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 19 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 20 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 21 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 22 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 23 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 24 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 25 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 26 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 27 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 28 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 29 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 30 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 31 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 32 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 33 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 34 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 35 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 36 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 37 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 38 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 39 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 40 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 41 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 42 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 43 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 44 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 45 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 46 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 47 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 48 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 49 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 50 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 51 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 52 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 53 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 54 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 55 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 56 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 57 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 58 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 59 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 60 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 61 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 62 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 63 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 64 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 65 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 66 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 67 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 68 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 69 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 70 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 71 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 72 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 73 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 74 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 75 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 76 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 77 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 78 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 79 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 80 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 81 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 82 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 83 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 84 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 85 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 86 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 87 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 88 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 89 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 90 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 91 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 92 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 93 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 94 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 95 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 96 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 97 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 98 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 99 at applicants training\n",
      "loss: 1.7033792734146118 at epoch 0 at applicants training\n",
      "loss: 1.7024656534194946 at epoch 1 at applicants training\n",
      "loss: 1.694339394569397 at epoch 2 at applicants training\n",
      "loss: 1.6929584741592407 at epoch 3 at applicants training\n",
      "loss: 1.6938306093215942 at epoch 4 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 5 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 6 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 7 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 8 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 9 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 10 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 11 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 12 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 13 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 14 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 15 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 16 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 17 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 18 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 19 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 20 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 21 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 22 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 23 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 24 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 25 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 26 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 27 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 28 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 29 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 30 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 31 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 32 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 33 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 34 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 35 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 36 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 37 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 38 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 39 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 40 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 41 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 42 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 43 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 44 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 45 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 46 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 47 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 48 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 49 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 50 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 51 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 52 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 53 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 54 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 55 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 56 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 57 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 58 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 59 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 60 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 61 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 62 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 63 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 64 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 65 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 66 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 67 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 68 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 69 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 70 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 71 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 72 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 73 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 74 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 75 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 76 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 77 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 78 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 79 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 80 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 81 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 82 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 83 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 84 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 85 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 86 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 87 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 88 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 89 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 90 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 91 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 92 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 93 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 94 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 95 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 96 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 97 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 98 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 99 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 0 at applicants training\n",
      "loss: 1.6908893585205078 at epoch 1 at applicants training\n",
      "loss: 1.6717817783355713 at epoch 2 at applicants training\n",
      "loss: 1.6728665828704834 at epoch 3 at applicants training\n",
      "loss: 1.6705527305603027 at epoch 4 at applicants training\n",
      "loss: 1.6771337985992432 at epoch 5 at applicants training\n",
      "loss: 1.6786636114120483 at epoch 6 at applicants training\n",
      "loss: 1.6788195371627808 at epoch 7 at applicants training\n",
      "loss: 1.6788307428359985 at epoch 8 at applicants training\n",
      "loss: 1.6788321733474731 at epoch 9 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.6788184642791748 at epoch 0 at applicants training\n",
      "loss: 1.6982529163360596 at epoch 1 at applicants training\n",
      "loss: 1.6904340982437134 at epoch 2 at applicants training\n",
      "loss: 1.678141474723816 at epoch 3 at applicants training\n",
      "loss: 1.6788311004638672 at epoch 4 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.72040855884552 at epoch 0 at applicants training\n",
      "loss: 1.7100272178649902 at epoch 1 at applicants training\n",
      "loss: 1.663177251815796 at epoch 2 at applicants training\n",
      "loss: 1.6714825630187988 at epoch 3 at applicants training\n",
      "loss: 1.6694234609603882 at epoch 4 at applicants training\n",
      "loss: 1.6581422090530396 at epoch 5 at applicants training\n",
      "loss: 1.6614859104156494 at epoch 6 at applicants training\n",
      "loss: 1.6845697164535522 at epoch 7 at applicants training\n",
      "loss: 1.677355170249939 at epoch 8 at applicants training\n",
      "loss: 1.6575944423675537 at epoch 9 at applicants training\n",
      "loss: 1.6559607982635498 at epoch 10 at applicants training\n",
      "loss: 1.659745216369629 at epoch 11 at applicants training\n",
      "loss: 1.6560651063919067 at epoch 12 at applicants training\n",
      "loss: 1.6510794162750244 at epoch 13 at applicants training\n",
      "loss: 1.6584147214889526 at epoch 14 at applicants training\n",
      "loss: 1.6505786180496216 at epoch 15 at applicants training\n",
      "loss: 1.6523854732513428 at epoch 16 at applicants training\n",
      "loss: 1.6490774154663086 at epoch 17 at applicants training\n",
      "loss: 1.6475471258163452 at epoch 18 at applicants training\n",
      "loss: 1.646386981010437 at epoch 19 at applicants training\n",
      "loss: 1.6415700912475586 at epoch 20 at applicants training\n",
      "loss: 1.6397584676742554 at epoch 21 at applicants training\n",
      "loss: 1.648146390914917 at epoch 22 at applicants training\n",
      "loss: 1.6342934370040894 at epoch 23 at applicants training\n",
      "loss: 1.6559877395629883 at epoch 24 at applicants training\n",
      "loss: 1.6557725667953491 at epoch 25 at applicants training\n",
      "loss: 1.6321710348129272 at epoch 26 at applicants training\n",
      "loss: 1.6547185182571411 at epoch 27 at applicants training\n",
      "loss: 1.6427901983261108 at epoch 28 at applicants training\n",
      "loss: 1.6474018096923828 at epoch 29 at applicants training\n",
      "loss: 1.6499541997909546 at epoch 30 at applicants training\n",
      "loss: 1.6333571672439575 at epoch 31 at applicants training\n",
      "loss: 1.6503019332885742 at epoch 32 at applicants training\n",
      "loss: 1.6430491209030151 at epoch 33 at applicants training\n",
      "loss: 1.637910008430481 at epoch 34 at applicants training\n",
      "loss: 1.6482471227645874 at epoch 35 at applicants training\n",
      "loss: 1.638979434967041 at epoch 36 at applicants training\n",
      "loss: 1.6338306665420532 at epoch 37 at applicants training\n",
      "loss: 1.6422719955444336 at epoch 38 at applicants training\n",
      "loss: 1.631785273551941 at epoch 39 at applicants training\n",
      "loss: 1.6361817121505737 at epoch 40 at applicants training\n",
      "loss: 1.6372071504592896 at epoch 41 at applicants training\n",
      "loss: 1.6300674676895142 at epoch 42 at applicants training\n",
      "loss: 1.635520339012146 at epoch 43 at applicants training\n",
      "loss: 1.6304852962493896 at epoch 44 at applicants training\n",
      "loss: 1.6307190656661987 at epoch 45 at applicants training\n",
      "loss: 1.63189697265625 at epoch 46 at applicants training\n",
      "loss: 1.6273751258850098 at epoch 47 at applicants training\n",
      "loss: 1.6318761110305786 at epoch 48 at applicants training\n",
      "loss: 1.6243298053741455 at epoch 49 at applicants training\n",
      "loss: 1.622368335723877 at epoch 50 at applicants training\n",
      "loss: 1.6265305280685425 at epoch 51 at applicants training\n",
      "loss: 1.6221458911895752 at epoch 52 at applicants training\n",
      "loss: 1.621451735496521 at epoch 53 at applicants training\n",
      "loss: 1.6261340379714966 at epoch 54 at applicants training\n",
      "loss: 1.6224024295806885 at epoch 55 at applicants training\n",
      "loss: 1.6248971223831177 at epoch 56 at applicants training\n",
      "loss: 1.6200857162475586 at epoch 57 at applicants training\n",
      "loss: 1.6213469505310059 at epoch 58 at applicants training\n",
      "loss: 1.6163674592971802 at epoch 59 at applicants training\n",
      "loss: 1.612770915031433 at epoch 60 at applicants training\n",
      "loss: 1.616368293762207 at epoch 61 at applicants training\n",
      "loss: 1.6136387586593628 at epoch 62 at applicants training\n",
      "loss: 1.6118091344833374 at epoch 63 at applicants training\n",
      "loss: 1.6133159399032593 at epoch 64 at applicants training\n",
      "loss: 1.6130350828170776 at epoch 65 at applicants training\n",
      "loss: 1.6126126050949097 at epoch 66 at applicants training\n",
      "loss: 1.6115751266479492 at epoch 67 at applicants training\n",
      "loss: 1.6095876693725586 at epoch 68 at applicants training\n",
      "loss: 1.6081197261810303 at epoch 69 at applicants training\n",
      "loss: 1.6090397834777832 at epoch 70 at applicants training\n",
      "loss: 1.6064938306808472 at epoch 71 at applicants training\n",
      "loss: 1.6070080995559692 at epoch 72 at applicants training\n",
      "loss: 1.6061878204345703 at epoch 73 at applicants training\n",
      "loss: 1.603492021560669 at epoch 74 at applicants training\n",
      "loss: 1.6052587032318115 at epoch 75 at applicants training\n",
      "loss: 1.602352261543274 at epoch 76 at applicants training\n",
      "loss: 1.603103756904602 at epoch 77 at applicants training\n",
      "loss: 1.601729154586792 at epoch 78 at applicants training\n",
      "loss: 1.6040135622024536 at epoch 79 at applicants training\n",
      "loss: 1.6045042276382446 at epoch 80 at applicants training\n",
      "loss: 1.6063930988311768 at epoch 81 at applicants training\n",
      "loss: 1.601660132408142 at epoch 82 at applicants training\n",
      "loss: 1.599511742591858 at epoch 83 at applicants training\n",
      "loss: 1.5987313985824585 at epoch 84 at applicants training\n",
      "loss: 1.6006653308868408 at epoch 85 at applicants training\n",
      "loss: 1.6041072607040405 at epoch 86 at applicants training\n",
      "loss: 1.6027247905731201 at epoch 87 at applicants training\n",
      "loss: 1.59988272190094 at epoch 88 at applicants training\n",
      "loss: 1.5972492694854736 at epoch 89 at applicants training\n",
      "loss: 1.5978641510009766 at epoch 90 at applicants training\n",
      "loss: 1.598105788230896 at epoch 91 at applicants training\n",
      "loss: 1.5987935066223145 at epoch 92 at applicants training\n",
      "loss: 1.5969531536102295 at epoch 93 at applicants training\n",
      "loss: 1.5962934494018555 at epoch 94 at applicants training\n",
      "loss: 1.5946552753448486 at epoch 95 at applicants training\n",
      "loss: 1.595271110534668 at epoch 96 at applicants training\n",
      "loss: 1.5948690176010132 at epoch 97 at applicants training\n",
      "loss: 1.5960566997528076 at epoch 98 at applicants training\n",
      "loss: 1.5964394807815552 at epoch 99 at applicants training\n",
      "loss: 1.6624542474746704 at epoch 0 at applicants training\n",
      "loss: 1.6775339841842651 at epoch 1 at applicants training\n",
      "loss: 1.6674937009811401 at epoch 2 at applicants training\n",
      "loss: 1.6887073516845703 at epoch 3 at applicants training\n",
      "loss: 1.6487869024276733 at epoch 4 at applicants training\n",
      "loss: 1.6537649631500244 at epoch 5 at applicants training\n",
      "loss: 1.6567823886871338 at epoch 6 at applicants training\n",
      "loss: 1.6547869443893433 at epoch 7 at applicants training\n",
      "loss: 1.6461564302444458 at epoch 8 at applicants training\n",
      "loss: 1.6430165767669678 at epoch 9 at applicants training\n",
      "loss: 1.6471619606018066 at epoch 10 at applicants training\n",
      "loss: 1.6451451778411865 at epoch 11 at applicants training\n",
      "loss: 1.634891390800476 at epoch 12 at applicants training\n",
      "loss: 1.6349124908447266 at epoch 13 at applicants training\n",
      "loss: 1.6357145309448242 at epoch 14 at applicants training\n",
      "loss: 1.63032865524292 at epoch 15 at applicants training\n",
      "loss: 1.6263118982315063 at epoch 16 at applicants training\n",
      "loss: 1.6259708404541016 at epoch 17 at applicants training\n",
      "loss: 1.6204473972320557 at epoch 18 at applicants training\n",
      "loss: 1.6153638362884521 at epoch 19 at applicants training\n",
      "loss: 1.6142122745513916 at epoch 20 at applicants training\n",
      "loss: 1.6139696836471558 at epoch 21 at applicants training\n",
      "loss: 1.614212155342102 at epoch 22 at applicants training\n",
      "loss: 1.6086552143096924 at epoch 23 at applicants training\n",
      "loss: 1.607884407043457 at epoch 24 at applicants training\n",
      "loss: 1.6076689958572388 at epoch 25 at applicants training\n",
      "loss: 1.6076734066009521 at epoch 26 at applicants training\n",
      "loss: 1.6062228679656982 at epoch 27 at applicants training\n",
      "loss: 1.6021702289581299 at epoch 28 at applicants training\n",
      "loss: 1.599336862564087 at epoch 29 at applicants training\n",
      "loss: 1.6008585691452026 at epoch 30 at applicants training\n",
      "loss: 1.5979526042938232 at epoch 31 at applicants training\n",
      "loss: 1.5945184230804443 at epoch 32 at applicants training\n",
      "loss: 1.5942951440811157 at epoch 33 at applicants training\n",
      "loss: 1.5934022665023804 at epoch 34 at applicants training\n",
      "loss: 1.591275691986084 at epoch 35 at applicants training\n",
      "loss: 1.5882209539413452 at epoch 36 at applicants training\n",
      "loss: 1.5889365673065186 at epoch 37 at applicants training\n",
      "loss: 1.5865789651870728 at epoch 38 at applicants training\n",
      "loss: 1.586031436920166 at epoch 39 at applicants training\n",
      "loss: 1.585951805114746 at epoch 40 at applicants training\n",
      "loss: 1.5839189291000366 at epoch 41 at applicants training\n",
      "loss: 1.5845612287521362 at epoch 42 at applicants training\n",
      "loss: 1.5833964347839355 at epoch 43 at applicants training\n",
      "loss: 1.5824217796325684 at epoch 44 at applicants training\n",
      "loss: 1.5831938982009888 at epoch 45 at applicants training\n",
      "loss: 1.5818928480148315 at epoch 46 at applicants training\n",
      "loss: 1.5813019275665283 at epoch 47 at applicants training\n",
      "loss: 1.5808851718902588 at epoch 48 at applicants training\n",
      "loss: 1.5800011157989502 at epoch 49 at applicants training\n",
      "loss: 1.5801070928573608 at epoch 50 at applicants training\n",
      "loss: 1.5790855884552002 at epoch 51 at applicants training\n",
      "loss: 1.5788525342941284 at epoch 52 at applicants training\n",
      "loss: 1.5781595706939697 at epoch 53 at applicants training\n",
      "loss: 1.5778133869171143 at epoch 54 at applicants training\n",
      "loss: 1.5775753259658813 at epoch 55 at applicants training\n",
      "loss: 1.5768195390701294 at epoch 56 at applicants training\n",
      "loss: 1.5767793655395508 at epoch 57 at applicants training\n",
      "loss: 1.5763881206512451 at epoch 58 at applicants training\n",
      "loss: 1.5761414766311646 at epoch 59 at applicants training\n",
      "loss: 1.5756173133850098 at epoch 60 at applicants training\n",
      "loss: 1.5755611658096313 at epoch 61 at applicants training\n",
      "loss: 1.5749279260635376 at epoch 62 at applicants training\n",
      "loss: 1.574750542640686 at epoch 63 at applicants training\n",
      "loss: 1.574212670326233 at epoch 64 at applicants training\n",
      "loss: 1.5742067098617554 at epoch 65 at applicants training\n",
      "loss: 1.5736641883850098 at epoch 66 at applicants training\n",
      "loss: 1.573467493057251 at epoch 67 at applicants training\n",
      "loss: 1.5729461908340454 at epoch 68 at applicants training\n",
      "loss: 1.572946310043335 at epoch 69 at applicants training\n",
      "loss: 1.572563886642456 at epoch 70 at applicants training\n",
      "loss: 1.5722582340240479 at epoch 71 at applicants training\n",
      "loss: 1.5721203088760376 at epoch 72 at applicants training\n",
      "loss: 1.5717521905899048 at epoch 73 at applicants training\n",
      "loss: 1.5716556310653687 at epoch 74 at applicants training\n",
      "loss: 1.5712890625 at epoch 75 at applicants training\n",
      "loss: 1.571040153503418 at epoch 76 at applicants training\n",
      "loss: 1.5708842277526855 at epoch 77 at applicants training\n",
      "loss: 1.5706413984298706 at epoch 78 at applicants training\n",
      "loss: 1.5705629587173462 at epoch 79 at applicants training\n",
      "loss: 1.5701541900634766 at epoch 80 at applicants training\n",
      "loss: 1.569848656654358 at epoch 81 at applicants training\n",
      "loss: 1.5696007013320923 at epoch 82 at applicants training\n",
      "loss: 1.5694658756256104 at epoch 83 at applicants training\n",
      "loss: 1.5694676637649536 at epoch 84 at applicants training\n",
      "loss: 1.5691741704940796 at epoch 85 at applicants training\n",
      "loss: 1.569009780883789 at epoch 86 at applicants training\n",
      "loss: 1.5687763690948486 at epoch 87 at applicants training\n",
      "loss: 1.5684928894042969 at epoch 88 at applicants training\n",
      "loss: 1.5681700706481934 at epoch 89 at applicants training\n",
      "loss: 1.568052887916565 at epoch 90 at applicants training\n",
      "loss: 1.5680028200149536 at epoch 91 at applicants training\n",
      "loss: 1.5678892135620117 at epoch 92 at applicants training\n",
      "loss: 1.5675355195999146 at epoch 93 at applicants training\n",
      "loss: 1.5671710968017578 at epoch 94 at applicants training\n",
      "loss: 1.5672438144683838 at epoch 95 at applicants training\n",
      "loss: 1.567029595375061 at epoch 96 at applicants training\n",
      "loss: 1.5671685934066772 at epoch 97 at applicants training\n",
      "loss: 1.5670136213302612 at epoch 98 at applicants training\n",
      "loss: 1.567384958267212 at epoch 99 at applicants training\n",
      "loss: 1.6804336309432983 at epoch 0 at applicants training\n",
      "loss: 1.718488335609436 at epoch 1 at applicants training\n",
      "loss: 1.6882388591766357 at epoch 2 at applicants training\n",
      "loss: 1.7135522365570068 at epoch 3 at applicants training\n",
      "loss: 1.7135084867477417 at epoch 4 at applicants training\n",
      "loss: 1.7128331661224365 at epoch 5 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 6 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 7 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 8 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 9 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 10 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 11 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 12 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 13 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 14 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 15 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 16 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 17 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 18 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 19 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 20 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 21 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 22 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 23 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 24 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 25 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 26 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 27 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 28 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 29 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 30 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 31 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 32 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 33 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 34 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 35 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 36 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 37 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 38 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 39 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 40 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 41 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 42 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 43 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 44 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 45 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 46 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 47 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 48 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 49 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 50 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 51 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 52 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 53 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 54 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 55 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 56 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 57 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 58 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 59 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 60 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 61 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 62 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 63 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 64 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 65 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 66 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 67 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 68 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 69 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 70 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 71 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 72 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 73 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 74 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 75 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 76 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 77 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 78 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 79 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 80 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 81 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 82 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 83 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 84 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 85 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 86 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 87 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 88 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 89 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 90 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 91 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 92 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 93 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 94 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 95 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 96 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 97 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 98 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 99 at applicants training\n",
      "loss: 1.7208327054977417 at epoch 0 at applicants training\n",
      "loss: 1.7209186553955078 at epoch 1 at applicants training\n",
      "loss: 1.7015981674194336 at epoch 2 at applicants training\n",
      "loss: 1.6975209712982178 at epoch 3 at applicants training\n",
      "loss: 1.6926403045654297 at epoch 4 at applicants training\n",
      "loss: 1.6881481409072876 at epoch 5 at applicants training\n",
      "loss: 1.6884194612503052 at epoch 6 at applicants training\n",
      "loss: 1.6822490692138672 at epoch 7 at applicants training\n",
      "loss: 1.677217721939087 at epoch 8 at applicants training\n",
      "loss: 1.675678014755249 at epoch 9 at applicants training\n",
      "loss: 1.6704881191253662 at epoch 10 at applicants training\n",
      "loss: 1.6706879138946533 at epoch 11 at applicants training\n",
      "loss: 1.6768437623977661 at epoch 12 at applicants training\n",
      "loss: 1.6641234159469604 at epoch 13 at applicants training\n",
      "loss: 1.6711844205856323 at epoch 14 at applicants training\n",
      "loss: 1.6722975969314575 at epoch 15 at applicants training\n",
      "loss: 1.6661436557769775 at epoch 16 at applicants training\n",
      "loss: 1.6574522256851196 at epoch 17 at applicants training\n",
      "loss: 1.6668872833251953 at epoch 18 at applicants training\n",
      "loss: 1.6642539501190186 at epoch 19 at applicants training\n",
      "loss: 1.66984224319458 at epoch 20 at applicants training\n",
      "loss: 1.6560848951339722 at epoch 21 at applicants training\n",
      "loss: 1.656158208847046 at epoch 22 at applicants training\n",
      "loss: 1.6610033512115479 at epoch 23 at applicants training\n",
      "loss: 1.6533634662628174 at epoch 24 at applicants training\n",
      "loss: 1.6530351638793945 at epoch 25 at applicants training\n",
      "loss: 1.6524949073791504 at epoch 26 at applicants training\n",
      "loss: 1.6524181365966797 at epoch 27 at applicants training\n",
      "loss: 1.6486215591430664 at epoch 28 at applicants training\n",
      "loss: 1.6463747024536133 at epoch 29 at applicants training\n",
      "loss: 1.6470857858657837 at epoch 30 at applicants training\n",
      "loss: 1.6455292701721191 at epoch 31 at applicants training\n",
      "loss: 1.6429096460342407 at epoch 32 at applicants training\n",
      "loss: 1.6419862508773804 at epoch 33 at applicants training\n",
      "loss: 1.6424617767333984 at epoch 34 at applicants training\n",
      "loss: 1.6393224000930786 at epoch 35 at applicants training\n",
      "loss: 1.6391353607177734 at epoch 36 at applicants training\n",
      "loss: 1.6382286548614502 at epoch 37 at applicants training\n",
      "loss: 1.6357754468917847 at epoch 38 at applicants training\n",
      "loss: 1.635588526725769 at epoch 39 at applicants training\n",
      "loss: 1.6335432529449463 at epoch 40 at applicants training\n",
      "loss: 1.6338926553726196 at epoch 41 at applicants training\n",
      "loss: 1.6329491138458252 at epoch 42 at applicants training\n",
      "loss: 1.6302341222763062 at epoch 43 at applicants training\n",
      "loss: 1.6302553415298462 at epoch 44 at applicants training\n",
      "loss: 1.6288697719573975 at epoch 45 at applicants training\n",
      "loss: 1.629321575164795 at epoch 46 at applicants training\n",
      "loss: 1.6280134916305542 at epoch 47 at applicants training\n",
      "loss: 1.6255711317062378 at epoch 48 at applicants training\n",
      "loss: 1.6253877878189087 at epoch 49 at applicants training\n",
      "loss: 1.6251755952835083 at epoch 50 at applicants training\n",
      "loss: 1.6260790824890137 at epoch 51 at applicants training\n",
      "loss: 1.6237183809280396 at epoch 52 at applicants training\n",
      "loss: 1.623956322669983 at epoch 53 at applicants training\n",
      "loss: 1.62490713596344 at epoch 54 at applicants training\n",
      "loss: 1.6244944334030151 at epoch 55 at applicants training\n",
      "loss: 1.6234790086746216 at epoch 56 at applicants training\n",
      "loss: 1.6221716403961182 at epoch 57 at applicants training\n",
      "loss: 1.6198716163635254 at epoch 58 at applicants training\n",
      "loss: 1.6200506687164307 at epoch 59 at applicants training\n",
      "loss: 1.6210845708847046 at epoch 60 at applicants training\n",
      "loss: 1.6207034587860107 at epoch 61 at applicants training\n",
      "loss: 1.6209467649459839 at epoch 62 at applicants training\n",
      "loss: 1.618899941444397 at epoch 63 at applicants training\n",
      "loss: 1.6161644458770752 at epoch 64 at applicants training\n",
      "loss: 1.616400122642517 at epoch 65 at applicants training\n",
      "loss: 1.6176319122314453 at epoch 66 at applicants training\n",
      "loss: 1.6176059246063232 at epoch 67 at applicants training\n",
      "loss: 1.6178065538406372 at epoch 68 at applicants training\n",
      "loss: 1.6141681671142578 at epoch 69 at applicants training\n",
      "loss: 1.614943504333496 at epoch 70 at applicants training\n",
      "loss: 1.6106598377227783 at epoch 71 at applicants training\n",
      "loss: 1.6115946769714355 at epoch 72 at applicants training\n",
      "loss: 1.612891435623169 at epoch 73 at applicants training\n",
      "loss: 1.6153957843780518 at epoch 74 at applicants training\n",
      "loss: 1.6149803400039673 at epoch 75 at applicants training\n",
      "loss: 1.6116588115692139 at epoch 76 at applicants training\n",
      "loss: 1.6086829900741577 at epoch 77 at applicants training\n",
      "loss: 1.6075340509414673 at epoch 78 at applicants training\n",
      "loss: 1.607453465461731 at epoch 79 at applicants training\n",
      "loss: 1.609678030014038 at epoch 80 at applicants training\n",
      "loss: 1.61350679397583 at epoch 81 at applicants training\n",
      "loss: 1.6092267036437988 at epoch 82 at applicants training\n",
      "loss: 1.6055244207382202 at epoch 83 at applicants training\n",
      "loss: 1.6039550304412842 at epoch 84 at applicants training\n",
      "loss: 1.606592059135437 at epoch 85 at applicants training\n",
      "loss: 1.6118054389953613 at epoch 86 at applicants training\n",
      "loss: 1.6043254137039185 at epoch 87 at applicants training\n",
      "loss: 1.601916790008545 at epoch 88 at applicants training\n",
      "loss: 1.6032484769821167 at epoch 89 at applicants training\n",
      "loss: 1.6044281721115112 at epoch 90 at applicants training\n",
      "loss: 1.6043306589126587 at epoch 91 at applicants training\n",
      "loss: 1.6011252403259277 at epoch 92 at applicants training\n",
      "loss: 1.6000937223434448 at epoch 93 at applicants training\n",
      "loss: 1.6015841960906982 at epoch 94 at applicants training\n",
      "loss: 1.6027553081512451 at epoch 95 at applicants training\n",
      "loss: 1.6009724140167236 at epoch 96 at applicants training\n",
      "loss: 1.5996452569961548 at epoch 97 at applicants training\n",
      "loss: 1.6004401445388794 at epoch 98 at applicants training\n",
      "loss: 1.605198860168457 at epoch 99 at applicants training\n",
      "loss: 1.6641590595245361 at epoch 0 at applicants training\n",
      "loss: 1.6938068866729736 at epoch 1 at applicants training\n",
      "loss: 1.6938244104385376 at epoch 2 at applicants training\n",
      "loss: 1.693817377090454 at epoch 3 at applicants training\n",
      "loss: 1.693655252456665 at epoch 4 at applicants training\n",
      "loss: 1.6904999017715454 at epoch 5 at applicants training\n",
      "loss: 1.6876055002212524 at epoch 6 at applicants training\n",
      "loss: 1.6838279962539673 at epoch 7 at applicants training\n",
      "loss: 1.6838055849075317 at epoch 8 at applicants training\n",
      "loss: 1.6790717840194702 at epoch 9 at applicants training\n",
      "loss: 1.6814936399459839 at epoch 10 at applicants training\n",
      "loss: 1.6791573762893677 at epoch 11 at applicants training\n",
      "loss: 1.6779826879501343 at epoch 12 at applicants training\n",
      "loss: 1.6784822940826416 at epoch 13 at applicants training\n",
      "loss: 1.6780719757080078 at epoch 14 at applicants training\n",
      "loss: 1.6729782819747925 at epoch 15 at applicants training\n",
      "loss: 1.6795202493667603 at epoch 16 at applicants training\n",
      "loss: 1.6722700595855713 at epoch 17 at applicants training\n",
      "loss: 1.6729826927185059 at epoch 18 at applicants training\n",
      "loss: 1.6719365119934082 at epoch 19 at applicants training\n",
      "loss: 1.6717560291290283 at epoch 20 at applicants training\n",
      "loss: 1.6725988388061523 at epoch 21 at applicants training\n",
      "loss: 1.672117829322815 at epoch 22 at applicants training\n",
      "loss: 1.668508529663086 at epoch 23 at applicants training\n",
      "loss: 1.6724509000778198 at epoch 24 at applicants training\n",
      "loss: 1.6668239831924438 at epoch 25 at applicants training\n",
      "loss: 1.6708812713623047 at epoch 26 at applicants training\n",
      "loss: 1.6651740074157715 at epoch 27 at applicants training\n",
      "loss: 1.667464017868042 at epoch 28 at applicants training\n",
      "loss: 1.6661860942840576 at epoch 29 at applicants training\n",
      "loss: 1.6648976802825928 at epoch 30 at applicants training\n",
      "loss: 1.6666780710220337 at epoch 31 at applicants training\n",
      "loss: 1.6652199029922485 at epoch 32 at applicants training\n",
      "loss: 1.667309045791626 at epoch 33 at applicants training\n",
      "loss: 1.6645995378494263 at epoch 34 at applicants training\n",
      "loss: 1.6684950590133667 at epoch 35 at applicants training\n",
      "loss: 1.6643604040145874 at epoch 36 at applicants training\n",
      "loss: 1.666189193725586 at epoch 37 at applicants training\n",
      "loss: 1.665035367012024 at epoch 38 at applicants training\n",
      "loss: 1.6642698049545288 at epoch 39 at applicants training\n",
      "loss: 1.6643315553665161 at epoch 40 at applicants training\n",
      "loss: 1.664278268814087 at epoch 41 at applicants training\n",
      "loss: 1.664517879486084 at epoch 42 at applicants training\n",
      "loss: 1.6631273031234741 at epoch 43 at applicants training\n",
      "loss: 1.6638579368591309 at epoch 44 at applicants training\n",
      "loss: 1.663856029510498 at epoch 45 at applicants training\n",
      "loss: 1.6634156703948975 at epoch 46 at applicants training\n",
      "loss: 1.6632728576660156 at epoch 47 at applicants training\n",
      "loss: 1.6617449522018433 at epoch 48 at applicants training\n",
      "loss: 1.6620328426361084 at epoch 49 at applicants training\n",
      "loss: 1.6612788438796997 at epoch 50 at applicants training\n",
      "loss: 1.6608902215957642 at epoch 51 at applicants training\n",
      "loss: 1.661061406135559 at epoch 52 at applicants training\n",
      "loss: 1.6603509187698364 at epoch 53 at applicants training\n",
      "loss: 1.6598507165908813 at epoch 54 at applicants training\n",
      "loss: 1.6595343351364136 at epoch 55 at applicants training\n",
      "loss: 1.659536600112915 at epoch 56 at applicants training\n",
      "loss: 1.6626619100570679 at epoch 57 at applicants training\n",
      "loss: 1.6735527515411377 at epoch 58 at applicants training\n",
      "loss: 1.6580476760864258 at epoch 59 at applicants training\n",
      "loss: 1.6775139570236206 at epoch 60 at applicants training\n",
      "loss: 1.6720515489578247 at epoch 61 at applicants training\n",
      "loss: 1.6683083772659302 at epoch 62 at applicants training\n",
      "loss: 1.6841074228286743 at epoch 63 at applicants training\n",
      "loss: 1.668648600578308 at epoch 64 at applicants training\n",
      "loss: 1.6819194555282593 at epoch 65 at applicants training\n",
      "loss: 1.6905356645584106 at epoch 66 at applicants training\n",
      "loss: 1.6921809911727905 at epoch 67 at applicants training\n",
      "loss: 1.6927931308746338 at epoch 68 at applicants training\n",
      "loss: 1.6923396587371826 at epoch 69 at applicants training\n",
      "loss: 1.691148042678833 at epoch 70 at applicants training\n",
      "loss: 1.688979983329773 at epoch 71 at applicants training\n",
      "loss: 1.6792513132095337 at epoch 72 at applicants training\n",
      "loss: 1.6657743453979492 at epoch 73 at applicants training\n",
      "loss: 1.6823556423187256 at epoch 74 at applicants training\n",
      "loss: 1.6735507249832153 at epoch 75 at applicants training\n",
      "loss: 1.6684536933898926 at epoch 76 at applicants training\n",
      "loss: 1.6770178079605103 at epoch 77 at applicants training\n",
      "loss: 1.6747692823410034 at epoch 78 at applicants training\n",
      "loss: 1.6650128364562988 at epoch 79 at applicants training\n",
      "loss: 1.6722564697265625 at epoch 80 at applicants training\n",
      "loss: 1.6743897199630737 at epoch 81 at applicants training\n",
      "loss: 1.66497004032135 at epoch 82 at applicants training\n",
      "loss: 1.6712807416915894 at epoch 83 at applicants training\n",
      "loss: 1.6751033067703247 at epoch 84 at applicants training\n",
      "loss: 1.6689666509628296 at epoch 85 at applicants training\n",
      "loss: 1.6650422811508179 at epoch 86 at applicants training\n",
      "loss: 1.6697686910629272 at epoch 87 at applicants training\n",
      "loss: 1.6685774326324463 at epoch 88 at applicants training\n",
      "loss: 1.6642930507659912 at epoch 89 at applicants training\n",
      "loss: 1.6657066345214844 at epoch 90 at applicants training\n",
      "loss: 1.6679270267486572 at epoch 91 at applicants training\n",
      "loss: 1.6649906635284424 at epoch 92 at applicants training\n",
      "loss: 1.6634416580200195 at epoch 93 at applicants training\n",
      "loss: 1.6655101776123047 at epoch 94 at applicants training\n",
      "loss: 1.6652584075927734 at epoch 95 at applicants training\n",
      "loss: 1.6629596948623657 at epoch 96 at applicants training\n",
      "loss: 1.663171410560608 at epoch 97 at applicants training\n",
      "loss: 1.6643805503845215 at epoch 98 at applicants training\n",
      "loss: 1.6622819900512695 at epoch 99 at applicants training\n",
      "loss: 1.7128328084945679 at epoch 0 at applicants training\n",
      "loss: 1.7127978801727295 at epoch 1 at applicants training\n",
      "loss: 1.7035459280014038 at epoch 2 at applicants training\n",
      "loss: 1.7009475231170654 at epoch 3 at applicants training\n",
      "loss: 1.6975709199905396 at epoch 4 at applicants training\n",
      "loss: 1.6961913108825684 at epoch 5 at applicants training\n",
      "loss: 1.6934596300125122 at epoch 6 at applicants training\n",
      "loss: 1.6925078630447388 at epoch 7 at applicants training\n",
      "loss: 1.6882141828536987 at epoch 8 at applicants training\n",
      "loss: 1.6822590827941895 at epoch 9 at applicants training\n",
      "loss: 1.6643507480621338 at epoch 10 at applicants training\n",
      "loss: 1.6749019622802734 at epoch 11 at applicants training\n",
      "loss: 1.6855344772338867 at epoch 12 at applicants training\n",
      "loss: 1.6751213073730469 at epoch 13 at applicants training\n",
      "loss: 1.6646826267242432 at epoch 14 at applicants training\n",
      "loss: 1.659073829650879 at epoch 15 at applicants training\n",
      "loss: 1.661146640777588 at epoch 16 at applicants training\n",
      "loss: 1.6504178047180176 at epoch 17 at applicants training\n",
      "loss: 1.648133635520935 at epoch 18 at applicants training\n",
      "loss: 1.6594456434249878 at epoch 19 at applicants training\n",
      "loss: 1.6503831148147583 at epoch 20 at applicants training\n",
      "loss: 1.645308494567871 at epoch 21 at applicants training\n",
      "loss: 1.639560580253601 at epoch 22 at applicants training\n",
      "loss: 1.6462690830230713 at epoch 23 at applicants training\n",
      "loss: 1.6339353322982788 at epoch 24 at applicants training\n",
      "loss: 1.6367454528808594 at epoch 25 at applicants training\n",
      "loss: 1.6385785341262817 at epoch 26 at applicants training\n",
      "loss: 1.6292009353637695 at epoch 27 at applicants training\n",
      "loss: 1.628679633140564 at epoch 28 at applicants training\n",
      "loss: 1.6308602094650269 at epoch 29 at applicants training\n",
      "loss: 1.6232353448867798 at epoch 30 at applicants training\n",
      "loss: 1.6265720129013062 at epoch 31 at applicants training\n",
      "loss: 1.6228374242782593 at epoch 32 at applicants training\n",
      "loss: 1.621604084968567 at epoch 33 at applicants training\n",
      "loss: 1.6218194961547852 at epoch 34 at applicants training\n",
      "loss: 1.6180897951126099 at epoch 35 at applicants training\n",
      "loss: 1.6192843914031982 at epoch 36 at applicants training\n",
      "loss: 1.6156527996063232 at epoch 37 at applicants training\n",
      "loss: 1.61611807346344 at epoch 38 at applicants training\n",
      "loss: 1.6142882108688354 at epoch 39 at applicants training\n",
      "loss: 1.6147615909576416 at epoch 40 at applicants training\n",
      "loss: 1.6127674579620361 at epoch 41 at applicants training\n",
      "loss: 1.6117087602615356 at epoch 42 at applicants training\n",
      "loss: 1.611405849456787 at epoch 43 at applicants training\n",
      "loss: 1.6102312803268433 at epoch 44 at applicants training\n",
      "loss: 1.6099681854248047 at epoch 45 at applicants training\n",
      "loss: 1.6093014478683472 at epoch 46 at applicants training\n",
      "loss: 1.6088547706604004 at epoch 47 at applicants training\n",
      "loss: 1.6076490879058838 at epoch 48 at applicants training\n",
      "loss: 1.6077847480773926 at epoch 49 at applicants training\n",
      "loss: 1.6073901653289795 at epoch 50 at applicants training\n",
      "loss: 1.606034517288208 at epoch 51 at applicants training\n",
      "loss: 1.6057908535003662 at epoch 52 at applicants training\n",
      "loss: 1.6050947904586792 at epoch 53 at applicants training\n",
      "loss: 1.6047581434249878 at epoch 54 at applicants training\n",
      "loss: 1.6042027473449707 at epoch 55 at applicants training\n",
      "loss: 1.6039127111434937 at epoch 56 at applicants training\n",
      "loss: 1.603062391281128 at epoch 57 at applicants training\n",
      "loss: 1.6025738716125488 at epoch 58 at applicants training\n",
      "loss: 1.6019890308380127 at epoch 59 at applicants training\n",
      "loss: 1.601730465888977 at epoch 60 at applicants training\n",
      "loss: 1.6012459993362427 at epoch 61 at applicants training\n",
      "loss: 1.6006901264190674 at epoch 62 at applicants training\n",
      "loss: 1.6003657579421997 at epoch 63 at applicants training\n",
      "loss: 1.5994820594787598 at epoch 64 at applicants training\n",
      "loss: 1.5992857217788696 at epoch 65 at applicants training\n",
      "loss: 1.5988253355026245 at epoch 66 at applicants training\n",
      "loss: 1.5983309745788574 at epoch 67 at applicants training\n",
      "loss: 1.5980783700942993 at epoch 68 at applicants training\n",
      "loss: 1.5978453159332275 at epoch 69 at applicants training\n",
      "loss: 1.5981234312057495 at epoch 70 at applicants training\n",
      "loss: 1.5989385843276978 at epoch 71 at applicants training\n",
      "loss: 1.6022721529006958 at epoch 72 at applicants training\n",
      "loss: 1.6037708520889282 at epoch 73 at applicants training\n",
      "loss: 1.59890615940094 at epoch 74 at applicants training\n",
      "loss: 1.597305417060852 at epoch 75 at applicants training\n",
      "loss: 1.6002861261367798 at epoch 76 at applicants training\n",
      "loss: 1.6058218479156494 at epoch 77 at applicants training\n",
      "loss: 1.5959547758102417 at epoch 78 at applicants training\n",
      "loss: 1.6101154088974 at epoch 79 at applicants training\n",
      "loss: 1.6055376529693604 at epoch 80 at applicants training\n",
      "loss: 1.6041479110717773 at epoch 81 at applicants training\n",
      "loss: 1.6065129041671753 at epoch 82 at applicants training\n",
      "loss: 1.60218346118927 at epoch 83 at applicants training\n",
      "loss: 1.5955538749694824 at epoch 84 at applicants training\n",
      "loss: 1.6001319885253906 at epoch 85 at applicants training\n",
      "loss: 1.596971035003662 at epoch 86 at applicants training\n",
      "loss: 1.6015267372131348 at epoch 87 at applicants training\n",
      "loss: 1.5976580381393433 at epoch 88 at applicants training\n",
      "loss: 1.59758460521698 at epoch 89 at applicants training\n",
      "loss: 1.599379539489746 at epoch 90 at applicants training\n",
      "loss: 1.5960835218429565 at epoch 91 at applicants training\n",
      "loss: 1.5940099954605103 at epoch 92 at applicants training\n",
      "loss: 1.5953258275985718 at epoch 93 at applicants training\n",
      "loss: 1.5935009717941284 at epoch 94 at applicants training\n",
      "loss: 1.5940659046173096 at epoch 95 at applicants training\n",
      "loss: 1.5891979932785034 at epoch 96 at applicants training\n",
      "loss: 1.603848934173584 at epoch 97 at applicants training\n",
      "loss: 1.590183973312378 at epoch 98 at applicants training\n",
      "loss: 1.5925331115722656 at epoch 99 at applicants training\n",
      "loss: 1.6852744817733765 at epoch 0 at applicants training\n",
      "loss: 1.7167772054672241 at epoch 1 at applicants training\n",
      "loss: 1.7178317308425903 at epoch 2 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 3 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 4 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 5 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 6 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 7 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 8 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 9 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 10 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 11 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 12 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 13 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 14 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 15 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 16 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 17 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 18 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 19 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 20 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 21 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 22 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 23 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 24 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 25 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 26 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 27 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 28 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 29 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 30 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 31 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 32 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 33 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 34 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 35 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 36 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 37 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 38 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 39 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 40 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 41 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 42 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 43 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 44 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 45 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 46 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 47 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 48 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 49 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 50 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 51 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 52 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 53 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 54 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 55 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 56 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 57 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 58 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 59 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 60 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 61 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 62 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 63 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 64 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 65 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 66 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 67 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 68 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 69 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 70 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 71 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 72 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 73 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 74 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 75 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 76 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 77 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 78 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 79 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 80 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 81 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 82 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 83 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 84 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 85 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 86 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 87 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 88 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 89 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 90 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 91 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 92 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 93 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 94 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 95 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 96 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 97 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 98 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 99 at applicants training\n",
      "loss: 1.6916202306747437 at epoch 0 at applicants training\n",
      "loss: 1.6782512664794922 at epoch 1 at applicants training\n",
      "loss: 1.6777019500732422 at epoch 2 at applicants training\n",
      "loss: 1.6768728494644165 at epoch 3 at applicants training\n",
      "loss: 1.6767514944076538 at epoch 4 at applicants training\n",
      "loss: 1.6767480373382568 at epoch 5 at applicants training\n",
      "loss: 1.6749900579452515 at epoch 6 at applicants training\n",
      "loss: 1.6763505935668945 at epoch 7 at applicants training\n",
      "loss: 1.67352294921875 at epoch 8 at applicants training\n",
      "loss: 1.674752950668335 at epoch 9 at applicants training\n",
      "loss: 1.6720411777496338 at epoch 10 at applicants training\n",
      "loss: 1.6730730533599854 at epoch 11 at applicants training\n",
      "loss: 1.668362021446228 at epoch 12 at applicants training\n",
      "loss: 1.6726200580596924 at epoch 13 at applicants training\n",
      "loss: 1.671755313873291 at epoch 14 at applicants training\n",
      "loss: 1.672530174255371 at epoch 15 at applicants training\n",
      "loss: 1.6679540872573853 at epoch 16 at applicants training\n",
      "loss: 1.6691179275512695 at epoch 17 at applicants training\n",
      "loss: 1.6677250862121582 at epoch 18 at applicants training\n",
      "loss: 1.6675035953521729 at epoch 19 at applicants training\n",
      "loss: 1.669061541557312 at epoch 20 at applicants training\n",
      "loss: 1.669387936592102 at epoch 21 at applicants training\n",
      "loss: 1.6670525074005127 at epoch 22 at applicants training\n",
      "loss: 1.6642768383026123 at epoch 23 at applicants training\n",
      "loss: 1.6670278310775757 at epoch 24 at applicants training\n",
      "loss: 1.6673946380615234 at epoch 25 at applicants training\n",
      "loss: 1.6613017320632935 at epoch 26 at applicants training\n",
      "loss: 1.6664642095565796 at epoch 27 at applicants training\n",
      "loss: 1.6674277782440186 at epoch 28 at applicants training\n",
      "loss: 1.66169011592865 at epoch 29 at applicants training\n",
      "loss: 1.6585824489593506 at epoch 30 at applicants training\n",
      "loss: 1.6638280153274536 at epoch 31 at applicants training\n",
      "loss: 1.660612940788269 at epoch 32 at applicants training\n",
      "loss: 1.6589912176132202 at epoch 33 at applicants training\n",
      "loss: 1.6579159498214722 at epoch 34 at applicants training\n",
      "loss: 1.6538290977478027 at epoch 35 at applicants training\n",
      "loss: 1.6527209281921387 at epoch 36 at applicants training\n",
      "loss: 1.6525635719299316 at epoch 37 at applicants training\n",
      "loss: 1.6501764059066772 at epoch 38 at applicants training\n",
      "loss: 1.6520583629608154 at epoch 39 at applicants training\n",
      "loss: 1.6488466262817383 at epoch 40 at applicants training\n",
      "loss: 1.6480474472045898 at epoch 41 at applicants training\n",
      "loss: 1.6505937576293945 at epoch 42 at applicants training\n",
      "loss: 1.6470078229904175 at epoch 43 at applicants training\n",
      "loss: 1.6463909149169922 at epoch 44 at applicants training\n",
      "loss: 1.6468114852905273 at epoch 45 at applicants training\n",
      "loss: 1.6443474292755127 at epoch 46 at applicants training\n",
      "loss: 1.6449350118637085 at epoch 47 at applicants training\n",
      "loss: 1.6435085535049438 at epoch 48 at applicants training\n",
      "loss: 1.6438332796096802 at epoch 49 at applicants training\n",
      "loss: 1.6431999206542969 at epoch 50 at applicants training\n",
      "loss: 1.6429139375686646 at epoch 51 at applicants training\n",
      "loss: 1.6424245834350586 at epoch 52 at applicants training\n",
      "loss: 1.6425238847732544 at epoch 53 at applicants training\n",
      "loss: 1.6429718732833862 at epoch 54 at applicants training\n",
      "loss: 1.643570065498352 at epoch 55 at applicants training\n",
      "loss: 1.6410927772521973 at epoch 56 at applicants training\n",
      "loss: 1.6411961317062378 at epoch 57 at applicants training\n",
      "loss: 1.64220130443573 at epoch 58 at applicants training\n",
      "loss: 1.6421430110931396 at epoch 59 at applicants training\n",
      "loss: 1.6402978897094727 at epoch 60 at applicants training\n",
      "loss: 1.6406328678131104 at epoch 61 at applicants training\n",
      "loss: 1.6427676677703857 at epoch 62 at applicants training\n",
      "loss: 1.639938473701477 at epoch 63 at applicants training\n",
      "loss: 1.6398096084594727 at epoch 64 at applicants training\n",
      "loss: 1.6407434940338135 at epoch 65 at applicants training\n",
      "loss: 1.6393674612045288 at epoch 66 at applicants training\n",
      "loss: 1.6397922039031982 at epoch 67 at applicants training\n",
      "loss: 1.6396337747573853 at epoch 68 at applicants training\n",
      "loss: 1.6388870477676392 at epoch 69 at applicants training\n",
      "loss: 1.63926362991333 at epoch 70 at applicants training\n",
      "loss: 1.6385647058486938 at epoch 71 at applicants training\n",
      "loss: 1.6382721662521362 at epoch 72 at applicants training\n",
      "loss: 1.6384929418563843 at epoch 73 at applicants training\n",
      "loss: 1.637902855873108 at epoch 74 at applicants training\n",
      "loss: 1.6374672651290894 at epoch 75 at applicants training\n",
      "loss: 1.6375735998153687 at epoch 76 at applicants training\n",
      "loss: 1.637254238128662 at epoch 77 at applicants training\n",
      "loss: 1.636474847793579 at epoch 78 at applicants training\n",
      "loss: 1.6363548040390015 at epoch 79 at applicants training\n",
      "loss: 1.6364549398422241 at epoch 80 at applicants training\n",
      "loss: 1.6373215913772583 at epoch 81 at applicants training\n",
      "loss: 1.639359712600708 at epoch 82 at applicants training\n",
      "loss: 1.6435142755508423 at epoch 83 at applicants training\n",
      "loss: 1.6356606483459473 at epoch 84 at applicants training\n",
      "loss: 1.642257809638977 at epoch 85 at applicants training\n",
      "loss: 1.6419100761413574 at epoch 86 at applicants training\n",
      "loss: 1.6362473964691162 at epoch 87 at applicants training\n",
      "loss: 1.6447924375534058 at epoch 88 at applicants training\n",
      "loss: 1.6364552974700928 at epoch 89 at applicants training\n",
      "loss: 1.639305830001831 at epoch 90 at applicants training\n",
      "loss: 1.6363375186920166 at epoch 91 at applicants training\n",
      "loss: 1.635108232498169 at epoch 92 at applicants training\n",
      "loss: 1.6367570161819458 at epoch 93 at applicants training\n",
      "loss: 1.6346977949142456 at epoch 94 at applicants training\n",
      "loss: 1.636690616607666 at epoch 95 at applicants training\n",
      "loss: 1.6339449882507324 at epoch 96 at applicants training\n",
      "loss: 1.6344554424285889 at epoch 97 at applicants training\n",
      "loss: 1.635759711265564 at epoch 98 at applicants training\n",
      "loss: 1.632597804069519 at epoch 99 at applicants training\n",
      "loss: 1.6918917894363403 at epoch 0 at applicants training\n",
      "loss: 1.6762816905975342 at epoch 1 at applicants training\n",
      "loss: 1.6759263277053833 at epoch 2 at applicants training\n",
      "loss: 1.6762205362319946 at epoch 3 at applicants training\n",
      "loss: 1.6746443510055542 at epoch 4 at applicants training\n",
      "loss: 1.6728825569152832 at epoch 5 at applicants training\n",
      "loss: 1.6697081327438354 at epoch 6 at applicants training\n",
      "loss: 1.6608905792236328 at epoch 7 at applicants training\n",
      "loss: 1.671746015548706 at epoch 8 at applicants training\n",
      "loss: 1.684090495109558 at epoch 9 at applicants training\n",
      "loss: 1.6583788394927979 at epoch 10 at applicants training\n",
      "loss: 1.6649311780929565 at epoch 11 at applicants training\n",
      "loss: 1.6701818704605103 at epoch 12 at applicants training\n",
      "loss: 1.6690459251403809 at epoch 13 at applicants training\n",
      "loss: 1.663754940032959 at epoch 14 at applicants training\n",
      "loss: 1.6638315916061401 at epoch 15 at applicants training\n",
      "loss: 1.67100989818573 at epoch 16 at applicants training\n",
      "loss: 1.6637377738952637 at epoch 17 at applicants training\n",
      "loss: 1.6626598834991455 at epoch 18 at applicants training\n",
      "loss: 1.665113091468811 at epoch 19 at applicants training\n",
      "loss: 1.6664024591445923 at epoch 20 at applicants training\n",
      "loss: 1.6626918315887451 at epoch 21 at applicants training\n",
      "loss: 1.6627529859542847 at epoch 22 at applicants training\n",
      "loss: 1.6633259057998657 at epoch 23 at applicants training\n",
      "loss: 1.6650267839431763 at epoch 24 at applicants training\n",
      "loss: 1.6636765003204346 at epoch 25 at applicants training\n",
      "loss: 1.6624212265014648 at epoch 26 at applicants training\n",
      "loss: 1.6615902185440063 at epoch 27 at applicants training\n",
      "loss: 1.6617748737335205 at epoch 28 at applicants training\n",
      "loss: 1.6627998352050781 at epoch 29 at applicants training\n",
      "loss: 1.6608067750930786 at epoch 30 at applicants training\n",
      "loss: 1.6607542037963867 at epoch 31 at applicants training\n",
      "loss: 1.6607493162155151 at epoch 32 at applicants training\n",
      "loss: 1.6606650352478027 at epoch 33 at applicants training\n",
      "loss: 1.6604481935501099 at epoch 34 at applicants training\n",
      "loss: 1.658976435661316 at epoch 35 at applicants training\n",
      "loss: 1.6583918333053589 at epoch 36 at applicants training\n",
      "loss: 1.6580803394317627 at epoch 37 at applicants training\n",
      "loss: 1.657138705253601 at epoch 38 at applicants training\n",
      "loss: 1.6562902927398682 at epoch 39 at applicants training\n",
      "loss: 1.6569243669509888 at epoch 40 at applicants training\n",
      "loss: 1.6557343006134033 at epoch 41 at applicants training\n",
      "loss: 1.6567710638046265 at epoch 42 at applicants training\n",
      "loss: 1.6547441482543945 at epoch 43 at applicants training\n",
      "loss: 1.6567749977111816 at epoch 44 at applicants training\n",
      "loss: 1.6543089151382446 at epoch 45 at applicants training\n",
      "loss: 1.6544780731201172 at epoch 46 at applicants training\n",
      "loss: 1.6542569398880005 at epoch 47 at applicants training\n",
      "loss: 1.6522305011749268 at epoch 48 at applicants training\n",
      "loss: 1.6531651020050049 at epoch 49 at applicants training\n",
      "loss: 1.6517491340637207 at epoch 50 at applicants training\n",
      "loss: 1.6514322757720947 at epoch 51 at applicants training\n",
      "loss: 1.6514099836349487 at epoch 52 at applicants training\n",
      "loss: 1.6502043008804321 at epoch 53 at applicants training\n",
      "loss: 1.6503983736038208 at epoch 54 at applicants training\n",
      "loss: 1.6494888067245483 at epoch 55 at applicants training\n",
      "loss: 1.6482069492340088 at epoch 56 at applicants training\n",
      "loss: 1.648611307144165 at epoch 57 at applicants training\n",
      "loss: 1.6508715152740479 at epoch 58 at applicants training\n",
      "loss: 1.6468520164489746 at epoch 59 at applicants training\n",
      "loss: 1.6456286907196045 at epoch 60 at applicants training\n",
      "loss: 1.6457808017730713 at epoch 61 at applicants training\n",
      "loss: 1.648496389389038 at epoch 62 at applicants training\n",
      "loss: 1.6549550294876099 at epoch 63 at applicants training\n",
      "loss: 1.6549407243728638 at epoch 64 at applicants training\n",
      "loss: 1.6443105936050415 at epoch 65 at applicants training\n",
      "loss: 1.654192328453064 at epoch 66 at applicants training\n",
      "loss: 1.652829885482788 at epoch 67 at applicants training\n",
      "loss: 1.6585005521774292 at epoch 68 at applicants training\n",
      "loss: 1.6474381685256958 at epoch 69 at applicants training\n",
      "loss: 1.6590713262557983 at epoch 70 at applicants training\n",
      "loss: 1.6432304382324219 at epoch 71 at applicants training\n",
      "loss: 1.649936318397522 at epoch 72 at applicants training\n",
      "loss: 1.649269461631775 at epoch 73 at applicants training\n",
      "loss: 1.6418640613555908 at epoch 74 at applicants training\n",
      "loss: 1.6521602869033813 at epoch 75 at applicants training\n",
      "loss: 1.645209550857544 at epoch 76 at applicants training\n",
      "loss: 1.6480575799942017 at epoch 77 at applicants training\n",
      "loss: 1.639007568359375 at epoch 78 at applicants training\n",
      "loss: 1.6477125883102417 at epoch 79 at applicants training\n",
      "loss: 1.642574667930603 at epoch 80 at applicants training\n",
      "loss: 1.6457210779190063 at epoch 81 at applicants training\n",
      "loss: 1.6380200386047363 at epoch 82 at applicants training\n",
      "loss: 1.6464757919311523 at epoch 83 at applicants training\n",
      "loss: 1.6392709016799927 at epoch 84 at applicants training\n",
      "loss: 1.6424915790557861 at epoch 85 at applicants training\n",
      "loss: 1.6365363597869873 at epoch 86 at applicants training\n",
      "loss: 1.6394572257995605 at epoch 87 at applicants training\n",
      "loss: 1.6387505531311035 at epoch 88 at applicants training\n",
      "loss: 1.6364377737045288 at epoch 89 at applicants training\n",
      "loss: 1.6400783061981201 at epoch 90 at applicants training\n",
      "loss: 1.633429765701294 at epoch 91 at applicants training\n",
      "loss: 1.6372629404067993 at epoch 92 at applicants training\n",
      "loss: 1.6324670314788818 at epoch 93 at applicants training\n",
      "loss: 1.6350829601287842 at epoch 94 at applicants training\n",
      "loss: 1.6333640813827515 at epoch 95 at applicants training\n",
      "loss: 1.6311594247817993 at epoch 96 at applicants training\n",
      "loss: 1.635023593902588 at epoch 97 at applicants training\n",
      "loss: 1.6319360733032227 at epoch 98 at applicants training\n",
      "loss: 1.6309515237808228 at epoch 99 at applicants training\n",
      "loss: 1.6726248264312744 at epoch 0 at applicants training\n",
      "loss: 1.6735351085662842 at epoch 1 at applicants training\n",
      "loss: 1.6686142683029175 at epoch 2 at applicants training\n",
      "loss: 1.6659517288208008 at epoch 3 at applicants training\n",
      "loss: 1.6650617122650146 at epoch 4 at applicants training\n",
      "loss: 1.6621301174163818 at epoch 5 at applicants training\n",
      "loss: 1.6537801027297974 at epoch 6 at applicants training\n",
      "loss: 1.648467779159546 at epoch 7 at applicants training\n",
      "loss: 1.6482133865356445 at epoch 8 at applicants training\n",
      "loss: 1.6554114818572998 at epoch 9 at applicants training\n",
      "loss: 1.6437463760375977 at epoch 10 at applicants training\n",
      "loss: 1.6439942121505737 at epoch 11 at applicants training\n",
      "loss: 1.644374132156372 at epoch 12 at applicants training\n",
      "loss: 1.640468716621399 at epoch 13 at applicants training\n",
      "loss: 1.639830470085144 at epoch 14 at applicants training\n",
      "loss: 1.6411081552505493 at epoch 15 at applicants training\n",
      "loss: 1.6394262313842773 at epoch 16 at applicants training\n",
      "loss: 1.6338921785354614 at epoch 17 at applicants training\n",
      "loss: 1.6433117389678955 at epoch 18 at applicants training\n",
      "loss: 1.6411598920822144 at epoch 19 at applicants training\n",
      "loss: 1.639682412147522 at epoch 20 at applicants training\n",
      "loss: 1.6372108459472656 at epoch 21 at applicants training\n",
      "loss: 1.632163643836975 at epoch 22 at applicants training\n",
      "loss: 1.636060357093811 at epoch 23 at applicants training\n",
      "loss: 1.6321918964385986 at epoch 24 at applicants training\n",
      "loss: 1.6349190473556519 at epoch 25 at applicants training\n",
      "loss: 1.6301556825637817 at epoch 26 at applicants training\n",
      "loss: 1.6326749324798584 at epoch 27 at applicants training\n",
      "loss: 1.6288228034973145 at epoch 28 at applicants training\n",
      "loss: 1.6321423053741455 at epoch 29 at applicants training\n",
      "loss: 1.6265404224395752 at epoch 30 at applicants training\n",
      "loss: 1.6288706064224243 at epoch 31 at applicants training\n",
      "loss: 1.6253753900527954 at epoch 32 at applicants training\n",
      "loss: 1.6250754594802856 at epoch 33 at applicants training\n",
      "loss: 1.6257857084274292 at epoch 34 at applicants training\n",
      "loss: 1.621910572052002 at epoch 35 at applicants training\n",
      "loss: 1.6242426633834839 at epoch 36 at applicants training\n",
      "loss: 1.623808741569519 at epoch 37 at applicants training\n",
      "loss: 1.6187291145324707 at epoch 38 at applicants training\n",
      "loss: 1.6222288608551025 at epoch 39 at applicants training\n",
      "loss: 1.6260050535202026 at epoch 40 at applicants training\n",
      "loss: 1.6160856485366821 at epoch 41 at applicants training\n",
      "loss: 1.6151946783065796 at epoch 42 at applicants training\n",
      "loss: 1.624656081199646 at epoch 43 at applicants training\n",
      "loss: 1.6205650568008423 at epoch 44 at applicants training\n",
      "loss: 1.6146187782287598 at epoch 45 at applicants training\n",
      "loss: 1.6120245456695557 at epoch 46 at applicants training\n",
      "loss: 1.6148353815078735 at epoch 47 at applicants training\n",
      "loss: 1.6221104860305786 at epoch 48 at applicants training\n",
      "loss: 1.6140167713165283 at epoch 49 at applicants training\n",
      "loss: 1.6104127168655396 at epoch 50 at applicants training\n",
      "loss: 1.6136343479156494 at epoch 51 at applicants training\n",
      "loss: 1.6129404306411743 at epoch 52 at applicants training\n",
      "loss: 1.6095738410949707 at epoch 53 at applicants training\n",
      "loss: 1.6092357635498047 at epoch 54 at applicants training\n",
      "loss: 1.6113721132278442 at epoch 55 at applicants training\n",
      "loss: 1.6131932735443115 at epoch 56 at applicants training\n",
      "loss: 1.610144019126892 at epoch 57 at applicants training\n",
      "loss: 1.608332872390747 at epoch 58 at applicants training\n",
      "loss: 1.608949065208435 at epoch 59 at applicants training\n",
      "loss: 1.6094077825546265 at epoch 60 at applicants training\n",
      "loss: 1.6085045337677002 at epoch 61 at applicants training\n",
      "loss: 1.6065903902053833 at epoch 62 at applicants training\n",
      "loss: 1.6064239740371704 at epoch 63 at applicants training\n",
      "loss: 1.6074413061141968 at epoch 64 at applicants training\n",
      "loss: 1.6066526174545288 at epoch 65 at applicants training\n",
      "loss: 1.605326771736145 at epoch 66 at applicants training\n",
      "loss: 1.605326533317566 at epoch 67 at applicants training\n",
      "loss: 1.60603666305542 at epoch 68 at applicants training\n",
      "loss: 1.606018304824829 at epoch 69 at applicants training\n",
      "loss: 1.6048847436904907 at epoch 70 at applicants training\n",
      "loss: 1.604010820388794 at epoch 71 at applicants training\n",
      "loss: 1.6036734580993652 at epoch 72 at applicants training\n",
      "loss: 1.6036888360977173 at epoch 73 at applicants training\n",
      "loss: 1.6040620803833008 at epoch 74 at applicants training\n",
      "loss: 1.603825569152832 at epoch 75 at applicants training\n",
      "loss: 1.6031124591827393 at epoch 76 at applicants training\n",
      "loss: 1.602055311203003 at epoch 77 at applicants training\n",
      "loss: 1.6012694835662842 at epoch 78 at applicants training\n",
      "loss: 1.600853443145752 at epoch 79 at applicants training\n",
      "loss: 1.6006715297698975 at epoch 80 at applicants training\n",
      "loss: 1.6007325649261475 at epoch 81 at applicants training\n",
      "loss: 1.601198434829712 at epoch 82 at applicants training\n",
      "loss: 1.602298617362976 at epoch 83 at applicants training\n",
      "loss: 1.6020276546478271 at epoch 84 at applicants training\n",
      "loss: 1.601883888244629 at epoch 85 at applicants training\n",
      "loss: 1.6005939245224 at epoch 86 at applicants training\n",
      "loss: 1.5993192195892334 at epoch 87 at applicants training\n",
      "loss: 1.5981392860412598 at epoch 88 at applicants training\n",
      "loss: 1.596811294555664 at epoch 89 at applicants training\n",
      "loss: 1.5962501764297485 at epoch 90 at applicants training\n",
      "loss: 1.5962402820587158 at epoch 91 at applicants training\n",
      "loss: 1.5971074104309082 at epoch 92 at applicants training\n",
      "loss: 1.5992540121078491 at epoch 93 at applicants training\n",
      "loss: 1.6009068489074707 at epoch 94 at applicants training\n",
      "loss: 1.599091649055481 at epoch 95 at applicants training\n",
      "loss: 1.5959985256195068 at epoch 96 at applicants training\n",
      "loss: 1.5943315029144287 at epoch 97 at applicants training\n",
      "loss: 1.593620777130127 at epoch 98 at applicants training\n",
      "loss: 1.5948799848556519 at epoch 99 at applicants training\n",
      "loss: 1.678870439529419 at epoch 0 at applicants training\n",
      "loss: 1.6912630796432495 at epoch 1 at applicants training\n",
      "loss: 1.685112714767456 at epoch 2 at applicants training\n",
      "loss: 1.6749329566955566 at epoch 3 at applicants training\n",
      "loss: 1.6490190029144287 at epoch 4 at applicants training\n",
      "loss: 1.656833529472351 at epoch 5 at applicants training\n",
      "loss: 1.6965240240097046 at epoch 6 at applicants training\n",
      "loss: 1.66120183467865 at epoch 7 at applicants training\n",
      "loss: 1.6767572164535522 at epoch 8 at applicants training\n",
      "loss: 1.6788051128387451 at epoch 9 at applicants training\n",
      "loss: 1.6788311004638672 at epoch 10 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 11 at applicants training\n",
      "loss: 1.678832769393921 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.6963443756103516 at epoch 0 at applicants training\n",
      "loss: 1.6907269954681396 at epoch 1 at applicants training\n",
      "loss: 1.6899833679199219 at epoch 2 at applicants training\n",
      "loss: 1.691431999206543 at epoch 3 at applicants training\n",
      "loss: 1.6898804903030396 at epoch 4 at applicants training\n",
      "loss: 1.688804030418396 at epoch 5 at applicants training\n",
      "loss: 1.6868339776992798 at epoch 6 at applicants training\n",
      "loss: 1.6857595443725586 at epoch 7 at applicants training\n",
      "loss: 1.6825730800628662 at epoch 8 at applicants training\n",
      "loss: 1.6837254762649536 at epoch 9 at applicants training\n",
      "loss: 1.679141640663147 at epoch 10 at applicants training\n",
      "loss: 1.6796127557754517 at epoch 11 at applicants training\n",
      "loss: 1.6787501573562622 at epoch 12 at applicants training\n",
      "loss: 1.6765800714492798 at epoch 13 at applicants training\n",
      "loss: 1.6761277914047241 at epoch 14 at applicants training\n",
      "loss: 1.6765117645263672 at epoch 15 at applicants training\n",
      "loss: 1.675370216369629 at epoch 16 at applicants training\n",
      "loss: 1.6711745262145996 at epoch 17 at applicants training\n",
      "loss: 1.6732228994369507 at epoch 18 at applicants training\n",
      "loss: 1.6753300428390503 at epoch 19 at applicants training\n",
      "loss: 1.6743345260620117 at epoch 20 at applicants training\n",
      "loss: 1.6685608625411987 at epoch 21 at applicants training\n",
      "loss: 1.672357201576233 at epoch 22 at applicants training\n",
      "loss: 1.6677790880203247 at epoch 23 at applicants training\n",
      "loss: 1.6691635847091675 at epoch 24 at applicants training\n",
      "loss: 1.6689549684524536 at epoch 25 at applicants training\n",
      "loss: 1.668373465538025 at epoch 26 at applicants training\n",
      "loss: 1.6668440103530884 at epoch 27 at applicants training\n",
      "loss: 1.6684155464172363 at epoch 28 at applicants training\n",
      "loss: 1.6657804250717163 at epoch 29 at applicants training\n",
      "loss: 1.6666862964630127 at epoch 30 at applicants training\n",
      "loss: 1.6666098833084106 at epoch 31 at applicants training\n",
      "loss: 1.6659599542617798 at epoch 32 at applicants training\n",
      "loss: 1.6646066904067993 at epoch 33 at applicants training\n",
      "loss: 1.6655548810958862 at epoch 34 at applicants training\n",
      "loss: 1.6642804145812988 at epoch 35 at applicants training\n",
      "loss: 1.6645464897155762 at epoch 36 at applicants training\n",
      "loss: 1.6640827655792236 at epoch 37 at applicants training\n",
      "loss: 1.662801742553711 at epoch 38 at applicants training\n",
      "loss: 1.663504719734192 at epoch 39 at applicants training\n",
      "loss: 1.6629137992858887 at epoch 40 at applicants training\n",
      "loss: 1.662835955619812 at epoch 41 at applicants training\n",
      "loss: 1.6624979972839355 at epoch 42 at applicants training\n",
      "loss: 1.6612215042114258 at epoch 43 at applicants training\n",
      "loss: 1.6618419885635376 at epoch 44 at applicants training\n",
      "loss: 1.6616955995559692 at epoch 45 at applicants training\n",
      "loss: 1.6611405611038208 at epoch 46 at applicants training\n",
      "loss: 1.659919261932373 at epoch 47 at applicants training\n",
      "loss: 1.6607050895690918 at epoch 48 at applicants training\n",
      "loss: 1.6613496541976929 at epoch 49 at applicants training\n",
      "loss: 1.6631484031677246 at epoch 50 at applicants training\n",
      "loss: 1.6614844799041748 at epoch 51 at applicants training\n",
      "loss: 1.6597604751586914 at epoch 52 at applicants training\n",
      "loss: 1.6613199710845947 at epoch 53 at applicants training\n",
      "loss: 1.6591242551803589 at epoch 54 at applicants training\n",
      "loss: 1.6591897010803223 at epoch 55 at applicants training\n",
      "loss: 1.6613671779632568 at epoch 56 at applicants training\n",
      "loss: 1.6578774452209473 at epoch 57 at applicants training\n",
      "loss: 1.6571563482284546 at epoch 58 at applicants training\n",
      "loss: 1.6584279537200928 at epoch 59 at applicants training\n",
      "loss: 1.6566338539123535 at epoch 60 at applicants training\n",
      "loss: 1.6557044982910156 at epoch 61 at applicants training\n",
      "loss: 1.6560170650482178 at epoch 62 at applicants training\n",
      "loss: 1.6590497493743896 at epoch 63 at applicants training\n",
      "loss: 1.664083480834961 at epoch 64 at applicants training\n",
      "loss: 1.6536232233047485 at epoch 65 at applicants training\n",
      "loss: 1.665658712387085 at epoch 66 at applicants training\n",
      "loss: 1.6589211225509644 at epoch 67 at applicants training\n",
      "loss: 1.6571675539016724 at epoch 68 at applicants training\n",
      "loss: 1.660735845565796 at epoch 69 at applicants training\n",
      "loss: 1.653119683265686 at epoch 70 at applicants training\n",
      "loss: 1.6604318618774414 at epoch 71 at applicants training\n",
      "loss: 1.6523418426513672 at epoch 72 at applicants training\n",
      "loss: 1.6577717065811157 at epoch 73 at applicants training\n",
      "loss: 1.6556005477905273 at epoch 74 at applicants training\n",
      "loss: 1.6520644426345825 at epoch 75 at applicants training\n",
      "loss: 1.65478515625 at epoch 76 at applicants training\n",
      "loss: 1.6508585214614868 at epoch 77 at applicants training\n",
      "loss: 1.6527221202850342 at epoch 78 at applicants training\n",
      "loss: 1.6539584398269653 at epoch 79 at applicants training\n",
      "loss: 1.649366021156311 at epoch 80 at applicants training\n",
      "loss: 1.6533613204956055 at epoch 81 at applicants training\n",
      "loss: 1.6491808891296387 at epoch 82 at applicants training\n",
      "loss: 1.6500147581100464 at epoch 83 at applicants training\n",
      "loss: 1.6498993635177612 at epoch 84 at applicants training\n",
      "loss: 1.6479923725128174 at epoch 85 at applicants training\n",
      "loss: 1.6480120420455933 at epoch 86 at applicants training\n",
      "loss: 1.6478434801101685 at epoch 87 at applicants training\n",
      "loss: 1.6468700170516968 at epoch 88 at applicants training\n",
      "loss: 1.646679162979126 at epoch 89 at applicants training\n",
      "loss: 1.6464897394180298 at epoch 90 at applicants training\n",
      "loss: 1.6464349031448364 at epoch 91 at applicants training\n",
      "loss: 1.645117998123169 at epoch 92 at applicants training\n",
      "loss: 1.6450692415237427 at epoch 93 at applicants training\n",
      "loss: 1.6442267894744873 at epoch 94 at applicants training\n",
      "loss: 1.6450059413909912 at epoch 95 at applicants training\n",
      "loss: 1.6449246406555176 at epoch 96 at applicants training\n",
      "loss: 1.6449470520019531 at epoch 97 at applicants training\n",
      "loss: 1.64432692527771 at epoch 98 at applicants training\n",
      "loss: 1.6427836418151855 at epoch 99 at applicants training\n",
      "loss: 1.688157558441162 at epoch 0 at applicants training\n",
      "loss: 1.67861807346344 at epoch 1 at applicants training\n",
      "loss: 1.6788330078125 at epoch 2 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 3 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 4 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.7069486379623413 at epoch 0 at applicants training\n",
      "loss: 1.7178595066070557 at epoch 1 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 2 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 3 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 4 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 5 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 6 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 7 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 8 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 9 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 10 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 11 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 12 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 13 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 14 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 15 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 16 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 17 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 18 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 19 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 20 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 21 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 22 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 23 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 24 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 25 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 26 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 27 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 28 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 29 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 30 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 31 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 32 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 33 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 34 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 35 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 36 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 37 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 38 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 39 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 40 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 41 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 42 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 43 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 44 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 45 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 46 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 47 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 48 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 49 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 50 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 51 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 52 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 53 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 54 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 55 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 56 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 57 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 58 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 59 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 60 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 61 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 62 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 63 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 64 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 65 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 66 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 67 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 68 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 69 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 70 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 71 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 72 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 73 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 74 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 75 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 76 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 77 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 78 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 79 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 80 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 81 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 82 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 83 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 84 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 85 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 86 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 87 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 88 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 89 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 90 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 91 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 92 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 93 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 94 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 95 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 96 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 97 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 98 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 99 at applicants training\n",
      "loss: 1.6827809810638428 at epoch 0 at applicants training\n",
      "loss: 1.677703619003296 at epoch 1 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 2 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 3 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 4 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.6884998083114624 at epoch 0 at applicants training\n",
      "loss: 1.661435842514038 at epoch 1 at applicants training\n",
      "loss: 1.6773662567138672 at epoch 2 at applicants training\n",
      "loss: 1.6642550230026245 at epoch 3 at applicants training\n",
      "loss: 1.6541389226913452 at epoch 4 at applicants training\n",
      "loss: 1.65333092212677 at epoch 5 at applicants training\n",
      "loss: 1.6598471403121948 at epoch 6 at applicants training\n",
      "loss: 1.649093508720398 at epoch 7 at applicants training\n",
      "loss: 1.6652276515960693 at epoch 8 at applicants training\n",
      "loss: 1.666607141494751 at epoch 9 at applicants training\n",
      "loss: 1.6487443447113037 at epoch 10 at applicants training\n",
      "loss: 1.660767674446106 at epoch 11 at applicants training\n",
      "loss: 1.6669299602508545 at epoch 12 at applicants training\n",
      "loss: 1.6632533073425293 at epoch 13 at applicants training\n",
      "loss: 1.6508989334106445 at epoch 14 at applicants training\n",
      "loss: 1.654168963432312 at epoch 15 at applicants training\n",
      "loss: 1.6588127613067627 at epoch 16 at applicants training\n",
      "loss: 1.6452915668487549 at epoch 17 at applicants training\n",
      "loss: 1.6475701332092285 at epoch 18 at applicants training\n",
      "loss: 1.6516931056976318 at epoch 19 at applicants training\n",
      "loss: 1.6489369869232178 at epoch 20 at applicants training\n",
      "loss: 1.6447817087173462 at epoch 21 at applicants training\n",
      "loss: 1.6457947492599487 at epoch 22 at applicants training\n",
      "loss: 1.6498128175735474 at epoch 23 at applicants training\n",
      "loss: 1.6444134712219238 at epoch 24 at applicants training\n",
      "loss: 1.6386033296585083 at epoch 25 at applicants training\n",
      "loss: 1.647707462310791 at epoch 26 at applicants training\n",
      "loss: 1.6430491209030151 at epoch 27 at applicants training\n",
      "loss: 1.6406790018081665 at epoch 28 at applicants training\n",
      "loss: 1.6444828510284424 at epoch 29 at applicants training\n",
      "loss: 1.6359553337097168 at epoch 30 at applicants training\n",
      "loss: 1.640418529510498 at epoch 31 at applicants training\n",
      "loss: 1.6418746709823608 at epoch 32 at applicants training\n",
      "loss: 1.6343061923980713 at epoch 33 at applicants training\n",
      "loss: 1.6430848836898804 at epoch 34 at applicants training\n",
      "loss: 1.6397581100463867 at epoch 35 at applicants training\n",
      "loss: 1.6340305805206299 at epoch 36 at applicants training\n",
      "loss: 1.6406718492507935 at epoch 37 at applicants training\n",
      "loss: 1.6333556175231934 at epoch 38 at applicants training\n",
      "loss: 1.6339762210845947 at epoch 39 at applicants training\n",
      "loss: 1.6378116607666016 at epoch 40 at applicants training\n",
      "loss: 1.633216142654419 at epoch 41 at applicants training\n",
      "loss: 1.6328352689743042 at epoch 42 at applicants training\n",
      "loss: 1.6372761726379395 at epoch 43 at applicants training\n",
      "loss: 1.6315251588821411 at epoch 44 at applicants training\n",
      "loss: 1.6345754861831665 at epoch 45 at applicants training\n",
      "loss: 1.634516716003418 at epoch 46 at applicants training\n",
      "loss: 1.6314836740493774 at epoch 47 at applicants training\n",
      "loss: 1.6321947574615479 at epoch 48 at applicants training\n",
      "loss: 1.6329540014266968 at epoch 49 at applicants training\n",
      "loss: 1.6311291456222534 at epoch 50 at applicants training\n",
      "loss: 1.6334989070892334 at epoch 51 at applicants training\n",
      "loss: 1.630948781967163 at epoch 52 at applicants training\n",
      "loss: 1.63037109375 at epoch 53 at applicants training\n",
      "loss: 1.6299632787704468 at epoch 54 at applicants training\n",
      "loss: 1.6285916566848755 at epoch 55 at applicants training\n",
      "loss: 1.6293264627456665 at epoch 56 at applicants training\n",
      "loss: 1.6263928413391113 at epoch 57 at applicants training\n",
      "loss: 1.6300524473190308 at epoch 58 at applicants training\n",
      "loss: 1.6309113502502441 at epoch 59 at applicants training\n",
      "loss: 1.6266597509384155 at epoch 60 at applicants training\n",
      "loss: 1.6339188814163208 at epoch 61 at applicants training\n",
      "loss: 1.628285527229309 at epoch 62 at applicants training\n",
      "loss: 1.6286741495132446 at epoch 63 at applicants training\n",
      "loss: 1.6307884454727173 at epoch 64 at applicants training\n",
      "loss: 1.6234540939331055 at epoch 65 at applicants training\n",
      "loss: 1.6268764734268188 at epoch 66 at applicants training\n",
      "loss: 1.6233766078948975 at epoch 67 at applicants training\n",
      "loss: 1.623978853225708 at epoch 68 at applicants training\n",
      "loss: 1.6251146793365479 at epoch 69 at applicants training\n",
      "loss: 1.6219266653060913 at epoch 70 at applicants training\n",
      "loss: 1.624810814857483 at epoch 71 at applicants training\n",
      "loss: 1.6228246688842773 at epoch 72 at applicants training\n",
      "loss: 1.6212995052337646 at epoch 73 at applicants training\n",
      "loss: 1.6241754293441772 at epoch 74 at applicants training\n",
      "loss: 1.6219515800476074 at epoch 75 at applicants training\n",
      "loss: 1.6205778121948242 at epoch 76 at applicants training\n",
      "loss: 1.6229177713394165 at epoch 77 at applicants training\n",
      "loss: 1.6212129592895508 at epoch 78 at applicants training\n",
      "loss: 1.619936227798462 at epoch 79 at applicants training\n",
      "loss: 1.6215099096298218 at epoch 80 at applicants training\n",
      "loss: 1.6209397315979004 at epoch 81 at applicants training\n",
      "loss: 1.6191798448562622 at epoch 82 at applicants training\n",
      "loss: 1.6196074485778809 at epoch 83 at applicants training\n",
      "loss: 1.6207607984542847 at epoch 84 at applicants training\n",
      "loss: 1.6196280717849731 at epoch 85 at applicants training\n",
      "loss: 1.6184271574020386 at epoch 86 at applicants training\n",
      "loss: 1.618463158607483 at epoch 87 at applicants training\n",
      "loss: 1.6191432476043701 at epoch 88 at applicants training\n",
      "loss: 1.6188738346099854 at epoch 89 at applicants training\n",
      "loss: 1.6179826259613037 at epoch 90 at applicants training\n",
      "loss: 1.6173810958862305 at epoch 91 at applicants training\n",
      "loss: 1.617542028427124 at epoch 92 at applicants training\n",
      "loss: 1.6179877519607544 at epoch 93 at applicants training\n",
      "loss: 1.618037223815918 at epoch 94 at applicants training\n",
      "loss: 1.617360234260559 at epoch 95 at applicants training\n",
      "loss: 1.616625189781189 at epoch 96 at applicants training\n",
      "loss: 1.6163625717163086 at epoch 97 at applicants training\n",
      "loss: 1.6164668798446655 at epoch 98 at applicants training\n",
      "loss: 1.6166934967041016 at epoch 99 at applicants training\n",
      "loss: 1.6791253089904785 at epoch 0 at applicants training\n",
      "loss: 1.7136892080307007 at epoch 1 at applicants training\n",
      "loss: 1.711409091949463 at epoch 2 at applicants training\n",
      "loss: 1.687072992324829 at epoch 3 at applicants training\n",
      "loss: 1.6982736587524414 at epoch 4 at applicants training\n",
      "loss: 1.685013771057129 at epoch 5 at applicants training\n",
      "loss: 1.7056772708892822 at epoch 6 at applicants training\n",
      "loss: 1.7104244232177734 at epoch 7 at applicants training\n",
      "loss: 1.702723741531372 at epoch 8 at applicants training\n",
      "loss: 1.682525873184204 at epoch 9 at applicants training\n",
      "loss: 1.7000497579574585 at epoch 10 at applicants training\n",
      "loss: 1.710274338722229 at epoch 11 at applicants training\n",
      "loss: 1.7120479345321655 at epoch 12 at applicants training\n",
      "loss: 1.7124840021133423 at epoch 13 at applicants training\n",
      "loss: 1.7126553058624268 at epoch 14 at applicants training\n",
      "loss: 1.7127326726913452 at epoch 15 at applicants training\n",
      "loss: 1.7127708196640015 at epoch 16 at applicants training\n",
      "loss: 1.7127913236618042 at epoch 17 at applicants training\n",
      "loss: 1.7128033638000488 at epoch 18 at applicants training\n",
      "loss: 1.7128105163574219 at epoch 19 at applicants training\n",
      "loss: 1.7128154039382935 at epoch 20 at applicants training\n",
      "loss: 1.7128187417984009 at epoch 21 at applicants training\n",
      "loss: 1.7128210067749023 at epoch 22 at applicants training\n",
      "loss: 1.712822675704956 at epoch 23 at applicants training\n",
      "loss: 1.7128238677978516 at epoch 24 at applicants training\n",
      "loss: 1.712824821472168 at epoch 25 at applicants training\n",
      "loss: 1.7128254175186157 at epoch 26 at applicants training\n",
      "loss: 1.7128260135650635 at epoch 27 at applicants training\n",
      "loss: 1.7128263711929321 at epoch 28 at applicants training\n",
      "loss: 1.7128267288208008 at epoch 29 at applicants training\n",
      "loss: 1.7128269672393799 at epoch 30 at applicants training\n",
      "loss: 1.7128273248672485 at epoch 31 at applicants training\n",
      "loss: 1.712827444076538 at epoch 32 at applicants training\n",
      "loss: 1.712827205657959 at epoch 33 at applicants training\n",
      "loss: 1.712824821472168 at epoch 34 at applicants training\n",
      "loss: 1.712805151939392 at epoch 35 at applicants training\n",
      "loss: 1.712710976600647 at epoch 36 at applicants training\n",
      "loss: 1.7126250267028809 at epoch 37 at applicants training\n",
      "loss: 1.7124334573745728 at epoch 38 at applicants training\n",
      "loss: 1.712283968925476 at epoch 39 at applicants training\n",
      "loss: 1.7119923830032349 at epoch 40 at applicants training\n",
      "loss: 1.7116035223007202 at epoch 41 at applicants training\n",
      "loss: 1.710920810699463 at epoch 42 at applicants training\n",
      "loss: 1.7094464302062988 at epoch 43 at applicants training\n",
      "loss: 1.7015846967697144 at epoch 44 at applicants training\n",
      "loss: 1.681080937385559 at epoch 45 at applicants training\n",
      "loss: 1.692657232284546 at epoch 46 at applicants training\n",
      "loss: 1.6938352584838867 at epoch 47 at applicants training\n",
      "loss: 1.6938327550888062 at epoch 48 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 49 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 50 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 51 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 52 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 53 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 54 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 55 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 56 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 57 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 58 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 59 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 60 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 61 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 62 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 63 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 64 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 65 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 66 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 67 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 68 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 69 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 70 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 71 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 72 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 73 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 74 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 75 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 76 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 77 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 78 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 79 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 80 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 81 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 82 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 83 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 84 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 85 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 86 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 87 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 88 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 89 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 90 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 91 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 92 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 93 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 94 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 95 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 96 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 97 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 98 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 99 at applicants training\n",
      "loss: 1.7212040424346924 at epoch 0 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 1 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 2 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 3 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 4 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 5 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 6 at applicants training\n",
      "loss: 1.7208324670791626 at epoch 7 at applicants training\n",
      "loss: 1.7208267450332642 at epoch 8 at applicants training\n",
      "loss: 1.720642328262329 at epoch 9 at applicants training\n",
      "loss: 1.7088841199874878 at epoch 10 at applicants training\n",
      "loss: 1.6763273477554321 at epoch 11 at applicants training\n",
      "loss: 1.6909033060073853 at epoch 12 at applicants training\n",
      "loss: 1.6930314302444458 at epoch 13 at applicants training\n",
      "loss: 1.6932227611541748 at epoch 14 at applicants training\n",
      "loss: 1.6925381422042847 at epoch 15 at applicants training\n",
      "loss: 1.6916098594665527 at epoch 16 at applicants training\n",
      "loss: 1.6854475736618042 at epoch 17 at applicants training\n",
      "loss: 1.6841944456100464 at epoch 18 at applicants training\n",
      "loss: 1.6850900650024414 at epoch 19 at applicants training\n",
      "loss: 1.6872544288635254 at epoch 20 at applicants training\n",
      "loss: 1.678402066230774 at epoch 21 at applicants training\n",
      "loss: 1.6818946599960327 at epoch 22 at applicants training\n",
      "loss: 1.6866850852966309 at epoch 23 at applicants training\n",
      "loss: 1.6861271858215332 at epoch 24 at applicants training\n",
      "loss: 1.679944634437561 at epoch 25 at applicants training\n",
      "loss: 1.6768244504928589 at epoch 26 at applicants training\n",
      "loss: 1.676117181777954 at epoch 27 at applicants training\n",
      "loss: 1.6853405237197876 at epoch 28 at applicants training\n",
      "loss: 1.6763792037963867 at epoch 29 at applicants training\n",
      "loss: 1.6778231859207153 at epoch 30 at applicants training\n",
      "loss: 1.6760183572769165 at epoch 31 at applicants training\n",
      "loss: 1.676813006401062 at epoch 32 at applicants training\n",
      "loss: 1.6792795658111572 at epoch 33 at applicants training\n",
      "loss: 1.6782798767089844 at epoch 34 at applicants training\n",
      "loss: 1.6763626337051392 at epoch 35 at applicants training\n",
      "loss: 1.6750457286834717 at epoch 36 at applicants training\n",
      "loss: 1.6766916513442993 at epoch 37 at applicants training\n",
      "loss: 1.6765638589859009 at epoch 38 at applicants training\n",
      "loss: 1.6758456230163574 at epoch 39 at applicants training\n",
      "loss: 1.6750837564468384 at epoch 40 at applicants training\n",
      "loss: 1.6731313467025757 at epoch 41 at applicants training\n",
      "loss: 1.669334053993225 at epoch 42 at applicants training\n",
      "loss: 1.669027328491211 at epoch 43 at applicants training\n",
      "loss: 1.6669790744781494 at epoch 44 at applicants training\n",
      "loss: 1.6631953716278076 at epoch 45 at applicants training\n",
      "loss: 1.663839340209961 at epoch 46 at applicants training\n",
      "loss: 1.657493233680725 at epoch 47 at applicants training\n",
      "loss: 1.6573015451431274 at epoch 48 at applicants training\n",
      "loss: 1.6525737047195435 at epoch 49 at applicants training\n",
      "loss: 1.6509640216827393 at epoch 50 at applicants training\n",
      "loss: 1.6516530513763428 at epoch 51 at applicants training\n",
      "loss: 1.6500275135040283 at epoch 52 at applicants training\n",
      "loss: 1.6407045125961304 at epoch 53 at applicants training\n",
      "loss: 1.652237057685852 at epoch 54 at applicants training\n",
      "loss: 1.644539475440979 at epoch 55 at applicants training\n",
      "loss: 1.6474026441574097 at epoch 56 at applicants training\n",
      "loss: 1.645807147026062 at epoch 57 at applicants training\n",
      "loss: 1.638390302658081 at epoch 58 at applicants training\n",
      "loss: 1.6421833038330078 at epoch 59 at applicants training\n",
      "loss: 1.6329364776611328 at epoch 60 at applicants training\n",
      "loss: 1.6352635622024536 at epoch 61 at applicants training\n",
      "loss: 1.6373615264892578 at epoch 62 at applicants training\n",
      "loss: 1.6284797191619873 at epoch 63 at applicants training\n",
      "loss: 1.6292837858200073 at epoch 64 at applicants training\n",
      "loss: 1.627431035041809 at epoch 65 at applicants training\n",
      "loss: 1.6255515813827515 at epoch 66 at applicants training\n",
      "loss: 1.626541018486023 at epoch 67 at applicants training\n",
      "loss: 1.621494174003601 at epoch 68 at applicants training\n",
      "loss: 1.6198590993881226 at epoch 69 at applicants training\n",
      "loss: 1.622390627861023 at epoch 70 at applicants training\n",
      "loss: 1.6188057661056519 at epoch 71 at applicants training\n",
      "loss: 1.6226338148117065 at epoch 72 at applicants training\n",
      "loss: 1.6184308528900146 at epoch 73 at applicants training\n",
      "loss: 1.6155182123184204 at epoch 74 at applicants training\n",
      "loss: 1.6178982257843018 at epoch 75 at applicants training\n",
      "loss: 1.614916205406189 at epoch 76 at applicants training\n",
      "loss: 1.6175858974456787 at epoch 77 at applicants training\n",
      "loss: 1.613290786743164 at epoch 78 at applicants training\n",
      "loss: 1.613450527191162 at epoch 79 at applicants training\n",
      "loss: 1.6141409873962402 at epoch 80 at applicants training\n",
      "loss: 1.6112979650497437 at epoch 81 at applicants training\n",
      "loss: 1.617471694946289 at epoch 82 at applicants training\n",
      "loss: 1.611121416091919 at epoch 83 at applicants training\n",
      "loss: 1.6151220798492432 at epoch 84 at applicants training\n",
      "loss: 1.6091359853744507 at epoch 85 at applicants training\n",
      "loss: 1.6145228147506714 at epoch 86 at applicants training\n",
      "loss: 1.6133081912994385 at epoch 87 at applicants training\n",
      "loss: 1.606785535812378 at epoch 88 at applicants training\n",
      "loss: 1.6132639646530151 at epoch 89 at applicants training\n",
      "loss: 1.6070560216903687 at epoch 90 at applicants training\n",
      "loss: 1.6077227592468262 at epoch 91 at applicants training\n",
      "loss: 1.6071103811264038 at epoch 92 at applicants training\n",
      "loss: 1.607550024986267 at epoch 93 at applicants training\n",
      "loss: 1.6057220697402954 at epoch 94 at applicants training\n",
      "loss: 1.6056544780731201 at epoch 95 at applicants training\n",
      "loss: 1.6105791330337524 at epoch 96 at applicants training\n",
      "loss: 1.6098862886428833 at epoch 97 at applicants training\n",
      "loss: 1.606950044631958 at epoch 98 at applicants training\n",
      "loss: 1.608435034751892 at epoch 99 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 0 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 1 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 2 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 3 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 4 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 5 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 6 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 7 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 8 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 9 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 10 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 11 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 12 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 13 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 14 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 15 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 16 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 17 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 18 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 19 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 20 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 21 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 22 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 23 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 24 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 25 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 26 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 27 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 28 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 29 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 30 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 31 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 32 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 33 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 34 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 35 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 36 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 37 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 38 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 39 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 40 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 41 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 42 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 43 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 44 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 45 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 46 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 47 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 48 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 49 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 50 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 51 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 52 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 53 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 54 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 55 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 56 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 57 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 58 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 59 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 60 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 61 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 62 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 63 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 64 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 65 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 66 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 67 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 68 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 69 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 70 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 71 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 72 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 73 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 74 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 75 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 76 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 77 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 78 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 79 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 80 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 81 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 82 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 83 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 84 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 85 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 86 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 87 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 88 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 89 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 90 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 91 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 92 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 93 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 94 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 95 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 96 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 97 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 98 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 99 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 0 at applicants training\n",
      "loss: 1.697886347770691 at epoch 1 at applicants training\n",
      "loss: 1.6770906448364258 at epoch 2 at applicants training\n",
      "loss: 1.676464557647705 at epoch 3 at applicants training\n",
      "loss: 1.6738237142562866 at epoch 4 at applicants training\n",
      "loss: 1.6715080738067627 at epoch 5 at applicants training\n",
      "loss: 1.6679874658584595 at epoch 6 at applicants training\n",
      "loss: 1.6613215208053589 at epoch 7 at applicants training\n",
      "loss: 1.656502604484558 at epoch 8 at applicants training\n",
      "loss: 1.6739740371704102 at epoch 9 at applicants training\n",
      "loss: 1.664742112159729 at epoch 10 at applicants training\n",
      "loss: 1.6777842044830322 at epoch 11 at applicants training\n",
      "loss: 1.6787868738174438 at epoch 12 at applicants training\n",
      "loss: 1.678830862045288 at epoch 13 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.7005549669265747 at epoch 0 at applicants training\n",
      "loss: 1.6785427331924438 at epoch 1 at applicants training\n",
      "loss: 1.6788333654403687 at epoch 2 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 3 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 4 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 7 at applicants training\n",
      "loss: 1.6788318157196045 at epoch 8 at applicants training\n",
      "loss: 1.6788285970687866 at epoch 9 at applicants training\n",
      "loss: 1.6788088083267212 at epoch 10 at applicants training\n",
      "loss: 1.6786171197891235 at epoch 11 at applicants training\n",
      "loss: 1.6767680644989014 at epoch 12 at applicants training\n",
      "loss: 1.6740734577178955 at epoch 13 at applicants training\n",
      "loss: 1.6757245063781738 at epoch 14 at applicants training\n",
      "loss: 1.6767817735671997 at epoch 15 at applicants training\n",
      "loss: 1.6741578578948975 at epoch 16 at applicants training\n",
      "loss: 1.6794490814208984 at epoch 17 at applicants training\n",
      "loss: 1.674344539642334 at epoch 18 at applicants training\n",
      "loss: 1.6778041124343872 at epoch 19 at applicants training\n",
      "loss: 1.6785197257995605 at epoch 20 at applicants training\n",
      "loss: 1.6787300109863281 at epoch 21 at applicants training\n",
      "loss: 1.6787934303283691 at epoch 22 at applicants training\n",
      "loss: 1.6788157224655151 at epoch 23 at applicants training\n",
      "loss: 1.6788246631622314 at epoch 24 at applicants training\n",
      "loss: 1.6788287162780762 at epoch 25 at applicants training\n",
      "loss: 1.6788303852081299 at epoch 26 at applicants training\n",
      "loss: 1.6788313388824463 at epoch 27 at applicants training\n",
      "loss: 1.678831934928894 at epoch 28 at applicants training\n",
      "loss: 1.6788321733474731 at epoch 29 at applicants training\n",
      "loss: 1.6788322925567627 at epoch 30 at applicants training\n",
      "loss: 1.6788324117660522 at epoch 31 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 32 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 33 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 34 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.687516212463379 at epoch 0 at applicants training\n",
      "loss: 1.6781553030014038 at epoch 1 at applicants training\n",
      "loss: 1.6751629114151 at epoch 2 at applicants training\n",
      "loss: 1.6647614240646362 at epoch 3 at applicants training\n",
      "loss: 1.694932222366333 at epoch 4 at applicants training\n",
      "loss: 1.6912075281143188 at epoch 5 at applicants training\n",
      "loss: 1.6643919944763184 at epoch 6 at applicants training\n",
      "loss: 1.6708414554595947 at epoch 7 at applicants training\n",
      "loss: 1.6785694360733032 at epoch 8 at applicants training\n",
      "loss: 1.6791300773620605 at epoch 9 at applicants training\n",
      "loss: 1.678852915763855 at epoch 10 at applicants training\n",
      "loss: 1.6788314580917358 at epoch 11 at applicants training\n",
      "loss: 1.678831696510315 at epoch 12 at applicants training\n",
      "loss: 1.6788322925567627 at epoch 13 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 14 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 15 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 16 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.7013274431228638 at epoch 0 at applicants training\n",
      "loss: 1.6942015886306763 at epoch 1 at applicants training\n",
      "loss: 1.6927101612091064 at epoch 2 at applicants training\n",
      "loss: 1.6885184049606323 at epoch 3 at applicants training\n",
      "loss: 1.6894335746765137 at epoch 4 at applicants training\n",
      "loss: 1.6858618259429932 at epoch 5 at applicants training\n",
      "loss: 1.685429334640503 at epoch 6 at applicants training\n",
      "loss: 1.6824150085449219 at epoch 7 at applicants training\n",
      "loss: 1.6667985916137695 at epoch 8 at applicants training\n",
      "loss: 1.66360342502594 at epoch 9 at applicants training\n",
      "loss: 1.6624746322631836 at epoch 10 at applicants training\n",
      "loss: 1.65281081199646 at epoch 11 at applicants training\n",
      "loss: 1.665669322013855 at epoch 12 at applicants training\n",
      "loss: 1.6522024869918823 at epoch 13 at applicants training\n",
      "loss: 1.6587879657745361 at epoch 14 at applicants training\n",
      "loss: 1.6509616374969482 at epoch 15 at applicants training\n",
      "loss: 1.635378122329712 at epoch 16 at applicants training\n",
      "loss: 1.6628412008285522 at epoch 17 at applicants training\n",
      "loss: 1.6455007791519165 at epoch 18 at applicants training\n",
      "loss: 1.637915849685669 at epoch 19 at applicants training\n",
      "loss: 1.6479511260986328 at epoch 20 at applicants training\n",
      "loss: 1.6461448669433594 at epoch 21 at applicants training\n",
      "loss: 1.6491602659225464 at epoch 22 at applicants training\n",
      "loss: 1.6442080736160278 at epoch 23 at applicants training\n",
      "loss: 1.6429766416549683 at epoch 24 at applicants training\n",
      "loss: 1.63872230052948 at epoch 25 at applicants training\n",
      "loss: 1.6361660957336426 at epoch 26 at applicants training\n",
      "loss: 1.6315151453018188 at epoch 27 at applicants training\n",
      "loss: 1.6241637468338013 at epoch 28 at applicants training\n",
      "loss: 1.6253613233566284 at epoch 29 at applicants training\n",
      "loss: 1.629388451576233 at epoch 30 at applicants training\n",
      "loss: 1.621559739112854 at epoch 31 at applicants training\n",
      "loss: 1.619950771331787 at epoch 32 at applicants training\n",
      "loss: 1.622246503829956 at epoch 33 at applicants training\n",
      "loss: 1.6202391386032104 at epoch 34 at applicants training\n",
      "loss: 1.6174486875534058 at epoch 35 at applicants training\n",
      "loss: 1.6142749786376953 at epoch 36 at applicants training\n",
      "loss: 1.6109675168991089 at epoch 37 at applicants training\n",
      "loss: 1.6107789278030396 at epoch 38 at applicants training\n",
      "loss: 1.6103105545043945 at epoch 39 at applicants training\n",
      "loss: 1.6089271306991577 at epoch 40 at applicants training\n",
      "loss: 1.6072009801864624 at epoch 41 at applicants training\n",
      "loss: 1.6037851572036743 at epoch 42 at applicants training\n",
      "loss: 1.6038050651550293 at epoch 43 at applicants training\n",
      "loss: 1.6017531156539917 at epoch 44 at applicants training\n",
      "loss: 1.6029605865478516 at epoch 45 at applicants training\n",
      "loss: 1.6006686687469482 at epoch 46 at applicants training\n",
      "loss: 1.5992650985717773 at epoch 47 at applicants training\n",
      "loss: 1.597915530204773 at epoch 48 at applicants training\n",
      "loss: 1.5980281829833984 at epoch 49 at applicants training\n",
      "loss: 1.597219705581665 at epoch 50 at applicants training\n",
      "loss: 1.5963681936264038 at epoch 51 at applicants training\n",
      "loss: 1.5958040952682495 at epoch 52 at applicants training\n",
      "loss: 1.5949763059616089 at epoch 53 at applicants training\n",
      "loss: 1.5927870273590088 at epoch 54 at applicants training\n",
      "loss: 1.5916913747787476 at epoch 55 at applicants training\n",
      "loss: 1.5904326438903809 at epoch 56 at applicants training\n",
      "loss: 1.588240146636963 at epoch 57 at applicants training\n",
      "loss: 1.5871344804763794 at epoch 58 at applicants training\n",
      "loss: 1.5869523286819458 at epoch 59 at applicants training\n",
      "loss: 1.5847296714782715 at epoch 60 at applicants training\n",
      "loss: 1.583632230758667 at epoch 61 at applicants training\n",
      "loss: 1.5832213163375854 at epoch 62 at applicants training\n",
      "loss: 1.582179307937622 at epoch 63 at applicants training\n",
      "loss: 1.5834013223648071 at epoch 64 at applicants training\n",
      "loss: 1.5859736204147339 at epoch 65 at applicants training\n",
      "loss: 1.5872269868850708 at epoch 66 at applicants training\n",
      "loss: 1.5884658098220825 at epoch 67 at applicants training\n",
      "loss: 1.5765891075134277 at epoch 68 at applicants training\n",
      "loss: 1.6017287969589233 at epoch 69 at applicants training\n",
      "loss: 1.5924896001815796 at epoch 70 at applicants training\n",
      "loss: 1.597445011138916 at epoch 71 at applicants training\n",
      "loss: 1.578432321548462 at epoch 72 at applicants training\n",
      "loss: 1.6037981510162354 at epoch 73 at applicants training\n",
      "loss: 1.5860445499420166 at epoch 74 at applicants training\n",
      "loss: 1.5838981866836548 at epoch 75 at applicants training\n",
      "loss: 1.5817539691925049 at epoch 76 at applicants training\n",
      "loss: 1.5855467319488525 at epoch 77 at applicants training\n",
      "loss: 1.5798941850662231 at epoch 78 at applicants training\n",
      "loss: 1.5766264200210571 at epoch 79 at applicants training\n",
      "loss: 1.5775671005249023 at epoch 80 at applicants training\n",
      "loss: 1.574763536453247 at epoch 81 at applicants training\n",
      "loss: 1.5746606588363647 at epoch 82 at applicants training\n",
      "loss: 1.5744330883026123 at epoch 83 at applicants training\n",
      "loss: 1.5693562030792236 at epoch 84 at applicants training\n",
      "loss: 1.57240891456604 at epoch 85 at applicants training\n",
      "loss: 1.5697153806686401 at epoch 86 at applicants training\n",
      "loss: 1.5669867992401123 at epoch 87 at applicants training\n",
      "loss: 1.5699231624603271 at epoch 88 at applicants training\n",
      "loss: 1.5649091005325317 at epoch 89 at applicants training\n",
      "loss: 1.565981388092041 at epoch 90 at applicants training\n",
      "loss: 1.564863920211792 at epoch 91 at applicants training\n",
      "loss: 1.563174843788147 at epoch 92 at applicants training\n",
      "loss: 1.563247799873352 at epoch 93 at applicants training\n",
      "loss: 1.561493158340454 at epoch 94 at applicants training\n",
      "loss: 1.5615322589874268 at epoch 95 at applicants training\n",
      "loss: 1.5613118410110474 at epoch 96 at applicants training\n",
      "loss: 1.559880256652832 at epoch 97 at applicants training\n",
      "loss: 1.559761881828308 at epoch 98 at applicants training\n",
      "loss: 1.5588291883468628 at epoch 99 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 0 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 1 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 2 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 3 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 4 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 5 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 6 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 7 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 8 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 9 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 10 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 11 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 12 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 13 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 14 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 15 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 16 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 17 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 18 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 19 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 20 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 21 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 22 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 23 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 24 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 25 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 26 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 27 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 28 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 29 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 30 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 31 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 32 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 33 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 34 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 35 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 36 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 37 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 38 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 39 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 40 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 41 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 42 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 43 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 44 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 45 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 46 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 47 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 48 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 49 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 50 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 51 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 52 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 53 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 54 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 55 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 56 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 57 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 58 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 59 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 60 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 61 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 62 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 63 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 64 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 65 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 66 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 67 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 68 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 69 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 70 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 71 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 72 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 73 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 74 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 75 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 76 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 77 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 78 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 79 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 80 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 81 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 82 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 83 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 84 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 85 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 86 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 87 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 88 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 89 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 90 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 91 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 92 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 93 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 94 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 95 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 96 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 97 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 98 at applicants training\n",
      "loss: 1.7128326892852783 at epoch 99 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 0 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 1 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 2 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 3 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 4 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 5 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 6 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 7 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 8 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 9 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 10 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 11 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 12 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 13 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 14 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 15 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 16 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 17 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 18 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 19 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 20 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 21 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 22 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 23 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 24 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 25 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 26 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 27 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 28 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 29 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 30 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 31 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 32 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 33 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 34 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 35 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 36 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 37 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 38 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 39 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 40 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 41 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 42 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 43 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 44 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 45 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 46 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 47 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 48 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 49 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 50 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 51 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 52 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 53 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 54 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 55 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 56 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 57 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 58 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 59 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 60 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 61 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 62 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 63 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 64 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 65 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 66 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 67 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 68 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 69 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 70 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 71 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 72 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 73 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 74 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 75 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 76 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 77 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 78 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 79 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 80 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 81 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 82 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 83 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 84 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 85 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 86 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 87 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 88 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 89 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 90 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 91 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 92 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 93 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 94 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 95 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 96 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 97 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 98 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 99 at applicants training\n",
      "loss: 1.7084170579910278 at epoch 0 at applicants training\n",
      "loss: 1.7187010049819946 at epoch 1 at applicants training\n",
      "loss: 1.7033435106277466 at epoch 2 at applicants training\n",
      "loss: 1.6733368635177612 at epoch 3 at applicants training\n",
      "loss: 1.675878882408142 at epoch 4 at applicants training\n",
      "loss: 1.6764593124389648 at epoch 5 at applicants training\n",
      "loss: 1.6754906177520752 at epoch 6 at applicants training\n",
      "loss: 1.6728910207748413 at epoch 7 at applicants training\n",
      "loss: 1.6699705123901367 at epoch 8 at applicants training\n",
      "loss: 1.6809828281402588 at epoch 9 at applicants training\n",
      "loss: 1.659517526626587 at epoch 10 at applicants training\n",
      "loss: 1.662806510925293 at epoch 11 at applicants training\n",
      "loss: 1.6632959842681885 at epoch 12 at applicants training\n",
      "loss: 1.656941294670105 at epoch 13 at applicants training\n",
      "loss: 1.6481519937515259 at epoch 14 at applicants training\n",
      "loss: 1.6500941514968872 at epoch 15 at applicants training\n",
      "loss: 1.6450499296188354 at epoch 16 at applicants training\n",
      "loss: 1.6430583000183105 at epoch 17 at applicants training\n",
      "loss: 1.642366647720337 at epoch 18 at applicants training\n",
      "loss: 1.6417410373687744 at epoch 19 at applicants training\n",
      "loss: 1.6354678869247437 at epoch 20 at applicants training\n",
      "loss: 1.639799952507019 at epoch 21 at applicants training\n",
      "loss: 1.6359014511108398 at epoch 22 at applicants training\n",
      "loss: 1.6333966255187988 at epoch 23 at applicants training\n",
      "loss: 1.6330264806747437 at epoch 24 at applicants training\n",
      "loss: 1.6309878826141357 at epoch 25 at applicants training\n",
      "loss: 1.6280442476272583 at epoch 26 at applicants training\n",
      "loss: 1.62850022315979 at epoch 27 at applicants training\n",
      "loss: 1.623723030090332 at epoch 28 at applicants training\n",
      "loss: 1.6233220100402832 at epoch 29 at applicants training\n",
      "loss: 1.620718002319336 at epoch 30 at applicants training\n",
      "loss: 1.6202201843261719 at epoch 31 at applicants training\n",
      "loss: 1.6197181940078735 at epoch 32 at applicants training\n",
      "loss: 1.6184155941009521 at epoch 33 at applicants training\n",
      "loss: 1.618265151977539 at epoch 34 at applicants training\n",
      "loss: 1.6156237125396729 at epoch 35 at applicants training\n",
      "loss: 1.6155805587768555 at epoch 36 at applicants training\n",
      "loss: 1.6143935918807983 at epoch 37 at applicants training\n",
      "loss: 1.6142377853393555 at epoch 38 at applicants training\n",
      "loss: 1.6146693229675293 at epoch 39 at applicants training\n",
      "loss: 1.618026852607727 at epoch 40 at applicants training\n",
      "loss: 1.6184924840927124 at epoch 41 at applicants training\n",
      "loss: 1.6187878847122192 at epoch 42 at applicants training\n",
      "loss: 1.6106420755386353 at epoch 43 at applicants training\n",
      "loss: 1.6200066804885864 at epoch 44 at applicants training\n",
      "loss: 1.6237659454345703 at epoch 45 at applicants training\n",
      "loss: 1.628583550453186 at epoch 46 at applicants training\n",
      "loss: 1.6097818613052368 at epoch 47 at applicants training\n",
      "loss: 1.6206085681915283 at epoch 48 at applicants training\n",
      "loss: 1.6254366636276245 at epoch 49 at applicants training\n",
      "loss: 1.6191794872283936 at epoch 50 at applicants training\n",
      "loss: 1.6087157726287842 at epoch 51 at applicants training\n",
      "loss: 1.6169241666793823 at epoch 52 at applicants training\n",
      "loss: 1.6083370447158813 at epoch 53 at applicants training\n",
      "loss: 1.6100213527679443 at epoch 54 at applicants training\n",
      "loss: 1.6100994348526 at epoch 55 at applicants training\n",
      "loss: 1.6073447465896606 at epoch 56 at applicants training\n",
      "loss: 1.6111770868301392 at epoch 57 at applicants training\n",
      "loss: 1.6048020124435425 at epoch 58 at applicants training\n",
      "loss: 1.6078506708145142 at epoch 59 at applicants training\n",
      "loss: 1.608022928237915 at epoch 60 at applicants training\n",
      "loss: 1.6035857200622559 at epoch 61 at applicants training\n",
      "loss: 1.6066592931747437 at epoch 62 at applicants training\n",
      "loss: 1.6039797067642212 at epoch 63 at applicants training\n",
      "loss: 1.6036432981491089 at epoch 64 at applicants training\n",
      "loss: 1.60547935962677 at epoch 65 at applicants training\n",
      "loss: 1.6029224395751953 at epoch 66 at applicants training\n",
      "loss: 1.6042762994766235 at epoch 67 at applicants training\n",
      "loss: 1.60166335105896 at epoch 68 at applicants training\n",
      "loss: 1.601380705833435 at epoch 69 at applicants training\n",
      "loss: 1.6037095785140991 at epoch 70 at applicants training\n",
      "loss: 1.6051619052886963 at epoch 71 at applicants training\n",
      "loss: 1.607060432434082 at epoch 72 at applicants training\n",
      "loss: 1.5991291999816895 at epoch 73 at applicants training\n",
      "loss: 1.6033961772918701 at epoch 74 at applicants training\n",
      "loss: 1.6130032539367676 at epoch 75 at applicants training\n",
      "loss: 1.6052699089050293 at epoch 76 at applicants training\n",
      "loss: 1.604477882385254 at epoch 77 at applicants training\n",
      "loss: 1.6012951135635376 at epoch 78 at applicants training\n",
      "loss: 1.6063108444213867 at epoch 79 at applicants training\n",
      "loss: 1.602217197418213 at epoch 80 at applicants training\n",
      "loss: 1.598366141319275 at epoch 81 at applicants training\n",
      "loss: 1.601389765739441 at epoch 82 at applicants training\n",
      "loss: 1.5969833135604858 at epoch 83 at applicants training\n",
      "loss: 1.5994386672973633 at epoch 84 at applicants training\n",
      "loss: 1.5974067449569702 at epoch 85 at applicants training\n",
      "loss: 1.6001665592193604 at epoch 86 at applicants training\n",
      "loss: 1.5973201990127563 at epoch 87 at applicants training\n",
      "loss: 1.5958698987960815 at epoch 88 at applicants training\n",
      "loss: 1.5971771478652954 at epoch 89 at applicants training\n",
      "loss: 1.5991712808609009 at epoch 90 at applicants training\n",
      "loss: 1.601387619972229 at epoch 91 at applicants training\n",
      "loss: 1.5945230722427368 at epoch 92 at applicants training\n",
      "loss: 1.5973007678985596 at epoch 93 at applicants training\n",
      "loss: 1.5984126329421997 at epoch 94 at applicants training\n",
      "loss: 1.6020013093948364 at epoch 95 at applicants training\n",
      "loss: 1.5971406698226929 at epoch 96 at applicants training\n",
      "loss: 1.5999855995178223 at epoch 97 at applicants training\n",
      "loss: 1.6066248416900635 at epoch 98 at applicants training\n",
      "loss: 1.597130298614502 at epoch 99 at applicants training\n",
      "loss: 1.6793495416641235 at epoch 0 at applicants training\n",
      "loss: 1.6751726865768433 at epoch 1 at applicants training\n",
      "loss: 1.6654185056686401 at epoch 2 at applicants training\n",
      "loss: 1.6806086301803589 at epoch 3 at applicants training\n",
      "loss: 1.681516170501709 at epoch 4 at applicants training\n",
      "loss: 1.656718134880066 at epoch 5 at applicants training\n",
      "loss: 1.6672486066818237 at epoch 6 at applicants training\n",
      "loss: 1.661916732788086 at epoch 7 at applicants training\n",
      "loss: 1.6721112728118896 at epoch 8 at applicants training\n",
      "loss: 1.6713824272155762 at epoch 9 at applicants training\n",
      "loss: 1.6572588682174683 at epoch 10 at applicants training\n",
      "loss: 1.6711921691894531 at epoch 11 at applicants training\n",
      "loss: 1.6743009090423584 at epoch 12 at applicants training\n",
      "loss: 1.672037124633789 at epoch 13 at applicants training\n",
      "loss: 1.6586682796478271 at epoch 14 at applicants training\n",
      "loss: 1.668129801750183 at epoch 15 at applicants training\n",
      "loss: 1.666393518447876 at epoch 16 at applicants training\n",
      "loss: 1.6516273021697998 at epoch 17 at applicants training\n",
      "loss: 1.6643325090408325 at epoch 18 at applicants training\n",
      "loss: 1.6670640707015991 at epoch 19 at applicants training\n",
      "loss: 1.6602507829666138 at epoch 20 at applicants training\n",
      "loss: 1.653824806213379 at epoch 21 at applicants training\n",
      "loss: 1.6599726676940918 at epoch 22 at applicants training\n",
      "loss: 1.659247636795044 at epoch 23 at applicants training\n",
      "loss: 1.6533585786819458 at epoch 24 at applicants training\n",
      "loss: 1.6530754566192627 at epoch 25 at applicants training\n",
      "loss: 1.6565316915512085 at epoch 26 at applicants training\n",
      "loss: 1.650415062904358 at epoch 27 at applicants training\n",
      "loss: 1.6511311531066895 at epoch 28 at applicants training\n",
      "loss: 1.6513233184814453 at epoch 29 at applicants training\n",
      "loss: 1.6476917266845703 at epoch 30 at applicants training\n",
      "loss: 1.6499942541122437 at epoch 31 at applicants training\n",
      "loss: 1.6461397409439087 at epoch 32 at applicants training\n",
      "loss: 1.6489354372024536 at epoch 33 at applicants training\n",
      "loss: 1.645268201828003 at epoch 34 at applicants training\n",
      "loss: 1.6465016603469849 at epoch 35 at applicants training\n",
      "loss: 1.6443755626678467 at epoch 36 at applicants training\n",
      "loss: 1.644504189491272 at epoch 37 at applicants training\n",
      "loss: 1.6440521478652954 at epoch 38 at applicants training\n",
      "loss: 1.6427030563354492 at epoch 39 at applicants training\n",
      "loss: 1.6435446739196777 at epoch 40 at applicants training\n",
      "loss: 1.6418544054031372 at epoch 41 at applicants training\n",
      "loss: 1.6416422128677368 at epoch 42 at applicants training\n",
      "loss: 1.6419540643692017 at epoch 43 at applicants training\n",
      "loss: 1.6402027606964111 at epoch 44 at applicants training\n",
      "loss: 1.63975191116333 at epoch 45 at applicants training\n",
      "loss: 1.6400827169418335 at epoch 46 at applicants training\n",
      "loss: 1.639129877090454 at epoch 47 at applicants training\n",
      "loss: 1.6379584074020386 at epoch 48 at applicants training\n",
      "loss: 1.637561321258545 at epoch 49 at applicants training\n",
      "loss: 1.637721300125122 at epoch 50 at applicants training\n",
      "loss: 1.6383241415023804 at epoch 51 at applicants training\n",
      "loss: 1.6384143829345703 at epoch 52 at applicants training\n",
      "loss: 1.63785982131958 at epoch 53 at applicants training\n",
      "loss: 1.6364452838897705 at epoch 54 at applicants training\n",
      "loss: 1.6357136964797974 at epoch 55 at applicants training\n",
      "loss: 1.6352529525756836 at epoch 56 at applicants training\n",
      "loss: 1.6350164413452148 at epoch 57 at applicants training\n",
      "loss: 1.6349934339523315 at epoch 58 at applicants training\n",
      "loss: 1.6351487636566162 at epoch 59 at applicants training\n",
      "loss: 1.6356852054595947 at epoch 60 at applicants training\n",
      "loss: 1.634883999824524 at epoch 61 at applicants training\n",
      "loss: 1.6353886127471924 at epoch 62 at applicants training\n",
      "loss: 1.6330091953277588 at epoch 63 at applicants training\n",
      "loss: 1.6301536560058594 at epoch 64 at applicants training\n",
      "loss: 1.630587100982666 at epoch 65 at applicants training\n",
      "loss: 1.6292592287063599 at epoch 66 at applicants training\n",
      "loss: 1.6269683837890625 at epoch 67 at applicants training\n",
      "loss: 1.6244969367980957 at epoch 68 at applicants training\n",
      "loss: 1.6228207349777222 at epoch 69 at applicants training\n",
      "loss: 1.621712327003479 at epoch 70 at applicants training\n",
      "loss: 1.6197861433029175 at epoch 71 at applicants training\n",
      "loss: 1.6185911893844604 at epoch 72 at applicants training\n",
      "loss: 1.6181361675262451 at epoch 73 at applicants training\n",
      "loss: 1.6175981760025024 at epoch 74 at applicants training\n",
      "loss: 1.6208807229995728 at epoch 75 at applicants training\n",
      "loss: 1.6343265771865845 at epoch 76 at applicants training\n",
      "loss: 1.6468753814697266 at epoch 77 at applicants training\n",
      "loss: 1.623773217201233 at epoch 78 at applicants training\n",
      "loss: 1.6423043012619019 at epoch 79 at applicants training\n",
      "loss: 1.6278107166290283 at epoch 80 at applicants training\n",
      "loss: 1.631014347076416 at epoch 81 at applicants training\n",
      "loss: 1.6387689113616943 at epoch 82 at applicants training\n",
      "loss: 1.624934196472168 at epoch 83 at applicants training\n",
      "loss: 1.6369153261184692 at epoch 84 at applicants training\n",
      "loss: 1.6368986368179321 at epoch 85 at applicants training\n",
      "loss: 1.626143455505371 at epoch 86 at applicants training\n",
      "loss: 1.6465976238250732 at epoch 87 at applicants training\n",
      "loss: 1.6266093254089355 at epoch 88 at applicants training\n",
      "loss: 1.6343728303909302 at epoch 89 at applicants training\n",
      "loss: 1.6261347532272339 at epoch 90 at applicants training\n",
      "loss: 1.6257363557815552 at epoch 91 at applicants training\n",
      "loss: 1.6305339336395264 at epoch 92 at applicants training\n",
      "loss: 1.6217442750930786 at epoch 93 at applicants training\n",
      "loss: 1.6329752206802368 at epoch 94 at applicants training\n",
      "loss: 1.6230216026306152 at epoch 95 at applicants training\n",
      "loss: 1.6256076097488403 at epoch 96 at applicants training\n",
      "loss: 1.6253540515899658 at epoch 97 at applicants training\n",
      "loss: 1.6190234422683716 at epoch 98 at applicants training\n",
      "loss: 1.6233727931976318 at epoch 99 at applicants training\n",
      "loss: 1.7152884006500244 at epoch 0 at applicants training\n",
      "loss: 1.6983813047409058 at epoch 1 at applicants training\n",
      "loss: 1.6744935512542725 at epoch 2 at applicants training\n",
      "loss: 1.6885839700698853 at epoch 3 at applicants training\n",
      "loss: 1.6680995225906372 at epoch 4 at applicants training\n",
      "loss: 1.6744259595870972 at epoch 5 at applicants training\n",
      "loss: 1.6657016277313232 at epoch 6 at applicants training\n",
      "loss: 1.6675763130187988 at epoch 7 at applicants training\n",
      "loss: 1.6616942882537842 at epoch 8 at applicants training\n",
      "loss: 1.663625717163086 at epoch 9 at applicants training\n",
      "loss: 1.6641079187393188 at epoch 10 at applicants training\n",
      "loss: 1.659903645515442 at epoch 11 at applicants training\n",
      "loss: 1.659565806388855 at epoch 12 at applicants training\n",
      "loss: 1.6625293493270874 at epoch 13 at applicants training\n",
      "loss: 1.6560114622116089 at epoch 14 at applicants training\n",
      "loss: 1.6580687761306763 at epoch 15 at applicants training\n",
      "loss: 1.6525912284851074 at epoch 16 at applicants training\n",
      "loss: 1.6557989120483398 at epoch 17 at applicants training\n",
      "loss: 1.654131293296814 at epoch 18 at applicants training\n",
      "loss: 1.6540379524230957 at epoch 19 at applicants training\n",
      "loss: 1.6542673110961914 at epoch 20 at applicants training\n",
      "loss: 1.65128755569458 at epoch 21 at applicants training\n",
      "loss: 1.650217890739441 at epoch 22 at applicants training\n",
      "loss: 1.6473703384399414 at epoch 23 at applicants training\n",
      "loss: 1.644384503364563 at epoch 24 at applicants training\n",
      "loss: 1.6429626941680908 at epoch 25 at applicants training\n",
      "loss: 1.644502878189087 at epoch 26 at applicants training\n",
      "loss: 1.6448688507080078 at epoch 27 at applicants training\n",
      "loss: 1.6415601968765259 at epoch 28 at applicants training\n",
      "loss: 1.636594533920288 at epoch 29 at applicants training\n",
      "loss: 1.6343803405761719 at epoch 30 at applicants training\n",
      "loss: 1.6352503299713135 at epoch 31 at applicants training\n",
      "loss: 1.6376537084579468 at epoch 32 at applicants training\n",
      "loss: 1.6350865364074707 at epoch 33 at applicants training\n",
      "loss: 1.632124423980713 at epoch 34 at applicants training\n",
      "loss: 1.6304398775100708 at epoch 35 at applicants training\n",
      "loss: 1.632510781288147 at epoch 36 at applicants training\n",
      "loss: 1.642430305480957 at epoch 37 at applicants training\n",
      "loss: 1.6368151903152466 at epoch 38 at applicants training\n",
      "loss: 1.6276775598526 at epoch 39 at applicants training\n",
      "loss: 1.6281406879425049 at epoch 40 at applicants training\n",
      "loss: 1.636836290359497 at epoch 41 at applicants training\n",
      "loss: 1.639876365661621 at epoch 42 at applicants training\n",
      "loss: 1.6227703094482422 at epoch 43 at applicants training\n",
      "loss: 1.6490213871002197 at epoch 44 at applicants training\n",
      "loss: 1.6278454065322876 at epoch 45 at applicants training\n",
      "loss: 1.6278576850891113 at epoch 46 at applicants training\n",
      "loss: 1.6396256685256958 at epoch 47 at applicants training\n",
      "loss: 1.6203161478042603 at epoch 48 at applicants training\n",
      "loss: 1.629656195640564 at epoch 49 at applicants training\n",
      "loss: 1.6272341012954712 at epoch 50 at applicants training\n",
      "loss: 1.6183840036392212 at epoch 51 at applicants training\n",
      "loss: 1.6268641948699951 at epoch 52 at applicants training\n",
      "loss: 1.624657392501831 at epoch 53 at applicants training\n",
      "loss: 1.6166603565216064 at epoch 54 at applicants training\n",
      "loss: 1.6207196712493896 at epoch 55 at applicants training\n",
      "loss: 1.624647855758667 at epoch 56 at applicants training\n",
      "loss: 1.6185075044631958 at epoch 57 at applicants training\n",
      "loss: 1.614570140838623 at epoch 58 at applicants training\n",
      "loss: 1.619256615638733 at epoch 59 at applicants training\n",
      "loss: 1.6242032051086426 at epoch 60 at applicants training\n",
      "loss: 1.6136925220489502 at epoch 61 at applicants training\n",
      "loss: 1.6146656274795532 at epoch 62 at applicants training\n",
      "loss: 1.6241950988769531 at epoch 63 at applicants training\n",
      "loss: 1.6157901287078857 at epoch 64 at applicants training\n",
      "loss: 1.6115416288375854 at epoch 65 at applicants training\n",
      "loss: 1.6130318641662598 at epoch 66 at applicants training\n",
      "loss: 1.6181747913360596 at epoch 67 at applicants training\n",
      "loss: 1.6249421834945679 at epoch 68 at applicants training\n",
      "loss: 1.6106711626052856 at epoch 69 at applicants training\n",
      "loss: 1.6118236780166626 at epoch 70 at applicants training\n",
      "loss: 1.625077724456787 at epoch 71 at applicants training\n",
      "loss: 1.6129693984985352 at epoch 72 at applicants training\n",
      "loss: 1.6087799072265625 at epoch 73 at applicants training\n",
      "loss: 1.6102440357208252 at epoch 74 at applicants training\n",
      "loss: 1.6151083707809448 at epoch 75 at applicants training\n",
      "loss: 1.6231119632720947 at epoch 76 at applicants training\n",
      "loss: 1.608278751373291 at epoch 77 at applicants training\n",
      "loss: 1.610683560371399 at epoch 78 at applicants training\n",
      "loss: 1.6245155334472656 at epoch 79 at applicants training\n",
      "loss: 1.607586145401001 at epoch 80 at applicants training\n",
      "loss: 1.6167947053909302 at epoch 81 at applicants training\n",
      "loss: 1.6292724609375 at epoch 82 at applicants training\n",
      "loss: 1.6125909090042114 at epoch 83 at applicants training\n",
      "loss: 1.6504302024841309 at epoch 84 at applicants training\n",
      "loss: 1.6084779500961304 at epoch 85 at applicants training\n",
      "loss: 1.6450788974761963 at epoch 86 at applicants training\n",
      "loss: 1.6436420679092407 at epoch 87 at applicants training\n",
      "loss: 1.6078283786773682 at epoch 88 at applicants training\n",
      "loss: 1.6456794738769531 at epoch 89 at applicants training\n",
      "loss: 1.6102474927902222 at epoch 90 at applicants training\n",
      "loss: 1.6350319385528564 at epoch 91 at applicants training\n",
      "loss: 1.633574366569519 at epoch 92 at applicants training\n",
      "loss: 1.60952889919281 at epoch 93 at applicants training\n",
      "loss: 1.6334415674209595 at epoch 94 at applicants training\n",
      "loss: 1.6083160638809204 at epoch 95 at applicants training\n",
      "loss: 1.6240293979644775 at epoch 96 at applicants training\n",
      "loss: 1.6144990921020508 at epoch 97 at applicants training\n",
      "loss: 1.6179465055465698 at epoch 98 at applicants training\n",
      "loss: 1.6130121946334839 at epoch 99 at applicants training\n",
      "loss: 1.6893339157104492 at epoch 0 at applicants training\n",
      "loss: 1.6788376569747925 at epoch 1 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 2 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 3 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 4 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.705479621887207 at epoch 0 at applicants training\n",
      "loss: 1.6939524412155151 at epoch 1 at applicants training\n",
      "loss: 1.69464111328125 at epoch 2 at applicants training\n",
      "loss: 1.694484829902649 at epoch 3 at applicants training\n",
      "loss: 1.6930228471755981 at epoch 4 at applicants training\n",
      "loss: 1.6879431009292603 at epoch 5 at applicants training\n",
      "loss: 1.6710021495819092 at epoch 6 at applicants training\n",
      "loss: 1.6653590202331543 at epoch 7 at applicants training\n",
      "loss: 1.659571886062622 at epoch 8 at applicants training\n",
      "loss: 1.6582015752792358 at epoch 9 at applicants training\n",
      "loss: 1.6647942066192627 at epoch 10 at applicants training\n",
      "loss: 1.6643171310424805 at epoch 11 at applicants training\n",
      "loss: 1.656178593635559 at epoch 12 at applicants training\n",
      "loss: 1.65480637550354 at epoch 13 at applicants training\n",
      "loss: 1.6655640602111816 at epoch 14 at applicants training\n",
      "loss: 1.6584320068359375 at epoch 15 at applicants training\n",
      "loss: 1.6526377201080322 at epoch 16 at applicants training\n",
      "loss: 1.66135835647583 at epoch 17 at applicants training\n",
      "loss: 1.6610664129257202 at epoch 18 at applicants training\n",
      "loss: 1.6506361961364746 at epoch 19 at applicants training\n",
      "loss: 1.6520731449127197 at epoch 20 at applicants training\n",
      "loss: 1.6558442115783691 at epoch 21 at applicants training\n",
      "loss: 1.653943657875061 at epoch 22 at applicants training\n",
      "loss: 1.6483120918273926 at epoch 23 at applicants training\n",
      "loss: 1.649802327156067 at epoch 24 at applicants training\n",
      "loss: 1.6536462306976318 at epoch 25 at applicants training\n",
      "loss: 1.6473000049591064 at epoch 26 at applicants training\n",
      "loss: 1.6457661390304565 at epoch 27 at applicants training\n",
      "loss: 1.6482336521148682 at epoch 28 at applicants training\n",
      "loss: 1.645175576210022 at epoch 29 at applicants training\n",
      "loss: 1.644547939300537 at epoch 30 at applicants training\n",
      "loss: 1.6461384296417236 at epoch 31 at applicants training\n",
      "loss: 1.644801378250122 at epoch 32 at applicants training\n",
      "loss: 1.6423674821853638 at epoch 33 at applicants training\n",
      "loss: 1.643257737159729 at epoch 34 at applicants training\n",
      "loss: 1.6433544158935547 at epoch 35 at applicants training\n",
      "loss: 1.6409727334976196 at epoch 36 at applicants training\n",
      "loss: 1.6417722702026367 at epoch 37 at applicants training\n",
      "loss: 1.6391561031341553 at epoch 38 at applicants training\n",
      "loss: 1.639898419380188 at epoch 39 at applicants training\n",
      "loss: 1.637305736541748 at epoch 40 at applicants training\n",
      "loss: 1.6378273963928223 at epoch 41 at applicants training\n",
      "loss: 1.6351022720336914 at epoch 42 at applicants training\n",
      "loss: 1.636163353919983 at epoch 43 at applicants training\n",
      "loss: 1.6342209577560425 at epoch 44 at applicants training\n",
      "loss: 1.6336828470230103 at epoch 45 at applicants training\n",
      "loss: 1.6338422298431396 at epoch 46 at applicants training\n",
      "loss: 1.6317700147628784 at epoch 47 at applicants training\n",
      "loss: 1.6328366994857788 at epoch 48 at applicants training\n",
      "loss: 1.6321771144866943 at epoch 49 at applicants training\n",
      "loss: 1.6304274797439575 at epoch 50 at applicants training\n",
      "loss: 1.6302814483642578 at epoch 51 at applicants training\n",
      "loss: 1.630953073501587 at epoch 52 at applicants training\n",
      "loss: 1.6344001293182373 at epoch 53 at applicants training\n",
      "loss: 1.6334943771362305 at epoch 54 at applicants training\n",
      "loss: 1.6282156705856323 at epoch 55 at applicants training\n",
      "loss: 1.632348656654358 at epoch 56 at applicants training\n",
      "loss: 1.628160834312439 at epoch 57 at applicants training\n",
      "loss: 1.6310800313949585 at epoch 58 at applicants training\n",
      "loss: 1.6263519525527954 at epoch 59 at applicants training\n",
      "loss: 1.630424976348877 at epoch 60 at applicants training\n",
      "loss: 1.6271153688430786 at epoch 61 at applicants training\n",
      "loss: 1.626952052116394 at epoch 62 at applicants training\n",
      "loss: 1.6261450052261353 at epoch 63 at applicants training\n",
      "loss: 1.623855710029602 at epoch 64 at applicants training\n",
      "loss: 1.6242690086364746 at epoch 65 at applicants training\n",
      "loss: 1.6229342222213745 at epoch 66 at applicants training\n",
      "loss: 1.62189781665802 at epoch 67 at applicants training\n",
      "loss: 1.6216785907745361 at epoch 68 at applicants training\n",
      "loss: 1.6209509372711182 at epoch 69 at applicants training\n",
      "loss: 1.6200604438781738 at epoch 70 at applicants training\n",
      "loss: 1.6194860935211182 at epoch 71 at applicants training\n",
      "loss: 1.6192246675491333 at epoch 72 at applicants training\n",
      "loss: 1.6185150146484375 at epoch 73 at applicants training\n",
      "loss: 1.6187509298324585 at epoch 74 at applicants training\n",
      "loss: 1.618678092956543 at epoch 75 at applicants training\n",
      "loss: 1.617628574371338 at epoch 76 at applicants training\n",
      "loss: 1.6160932779312134 at epoch 77 at applicants training\n",
      "loss: 1.6160959005355835 at epoch 78 at applicants training\n",
      "loss: 1.6174895763397217 at epoch 79 at applicants training\n",
      "loss: 1.6179660558700562 at epoch 80 at applicants training\n",
      "loss: 1.6223671436309814 at epoch 81 at applicants training\n",
      "loss: 1.611796259880066 at epoch 82 at applicants training\n",
      "loss: 1.6188126802444458 at epoch 83 at applicants training\n",
      "loss: 1.6252641677856445 at epoch 84 at applicants training\n",
      "loss: 1.6141549348831177 at epoch 85 at applicants training\n",
      "loss: 1.6320738792419434 at epoch 86 at applicants training\n",
      "loss: 1.6187142133712769 at epoch 87 at applicants training\n",
      "loss: 1.6316787004470825 at epoch 88 at applicants training\n",
      "loss: 1.6192582845687866 at epoch 89 at applicants training\n",
      "loss: 1.6229281425476074 at epoch 90 at applicants training\n",
      "loss: 1.6177321672439575 at epoch 91 at applicants training\n",
      "loss: 1.6195781230926514 at epoch 92 at applicants training\n",
      "loss: 1.6246658563613892 at epoch 93 at applicants training\n",
      "loss: 1.617477297782898 at epoch 94 at applicants training\n",
      "loss: 1.6151548624038696 at epoch 95 at applicants training\n",
      "loss: 1.6097257137298584 at epoch 96 at applicants training\n",
      "loss: 1.6084011793136597 at epoch 97 at applicants training\n",
      "loss: 1.6092867851257324 at epoch 98 at applicants training\n",
      "loss: 1.60546875 at epoch 99 at applicants training\n",
      "loss: 1.7117786407470703 at epoch 0 at applicants training\n",
      "loss: 1.7220157384872437 at epoch 1 at applicants training\n",
      "loss: 1.7207924127578735 at epoch 2 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 3 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 4 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 5 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 6 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 7 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 8 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 9 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 10 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 11 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 12 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 13 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 14 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 15 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 16 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 17 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 18 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 19 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 20 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 21 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 22 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 23 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 24 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 25 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 26 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 27 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 28 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 29 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 30 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 31 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 32 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 33 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 34 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 35 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 36 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 37 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 38 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 39 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 40 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 41 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 42 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 43 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 44 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 45 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 46 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 47 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 48 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 49 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 50 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 51 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 52 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 53 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 54 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 55 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 56 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 57 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 58 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 59 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 60 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 61 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 62 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 63 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 64 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 65 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 66 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 67 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 68 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 69 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 70 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 71 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 72 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 73 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 74 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 75 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 76 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 77 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 78 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 79 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 80 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 81 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 82 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 83 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 84 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 85 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 86 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 87 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 88 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 89 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 90 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 91 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 92 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 93 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 94 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 95 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 96 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 97 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 98 at applicants training\n",
      "loss: 1.7208325862884521 at epoch 99 at applicants training\n",
      "loss: 1.7100063562393188 at epoch 0 at applicants training\n",
      "loss: 1.6899635791778564 at epoch 1 at applicants training\n",
      "loss: 1.663516640663147 at epoch 2 at applicants training\n",
      "loss: 1.656628131866455 at epoch 3 at applicants training\n",
      "loss: 1.6751376390457153 at epoch 4 at applicants training\n",
      "loss: 1.678354024887085 at epoch 5 at applicants training\n",
      "loss: 1.677795171737671 at epoch 6 at applicants training\n",
      "loss: 1.6755236387252808 at epoch 7 at applicants training\n",
      "loss: 1.6729621887207031 at epoch 8 at applicants training\n",
      "loss: 1.674699068069458 at epoch 9 at applicants training\n",
      "loss: 1.6737189292907715 at epoch 10 at applicants training\n",
      "loss: 1.67234206199646 at epoch 11 at applicants training\n",
      "loss: 1.6733734607696533 at epoch 12 at applicants training\n",
      "loss: 1.6707831621170044 at epoch 13 at applicants training\n",
      "loss: 1.670528769493103 at epoch 14 at applicants training\n",
      "loss: 1.6682171821594238 at epoch 15 at applicants training\n",
      "loss: 1.6681691408157349 at epoch 16 at applicants training\n",
      "loss: 1.665878415107727 at epoch 17 at applicants training\n",
      "loss: 1.6663024425506592 at epoch 18 at applicants training\n",
      "loss: 1.6647417545318604 at epoch 19 at applicants training\n",
      "loss: 1.6654061079025269 at epoch 20 at applicants training\n",
      "loss: 1.6643688678741455 at epoch 21 at applicants training\n",
      "loss: 1.6622395515441895 at epoch 22 at applicants training\n",
      "loss: 1.6650974750518799 at epoch 23 at applicants training\n",
      "loss: 1.6615113019943237 at epoch 24 at applicants training\n",
      "loss: 1.6631028652191162 at epoch 25 at applicants training\n",
      "loss: 1.6610889434814453 at epoch 26 at applicants training\n",
      "loss: 1.6620497703552246 at epoch 27 at applicants training\n",
      "loss: 1.6602113246917725 at epoch 28 at applicants training\n",
      "loss: 1.6594284772872925 at epoch 29 at applicants training\n",
      "loss: 1.6609853506088257 at epoch 30 at applicants training\n",
      "loss: 1.6576552391052246 at epoch 31 at applicants training\n",
      "loss: 1.6561927795410156 at epoch 32 at applicants training\n",
      "loss: 1.6568790674209595 at epoch 33 at applicants training\n",
      "loss: 1.6587966680526733 at epoch 34 at applicants training\n",
      "loss: 1.6565744876861572 at epoch 35 at applicants training\n",
      "loss: 1.6555838584899902 at epoch 36 at applicants training\n",
      "loss: 1.6565515995025635 at epoch 37 at applicants training\n",
      "loss: 1.6549184322357178 at epoch 38 at applicants training\n",
      "loss: 1.655145525932312 at epoch 39 at applicants training\n",
      "loss: 1.6552479267120361 at epoch 40 at applicants training\n",
      "loss: 1.6534614562988281 at epoch 41 at applicants training\n",
      "loss: 1.6534396409988403 at epoch 42 at applicants training\n",
      "loss: 1.6561646461486816 at epoch 43 at applicants training\n",
      "loss: 1.656752586364746 at epoch 44 at applicants training\n",
      "loss: 1.652631402015686 at epoch 45 at applicants training\n",
      "loss: 1.653774619102478 at epoch 46 at applicants training\n",
      "loss: 1.656173586845398 at epoch 47 at applicants training\n",
      "loss: 1.6524993181228638 at epoch 48 at applicants training\n",
      "loss: 1.6557729244232178 at epoch 49 at applicants training\n",
      "loss: 1.6545913219451904 at epoch 50 at applicants training\n",
      "loss: 1.651605248451233 at epoch 51 at applicants training\n",
      "loss: 1.6550219058990479 at epoch 52 at applicants training\n",
      "loss: 1.651397943496704 at epoch 53 at applicants training\n",
      "loss: 1.6526474952697754 at epoch 54 at applicants training\n",
      "loss: 1.6531213521957397 at epoch 55 at applicants training\n",
      "loss: 1.6510024070739746 at epoch 56 at applicants training\n",
      "loss: 1.6534909009933472 at epoch 57 at applicants training\n",
      "loss: 1.6509320735931396 at epoch 58 at applicants training\n",
      "loss: 1.65154230594635 at epoch 59 at applicants training\n",
      "loss: 1.6517847776412964 at epoch 60 at applicants training\n",
      "loss: 1.6498990058898926 at epoch 61 at applicants training\n",
      "loss: 1.6513017416000366 at epoch 62 at applicants training\n",
      "loss: 1.6501511335372925 at epoch 63 at applicants training\n",
      "loss: 1.6495181322097778 at epoch 64 at applicants training\n",
      "loss: 1.6503760814666748 at epoch 65 at applicants training\n",
      "loss: 1.6493076086044312 at epoch 66 at applicants training\n",
      "loss: 1.6491144895553589 at epoch 67 at applicants training\n",
      "loss: 1.6495448350906372 at epoch 68 at applicants training\n",
      "loss: 1.648520827293396 at epoch 69 at applicants training\n",
      "loss: 1.6482986211776733 at epoch 70 at applicants training\n",
      "loss: 1.6486963033676147 at epoch 71 at applicants training\n",
      "loss: 1.6481709480285645 at epoch 72 at applicants training\n",
      "loss: 1.6471853256225586 at epoch 73 at applicants training\n",
      "loss: 1.646758794784546 at epoch 74 at applicants training\n",
      "loss: 1.6470847129821777 at epoch 75 at applicants training\n",
      "loss: 1.6469470262527466 at epoch 76 at applicants training\n",
      "loss: 1.6461132764816284 at epoch 77 at applicants training\n",
      "loss: 1.6453311443328857 at epoch 78 at applicants training\n",
      "loss: 1.6445741653442383 at epoch 79 at applicants training\n",
      "loss: 1.6443647146224976 at epoch 80 at applicants training\n",
      "loss: 1.644766092300415 at epoch 81 at applicants training\n",
      "loss: 1.6473509073257446 at epoch 82 at applicants training\n",
      "loss: 1.6519370079040527 at epoch 83 at applicants training\n",
      "loss: 1.6447579860687256 at epoch 84 at applicants training\n",
      "loss: 1.6421884298324585 at epoch 85 at applicants training\n",
      "loss: 1.6443032026290894 at epoch 86 at applicants training\n",
      "loss: 1.64799165725708 at epoch 87 at applicants training\n",
      "loss: 1.6477577686309814 at epoch 88 at applicants training\n",
      "loss: 1.6422755718231201 at epoch 89 at applicants training\n",
      "loss: 1.64084792137146 at epoch 90 at applicants training\n",
      "loss: 1.6436357498168945 at epoch 91 at applicants training\n",
      "loss: 1.644595980644226 at epoch 92 at applicants training\n",
      "loss: 1.6400939226150513 at epoch 93 at applicants training\n",
      "loss: 1.640418291091919 at epoch 94 at applicants training\n",
      "loss: 1.644511103630066 at epoch 95 at applicants training\n",
      "loss: 1.6448686122894287 at epoch 96 at applicants training\n",
      "loss: 1.639768123626709 at epoch 97 at applicants training\n",
      "loss: 1.6385812759399414 at epoch 98 at applicants training\n",
      "loss: 1.6415736675262451 at epoch 99 at applicants training\n",
      "loss: 1.6786638498306274 at epoch 0 at applicants training\n",
      "loss: 1.67271888256073 at epoch 1 at applicants training\n",
      "loss: 1.6739717721939087 at epoch 2 at applicants training\n",
      "loss: 1.651700496673584 at epoch 3 at applicants training\n",
      "loss: 1.6482549905776978 at epoch 4 at applicants training\n",
      "loss: 1.63869047164917 at epoch 5 at applicants training\n",
      "loss: 1.645251989364624 at epoch 6 at applicants training\n",
      "loss: 1.6353100538253784 at epoch 7 at applicants training\n",
      "loss: 1.6418182849884033 at epoch 8 at applicants training\n",
      "loss: 1.6340168714523315 at epoch 9 at applicants training\n",
      "loss: 1.6349387168884277 at epoch 10 at applicants training\n",
      "loss: 1.6283674240112305 at epoch 11 at applicants training\n",
      "loss: 1.6276013851165771 at epoch 12 at applicants training\n",
      "loss: 1.6281483173370361 at epoch 13 at applicants training\n",
      "loss: 1.6236927509307861 at epoch 14 at applicants training\n",
      "loss: 1.6249449253082275 at epoch 15 at applicants training\n",
      "loss: 1.6180051565170288 at epoch 16 at applicants training\n",
      "loss: 1.62309730052948 at epoch 17 at applicants training\n",
      "loss: 1.615327000617981 at epoch 18 at applicants training\n",
      "loss: 1.6176103353500366 at epoch 19 at applicants training\n",
      "loss: 1.6141918897628784 at epoch 20 at applicants training\n",
      "loss: 1.610984206199646 at epoch 21 at applicants training\n",
      "loss: 1.615593671798706 at epoch 22 at applicants training\n",
      "loss: 1.6117409467697144 at epoch 23 at applicants training\n",
      "loss: 1.6104519367218018 at epoch 24 at applicants training\n",
      "loss: 1.6142857074737549 at epoch 25 at applicants training\n",
      "loss: 1.6107996702194214 at epoch 26 at applicants training\n",
      "loss: 1.6091221570968628 at epoch 27 at applicants training\n",
      "loss: 1.6103476285934448 at epoch 28 at applicants training\n",
      "loss: 1.6079068183898926 at epoch 29 at applicants training\n",
      "loss: 1.608831763267517 at epoch 30 at applicants training\n",
      "loss: 1.607861042022705 at epoch 31 at applicants training\n",
      "loss: 1.6064637899398804 at epoch 32 at applicants training\n",
      "loss: 1.607078194618225 at epoch 33 at applicants training\n",
      "loss: 1.6050267219543457 at epoch 34 at applicants training\n",
      "loss: 1.6048659086227417 at epoch 35 at applicants training\n",
      "loss: 1.6054893732070923 at epoch 36 at applicants training\n",
      "loss: 1.6039284467697144 at epoch 37 at applicants training\n",
      "loss: 1.6035319566726685 at epoch 38 at applicants training\n",
      "loss: 1.6037904024124146 at epoch 39 at applicants training\n",
      "loss: 1.6026302576065063 at epoch 40 at applicants training\n",
      "loss: 1.6023467779159546 at epoch 41 at applicants training\n",
      "loss: 1.6025452613830566 at epoch 42 at applicants training\n",
      "loss: 1.6014653444290161 at epoch 43 at applicants training\n",
      "loss: 1.600876808166504 at epoch 44 at applicants training\n",
      "loss: 1.600881814956665 at epoch 45 at applicants training\n",
      "loss: 1.6008667945861816 at epoch 46 at applicants training\n",
      "loss: 1.600242257118225 at epoch 47 at applicants training\n",
      "loss: 1.5995699167251587 at epoch 48 at applicants training\n",
      "loss: 1.5993289947509766 at epoch 49 at applicants training\n",
      "loss: 1.5992623567581177 at epoch 50 at applicants training\n",
      "loss: 1.5991758108139038 at epoch 51 at applicants training\n",
      "loss: 1.5988593101501465 at epoch 52 at applicants training\n",
      "loss: 1.5983904600143433 at epoch 53 at applicants training\n",
      "loss: 1.5978519916534424 at epoch 54 at applicants training\n",
      "loss: 1.5976414680480957 at epoch 55 at applicants training\n",
      "loss: 1.597504734992981 at epoch 56 at applicants training\n",
      "loss: 1.5976362228393555 at epoch 57 at applicants training\n",
      "loss: 1.5974215269088745 at epoch 58 at applicants training\n",
      "loss: 1.5973830223083496 at epoch 59 at applicants training\n",
      "loss: 1.5969464778900146 at epoch 60 at applicants training\n",
      "loss: 1.5965317487716675 at epoch 61 at applicants training\n",
      "loss: 1.595970630645752 at epoch 62 at applicants training\n",
      "loss: 1.5955331325531006 at epoch 63 at applicants training\n",
      "loss: 1.595000147819519 at epoch 64 at applicants training\n",
      "loss: 1.5944098234176636 at epoch 65 at applicants training\n",
      "loss: 1.5939252376556396 at epoch 66 at applicants training\n",
      "loss: 1.5934656858444214 at epoch 67 at applicants training\n",
      "loss: 1.5929120779037476 at epoch 68 at applicants training\n",
      "loss: 1.592589259147644 at epoch 69 at applicants training\n",
      "loss: 1.592630386352539 at epoch 70 at applicants training\n",
      "loss: 1.5937983989715576 at epoch 71 at applicants training\n",
      "loss: 1.5961827039718628 at epoch 72 at applicants training\n",
      "loss: 1.596901297569275 at epoch 73 at applicants training\n",
      "loss: 1.5951026678085327 at epoch 74 at applicants training\n",
      "loss: 1.5900928974151611 at epoch 75 at applicants training\n",
      "loss: 1.589634656906128 at epoch 76 at applicants training\n",
      "loss: 1.592617154121399 at epoch 77 at applicants training\n",
      "loss: 1.5926684141159058 at epoch 78 at applicants training\n",
      "loss: 1.5895713567733765 at epoch 79 at applicants training\n",
      "loss: 1.5873115062713623 at epoch 80 at applicants training\n",
      "loss: 1.5882611274719238 at epoch 81 at applicants training\n",
      "loss: 1.5895917415618896 at epoch 82 at applicants training\n",
      "loss: 1.588575005531311 at epoch 83 at applicants training\n",
      "loss: 1.5865615606307983 at epoch 84 at applicants training\n",
      "loss: 1.5852974653244019 at epoch 85 at applicants training\n",
      "loss: 1.5861819982528687 at epoch 86 at applicants training\n",
      "loss: 1.5875451564788818 at epoch 87 at applicants training\n",
      "loss: 1.5874402523040771 at epoch 88 at applicants training\n",
      "loss: 1.5858513116836548 at epoch 89 at applicants training\n",
      "loss: 1.5835808515548706 at epoch 90 at applicants training\n",
      "loss: 1.583512306213379 at epoch 91 at applicants training\n",
      "loss: 1.5837141275405884 at epoch 92 at applicants training\n",
      "loss: 1.5846377611160278 at epoch 93 at applicants training\n",
      "loss: 1.5846439599990845 at epoch 94 at applicants training\n",
      "loss: 1.5828118324279785 at epoch 95 at applicants training\n",
      "loss: 1.5819324254989624 at epoch 96 at applicants training\n",
      "loss: 1.5806846618652344 at epoch 97 at applicants training\n",
      "loss: 1.5808228254318237 at epoch 98 at applicants training\n",
      "loss: 1.58042573928833 at epoch 99 at applicants training\n",
      "loss: 1.6813809871673584 at epoch 0 at applicants training\n",
      "loss: 1.6827317476272583 at epoch 1 at applicants training\n",
      "loss: 1.7096115350723267 at epoch 2 at applicants training\n",
      "loss: 1.7036269903182983 at epoch 3 at applicants training\n",
      "loss: 1.6816877126693726 at epoch 4 at applicants training\n",
      "loss: 1.6936149597167969 at epoch 5 at applicants training\n",
      "loss: 1.6955100297927856 at epoch 6 at applicants training\n",
      "loss: 1.6900086402893066 at epoch 7 at applicants training\n",
      "loss: 1.6855368614196777 at epoch 8 at applicants training\n",
      "loss: 1.6956231594085693 at epoch 9 at applicants training\n",
      "loss: 1.6770045757293701 at epoch 10 at applicants training\n",
      "loss: 1.6754589080810547 at epoch 11 at applicants training\n",
      "loss: 1.6771351099014282 at epoch 12 at applicants training\n",
      "loss: 1.6758754253387451 at epoch 13 at applicants training\n",
      "loss: 1.6743443012237549 at epoch 14 at applicants training\n",
      "loss: 1.6693968772888184 at epoch 15 at applicants training\n",
      "loss: 1.666332483291626 at epoch 16 at applicants training\n",
      "loss: 1.6631901264190674 at epoch 17 at applicants training\n",
      "loss: 1.6591134071350098 at epoch 18 at applicants training\n",
      "loss: 1.656005620956421 at epoch 19 at applicants training\n",
      "loss: 1.6551460027694702 at epoch 20 at applicants training\n",
      "loss: 1.655785322189331 at epoch 21 at applicants training\n",
      "loss: 1.6573922634124756 at epoch 22 at applicants training\n",
      "loss: 1.6540521383285522 at epoch 23 at applicants training\n",
      "loss: 1.6516567468643188 at epoch 24 at applicants training\n",
      "loss: 1.6491025686264038 at epoch 25 at applicants training\n",
      "loss: 1.6480406522750854 at epoch 26 at applicants training\n",
      "loss: 1.6541448831558228 at epoch 27 at applicants training\n",
      "loss: 1.6517443656921387 at epoch 28 at applicants training\n",
      "loss: 1.6486425399780273 at epoch 29 at applicants training\n",
      "loss: 1.6546630859375 at epoch 30 at applicants training\n",
      "loss: 1.6466940641403198 at epoch 31 at applicants training\n",
      "loss: 1.6459861993789673 at epoch 32 at applicants training\n",
      "loss: 1.6438794136047363 at epoch 33 at applicants training\n",
      "loss: 1.645668625831604 at epoch 34 at applicants training\n",
      "loss: 1.6466249227523804 at epoch 35 at applicants training\n",
      "loss: 1.6431137323379517 at epoch 36 at applicants training\n",
      "loss: 1.6430332660675049 at epoch 37 at applicants training\n",
      "loss: 1.6432384252548218 at epoch 38 at applicants training\n",
      "loss: 1.6431480646133423 at epoch 39 at applicants training\n",
      "loss: 1.639744758605957 at epoch 40 at applicants training\n",
      "loss: 1.6401045322418213 at epoch 41 at applicants training\n",
      "loss: 1.638114333152771 at epoch 42 at applicants training\n",
      "loss: 1.640107274055481 at epoch 43 at applicants training\n",
      "loss: 1.6450695991516113 at epoch 44 at applicants training\n",
      "loss: 1.6403043270111084 at epoch 45 at applicants training\n",
      "loss: 1.6367360353469849 at epoch 46 at applicants training\n",
      "loss: 1.64125657081604 at epoch 47 at applicants training\n",
      "loss: 1.644763469696045 at epoch 48 at applicants training\n",
      "loss: 1.6364811658859253 at epoch 49 at applicants training\n",
      "loss: 1.642748236656189 at epoch 50 at applicants training\n",
      "loss: 1.639400839805603 at epoch 51 at applicants training\n",
      "loss: 1.6345776319503784 at epoch 52 at applicants training\n",
      "loss: 1.639877200126648 at epoch 53 at applicants training\n",
      "loss: 1.6348521709442139 at epoch 54 at applicants training\n",
      "loss: 1.6367136240005493 at epoch 55 at applicants training\n",
      "loss: 1.6360710859298706 at epoch 56 at applicants training\n",
      "loss: 1.633315086364746 at epoch 57 at applicants training\n",
      "loss: 1.634647011756897 at epoch 58 at applicants training\n",
      "loss: 1.6342169046401978 at epoch 59 at applicants training\n",
      "loss: 1.632527470588684 at epoch 60 at applicants training\n",
      "loss: 1.6323802471160889 at epoch 61 at applicants training\n",
      "loss: 1.633090853691101 at epoch 62 at applicants training\n",
      "loss: 1.6314501762390137 at epoch 63 at applicants training\n",
      "loss: 1.6312686204910278 at epoch 64 at applicants training\n",
      "loss: 1.6309540271759033 at epoch 65 at applicants training\n",
      "loss: 1.632332682609558 at epoch 66 at applicants training\n",
      "loss: 1.6310856342315674 at epoch 67 at applicants training\n",
      "loss: 1.6302140951156616 at epoch 68 at applicants training\n",
      "loss: 1.629265308380127 at epoch 69 at applicants training\n",
      "loss: 1.6294994354248047 at epoch 70 at applicants training\n",
      "loss: 1.6293009519577026 at epoch 71 at applicants training\n",
      "loss: 1.6292264461517334 at epoch 72 at applicants training\n",
      "loss: 1.628340482711792 at epoch 73 at applicants training\n",
      "loss: 1.6277858018875122 at epoch 74 at applicants training\n",
      "loss: 1.627665638923645 at epoch 75 at applicants training\n",
      "loss: 1.6275838613510132 at epoch 76 at applicants training\n",
      "loss: 1.6280896663665771 at epoch 77 at applicants training\n",
      "loss: 1.6296871900558472 at epoch 78 at applicants training\n",
      "loss: 1.629051923751831 at epoch 79 at applicants training\n",
      "loss: 1.6284475326538086 at epoch 80 at applicants training\n",
      "loss: 1.625962257385254 at epoch 81 at applicants training\n",
      "loss: 1.6258959770202637 at epoch 82 at applicants training\n",
      "loss: 1.6278332471847534 at epoch 83 at applicants training\n",
      "loss: 1.6272648572921753 at epoch 84 at applicants training\n",
      "loss: 1.626529574394226 at epoch 85 at applicants training\n",
      "loss: 1.6243736743927002 at epoch 86 at applicants training\n",
      "loss: 1.6230988502502441 at epoch 87 at applicants training\n",
      "loss: 1.6228139400482178 at epoch 88 at applicants training\n",
      "loss: 1.622779369354248 at epoch 89 at applicants training\n",
      "loss: 1.623996615409851 at epoch 90 at applicants training\n",
      "loss: 1.626468539237976 at epoch 91 at applicants training\n",
      "loss: 1.6262675523757935 at epoch 92 at applicants training\n",
      "loss: 1.6208065748214722 at epoch 93 at applicants training\n",
      "loss: 1.6207489967346191 at epoch 94 at applicants training\n",
      "loss: 1.623363971710205 at epoch 95 at applicants training\n",
      "loss: 1.6258355379104614 at epoch 96 at applicants training\n",
      "loss: 1.619941234588623 at epoch 97 at applicants training\n",
      "loss: 1.6186176538467407 at epoch 98 at applicants training\n",
      "loss: 1.6180082559585571 at epoch 99 at applicants training\n",
      "loss: 1.6788291931152344 at epoch 0 at applicants training\n",
      "loss: 1.7039036750793457 at epoch 1 at applicants training\n",
      "loss: 1.677694320678711 at epoch 2 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 3 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 4 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.7181949615478516 at epoch 0 at applicants training\n",
      "loss: 1.690269947052002 at epoch 1 at applicants training\n",
      "loss: 1.6806950569152832 at epoch 2 at applicants training\n",
      "loss: 1.6809297800064087 at epoch 3 at applicants training\n",
      "loss: 1.6885851621627808 at epoch 4 at applicants training\n",
      "loss: 1.6846928596496582 at epoch 5 at applicants training\n",
      "loss: 1.676586627960205 at epoch 6 at applicants training\n",
      "loss: 1.6723958253860474 at epoch 7 at applicants training\n",
      "loss: 1.6652610301971436 at epoch 8 at applicants training\n",
      "loss: 1.6662956476211548 at epoch 9 at applicants training\n",
      "loss: 1.659489631652832 at epoch 10 at applicants training\n",
      "loss: 1.6691557168960571 at epoch 11 at applicants training\n",
      "loss: 1.6661118268966675 at epoch 12 at applicants training\n",
      "loss: 1.6758334636688232 at epoch 13 at applicants training\n",
      "loss: 1.6775281429290771 at epoch 14 at applicants training\n",
      "loss: 1.6756521463394165 at epoch 15 at applicants training\n",
      "loss: 1.666910171508789 at epoch 16 at applicants training\n",
      "loss: 1.6550421714782715 at epoch 17 at applicants training\n",
      "loss: 1.6793553829193115 at epoch 18 at applicants training\n",
      "loss: 1.6718202829360962 at epoch 19 at applicants training\n",
      "loss: 1.6567163467407227 at epoch 20 at applicants training\n",
      "loss: 1.6655372381210327 at epoch 21 at applicants training\n",
      "loss: 1.6696863174438477 at epoch 22 at applicants training\n",
      "loss: 1.6650270223617554 at epoch 23 at applicants training\n",
      "loss: 1.6571279764175415 at epoch 24 at applicants training\n",
      "loss: 1.6570416688919067 at epoch 25 at applicants training\n",
      "loss: 1.6623444557189941 at epoch 26 at applicants training\n",
      "loss: 1.6593393087387085 at epoch 27 at applicants training\n",
      "loss: 1.6566911935806274 at epoch 28 at applicants training\n",
      "loss: 1.6557466983795166 at epoch 29 at applicants training\n",
      "loss: 1.657014012336731 at epoch 30 at applicants training\n",
      "loss: 1.6584293842315674 at epoch 31 at applicants training\n",
      "loss: 1.6569722890853882 at epoch 32 at applicants training\n",
      "loss: 1.655402660369873 at epoch 33 at applicants training\n",
      "loss: 1.6550719738006592 at epoch 34 at applicants training\n",
      "loss: 1.6517503261566162 at epoch 35 at applicants training\n",
      "loss: 1.6616617441177368 at epoch 36 at applicants training\n",
      "loss: 1.6504673957824707 at epoch 37 at applicants training\n",
      "loss: 1.6523516178131104 at epoch 38 at applicants training\n",
      "loss: 1.6598411798477173 at epoch 39 at applicants training\n",
      "loss: 1.6631101369857788 at epoch 40 at applicants training\n",
      "loss: 1.6567317247390747 at epoch 41 at applicants training\n",
      "loss: 1.6505287885665894 at epoch 42 at applicants training\n",
      "loss: 1.6492972373962402 at epoch 43 at applicants training\n",
      "loss: 1.6531676054000854 at epoch 44 at applicants training\n",
      "loss: 1.649836540222168 at epoch 45 at applicants training\n",
      "loss: 1.6525914669036865 at epoch 46 at applicants training\n",
      "loss: 1.6567836999893188 at epoch 47 at applicants training\n",
      "loss: 1.6540052890777588 at epoch 48 at applicants training\n",
      "loss: 1.6505812406539917 at epoch 49 at applicants training\n",
      "loss: 1.648826241493225 at epoch 50 at applicants training\n",
      "loss: 1.656175971031189 at epoch 51 at applicants training\n",
      "loss: 1.648868441581726 at epoch 52 at applicants training\n",
      "loss: 1.6498233079910278 at epoch 53 at applicants training\n",
      "loss: 1.6507086753845215 at epoch 54 at applicants training\n",
      "loss: 1.6508511304855347 at epoch 55 at applicants training\n",
      "loss: 1.6511147022247314 at epoch 56 at applicants training\n",
      "loss: 1.649753451347351 at epoch 57 at applicants training\n",
      "loss: 1.6477644443511963 at epoch 58 at applicants training\n",
      "loss: 1.6468178033828735 at epoch 59 at applicants training\n",
      "loss: 1.6476805210113525 at epoch 60 at applicants training\n",
      "loss: 1.6464619636535645 at epoch 61 at applicants training\n",
      "loss: 1.6442335844039917 at epoch 62 at applicants training\n",
      "loss: 1.645719051361084 at epoch 63 at applicants training\n",
      "loss: 1.6447956562042236 at epoch 64 at applicants training\n",
      "loss: 1.6422120332717896 at epoch 65 at applicants training\n",
      "loss: 1.6432337760925293 at epoch 66 at applicants training\n",
      "loss: 1.64127516746521 at epoch 67 at applicants training\n",
      "loss: 1.6415444612503052 at epoch 68 at applicants training\n",
      "loss: 1.640119194984436 at epoch 69 at applicants training\n",
      "loss: 1.6391645669937134 at epoch 70 at applicants training\n",
      "loss: 1.6399821043014526 at epoch 71 at applicants training\n",
      "loss: 1.6379326581954956 at epoch 72 at applicants training\n",
      "loss: 1.6388144493103027 at epoch 73 at applicants training\n",
      "loss: 1.6375082731246948 at epoch 74 at applicants training\n",
      "loss: 1.6368687152862549 at epoch 75 at applicants training\n",
      "loss: 1.6375271081924438 at epoch 76 at applicants training\n",
      "loss: 1.6359889507293701 at epoch 77 at applicants training\n",
      "loss: 1.6357128620147705 at epoch 78 at applicants training\n",
      "loss: 1.636039137840271 at epoch 79 at applicants training\n",
      "loss: 1.6347564458847046 at epoch 80 at applicants training\n",
      "loss: 1.6339666843414307 at epoch 81 at applicants training\n",
      "loss: 1.6344083547592163 at epoch 82 at applicants training\n",
      "loss: 1.6341488361358643 at epoch 83 at applicants training\n",
      "loss: 1.6325584650039673 at epoch 84 at applicants training\n",
      "loss: 1.6327968835830688 at epoch 85 at applicants training\n",
      "loss: 1.6338140964508057 at epoch 86 at applicants training\n",
      "loss: 1.631824254989624 at epoch 87 at applicants training\n",
      "loss: 1.631095290184021 at epoch 88 at applicants training\n",
      "loss: 1.6315438747406006 at epoch 89 at applicants training\n",
      "loss: 1.63232421875 at epoch 90 at applicants training\n",
      "loss: 1.6331417560577393 at epoch 91 at applicants training\n",
      "loss: 1.6319947242736816 at epoch 92 at applicants training\n",
      "loss: 1.6306520700454712 at epoch 93 at applicants training\n",
      "loss: 1.6297543048858643 at epoch 94 at applicants training\n",
      "loss: 1.6293973922729492 at epoch 95 at applicants training\n",
      "loss: 1.629250168800354 at epoch 96 at applicants training\n",
      "loss: 1.6292970180511475 at epoch 97 at applicants training\n",
      "loss: 1.6302640438079834 at epoch 98 at applicants training\n",
      "loss: 1.6324248313903809 at epoch 99 at applicants training\n",
      "loss: 1.7121975421905518 at epoch 0 at applicants training\n",
      "loss: 1.704864263534546 at epoch 1 at applicants training\n",
      "loss: 1.6766537427902222 at epoch 2 at applicants training\n",
      "loss: 1.6798808574676514 at epoch 3 at applicants training\n",
      "loss: 1.6791847944259644 at epoch 4 at applicants training\n",
      "loss: 1.672539234161377 at epoch 5 at applicants training\n",
      "loss: 1.669488549232483 at epoch 6 at applicants training\n",
      "loss: 1.6633604764938354 at epoch 7 at applicants training\n",
      "loss: 1.6572372913360596 at epoch 8 at applicants training\n",
      "loss: 1.6536717414855957 at epoch 9 at applicants training\n",
      "loss: 1.6499178409576416 at epoch 10 at applicants training\n",
      "loss: 1.6505458354949951 at epoch 11 at applicants training\n",
      "loss: 1.6504640579223633 at epoch 12 at applicants training\n",
      "loss: 1.6424580812454224 at epoch 13 at applicants training\n",
      "loss: 1.6351101398468018 at epoch 14 at applicants training\n",
      "loss: 1.6635421514511108 at epoch 15 at applicants training\n",
      "loss: 1.6451224088668823 at epoch 16 at applicants training\n",
      "loss: 1.6534430980682373 at epoch 17 at applicants training\n",
      "loss: 1.6467126607894897 at epoch 18 at applicants training\n",
      "loss: 1.6458632946014404 at epoch 19 at applicants training\n",
      "loss: 1.6370041370391846 at epoch 20 at applicants training\n",
      "loss: 1.6334078311920166 at epoch 21 at applicants training\n",
      "loss: 1.627929925918579 at epoch 22 at applicants training\n",
      "loss: 1.6271135807037354 at epoch 23 at applicants training\n",
      "loss: 1.63167405128479 at epoch 24 at applicants training\n",
      "loss: 1.624462366104126 at epoch 25 at applicants training\n",
      "loss: 1.6176677942276 at epoch 26 at applicants training\n",
      "loss: 1.616405963897705 at epoch 27 at applicants training\n",
      "loss: 1.6191225051879883 at epoch 28 at applicants training\n",
      "loss: 1.6159484386444092 at epoch 29 at applicants training\n",
      "loss: 1.6175016164779663 at epoch 30 at applicants training\n",
      "loss: 1.6136661767959595 at epoch 31 at applicants training\n",
      "loss: 1.611297607421875 at epoch 32 at applicants training\n",
      "loss: 1.6123191118240356 at epoch 33 at applicants training\n",
      "loss: 1.6084576845169067 at epoch 34 at applicants training\n",
      "loss: 1.6110918521881104 at epoch 35 at applicants training\n",
      "loss: 1.606359839439392 at epoch 36 at applicants training\n",
      "loss: 1.6046627759933472 at epoch 37 at applicants training\n",
      "loss: 1.6062439680099487 at epoch 38 at applicants training\n",
      "loss: 1.6034573316574097 at epoch 39 at applicants training\n",
      "loss: 1.6032766103744507 at epoch 40 at applicants training\n",
      "loss: 1.6022952795028687 at epoch 41 at applicants training\n",
      "loss: 1.6017892360687256 at epoch 42 at applicants training\n",
      "loss: 1.6018768548965454 at epoch 43 at applicants training\n",
      "loss: 1.5977373123168945 at epoch 44 at applicants training\n",
      "loss: 1.5989857912063599 at epoch 45 at applicants training\n",
      "loss: 1.5968009233474731 at epoch 46 at applicants training\n",
      "loss: 1.59603750705719 at epoch 47 at applicants training\n",
      "loss: 1.5967730283737183 at epoch 48 at applicants training\n",
      "loss: 1.5948787927627563 at epoch 49 at applicants training\n",
      "loss: 1.595855712890625 at epoch 50 at applicants training\n",
      "loss: 1.5938043594360352 at epoch 51 at applicants training\n",
      "loss: 1.593855619430542 at epoch 52 at applicants training\n",
      "loss: 1.5926668643951416 at epoch 53 at applicants training\n",
      "loss: 1.5924420356750488 at epoch 54 at applicants training\n",
      "loss: 1.592171311378479 at epoch 55 at applicants training\n",
      "loss: 1.5909676551818848 at epoch 56 at applicants training\n",
      "loss: 1.5903009176254272 at epoch 57 at applicants training\n",
      "loss: 1.5893033742904663 at epoch 58 at applicants training\n",
      "loss: 1.5882327556610107 at epoch 59 at applicants training\n",
      "loss: 1.5881379842758179 at epoch 60 at applicants training\n",
      "loss: 1.5869663953781128 at epoch 61 at applicants training\n",
      "loss: 1.586851716041565 at epoch 62 at applicants training\n",
      "loss: 1.5864993333816528 at epoch 63 at applicants training\n",
      "loss: 1.5855662822723389 at epoch 64 at applicants training\n",
      "loss: 1.5849823951721191 at epoch 65 at applicants training\n",
      "loss: 1.5851484537124634 at epoch 66 at applicants training\n",
      "loss: 1.5851387977600098 at epoch 67 at applicants training\n",
      "loss: 1.5836366415023804 at epoch 68 at applicants training\n",
      "loss: 1.5862343311309814 at epoch 69 at applicants training\n",
      "loss: 1.5873905420303345 at epoch 70 at applicants training\n",
      "loss: 1.5890941619873047 at epoch 71 at applicants training\n",
      "loss: 1.5828373432159424 at epoch 72 at applicants training\n",
      "loss: 1.5937734842300415 at epoch 73 at applicants training\n",
      "loss: 1.5900647640228271 at epoch 74 at applicants training\n",
      "loss: 1.5985825061798096 at epoch 75 at applicants training\n",
      "loss: 1.5990244150161743 at epoch 76 at applicants training\n",
      "loss: 1.5953688621520996 at epoch 77 at applicants training\n",
      "loss: 1.5975675582885742 at epoch 78 at applicants training\n",
      "loss: 1.6003342866897583 at epoch 79 at applicants training\n",
      "loss: 1.5953034162521362 at epoch 80 at applicants training\n",
      "loss: 1.5946751832962036 at epoch 81 at applicants training\n",
      "loss: 1.5988553762435913 at epoch 82 at applicants training\n",
      "loss: 1.5964672565460205 at epoch 83 at applicants training\n",
      "loss: 1.5926809310913086 at epoch 84 at applicants training\n",
      "loss: 1.5941344499588013 at epoch 85 at applicants training\n",
      "loss: 1.5954008102416992 at epoch 86 at applicants training\n",
      "loss: 1.5938392877578735 at epoch 87 at applicants training\n",
      "loss: 1.591904640197754 at epoch 88 at applicants training\n",
      "loss: 1.5942504405975342 at epoch 89 at applicants training\n",
      "loss: 1.594103217124939 at epoch 90 at applicants training\n",
      "loss: 1.5909374952316284 at epoch 91 at applicants training\n",
      "loss: 1.5930448770523071 at epoch 92 at applicants training\n",
      "loss: 1.5954135656356812 at epoch 93 at applicants training\n",
      "loss: 1.5910661220550537 at epoch 94 at applicants training\n",
      "loss: 1.5909932851791382 at epoch 95 at applicants training\n",
      "loss: 1.5943660736083984 at epoch 96 at applicants training\n",
      "loss: 1.5898995399475098 at epoch 97 at applicants training\n",
      "loss: 1.5900694131851196 at epoch 98 at applicants training\n",
      "loss: 1.5920759439468384 at epoch 99 at applicants training\n",
      "loss: 1.7113642692565918 at epoch 0 at applicants training\n",
      "loss: 1.6851377487182617 at epoch 1 at applicants training\n",
      "loss: 1.6787687540054321 at epoch 2 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 3 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 4 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.7114505767822266 at epoch 0 at applicants training\n",
      "loss: 1.6937572956085205 at epoch 1 at applicants training\n",
      "loss: 1.7113661766052246 at epoch 2 at applicants training\n",
      "loss: 1.7088779211044312 at epoch 3 at applicants training\n",
      "loss: 1.6989693641662598 at epoch 4 at applicants training\n",
      "loss: 1.688446044921875 at epoch 5 at applicants training\n",
      "loss: 1.6937224864959717 at epoch 6 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 7 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 8 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 9 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 10 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 11 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 12 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 13 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 14 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 15 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 16 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 17 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 18 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 19 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 20 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 21 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 22 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 23 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 24 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 25 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 26 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 27 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 28 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 29 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 30 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 31 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 32 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 33 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 34 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 35 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 36 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 37 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 38 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 39 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 40 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 41 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 42 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 43 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 44 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 45 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 46 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 47 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 48 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 49 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 50 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 51 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 52 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 53 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 54 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 55 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 56 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 57 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 58 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 59 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 60 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 61 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 62 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 63 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 64 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 65 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 66 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 67 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 68 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 69 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 70 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 71 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 72 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 73 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 74 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 75 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 76 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 77 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 78 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 79 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 80 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 81 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 82 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 83 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 84 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 85 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 86 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 87 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 88 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 89 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 90 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 91 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 92 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 93 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 94 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 95 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 96 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 97 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 98 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 99 at applicants training\n",
      "loss: 1.6565779447555542 at epoch 0 at applicants training\n",
      "loss: 1.6677945852279663 at epoch 1 at applicants training\n",
      "loss: 1.6699702739715576 at epoch 2 at applicants training\n",
      "loss: 1.6686439514160156 at epoch 3 at applicants training\n",
      "loss: 1.6619236469268799 at epoch 4 at applicants training\n",
      "loss: 1.667157769203186 at epoch 5 at applicants training\n",
      "loss: 1.6579482555389404 at epoch 6 at applicants training\n",
      "loss: 1.6653493642807007 at epoch 7 at applicants training\n",
      "loss: 1.658504605293274 at epoch 8 at applicants training\n",
      "loss: 1.6564136743545532 at epoch 9 at applicants training\n",
      "loss: 1.6555290222167969 at epoch 10 at applicants training\n",
      "loss: 1.64845871925354 at epoch 11 at applicants training\n",
      "loss: 1.6536047458648682 at epoch 12 at applicants training\n",
      "loss: 1.645702600479126 at epoch 13 at applicants training\n",
      "loss: 1.6434613466262817 at epoch 14 at applicants training\n",
      "loss: 1.650166392326355 at epoch 15 at applicants training\n",
      "loss: 1.6408100128173828 at epoch 16 at applicants training\n",
      "loss: 1.6484133005142212 at epoch 17 at applicants training\n",
      "loss: 1.65145742893219 at epoch 18 at applicants training\n",
      "loss: 1.6503751277923584 at epoch 19 at applicants training\n",
      "loss: 1.6477242708206177 at epoch 20 at applicants training\n",
      "loss: 1.6375874280929565 at epoch 21 at applicants training\n",
      "loss: 1.6436251401901245 at epoch 22 at applicants training\n",
      "loss: 1.6447806358337402 at epoch 23 at applicants training\n",
      "loss: 1.6369235515594482 at epoch 24 at applicants training\n",
      "loss: 1.6304982900619507 at epoch 25 at applicants training\n",
      "loss: 1.6365506649017334 at epoch 26 at applicants training\n",
      "loss: 1.6370021104812622 at epoch 27 at applicants training\n",
      "loss: 1.6294314861297607 at epoch 28 at applicants training\n",
      "loss: 1.6283953189849854 at epoch 29 at applicants training\n",
      "loss: 1.6285325288772583 at epoch 30 at applicants training\n",
      "loss: 1.6273959875106812 at epoch 31 at applicants training\n",
      "loss: 1.6264560222625732 at epoch 32 at applicants training\n",
      "loss: 1.6241223812103271 at epoch 33 at applicants training\n",
      "loss: 1.6215208768844604 at epoch 34 at applicants training\n",
      "loss: 1.620276689529419 at epoch 35 at applicants training\n",
      "loss: 1.6199811697006226 at epoch 36 at applicants training\n",
      "loss: 1.6172640323638916 at epoch 37 at applicants training\n",
      "loss: 1.6172735691070557 at epoch 38 at applicants training\n",
      "loss: 1.6200850009918213 at epoch 39 at applicants training\n",
      "loss: 1.6186633110046387 at epoch 40 at applicants training\n",
      "loss: 1.6143971681594849 at epoch 41 at applicants training\n",
      "loss: 1.613633632659912 at epoch 42 at applicants training\n",
      "loss: 1.6137036085128784 at epoch 43 at applicants training\n",
      "loss: 1.6162689924240112 at epoch 44 at applicants training\n",
      "loss: 1.6112815141677856 at epoch 45 at applicants training\n",
      "loss: 1.6099669933319092 at epoch 46 at applicants training\n",
      "loss: 1.6088799238204956 at epoch 47 at applicants training\n",
      "loss: 1.610261082649231 at epoch 48 at applicants training\n",
      "loss: 1.6120160818099976 at epoch 49 at applicants training\n",
      "loss: 1.6080408096313477 at epoch 50 at applicants training\n",
      "loss: 1.6080584526062012 at epoch 51 at applicants training\n",
      "loss: 1.6095314025878906 at epoch 52 at applicants training\n",
      "loss: 1.6084256172180176 at epoch 53 at applicants training\n",
      "loss: 1.6101577281951904 at epoch 54 at applicants training\n",
      "loss: 1.6073787212371826 at epoch 55 at applicants training\n",
      "loss: 1.616252064704895 at epoch 56 at applicants training\n",
      "loss: 1.6056246757507324 at epoch 57 at applicants training\n",
      "loss: 1.610877513885498 at epoch 58 at applicants training\n",
      "loss: 1.6030384302139282 at epoch 59 at applicants training\n",
      "loss: 1.6080152988433838 at epoch 60 at applicants training\n",
      "loss: 1.6043221950531006 at epoch 61 at applicants training\n",
      "loss: 1.6055216789245605 at epoch 62 at applicants training\n",
      "loss: 1.6015721559524536 at epoch 63 at applicants training\n",
      "loss: 1.6004327535629272 at epoch 64 at applicants training\n",
      "loss: 1.6012109518051147 at epoch 65 at applicants training\n",
      "loss: 1.5994755029678345 at epoch 66 at applicants training\n",
      "loss: 1.5996174812316895 at epoch 67 at applicants training\n",
      "loss: 1.595371127128601 at epoch 68 at applicants training\n",
      "loss: 1.5970350503921509 at epoch 69 at applicants training\n",
      "loss: 1.5954760313034058 at epoch 70 at applicants training\n",
      "loss: 1.5964405536651611 at epoch 71 at applicants training\n",
      "loss: 1.5945074558258057 at epoch 72 at applicants training\n",
      "loss: 1.591871738433838 at epoch 73 at applicants training\n",
      "loss: 1.5929780006408691 at epoch 74 at applicants training\n",
      "loss: 1.5921212434768677 at epoch 75 at applicants training\n",
      "loss: 1.5936964750289917 at epoch 76 at applicants training\n",
      "loss: 1.5968269109725952 at epoch 77 at applicants training\n",
      "loss: 1.5913488864898682 at epoch 78 at applicants training\n",
      "loss: 1.5883883237838745 at epoch 79 at applicants training\n",
      "loss: 1.5928579568862915 at epoch 80 at applicants training\n",
      "loss: 1.5941590070724487 at epoch 81 at applicants training\n",
      "loss: 1.5877138376235962 at epoch 82 at applicants training\n",
      "loss: 1.5865880250930786 at epoch 83 at applicants training\n",
      "loss: 1.5890328884124756 at epoch 84 at applicants training\n",
      "loss: 1.5876579284667969 at epoch 85 at applicants training\n",
      "loss: 1.586742877960205 at epoch 86 at applicants training\n",
      "loss: 1.5871715545654297 at epoch 87 at applicants training\n",
      "loss: 1.5872944593429565 at epoch 88 at applicants training\n",
      "loss: 1.5893380641937256 at epoch 89 at applicants training\n",
      "loss: 1.5835151672363281 at epoch 90 at applicants training\n",
      "loss: 1.5888069868087769 at epoch 91 at applicants training\n",
      "loss: 1.5836724042892456 at epoch 92 at applicants training\n",
      "loss: 1.5814192295074463 at epoch 93 at applicants training\n",
      "loss: 1.5877794027328491 at epoch 94 at applicants training\n",
      "loss: 1.5832732915878296 at epoch 95 at applicants training\n",
      "loss: 1.582800030708313 at epoch 96 at applicants training\n",
      "loss: 1.5829473733901978 at epoch 97 at applicants training\n",
      "loss: 1.5828607082366943 at epoch 98 at applicants training\n",
      "loss: 1.583566665649414 at epoch 99 at applicants training\n",
      "loss: 1.6788403987884521 at epoch 0 at applicants training\n",
      "loss: 1.7038183212280273 at epoch 1 at applicants training\n",
      "loss: 1.67681884765625 at epoch 2 at applicants training\n",
      "loss: 1.6782398223876953 at epoch 3 at applicants training\n",
      "loss: 1.678829312324524 at epoch 4 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.7178325653076172 at epoch 0 at applicants training\n",
      "loss: 1.7113432884216309 at epoch 1 at applicants training\n",
      "loss: 1.6901946067810059 at epoch 2 at applicants training\n",
      "loss: 1.675445556640625 at epoch 3 at applicants training\n",
      "loss: 1.6842737197875977 at epoch 4 at applicants training\n",
      "loss: 1.6755363941192627 at epoch 5 at applicants training\n",
      "loss: 1.6785190105438232 at epoch 6 at applicants training\n",
      "loss: 1.6789946556091309 at epoch 7 at applicants training\n",
      "loss: 1.6790648698806763 at epoch 8 at applicants training\n",
      "loss: 1.6790072917938232 at epoch 9 at applicants training\n",
      "loss: 1.678955316543579 at epoch 10 at applicants training\n",
      "loss: 1.6788805723190308 at epoch 11 at applicants training\n",
      "loss: 1.6788036823272705 at epoch 12 at applicants training\n",
      "loss: 1.6786772012710571 at epoch 13 at applicants training\n",
      "loss: 1.678177833557129 at epoch 14 at applicants training\n",
      "loss: 1.6760246753692627 at epoch 15 at applicants training\n",
      "loss: 1.6688700914382935 at epoch 16 at applicants training\n",
      "loss: 1.668940544128418 at epoch 17 at applicants training\n",
      "loss: 1.6816391944885254 at epoch 18 at applicants training\n",
      "loss: 1.6667561531066895 at epoch 19 at applicants training\n",
      "loss: 1.6688481569290161 at epoch 20 at applicants training\n",
      "loss: 1.6710811853408813 at epoch 21 at applicants training\n",
      "loss: 1.664613962173462 at epoch 22 at applicants training\n",
      "loss: 1.6662226915359497 at epoch 23 at applicants training\n",
      "loss: 1.6683579683303833 at epoch 24 at applicants training\n",
      "loss: 1.6667466163635254 at epoch 25 at applicants training\n",
      "loss: 1.6641061305999756 at epoch 26 at applicants training\n",
      "loss: 1.66245436668396 at epoch 27 at applicants training\n",
      "loss: 1.6650545597076416 at epoch 28 at applicants training\n",
      "loss: 1.661757230758667 at epoch 29 at applicants training\n",
      "loss: 1.662247657775879 at epoch 30 at applicants training\n",
      "loss: 1.662024974822998 at epoch 31 at applicants training\n",
      "loss: 1.6592901945114136 at epoch 32 at applicants training\n",
      "loss: 1.6589120626449585 at epoch 33 at applicants training\n",
      "loss: 1.658029317855835 at epoch 34 at applicants training\n",
      "loss: 1.6576483249664307 at epoch 35 at applicants training\n",
      "loss: 1.658023715019226 at epoch 36 at applicants training\n",
      "loss: 1.6564695835113525 at epoch 37 at applicants training\n",
      "loss: 1.6577374935150146 at epoch 38 at applicants training\n",
      "loss: 1.6565412282943726 at epoch 39 at applicants training\n",
      "loss: 1.6563423871994019 at epoch 40 at applicants training\n",
      "loss: 1.657579779624939 at epoch 41 at applicants training\n",
      "loss: 1.6556098461151123 at epoch 42 at applicants training\n",
      "loss: 1.6575219631195068 at epoch 43 at applicants training\n",
      "loss: 1.6555163860321045 at epoch 44 at applicants training\n",
      "loss: 1.6561031341552734 at epoch 45 at applicants training\n",
      "loss: 1.6554604768753052 at epoch 46 at applicants training\n",
      "loss: 1.6543484926223755 at epoch 47 at applicants training\n",
      "loss: 1.6560636758804321 at epoch 48 at applicants training\n",
      "loss: 1.6529351472854614 at epoch 49 at applicants training\n",
      "loss: 1.6552395820617676 at epoch 50 at applicants training\n",
      "loss: 1.6572238206863403 at epoch 51 at applicants training\n",
      "loss: 1.6540167331695557 at epoch 52 at applicants training\n",
      "loss: 1.6608846187591553 at epoch 53 at applicants training\n",
      "loss: 1.652664303779602 at epoch 54 at applicants training\n",
      "loss: 1.6589256525039673 at epoch 55 at applicants training\n",
      "loss: 1.6557751893997192 at epoch 56 at applicants training\n",
      "loss: 1.656207799911499 at epoch 57 at applicants training\n",
      "loss: 1.6539777517318726 at epoch 58 at applicants training\n",
      "loss: 1.654883861541748 at epoch 59 at applicants training\n",
      "loss: 1.6551826000213623 at epoch 60 at applicants training\n",
      "loss: 1.6524134874343872 at epoch 61 at applicants training\n",
      "loss: 1.6561470031738281 at epoch 62 at applicants training\n",
      "loss: 1.6538176536560059 at epoch 63 at applicants training\n",
      "loss: 1.6555330753326416 at epoch 64 at applicants training\n",
      "loss: 1.651600956916809 at epoch 65 at applicants training\n",
      "loss: 1.6565771102905273 at epoch 66 at applicants training\n",
      "loss: 1.6534055471420288 at epoch 67 at applicants training\n",
      "loss: 1.6551166772842407 at epoch 68 at applicants training\n",
      "loss: 1.651126503944397 at epoch 69 at applicants training\n",
      "loss: 1.6574496030807495 at epoch 70 at applicants training\n",
      "loss: 1.6518330574035645 at epoch 71 at applicants training\n",
      "loss: 1.653926968574524 at epoch 72 at applicants training\n",
      "loss: 1.650985836982727 at epoch 73 at applicants training\n",
      "loss: 1.6548986434936523 at epoch 74 at applicants training\n",
      "loss: 1.6511470079421997 at epoch 75 at applicants training\n",
      "loss: 1.6523244380950928 at epoch 76 at applicants training\n",
      "loss: 1.6500627994537354 at epoch 77 at applicants training\n",
      "loss: 1.6508148908615112 at epoch 78 at applicants training\n",
      "loss: 1.651602864265442 at epoch 79 at applicants training\n",
      "loss: 1.6500276327133179 at epoch 80 at applicants training\n",
      "loss: 1.6530629396438599 at epoch 81 at applicants training\n",
      "loss: 1.6496586799621582 at epoch 82 at applicants training\n",
      "loss: 1.6502195596694946 at epoch 83 at applicants training\n",
      "loss: 1.650076150894165 at epoch 84 at applicants training\n",
      "loss: 1.648236870765686 at epoch 85 at applicants training\n",
      "loss: 1.648853063583374 at epoch 86 at applicants training\n",
      "loss: 1.648608684539795 at epoch 87 at applicants training\n",
      "loss: 1.6477153301239014 at epoch 88 at applicants training\n",
      "loss: 1.6475411653518677 at epoch 89 at applicants training\n",
      "loss: 1.647922396659851 at epoch 90 at applicants training\n",
      "loss: 1.6477086544036865 at epoch 91 at applicants training\n",
      "loss: 1.646915316581726 at epoch 92 at applicants training\n",
      "loss: 1.6470175981521606 at epoch 93 at applicants training\n",
      "loss: 1.647443413734436 at epoch 94 at applicants training\n",
      "loss: 1.6467370986938477 at epoch 95 at applicants training\n",
      "loss: 1.6462202072143555 at epoch 96 at applicants training\n",
      "loss: 1.6462346315383911 at epoch 97 at applicants training\n",
      "loss: 1.646410346031189 at epoch 98 at applicants training\n",
      "loss: 1.6464985609054565 at epoch 99 at applicants training\n",
      "loss: 1.7006837129592896 at epoch 0 at applicants training\n",
      "loss: 1.6787950992584229 at epoch 1 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 2 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 3 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 4 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.693877935409546 at epoch 0 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 1 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 2 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 3 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 4 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 5 at applicants training\n",
      "loss: 1.6938319206237793 at epoch 6 at applicants training\n",
      "loss: 1.6937978267669678 at epoch 7 at applicants training\n",
      "loss: 1.6933034658432007 at epoch 8 at applicants training\n",
      "loss: 1.693832278251648 at epoch 9 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 10 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 11 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 12 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 13 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 14 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 15 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 16 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 17 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 18 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 19 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 20 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 21 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 22 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 23 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 24 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 25 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 26 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 27 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 28 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 29 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 30 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 31 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 32 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 33 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 34 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 35 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 36 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 37 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 38 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 39 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 40 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 41 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 42 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 43 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 44 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 45 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 46 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 47 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 48 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 49 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 50 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 51 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 52 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 53 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 54 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 55 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 56 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 57 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 58 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 59 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 60 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 61 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 62 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 63 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 64 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 65 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 66 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 67 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 68 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 69 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 70 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 71 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 72 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 73 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 74 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 75 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 76 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 77 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 78 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 79 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 80 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 81 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 82 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 83 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 84 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 85 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 86 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 87 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 88 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 89 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 90 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 91 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 92 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 93 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 94 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 95 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 96 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 97 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 98 at applicants training\n",
      "loss: 1.6938326358795166 at epoch 99 at applicants training\n",
      "loss: 1.7107677459716797 at epoch 0 at applicants training\n",
      "loss: 1.720255732536316 at epoch 1 at applicants training\n",
      "loss: 1.7186952829360962 at epoch 2 at applicants training\n",
      "loss: 1.70619535446167 at epoch 3 at applicants training\n",
      "loss: 1.7036654949188232 at epoch 4 at applicants training\n",
      "loss: 1.7111808061599731 at epoch 5 at applicants training\n",
      "loss: 1.7124308347702026 at epoch 6 at applicants training\n",
      "loss: 1.7121974229812622 at epoch 7 at applicants training\n",
      "loss: 1.7119643688201904 at epoch 8 at applicants training\n",
      "loss: 1.7118005752563477 at epoch 9 at applicants training\n",
      "loss: 1.7111153602600098 at epoch 10 at applicants training\n",
      "loss: 1.706222414970398 at epoch 11 at applicants training\n",
      "loss: 1.7027454376220703 at epoch 12 at applicants training\n",
      "loss: 1.7032010555267334 at epoch 13 at applicants training\n",
      "loss: 1.6991716623306274 at epoch 14 at applicants training\n",
      "loss: 1.6974718570709229 at epoch 15 at applicants training\n",
      "loss: 1.6940122842788696 at epoch 16 at applicants training\n",
      "loss: 1.689440131187439 at epoch 17 at applicants training\n",
      "loss: 1.684298038482666 at epoch 18 at applicants training\n",
      "loss: 1.6846767663955688 at epoch 19 at applicants training\n",
      "loss: 1.6918631792068481 at epoch 20 at applicants training\n",
      "loss: 1.6841844320297241 at epoch 21 at applicants training\n",
      "loss: 1.6853446960449219 at epoch 22 at applicants training\n",
      "loss: 1.68104887008667 at epoch 23 at applicants training\n",
      "loss: 1.6769381761550903 at epoch 24 at applicants training\n",
      "loss: 1.684017300605774 at epoch 25 at applicants training\n",
      "loss: 1.672065258026123 at epoch 26 at applicants training\n",
      "loss: 1.6784489154815674 at epoch 27 at applicants training\n",
      "loss: 1.671865463256836 at epoch 28 at applicants training\n",
      "loss: 1.674271821975708 at epoch 29 at applicants training\n",
      "loss: 1.669499158859253 at epoch 30 at applicants training\n",
      "loss: 1.6700466871261597 at epoch 31 at applicants training\n",
      "loss: 1.666562795639038 at epoch 32 at applicants training\n",
      "loss: 1.6678828001022339 at epoch 33 at applicants training\n",
      "loss: 1.6664522886276245 at epoch 34 at applicants training\n",
      "loss: 1.6655399799346924 at epoch 35 at applicants training\n",
      "loss: 1.6650210618972778 at epoch 36 at applicants training\n",
      "loss: 1.6641689538955688 at epoch 37 at applicants training\n",
      "loss: 1.6631275415420532 at epoch 38 at applicants training\n",
      "loss: 1.6634246110916138 at epoch 39 at applicants training\n",
      "loss: 1.6610479354858398 at epoch 40 at applicants training\n",
      "loss: 1.6635948419570923 at epoch 41 at applicants training\n",
      "loss: 1.6592652797698975 at epoch 42 at applicants training\n",
      "loss: 1.6618845462799072 at epoch 43 at applicants training\n",
      "loss: 1.6597874164581299 at epoch 44 at applicants training\n",
      "loss: 1.6585466861724854 at epoch 45 at applicants training\n",
      "loss: 1.659844160079956 at epoch 46 at applicants training\n",
      "loss: 1.6574795246124268 at epoch 47 at applicants training\n",
      "loss: 1.6583003997802734 at epoch 48 at applicants training\n",
      "loss: 1.6583212614059448 at epoch 49 at applicants training\n",
      "loss: 1.656412124633789 at epoch 50 at applicants training\n",
      "loss: 1.6573963165283203 at epoch 51 at applicants training\n",
      "loss: 1.6574833393096924 at epoch 52 at applicants training\n",
      "loss: 1.6554880142211914 at epoch 53 at applicants training\n",
      "loss: 1.655930519104004 at epoch 54 at applicants training\n",
      "loss: 1.6568694114685059 at epoch 55 at applicants training\n",
      "loss: 1.6550959348678589 at epoch 56 at applicants training\n",
      "loss: 1.6541022062301636 at epoch 57 at applicants training\n",
      "loss: 1.6549100875854492 at epoch 58 at applicants training\n",
      "loss: 1.6553349494934082 at epoch 59 at applicants training\n",
      "loss: 1.6543351411819458 at epoch 60 at applicants training\n",
      "loss: 1.653106451034546 at epoch 61 at applicants training\n",
      "loss: 1.6529444456100464 at epoch 62 at applicants training\n",
      "loss: 1.6535816192626953 at epoch 63 at applicants training\n",
      "loss: 1.6536643505096436 at epoch 64 at applicants training\n",
      "loss: 1.6528419256210327 at epoch 65 at applicants training\n",
      "loss: 1.651859998703003 at epoch 66 at applicants training\n",
      "loss: 1.651482343673706 at epoch 67 at applicants training\n",
      "loss: 1.6517173051834106 at epoch 68 at applicants training\n",
      "loss: 1.6524527072906494 at epoch 69 at applicants training\n",
      "loss: 1.65377938747406 at epoch 70 at applicants training\n",
      "loss: 1.6541203260421753 at epoch 71 at applicants training\n",
      "loss: 1.6534335613250732 at epoch 72 at applicants training\n",
      "loss: 1.6511352062225342 at epoch 73 at applicants training\n",
      "loss: 1.6497143507003784 at epoch 74 at applicants training\n",
      "loss: 1.6492230892181396 at epoch 75 at applicants training\n",
      "loss: 1.649592399597168 at epoch 76 at applicants training\n",
      "loss: 1.6510342359542847 at epoch 77 at applicants training\n",
      "loss: 1.653262972831726 at epoch 78 at applicants training\n",
      "loss: 1.6545428037643433 at epoch 79 at applicants training\n",
      "loss: 1.6505125761032104 at epoch 80 at applicants training\n",
      "loss: 1.6481432914733887 at epoch 81 at applicants training\n",
      "loss: 1.6483393907546997 at epoch 82 at applicants training\n",
      "loss: 1.6503764390945435 at epoch 83 at applicants training\n",
      "loss: 1.652327299118042 at epoch 84 at applicants training\n",
      "loss: 1.6499680280685425 at epoch 85 at applicants training\n",
      "loss: 1.6476936340332031 at epoch 86 at applicants training\n",
      "loss: 1.646895408630371 at epoch 87 at applicants training\n",
      "loss: 1.6479440927505493 at epoch 88 at applicants training\n",
      "loss: 1.6499539613723755 at epoch 89 at applicants training\n",
      "loss: 1.6497869491577148 at epoch 90 at applicants training\n",
      "loss: 1.6482548713684082 at epoch 91 at applicants training\n",
      "loss: 1.6464123725891113 at epoch 92 at applicants training\n",
      "loss: 1.645754337310791 at epoch 93 at applicants training\n",
      "loss: 1.64640212059021 at epoch 94 at applicants training\n",
      "loss: 1.6478800773620605 at epoch 95 at applicants training\n",
      "loss: 1.6491854190826416 at epoch 96 at applicants training\n",
      "loss: 1.6479792594909668 at epoch 97 at applicants training\n",
      "loss: 1.6461718082427979 at epoch 98 at applicants training\n",
      "loss: 1.6448538303375244 at epoch 99 at applicants training\n",
      "loss: 1.706058382987976 at epoch 0 at applicants training\n",
      "loss: 1.7169702053070068 at epoch 1 at applicants training\n",
      "loss: 1.7073622941970825 at epoch 2 at applicants training\n",
      "loss: 1.6956783533096313 at epoch 3 at applicants training\n",
      "loss: 1.6784437894821167 at epoch 4 at applicants training\n",
      "loss: 1.6788325309753418 at epoch 5 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 6 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 7 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 8 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 9 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 10 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 11 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 12 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 13 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 14 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 15 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 16 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 17 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 18 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 19 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 20 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 21 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 22 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 23 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 24 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 25 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 26 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 27 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 28 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 29 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 30 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 31 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 32 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 33 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 34 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 35 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 36 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 37 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 38 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 39 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 40 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 41 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 42 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 43 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 44 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 45 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 46 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 47 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 48 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 49 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 50 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 51 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 52 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 53 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 54 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 55 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 56 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 57 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 58 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 59 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 60 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 61 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 62 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 63 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 64 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 65 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 66 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 67 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 68 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 69 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 70 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 71 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 72 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 73 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 74 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 75 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 76 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 77 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 78 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 79 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 80 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 81 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 82 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 83 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 84 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 85 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 86 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 87 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 88 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 89 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 90 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 91 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 92 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 93 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 94 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 95 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 96 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 97 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 98 at applicants training\n",
      "loss: 1.6788326501846313 at epoch 99 at applicants training\n",
      "loss: 1.712825059890747 at epoch 0 at applicants training\n",
      "loss: 1.698703646659851 at epoch 1 at applicants training\n",
      "loss: 1.7069289684295654 at epoch 2 at applicants training\n",
      "loss: 1.7054033279418945 at epoch 3 at applicants training\n",
      "loss: 1.6624013185501099 at epoch 4 at applicants training\n",
      "loss: 1.6570491790771484 at epoch 5 at applicants training\n",
      "loss: 1.658591866493225 at epoch 6 at applicants training\n",
      "loss: 1.650367021560669 at epoch 7 at applicants training\n",
      "loss: 1.6573542356491089 at epoch 8 at applicants training\n",
      "loss: 1.6529346704483032 at epoch 9 at applicants training\n",
      "loss: 1.6528780460357666 at epoch 10 at applicants training\n",
      "loss: 1.6471049785614014 at epoch 11 at applicants training\n",
      "loss: 1.6492644548416138 at epoch 12 at applicants training\n",
      "loss: 1.6503870487213135 at epoch 13 at applicants training\n",
      "loss: 1.6390000581741333 at epoch 14 at applicants training\n",
      "loss: 1.6433544158935547 at epoch 15 at applicants training\n",
      "loss: 1.638757586479187 at epoch 16 at applicants training\n",
      "loss: 1.6372590065002441 at epoch 17 at applicants training\n",
      "loss: 1.6366082429885864 at epoch 18 at applicants training\n",
      "loss: 1.6338900327682495 at epoch 19 at applicants training\n",
      "loss: 1.630920171737671 at epoch 20 at applicants training\n",
      "loss: 1.6336818933486938 at epoch 21 at applicants training\n",
      "loss: 1.6273735761642456 at epoch 22 at applicants training\n",
      "loss: 1.6324185132980347 at epoch 23 at applicants training\n",
      "loss: 1.6271865367889404 at epoch 24 at applicants training\n",
      "loss: 1.6261756420135498 at epoch 25 at applicants training\n",
      "loss: 1.6294995546340942 at epoch 26 at applicants training\n",
      "loss: 1.6232671737670898 at epoch 27 at applicants training\n",
      "loss: 1.6262619495391846 at epoch 28 at applicants training\n",
      "loss: 1.629414439201355 at epoch 29 at applicants training\n",
      "loss: 1.622186303138733 at epoch 30 at applicants training\n",
      "loss: 1.6357600688934326 at epoch 31 at applicants training\n",
      "loss: 1.6207715272903442 at epoch 32 at applicants training\n",
      "loss: 1.6248921155929565 at epoch 33 at applicants training\n",
      "loss: 1.6235301494598389 at epoch 34 at applicants training\n",
      "loss: 1.6198354959487915 at epoch 35 at applicants training\n",
      "loss: 1.6240017414093018 at epoch 36 at applicants training\n",
      "loss: 1.6177424192428589 at epoch 37 at applicants training\n",
      "loss: 1.6201000213623047 at epoch 38 at applicants training\n",
      "loss: 1.6215680837631226 at epoch 39 at applicants training\n",
      "loss: 1.6165947914123535 at epoch 40 at applicants training\n",
      "loss: 1.6193554401397705 at epoch 41 at applicants training\n",
      "loss: 1.6199009418487549 at epoch 42 at applicants training\n",
      "loss: 1.614972710609436 at epoch 43 at applicants training\n",
      "loss: 1.6159363985061646 at epoch 44 at applicants training\n",
      "loss: 1.6195969581604004 at epoch 45 at applicants training\n",
      "loss: 1.6164100170135498 at epoch 46 at applicants training\n",
      "loss: 1.6131610870361328 at epoch 47 at applicants training\n",
      "loss: 1.61417555809021 at epoch 48 at applicants training\n",
      "loss: 1.6163079738616943 at epoch 49 at applicants training\n",
      "loss: 1.615578055381775 at epoch 50 at applicants training\n",
      "loss: 1.6125438213348389 at epoch 51 at applicants training\n",
      "loss: 1.6111525297164917 at epoch 52 at applicants training\n",
      "loss: 1.6117913722991943 at epoch 53 at applicants training\n",
      "loss: 1.6144956350326538 at epoch 54 at applicants training\n",
      "loss: 1.6175451278686523 at epoch 55 at applicants training\n",
      "loss: 1.6176382303237915 at epoch 56 at applicants training\n",
      "loss: 1.6111929416656494 at epoch 57 at applicants training\n",
      "loss: 1.6091363430023193 at epoch 58 at applicants training\n",
      "loss: 1.6117154359817505 at epoch 59 at applicants training\n",
      "loss: 1.6166245937347412 at epoch 60 at applicants training\n",
      "loss: 1.6164863109588623 at epoch 61 at applicants training\n",
      "loss: 1.61030912399292 at epoch 62 at applicants training\n",
      "loss: 1.6073429584503174 at epoch 63 at applicants training\n",
      "loss: 1.6082267761230469 at epoch 64 at applicants training\n",
      "loss: 1.6126418113708496 at epoch 65 at applicants training\n",
      "loss: 1.6178780794143677 at epoch 66 at applicants training\n",
      "loss: 1.6125428676605225 at epoch 67 at applicants training\n",
      "loss: 1.6070952415466309 at epoch 68 at applicants training\n",
      "loss: 1.6057264804840088 at epoch 69 at applicants training\n",
      "loss: 1.608100175857544 at epoch 70 at applicants training\n",
      "loss: 1.6134006977081299 at epoch 71 at applicants training\n",
      "loss: 1.614414930343628 at epoch 72 at applicants training\n",
      "loss: 1.6093475818634033 at epoch 73 at applicants training\n",
      "loss: 1.6051454544067383 at epoch 74 at applicants training\n",
      "loss: 1.6049396991729736 at epoch 75 at applicants training\n",
      "loss: 1.6072771549224854 at epoch 76 at applicants training\n",
      "loss: 1.6129709482192993 at epoch 77 at applicants training\n",
      "loss: 1.6143872737884521 at epoch 78 at applicants training\n",
      "loss: 1.607530951499939 at epoch 79 at applicants training\n",
      "loss: 1.6035828590393066 at epoch 80 at applicants training\n",
      "loss: 1.6040754318237305 at epoch 81 at applicants training\n",
      "loss: 1.6070404052734375 at epoch 82 at applicants training\n",
      "loss: 1.6108478307724 at epoch 83 at applicants training\n",
      "loss: 1.6092370748519897 at epoch 84 at applicants training\n",
      "loss: 1.6049904823303223 at epoch 85 at applicants training\n",
      "loss: 1.6024454832077026 at epoch 86 at applicants training\n",
      "loss: 1.601996660232544 at epoch 87 at applicants training\n",
      "loss: 1.6029201745986938 at epoch 88 at applicants training\n",
      "loss: 1.605404257774353 at epoch 89 at applicants training\n",
      "loss: 1.6085822582244873 at epoch 90 at applicants training\n",
      "loss: 1.6084531545639038 at epoch 91 at applicants training\n",
      "loss: 1.606676459312439 at epoch 92 at applicants training\n",
      "loss: 1.6031692028045654 at epoch 93 at applicants training\n",
      "loss: 1.6010541915893555 at epoch 94 at applicants training\n",
      "loss: 1.6008672714233398 at epoch 95 at applicants training\n",
      "loss: 1.602695345878601 at epoch 96 at applicants training\n",
      "loss: 1.6064069271087646 at epoch 97 at applicants training\n",
      "loss: 1.6091454029083252 at epoch 98 at applicants training\n",
      "loss: 1.607707142829895 at epoch 99 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 0 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 1 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 2 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 3 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 4 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 5 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 6 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 7 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 8 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 9 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 10 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 11 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 12 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 13 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 14 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 15 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 16 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 17 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 18 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 19 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 20 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 21 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 22 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 23 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 24 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 25 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 26 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 27 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 28 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 29 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 30 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 31 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 32 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 33 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 34 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 35 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 36 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 37 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 38 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 39 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 40 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 41 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 42 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 43 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 44 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 45 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 46 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 47 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 48 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 49 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 50 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 51 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 52 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 53 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 54 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 55 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 56 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 57 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 58 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 59 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 60 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 61 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 62 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 63 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 64 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 65 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 66 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 67 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 68 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 69 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 70 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 71 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 72 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 73 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 74 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 75 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 76 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 77 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 78 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 79 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 80 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 81 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 82 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 83 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 84 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 85 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 86 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 87 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 88 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 89 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 90 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 91 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 92 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 93 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 94 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 95 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 96 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 97 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 98 at applicants training\n",
      "loss: 1.7178326845169067 at epoch 99 at applicants training\n",
      "loss: 4913.1953125 at epoch 0\n",
      "loss: 3938.3779296875 at epoch 0\n",
      "loss: 2585.570068359375 at epoch 0\n",
      "loss: 1382.3402099609375 at epoch 0\n",
      "loss: 933.8739013671875 at epoch 0\n",
      "loss: 336.4509582519531 at epoch 0\n",
      "loss: 359.93841552734375 at epoch 0\n",
      "loss: 409.60797119140625 at epoch 0\n",
      "loss: 517.1165771484375 at epoch 1\n",
      "loss: 479.5953369140625 at epoch 1\n",
      "loss: 369.6922607421875 at epoch 1\n",
      "loss: 212.8184051513672 at epoch 1\n",
      "loss: 158.6352996826172 at epoch 1\n",
      "loss: 117.27214813232422 at epoch 1\n",
      "loss: 133.54551696777344 at epoch 1\n",
      "loss: 214.41690063476562 at epoch 1\n",
      "loss: 175.5971221923828 at epoch 2\n",
      "loss: 263.1422119140625 at epoch 2\n",
      "loss: 281.3028564453125 at epoch 2\n",
      "loss: 221.27273559570312 at epoch 2\n",
      "loss: 167.53504943847656 at epoch 2\n",
      "loss: 101.62715148925781 at epoch 2\n",
      "loss: 82.7318344116211 at epoch 2\n",
      "loss: 51.014163970947266 at epoch 2\n",
      "loss: 41.87892150878906 at epoch 3\n",
      "loss: 57.605308532714844 at epoch 3\n",
      "loss: 110.3450698852539 at epoch 3\n",
      "loss: 98.65135955810547 at epoch 3\n",
      "loss: 97.97590637207031 at epoch 3\n",
      "loss: 98.70359802246094 at epoch 3\n",
      "loss: 84.58052062988281 at epoch 3\n",
      "loss: 72.58981323242188 at epoch 3\n",
      "loss: 56.95426940917969 at epoch 4\n",
      "loss: 61.39173126220703 at epoch 4\n",
      "loss: 53.16127014160156 at epoch 4\n",
      "loss: 69.74519348144531 at epoch 4\n",
      "loss: 51.181339263916016 at epoch 4\n",
      "loss: 56.19007110595703 at epoch 4\n",
      "loss: 56.43692398071289 at epoch 4\n",
      "loss: 67.05316925048828 at epoch 4\n",
      "loss: 68.22408294677734 at epoch 5\n",
      "loss: 48.682430267333984 at epoch 5\n",
      "loss: 35.45002746582031 at epoch 5\n",
      "loss: 46.160400390625 at epoch 5\n",
      "loss: 36.01750183105469 at epoch 5\n",
      "loss: 46.51837158203125 at epoch 5\n",
      "loss: 38.595787048339844 at epoch 5\n",
      "loss: 49.84098434448242 at epoch 5\n",
      "loss: 44.46495819091797 at epoch 6\n",
      "loss: 38.030372619628906 at epoch 6\n",
      "loss: 44.78007888793945 at epoch 6\n",
      "loss: 37.93613052368164 at epoch 6\n",
      "loss: 35.892818450927734 at epoch 6\n",
      "loss: 35.55855178833008 at epoch 6\n",
      "loss: 46.83050537109375 at epoch 6\n",
      "loss: 37.855743408203125 at epoch 6\n",
      "loss: 35.9549674987793 at epoch 7\n",
      "loss: 36.08618927001953 at epoch 7\n",
      "loss: 44.606048583984375 at epoch 7\n",
      "loss: 34.9765625 at epoch 7\n",
      "loss: 32.80079650878906 at epoch 7\n",
      "loss: 26.982622146606445 at epoch 7\n",
      "loss: 31.973087310791016 at epoch 7\n",
      "loss: 29.087583541870117 at epoch 7\n",
      "loss: 30.94548988342285 at epoch 8\n",
      "loss: 34.279842376708984 at epoch 8\n",
      "loss: 25.360614776611328 at epoch 8\n",
      "loss: 36.562400817871094 at epoch 8\n",
      "loss: 28.4628849029541 at epoch 8\n",
      "loss: 30.643659591674805 at epoch 8\n",
      "loss: 30.38864517211914 at epoch 8\n",
      "loss: 31.609130859375 at epoch 8\n",
      "loss: 31.321401596069336 at epoch 9\n",
      "loss: 22.88259506225586 at epoch 9\n",
      "loss: 25.34162139892578 at epoch 9\n",
      "loss: 27.600950241088867 at epoch 9\n",
      "loss: 27.10451889038086 at epoch 9\n",
      "loss: 24.966642379760742 at epoch 9\n",
      "loss: 31.880334854125977 at epoch 9\n",
      "loss: 37.90387725830078 at epoch 9\n",
      "loss: 29.344615936279297 at epoch 10\n",
      "loss: 25.436508178710938 at epoch 10\n",
      "loss: 28.233732223510742 at epoch 10\n",
      "loss: 24.65818977355957 at epoch 10\n",
      "loss: 30.895910263061523 at epoch 10\n",
      "loss: 19.68365478515625 at epoch 10\n",
      "loss: 20.59807586669922 at epoch 10\n",
      "loss: 31.54640007019043 at epoch 10\n",
      "loss: 26.87984275817871 at epoch 11\n",
      "loss: 28.725357055664062 at epoch 11\n",
      "loss: 18.7636775970459 at epoch 11\n",
      "loss: 24.6971378326416 at epoch 11\n",
      "loss: 28.593473434448242 at epoch 11\n",
      "loss: 19.84037971496582 at epoch 11\n",
      "loss: 21.062786102294922 at epoch 11\n",
      "loss: 24.209415435791016 at epoch 11\n",
      "loss: 21.173404693603516 at epoch 12\n",
      "loss: 18.215715408325195 at epoch 12\n",
      "loss: 25.696868896484375 at epoch 12\n",
      "loss: 28.944900512695312 at epoch 12\n",
      "loss: 24.338970184326172 at epoch 12\n",
      "loss: 21.41844367980957 at epoch 12\n",
      "loss: 18.11530303955078 at epoch 12\n",
      "loss: 21.68946075439453 at epoch 12\n",
      "loss: 17.55534553527832 at epoch 13\n",
      "loss: 20.27267837524414 at epoch 13\n",
      "loss: 17.351205825805664 at epoch 13\n",
      "loss: 24.992938995361328 at epoch 13\n",
      "loss: 25.005285263061523 at epoch 13\n",
      "loss: 20.568681716918945 at epoch 13\n",
      "loss: 17.975299835205078 at epoch 13\n",
      "loss: 20.994590759277344 at epoch 13\n",
      "loss: 16.21633529663086 at epoch 14\n",
      "loss: 26.662511825561523 at epoch 14\n",
      "loss: 20.228952407836914 at epoch 14\n",
      "loss: 16.11663246154785 at epoch 14\n",
      "loss: 21.276092529296875 at epoch 14\n",
      "loss: 18.359294891357422 at epoch 14\n",
      "loss: 20.538402557373047 at epoch 14\n",
      "loss: 16.67979621887207 at epoch 14\n",
      "loss: 19.399038314819336 at epoch 15\n",
      "loss: 12.981148719787598 at epoch 15\n",
      "loss: 19.244857788085938 at epoch 15\n",
      "loss: 19.919580459594727 at epoch 15\n",
      "loss: 17.255783081054688 at epoch 15\n",
      "loss: 19.983957290649414 at epoch 15\n",
      "loss: 20.06848907470703 at epoch 15\n",
      "loss: 16.88800811767578 at epoch 15\n",
      "loss: 23.45323944091797 at epoch 16\n",
      "loss: 19.176836013793945 at epoch 16\n",
      "loss: 20.76524543762207 at epoch 16\n",
      "loss: 14.89474105834961 at epoch 16\n",
      "loss: 15.489165306091309 at epoch 16\n",
      "loss: 15.996240615844727 at epoch 16\n",
      "loss: 16.509326934814453 at epoch 16\n",
      "loss: 12.779111862182617 at epoch 16\n",
      "loss: 12.728984832763672 at epoch 17\n",
      "loss: 21.406360626220703 at epoch 17\n",
      "loss: 16.7319278717041 at epoch 17\n",
      "loss: 21.486522674560547 at epoch 17\n",
      "loss: 13.82518482208252 at epoch 17\n",
      "loss: 16.82025909423828 at epoch 17\n",
      "loss: 11.45985221862793 at epoch 17\n",
      "loss: 14.502888679504395 at epoch 17\n",
      "loss: 15.1556396484375 at epoch 18\n",
      "loss: 13.896528244018555 at epoch 18\n",
      "loss: 16.433094024658203 at epoch 18\n",
      "loss: 15.616790771484375 at epoch 18\n",
      "loss: 15.619271278381348 at epoch 18\n",
      "loss: 17.68092918395996 at epoch 18\n",
      "loss: 13.275546073913574 at epoch 18\n",
      "loss: 16.22553825378418 at epoch 18\n",
      "loss: 16.600261688232422 at epoch 19\n",
      "loss: 14.61051082611084 at epoch 19\n",
      "loss: 17.12270164489746 at epoch 19\n",
      "loss: 13.111733436584473 at epoch 19\n",
      "loss: 13.827058792114258 at epoch 19\n",
      "loss: 15.337815284729004 at epoch 19\n",
      "loss: 12.41694164276123 at epoch 19\n",
      "loss: 15.272902488708496 at epoch 19\n",
      "loss: 10.736425399780273 at epoch 20\n",
      "loss: 14.665107727050781 at epoch 20\n",
      "loss: 11.831811904907227 at epoch 20\n",
      "loss: 16.918811798095703 at epoch 20\n",
      "loss: 15.055058479309082 at epoch 20\n",
      "loss: 14.448583602905273 at epoch 20\n",
      "loss: 15.35755729675293 at epoch 20\n",
      "loss: 13.732807159423828 at epoch 20\n",
      "loss: 15.03742790222168 at epoch 21\n",
      "loss: 15.321877479553223 at epoch 21\n",
      "loss: 10.224100112915039 at epoch 21\n",
      "loss: 13.664417266845703 at epoch 21\n",
      "loss: 13.933956146240234 at epoch 21\n",
      "loss: 13.235135078430176 at epoch 21\n",
      "loss: 12.571470260620117 at epoch 21\n",
      "loss: 13.170610427856445 at epoch 21\n",
      "loss: 12.19439697265625 at epoch 22\n",
      "loss: 13.07603931427002 at epoch 22\n",
      "loss: 14.456158638000488 at epoch 22\n",
      "loss: 15.142423629760742 at epoch 22\n",
      "loss: 12.385797500610352 at epoch 22\n",
      "loss: 13.384835243225098 at epoch 22\n",
      "loss: 11.064785957336426 at epoch 22\n",
      "loss: 12.77703857421875 at epoch 22\n",
      "loss: 10.880473136901855 at epoch 23\n",
      "loss: 13.445053100585938 at epoch 23\n",
      "loss: 12.490689277648926 at epoch 23\n",
      "loss: 11.547918319702148 at epoch 23\n",
      "loss: 12.865779876708984 at epoch 23\n",
      "loss: 12.863866806030273 at epoch 23\n",
      "loss: 14.00107192993164 at epoch 23\n",
      "loss: 13.38106918334961 at epoch 23\n",
      "loss: 11.41775894165039 at epoch 24\n",
      "loss: 13.401458740234375 at epoch 24\n",
      "loss: 14.02434253692627 at epoch 24\n",
      "loss: 11.38609504699707 at epoch 24\n",
      "loss: 13.401494979858398 at epoch 24\n",
      "loss: 11.370125770568848 at epoch 24\n",
      "loss: 11.347967147827148 at epoch 24\n",
      "loss: 12.098926544189453 at epoch 24\n",
      "loss: 11.288232803344727 at epoch 25\n",
      "loss: 14.138943672180176 at epoch 25\n",
      "loss: 8.56883716583252 at epoch 25\n",
      "loss: 11.566925048828125 at epoch 25\n",
      "loss: 13.572328567504883 at epoch 25\n",
      "loss: 12.836732864379883 at epoch 25\n",
      "loss: 10.5894136428833 at epoch 25\n",
      "loss: 10.528849601745605 at epoch 25\n",
      "loss: 12.153067588806152 at epoch 26\n",
      "loss: 12.609347343444824 at epoch 26\n",
      "loss: 10.969152450561523 at epoch 26\n",
      "loss: 11.384575843811035 at epoch 26\n",
      "loss: 10.563932418823242 at epoch 26\n",
      "loss: 9.136817932128906 at epoch 26\n",
      "loss: 13.10405445098877 at epoch 26\n",
      "loss: 10.40572452545166 at epoch 26\n",
      "loss: 9.438868522644043 at epoch 27\n",
      "loss: 11.755790710449219 at epoch 27\n",
      "loss: 10.885906219482422 at epoch 27\n",
      "loss: 10.933541297912598 at epoch 27\n",
      "loss: 10.490726470947266 at epoch 27\n",
      "loss: 10.727014541625977 at epoch 27\n",
      "loss: 10.788935661315918 at epoch 27\n",
      "loss: 10.485118865966797 at epoch 27\n",
      "loss: 10.503693580627441 at epoch 28\n",
      "loss: 9.19343376159668 at epoch 28\n",
      "loss: 10.988092422485352 at epoch 28\n",
      "loss: 13.158082962036133 at epoch 28\n",
      "loss: 9.624441146850586 at epoch 28\n",
      "loss: 9.326558113098145 at epoch 28\n",
      "loss: 12.528181076049805 at epoch 28\n",
      "loss: 9.49419116973877 at epoch 28\n",
      "loss: 8.898877143859863 at epoch 29\n",
      "loss: 11.93169116973877 at epoch 29\n",
      "loss: 9.352615356445312 at epoch 29\n",
      "loss: 10.650485038757324 at epoch 29\n",
      "loss: 9.45264720916748 at epoch 29\n",
      "loss: 11.597428321838379 at epoch 29\n",
      "loss: 12.125273704528809 at epoch 29\n",
      "loss: 8.733019828796387 at epoch 29\n",
      "loss: 10.340422630310059 at epoch 30\n",
      "loss: 9.227807998657227 at epoch 30\n",
      "loss: 9.187021255493164 at epoch 30\n",
      "loss: 10.77560043334961 at epoch 30\n",
      "loss: 10.250448226928711 at epoch 30\n",
      "loss: 12.066137313842773 at epoch 30\n",
      "loss: 9.547276496887207 at epoch 30\n",
      "loss: 8.936931610107422 at epoch 30\n",
      "loss: 10.962590217590332 at epoch 31\n",
      "loss: 9.30241584777832 at epoch 31\n",
      "loss: 11.371382713317871 at epoch 31\n",
      "loss: 9.422113418579102 at epoch 31\n",
      "loss: 8.356587409973145 at epoch 31\n",
      "loss: 10.791116714477539 at epoch 31\n",
      "loss: 9.632680892944336 at epoch 31\n",
      "loss: 9.433895111083984 at epoch 31\n",
      "loss: 9.616336822509766 at epoch 32\n",
      "loss: 10.092796325683594 at epoch 32\n",
      "loss: 10.238662719726562 at epoch 32\n",
      "loss: 10.473249435424805 at epoch 32\n",
      "loss: 8.910102844238281 at epoch 32\n",
      "loss: 7.922037124633789 at epoch 32\n",
      "loss: 9.510359764099121 at epoch 32\n",
      "loss: 10.038022994995117 at epoch 32\n",
      "loss: 9.500473022460938 at epoch 33\n",
      "loss: 8.912109375 at epoch 33\n",
      "loss: 9.874618530273438 at epoch 33\n",
      "loss: 8.860973358154297 at epoch 33\n",
      "loss: 9.847027778625488 at epoch 33\n",
      "loss: 9.769998550415039 at epoch 33\n",
      "loss: 9.358938217163086 at epoch 33\n",
      "loss: 8.325037002563477 at epoch 33\n",
      "loss: 9.941450119018555 at epoch 34\n",
      "loss: 7.8305158615112305 at epoch 34\n",
      "loss: 10.432368278503418 at epoch 34\n",
      "loss: 9.152819633483887 at epoch 34\n",
      "loss: 10.908987998962402 at epoch 34\n",
      "loss: 8.958929061889648 at epoch 34\n",
      "loss: 10.48045539855957 at epoch 34\n",
      "loss: 8.790544509887695 at epoch 34\n",
      "loss: 8.556804656982422 at epoch 35\n",
      "loss: 7.863805294036865 at epoch 35\n",
      "loss: 11.476179122924805 at epoch 35\n",
      "loss: 8.197043418884277 at epoch 35\n",
      "loss: 10.053393363952637 at epoch 35\n",
      "loss: 8.695718765258789 at epoch 35\n",
      "loss: 7.604709625244141 at epoch 35\n",
      "loss: 9.887206077575684 at epoch 35\n",
      "loss: 9.752991676330566 at epoch 36\n",
      "loss: 10.377208709716797 at epoch 36\n",
      "loss: 7.865407943725586 at epoch 36\n",
      "loss: 7.589207649230957 at epoch 36\n",
      "loss: 10.847892761230469 at epoch 36\n",
      "loss: 12.008330345153809 at epoch 36\n",
      "loss: 8.9110107421875 at epoch 36\n",
      "loss: 8.528494834899902 at epoch 36\n",
      "loss: 10.626007080078125 at epoch 37\n",
      "loss: 8.870448112487793 at epoch 37\n",
      "loss: 8.356058120727539 at epoch 37\n",
      "loss: 7.479153633117676 at epoch 37\n",
      "loss: 7.5638933181762695 at epoch 37\n",
      "loss: 9.610623359680176 at epoch 37\n",
      "loss: 8.674310684204102 at epoch 37\n",
      "loss: 8.409323692321777 at epoch 37\n",
      "loss: 8.295381546020508 at epoch 38\n",
      "loss: 6.900449752807617 at epoch 38\n",
      "loss: 7.195363998413086 at epoch 38\n",
      "loss: 9.567795753479004 at epoch 38\n",
      "loss: 8.560060501098633 at epoch 38\n",
      "loss: 8.911728858947754 at epoch 38\n",
      "loss: 10.582015991210938 at epoch 38\n",
      "loss: 8.885107040405273 at epoch 38\n",
      "loss: 8.474311828613281 at epoch 39\n",
      "loss: 8.670026779174805 at epoch 39\n",
      "loss: 9.388123512268066 at epoch 39\n",
      "loss: 8.662712097167969 at epoch 39\n",
      "loss: 9.603778839111328 at epoch 39\n",
      "loss: 8.061341285705566 at epoch 39\n",
      "loss: 7.812265396118164 at epoch 39\n",
      "loss: 6.820784568786621 at epoch 39\n",
      "loss: 7.216675758361816 at epoch 40\n",
      "loss: 9.07956314086914 at epoch 40\n",
      "loss: 8.49917221069336 at epoch 40\n",
      "loss: 9.60489273071289 at epoch 40\n",
      "loss: 7.830499649047852 at epoch 40\n",
      "loss: 7.4768781661987305 at epoch 40\n",
      "loss: 8.743695259094238 at epoch 40\n",
      "loss: 8.903240203857422 at epoch 40\n",
      "loss: 8.11683464050293 at epoch 41\n",
      "loss: 8.226308822631836 at epoch 41\n",
      "loss: 8.020641326904297 at epoch 41\n",
      "loss: 8.21616268157959 at epoch 41\n",
      "loss: 7.35161828994751 at epoch 41\n",
      "loss: 9.405457496643066 at epoch 41\n",
      "loss: 8.035634994506836 at epoch 41\n",
      "loss: 10.173479080200195 at epoch 41\n",
      "loss: 7.914783954620361 at epoch 42\n",
      "loss: 8.659880638122559 at epoch 42\n",
      "loss: 7.311919689178467 at epoch 42\n",
      "loss: 9.427040100097656 at epoch 42\n",
      "loss: 8.161770820617676 at epoch 42\n",
      "loss: 8.68490982055664 at epoch 42\n",
      "loss: 7.982133865356445 at epoch 42\n",
      "loss: 7.802408218383789 at epoch 42\n",
      "loss: 8.843778610229492 at epoch 43\n",
      "loss: 8.52359390258789 at epoch 43\n",
      "loss: 7.666608810424805 at epoch 43\n",
      "loss: 8.705517768859863 at epoch 43\n",
      "loss: 7.289900779724121 at epoch 43\n",
      "loss: 9.101415634155273 at epoch 43\n",
      "loss: 7.782346248626709 at epoch 43\n",
      "loss: 8.072556495666504 at epoch 43\n",
      "loss: 8.659927368164062 at epoch 44\n",
      "loss: 7.555281162261963 at epoch 44\n",
      "loss: 7.2092719078063965 at epoch 44\n",
      "loss: 8.800192832946777 at epoch 44\n",
      "loss: 8.294677734375 at epoch 44\n",
      "loss: 8.712660789489746 at epoch 44\n",
      "loss: 7.777994155883789 at epoch 44\n",
      "loss: 8.13182544708252 at epoch 44\n",
      "loss: 6.963811874389648 at epoch 45\n",
      "loss: 8.058206558227539 at epoch 45\n",
      "loss: 8.79633903503418 at epoch 45\n",
      "loss: 8.889717102050781 at epoch 45\n",
      "loss: 7.792688369750977 at epoch 45\n",
      "loss: 7.763976097106934 at epoch 45\n",
      "loss: 6.910966396331787 at epoch 45\n",
      "loss: 9.930965423583984 at epoch 45\n",
      "loss: 8.065834045410156 at epoch 46\n",
      "loss: 7.888740062713623 at epoch 46\n",
      "loss: 7.006495475769043 at epoch 46\n",
      "loss: 8.77634048461914 at epoch 46\n",
      "loss: 9.539172172546387 at epoch 46\n",
      "loss: 7.465308666229248 at epoch 46\n",
      "loss: 7.910671234130859 at epoch 46\n",
      "loss: 9.192638397216797 at epoch 46\n",
      "loss: 7.970193386077881 at epoch 47\n",
      "loss: 9.295441627502441 at epoch 47\n",
      "loss: 8.906353950500488 at epoch 47\n",
      "loss: 8.092391014099121 at epoch 47\n",
      "loss: 7.446540355682373 at epoch 47\n",
      "loss: 8.068672180175781 at epoch 47\n",
      "loss: 8.99750804901123 at epoch 47\n",
      "loss: 7.2182207107543945 at epoch 47\n",
      "loss: 6.2186970710754395 at epoch 48\n",
      "loss: 8.720169067382812 at epoch 48\n",
      "loss: 8.194497108459473 at epoch 48\n",
      "loss: 7.885765552520752 at epoch 48\n",
      "loss: 8.371376037597656 at epoch 48\n",
      "loss: 8.262149810791016 at epoch 48\n",
      "loss: 7.831148147583008 at epoch 48\n",
      "loss: 7.958519458770752 at epoch 48\n",
      "loss: 7.472761154174805 at epoch 49\n",
      "loss: 8.317594528198242 at epoch 49\n",
      "loss: 8.285284042358398 at epoch 49\n",
      "loss: 8.196006774902344 at epoch 49\n",
      "loss: 9.55461597442627 at epoch 49\n",
      "loss: 6.80761194229126 at epoch 49\n",
      "loss: 8.113495826721191 at epoch 49\n",
      "loss: 7.5220417976379395 at epoch 49\n",
      "loss: 6.714999198913574 at epoch 50\n",
      "loss: 8.653973579406738 at epoch 50\n",
      "loss: 9.170886993408203 at epoch 50\n",
      "loss: 7.023528575897217 at epoch 50\n",
      "loss: 10.522038459777832 at epoch 50\n",
      "loss: 7.980156421661377 at epoch 50\n",
      "loss: 7.253573894500732 at epoch 50\n",
      "loss: 8.153584480285645 at epoch 50\n",
      "loss: 8.28744125366211 at epoch 51\n",
      "loss: 7.706567287445068 at epoch 51\n",
      "loss: 6.485569000244141 at epoch 51\n",
      "loss: 8.846573829650879 at epoch 51\n",
      "loss: 7.185993194580078 at epoch 51\n",
      "loss: 8.754331588745117 at epoch 51\n",
      "loss: 9.210759162902832 at epoch 51\n",
      "loss: 6.970766067504883 at epoch 51\n",
      "loss: 7.713910102844238 at epoch 52\n",
      "loss: 8.249581336975098 at epoch 52\n",
      "loss: 7.071544170379639 at epoch 52\n",
      "loss: 9.698538780212402 at epoch 52\n",
      "loss: 8.669590950012207 at epoch 52\n",
      "loss: 7.212716102600098 at epoch 52\n",
      "loss: 6.9638800621032715 at epoch 52\n",
      "loss: 8.165252685546875 at epoch 52\n",
      "loss: 8.741613388061523 at epoch 53\n",
      "loss: 7.3927741050720215 at epoch 53\n",
      "loss: 7.029386520385742 at epoch 53\n",
      "loss: 7.905776023864746 at epoch 53\n",
      "loss: 8.315764427185059 at epoch 53\n",
      "loss: 7.043727874755859 at epoch 53\n",
      "loss: 7.384069442749023 at epoch 53\n",
      "loss: 8.428766250610352 at epoch 53\n",
      "loss: 10.176877975463867 at epoch 54\n",
      "loss: 7.729691982269287 at epoch 54\n",
      "loss: 7.846596717834473 at epoch 54\n",
      "loss: 7.224160671234131 at epoch 54\n",
      "loss: 7.706282138824463 at epoch 54\n",
      "loss: 6.8688178062438965 at epoch 54\n",
      "loss: 7.108187675476074 at epoch 54\n",
      "loss: 6.446627616882324 at epoch 54\n",
      "loss: 8.346693992614746 at epoch 55\n",
      "loss: 6.499961853027344 at epoch 55\n",
      "loss: 7.9449615478515625 at epoch 55\n",
      "loss: 6.826506614685059 at epoch 55\n",
      "loss: 7.995485305786133 at epoch 55\n",
      "loss: 7.974676609039307 at epoch 55\n",
      "loss: 6.563831806182861 at epoch 55\n",
      "loss: 9.147323608398438 at epoch 55\n",
      "loss: 8.213708877563477 at epoch 56\n",
      "loss: 7.457188606262207 at epoch 56\n",
      "loss: 7.2951130867004395 at epoch 56\n",
      "loss: 8.385340690612793 at epoch 56\n",
      "loss: 7.229615688323975 at epoch 56\n",
      "loss: 6.997784614562988 at epoch 56\n",
      "loss: 8.191590309143066 at epoch 56\n",
      "loss: 7.224030494689941 at epoch 56\n",
      "loss: 7.316153049468994 at epoch 57\n",
      "loss: 6.453529357910156 at epoch 57\n",
      "loss: 8.057424545288086 at epoch 57\n",
      "loss: 7.754350185394287 at epoch 57\n",
      "loss: 8.042859077453613 at epoch 57\n",
      "loss: 7.783869743347168 at epoch 57\n",
      "loss: 8.601734161376953 at epoch 57\n",
      "loss: 8.267050743103027 at epoch 57\n",
      "loss: 8.045161247253418 at epoch 58\n",
      "loss: 7.773481845855713 at epoch 58\n",
      "loss: 8.799362182617188 at epoch 58\n",
      "loss: 7.961820602416992 at epoch 58\n",
      "loss: 7.776803493499756 at epoch 58\n",
      "loss: 7.071231365203857 at epoch 58\n",
      "loss: 6.765644550323486 at epoch 58\n",
      "loss: 7.372347831726074 at epoch 58\n",
      "loss: 7.785945892333984 at epoch 59\n",
      "loss: 7.729998588562012 at epoch 59\n",
      "loss: 9.649209022521973 at epoch 59\n",
      "loss: 8.307641983032227 at epoch 59\n",
      "loss: 6.370532989501953 at epoch 59\n",
      "loss: 9.69244384765625 at epoch 59\n",
      "loss: 7.84669828414917 at epoch 59\n",
      "loss: 7.669975757598877 at epoch 59\n",
      "loss: 7.367908477783203 at epoch 60\n",
      "loss: 8.50975227355957 at epoch 60\n",
      "loss: 9.080151557922363 at epoch 60\n",
      "loss: 7.582904815673828 at epoch 60\n",
      "loss: 8.066988945007324 at epoch 60\n",
      "loss: 8.257933616638184 at epoch 60\n",
      "loss: 9.168719291687012 at epoch 60\n",
      "loss: 7.568871021270752 at epoch 60\n",
      "loss: 8.58942985534668 at epoch 61\n",
      "loss: 7.381180763244629 at epoch 61\n",
      "loss: 9.562623977661133 at epoch 61\n",
      "loss: 6.637009143829346 at epoch 61\n",
      "loss: 7.824540138244629 at epoch 61\n",
      "loss: 8.384729385375977 at epoch 61\n",
      "loss: 6.790153503417969 at epoch 61\n",
      "loss: 6.761316299438477 at epoch 61\n",
      "loss: 7.090297698974609 at epoch 62\n",
      "loss: 9.513957977294922 at epoch 62\n",
      "loss: 6.991481781005859 at epoch 62\n",
      "loss: 7.297801971435547 at epoch 62\n",
      "loss: 8.075833320617676 at epoch 62\n",
      "loss: 7.503237724304199 at epoch 62\n",
      "loss: 7.491470813751221 at epoch 62\n",
      "loss: 8.138590812683105 at epoch 62\n",
      "loss: 8.757671356201172 at epoch 63\n",
      "loss: 7.1428422927856445 at epoch 63\n",
      "loss: 7.2578935623168945 at epoch 63\n",
      "loss: 7.947530269622803 at epoch 63\n",
      "loss: 7.071892738342285 at epoch 63\n",
      "loss: 6.62082052230835 at epoch 63\n",
      "loss: 7.660303115844727 at epoch 63\n",
      "loss: 8.88227367401123 at epoch 63\n",
      "loss: 8.618339538574219 at epoch 64\n",
      "loss: 8.461197853088379 at epoch 64\n",
      "loss: 7.021823406219482 at epoch 64\n",
      "loss: 6.9279046058654785 at epoch 64\n",
      "loss: 6.582125663757324 at epoch 64\n",
      "loss: 7.122354507446289 at epoch 64\n",
      "loss: 8.107011795043945 at epoch 64\n",
      "loss: 7.275057315826416 at epoch 64\n",
      "loss: 7.42080545425415 at epoch 65\n",
      "loss: 7.362231254577637 at epoch 65\n",
      "loss: 8.658682823181152 at epoch 65\n",
      "loss: 8.253875732421875 at epoch 65\n",
      "loss: 7.164323806762695 at epoch 65\n",
      "loss: 7.326443195343018 at epoch 65\n",
      "loss: 7.933440685272217 at epoch 65\n",
      "loss: 8.844816207885742 at epoch 65\n",
      "loss: 7.537019729614258 at epoch 66\n",
      "loss: 7.620215892791748 at epoch 66\n",
      "loss: 6.697807312011719 at epoch 66\n",
      "loss: 8.13931655883789 at epoch 66\n",
      "loss: 7.992350101470947 at epoch 66\n",
      "loss: 10.080343246459961 at epoch 66\n",
      "loss: 6.581408977508545 at epoch 66\n",
      "loss: 7.486391544342041 at epoch 66\n",
      "loss: 9.223370552062988 at epoch 67\n",
      "loss: 7.575169563293457 at epoch 67\n",
      "loss: 7.168099880218506 at epoch 67\n",
      "loss: 7.8238372802734375 at epoch 67\n",
      "loss: 8.196072578430176 at epoch 67\n",
      "loss: 8.166620254516602 at epoch 67\n",
      "loss: 7.457042694091797 at epoch 67\n",
      "loss: 6.568477153778076 at epoch 67\n",
      "loss: 7.4856085777282715 at epoch 68\n",
      "loss: 7.442721366882324 at epoch 68\n",
      "loss: 7.462404251098633 at epoch 68\n",
      "loss: 6.675113677978516 at epoch 68\n",
      "loss: 7.52599573135376 at epoch 68\n",
      "loss: 7.675848007202148 at epoch 68\n",
      "loss: 8.295241355895996 at epoch 68\n",
      "loss: 7.405211925506592 at epoch 68\n",
      "loss: 8.130881309509277 at epoch 69\n",
      "loss: 7.114197731018066 at epoch 69\n",
      "loss: 8.171048164367676 at epoch 69\n",
      "loss: 7.8195343017578125 at epoch 69\n",
      "loss: 6.797643661499023 at epoch 69\n",
      "loss: 7.789085865020752 at epoch 69\n",
      "loss: 8.59939956665039 at epoch 69\n",
      "loss: 7.943236351013184 at epoch 69\n",
      "loss: 8.606287956237793 at epoch 70\n",
      "loss: 7.197114944458008 at epoch 70\n",
      "loss: 8.074193000793457 at epoch 70\n",
      "loss: 7.509893894195557 at epoch 70\n",
      "loss: 8.51565170288086 at epoch 70\n",
      "loss: 6.9584574699401855 at epoch 70\n",
      "loss: 7.611721038818359 at epoch 70\n",
      "loss: 9.80179214477539 at epoch 70\n",
      "loss: 6.372984886169434 at epoch 71\n",
      "loss: 6.957637786865234 at epoch 71\n",
      "loss: 8.205099105834961 at epoch 71\n",
      "loss: 9.66270637512207 at epoch 71\n",
      "loss: 9.141732215881348 at epoch 71\n",
      "loss: 6.5145158767700195 at epoch 71\n",
      "loss: 7.065230846405029 at epoch 71\n",
      "loss: 8.118040084838867 at epoch 71\n",
      "loss: 8.110030174255371 at epoch 72\n",
      "loss: 8.272146224975586 at epoch 72\n",
      "loss: 7.139291763305664 at epoch 72\n",
      "loss: 8.506869316101074 at epoch 72\n",
      "loss: 7.118168830871582 at epoch 72\n",
      "loss: 7.682117938995361 at epoch 72\n",
      "loss: 7.955856800079346 at epoch 72\n",
      "loss: 8.534668922424316 at epoch 72\n",
      "loss: 7.240983963012695 at epoch 73\n",
      "loss: 7.6223649978637695 at epoch 73\n",
      "loss: 7.418997287750244 at epoch 73\n",
      "loss: 7.800086498260498 at epoch 73\n",
      "loss: 7.3521881103515625 at epoch 73\n",
      "loss: 6.5226006507873535 at epoch 73\n",
      "loss: 9.599832534790039 at epoch 73\n",
      "loss: 7.348919868469238 at epoch 73\n",
      "loss: 7.709866523742676 at epoch 74\n",
      "loss: 7.730627536773682 at epoch 74\n",
      "loss: 7.356072425842285 at epoch 74\n",
      "loss: 8.512154579162598 at epoch 74\n",
      "loss: 6.039295673370361 at epoch 74\n",
      "loss: 8.064298629760742 at epoch 74\n",
      "loss: 9.163553237915039 at epoch 74\n",
      "loss: 7.503281593322754 at epoch 74\n",
      "loss: 7.264865875244141 at epoch 75\n",
      "loss: 6.642945766448975 at epoch 75\n",
      "loss: 8.06365966796875 at epoch 75\n",
      "loss: 9.352241516113281 at epoch 75\n",
      "loss: 8.33847427368164 at epoch 75\n",
      "loss: 8.139518737792969 at epoch 75\n",
      "loss: 6.524666786193848 at epoch 75\n",
      "loss: 8.340432167053223 at epoch 75\n",
      "loss: 9.138101577758789 at epoch 76\n",
      "loss: 7.151443958282471 at epoch 76\n",
      "loss: 7.481271743774414 at epoch 76\n",
      "loss: 6.98771333694458 at epoch 76\n",
      "loss: 7.517413139343262 at epoch 76\n",
      "loss: 8.029925346374512 at epoch 76\n",
      "loss: 7.445691108703613 at epoch 76\n",
      "loss: 8.169533729553223 at epoch 76\n",
      "loss: 7.876131057739258 at epoch 77\n",
      "loss: 11.231186866760254 at epoch 77\n",
      "loss: 7.363658428192139 at epoch 77\n",
      "loss: 7.899264335632324 at epoch 77\n",
      "loss: 8.963961601257324 at epoch 77\n",
      "loss: 9.353269577026367 at epoch 77\n",
      "loss: 7.442018985748291 at epoch 77\n",
      "loss: 5.976432800292969 at epoch 77\n",
      "loss: 7.706399440765381 at epoch 78\n",
      "loss: 8.118741989135742 at epoch 78\n",
      "loss: 8.369880676269531 at epoch 78\n",
      "loss: 8.3174467086792 at epoch 78\n",
      "loss: 6.58435583114624 at epoch 78\n",
      "loss: 9.473808288574219 at epoch 78\n",
      "loss: 7.223588943481445 at epoch 78\n",
      "loss: 7.558740615844727 at epoch 78\n",
      "loss: 8.398085594177246 at epoch 79\n",
      "loss: 9.543495178222656 at epoch 79\n",
      "loss: 8.134712219238281 at epoch 79\n",
      "loss: 5.802719593048096 at epoch 79\n",
      "loss: 6.96985387802124 at epoch 79\n",
      "loss: 7.287716865539551 at epoch 79\n",
      "loss: 8.289043426513672 at epoch 79\n",
      "loss: 9.228757858276367 at epoch 79\n",
      "loss: 7.2706618309021 at epoch 80\n",
      "loss: 6.738718032836914 at epoch 80\n",
      "loss: 7.652237415313721 at epoch 80\n",
      "loss: 7.875977516174316 at epoch 80\n",
      "loss: 8.8649320602417 at epoch 80\n",
      "loss: 8.227090835571289 at epoch 80\n",
      "loss: 6.813365936279297 at epoch 80\n",
      "loss: 7.602989196777344 at epoch 80\n",
      "loss: 7.854541778564453 at epoch 81\n",
      "loss: 9.474540710449219 at epoch 81\n",
      "loss: 7.7550129890441895 at epoch 81\n",
      "loss: 8.343077659606934 at epoch 81\n",
      "loss: 7.001684188842773 at epoch 81\n",
      "loss: 6.360991954803467 at epoch 81\n",
      "loss: 9.082965850830078 at epoch 81\n",
      "loss: 7.572791576385498 at epoch 81\n",
      "loss: 8.292729377746582 at epoch 82\n",
      "loss: 7.1964826583862305 at epoch 82\n",
      "loss: 7.3199238777160645 at epoch 82\n",
      "loss: 7.589167594909668 at epoch 82\n",
      "loss: 6.533061981201172 at epoch 82\n",
      "loss: 8.906298637390137 at epoch 82\n",
      "loss: 7.847304344177246 at epoch 82\n",
      "loss: 7.5890984535217285 at epoch 82\n",
      "loss: 6.767779350280762 at epoch 83\n",
      "loss: 7.468155860900879 at epoch 83\n",
      "loss: 7.467644214630127 at epoch 83\n",
      "loss: 8.560321807861328 at epoch 83\n",
      "loss: 6.839339256286621 at epoch 83\n",
      "loss: 8.480219841003418 at epoch 83\n",
      "loss: 7.629741191864014 at epoch 83\n",
      "loss: 7.82106351852417 at epoch 83\n",
      "loss: 8.050320625305176 at epoch 84\n",
      "loss: 7.297093391418457 at epoch 84\n",
      "loss: 6.899611473083496 at epoch 84\n",
      "loss: 7.019286155700684 at epoch 84\n",
      "loss: 7.374664783477783 at epoch 84\n",
      "loss: 7.995880126953125 at epoch 84\n",
      "loss: 7.515448093414307 at epoch 84\n",
      "loss: 7.2512288093566895 at epoch 84\n",
      "loss: 8.040494918823242 at epoch 85\n",
      "loss: 7.055394649505615 at epoch 85\n",
      "loss: 7.985840797424316 at epoch 85\n",
      "loss: 7.556640148162842 at epoch 85\n",
      "loss: 6.361517429351807 at epoch 85\n",
      "loss: 9.169130325317383 at epoch 85\n",
      "loss: 8.171832084655762 at epoch 85\n",
      "loss: 7.549355506896973 at epoch 85\n",
      "loss: 7.6750712394714355 at epoch 86\n",
      "loss: 6.904651165008545 at epoch 86\n",
      "loss: 7.413649082183838 at epoch 86\n",
      "loss: 9.438539505004883 at epoch 86\n",
      "loss: 6.651368141174316 at epoch 86\n",
      "loss: 7.094099521636963 at epoch 86\n",
      "loss: 8.47121524810791 at epoch 86\n",
      "loss: 7.262094974517822 at epoch 86\n",
      "loss: 7.414723873138428 at epoch 87\n",
      "loss: 7.845856666564941 at epoch 87\n",
      "loss: 8.529937744140625 at epoch 87\n",
      "loss: 8.492899894714355 at epoch 87\n",
      "loss: 7.050610542297363 at epoch 87\n",
      "loss: 8.075714111328125 at epoch 87\n",
      "loss: 8.352153778076172 at epoch 87\n",
      "loss: 8.260003089904785 at epoch 87\n",
      "loss: 8.671671867370605 at epoch 88\n",
      "loss: 8.781598091125488 at epoch 88\n",
      "loss: 8.89881706237793 at epoch 88\n",
      "loss: 7.613527774810791 at epoch 88\n",
      "loss: 6.931472301483154 at epoch 88\n",
      "loss: 7.639312744140625 at epoch 88\n",
      "loss: 7.551994800567627 at epoch 88\n",
      "loss: 8.591347694396973 at epoch 88\n",
      "loss: 7.591108322143555 at epoch 89\n",
      "loss: 7.3344807624816895 at epoch 89\n",
      "loss: 9.0939359664917 at epoch 89\n",
      "loss: 9.275158882141113 at epoch 89\n",
      "loss: 6.782797813415527 at epoch 89\n",
      "loss: 8.984797477722168 at epoch 89\n",
      "loss: 7.880897521972656 at epoch 89\n",
      "loss: 8.571531295776367 at epoch 89\n",
      "loss: 7.593782424926758 at epoch 90\n",
      "loss: 6.808177947998047 at epoch 90\n",
      "loss: 10.573775291442871 at epoch 90\n",
      "loss: 8.260483741760254 at epoch 90\n",
      "loss: 7.052952766418457 at epoch 90\n",
      "loss: 8.60272216796875 at epoch 90\n",
      "loss: 9.239940643310547 at epoch 90\n",
      "loss: 8.004632949829102 at epoch 90\n",
      "loss: 8.64643669128418 at epoch 91\n",
      "loss: 8.451004028320312 at epoch 91\n",
      "loss: 8.366806030273438 at epoch 91\n",
      "loss: 8.013337135314941 at epoch 91\n",
      "loss: 7.489645004272461 at epoch 91\n",
      "loss: 6.18260383605957 at epoch 91\n",
      "loss: 7.843361854553223 at epoch 91\n",
      "loss: 7.915563106536865 at epoch 91\n",
      "loss: 8.755932807922363 at epoch 92\n",
      "loss: 8.51830768585205 at epoch 92\n",
      "loss: 7.888507843017578 at epoch 92\n",
      "loss: 6.9841742515563965 at epoch 92\n",
      "loss: 7.570112705230713 at epoch 92\n",
      "loss: 8.233977317810059 at epoch 92\n",
      "loss: 8.9082670211792 at epoch 92\n",
      "loss: 6.946985244750977 at epoch 92\n",
      "loss: 7.684804439544678 at epoch 93\n",
      "loss: 7.139291763305664 at epoch 93\n",
      "loss: 8.786259651184082 at epoch 93\n",
      "loss: 7.275816917419434 at epoch 93\n",
      "loss: 7.902842044830322 at epoch 93\n",
      "loss: 7.214604377746582 at epoch 93\n",
      "loss: 7.305072784423828 at epoch 93\n",
      "loss: 8.093912124633789 at epoch 93\n",
      "loss: 7.618682861328125 at epoch 94\n",
      "loss: 6.401076793670654 at epoch 94\n",
      "loss: 7.377257823944092 at epoch 94\n",
      "loss: 8.362738609313965 at epoch 94\n",
      "loss: 8.59926700592041 at epoch 94\n",
      "loss: 8.801826477050781 at epoch 94\n",
      "loss: 7.5966315269470215 at epoch 94\n",
      "loss: 7.846940040588379 at epoch 94\n",
      "loss: 8.229978561401367 at epoch 95\n",
      "loss: 8.059845924377441 at epoch 95\n",
      "loss: 8.372281074523926 at epoch 95\n",
      "loss: 6.419938564300537 at epoch 95\n",
      "loss: 6.811509132385254 at epoch 95\n",
      "loss: 8.519152641296387 at epoch 95\n",
      "loss: 9.118708610534668 at epoch 95\n",
      "loss: 7.052582740783691 at epoch 95\n",
      "loss: 7.019234657287598 at epoch 96\n",
      "loss: 6.532744407653809 at epoch 96\n",
      "loss: 11.011068344116211 at epoch 96\n",
      "loss: 7.430337905883789 at epoch 96\n",
      "loss: 8.873908996582031 at epoch 96\n",
      "loss: 8.016693115234375 at epoch 96\n",
      "loss: 8.815906524658203 at epoch 96\n",
      "loss: 8.52531909942627 at epoch 96\n",
      "loss: 9.09438705444336 at epoch 97\n",
      "loss: 8.718618392944336 at epoch 97\n",
      "loss: 9.730873107910156 at epoch 97\n",
      "loss: 6.891715049743652 at epoch 97\n",
      "loss: 7.5766520500183105 at epoch 97\n",
      "loss: 9.4423246383667 at epoch 97\n",
      "loss: 7.878791809082031 at epoch 97\n",
      "loss: 8.646201133728027 at epoch 97\n",
      "loss: 7.375131607055664 at epoch 98\n",
      "loss: 8.62042236328125 at epoch 98\n",
      "loss: 9.885795593261719 at epoch 98\n",
      "loss: 8.335451126098633 at epoch 98\n",
      "loss: 10.175851821899414 at epoch 98\n",
      "loss: 9.645671844482422 at epoch 98\n",
      "loss: 7.887790679931641 at epoch 98\n",
      "loss: 8.322832107543945 at epoch 98\n",
      "loss: 9.164257049560547 at epoch 99\n",
      "loss: 9.699063301086426 at epoch 99\n",
      "loss: 7.543338775634766 at epoch 99\n",
      "loss: 8.075572967529297 at epoch 99\n",
      "loss: 7.371677875518799 at epoch 99\n",
      "loss: 6.988359451293945 at epoch 99\n",
      "loss: 6.728660583496094 at epoch 99\n",
      "loss: 6.70629358291626 at epoch 99\n",
      "loss: 7.788046836853027 at epoch 100\n",
      "loss: 7.661633491516113 at epoch 100\n",
      "loss: 7.553171157836914 at epoch 100\n",
      "loss: 7.647904396057129 at epoch 100\n",
      "loss: 7.518268585205078 at epoch 100\n",
      "loss: 7.4331817626953125 at epoch 100\n",
      "loss: 8.099324226379395 at epoch 100\n",
      "loss: 7.9449076652526855 at epoch 100\n",
      "loss: 7.483198642730713 at epoch 101\n",
      "loss: 6.59122371673584 at epoch 101\n",
      "loss: 8.467451095581055 at epoch 101\n",
      "loss: 7.602045059204102 at epoch 101\n",
      "loss: 7.401715278625488 at epoch 101\n",
      "loss: 8.267923355102539 at epoch 101\n",
      "loss: 7.450346946716309 at epoch 101\n",
      "loss: 6.7649126052856445 at epoch 101\n",
      "loss: 8.297636032104492 at epoch 102\n",
      "loss: 6.593276500701904 at epoch 102\n",
      "loss: 7.067545413970947 at epoch 102\n",
      "loss: 7.987785816192627 at epoch 102\n",
      "loss: 6.936794757843018 at epoch 102\n",
      "loss: 8.617594718933105 at epoch 102\n",
      "loss: 8.278573989868164 at epoch 102\n",
      "loss: 6.745144367218018 at epoch 102\n",
      "loss: 7.457056045532227 at epoch 103\n",
      "loss: 8.429393768310547 at epoch 103\n",
      "loss: 7.173131942749023 at epoch 103\n",
      "loss: 7.922186851501465 at epoch 103\n",
      "loss: 6.209540843963623 at epoch 103\n",
      "loss: 7.940160274505615 at epoch 103\n",
      "loss: 8.257291793823242 at epoch 103\n",
      "loss: 7.210773944854736 at epoch 103\n",
      "loss: 8.025808334350586 at epoch 104\n",
      "loss: 7.392977237701416 at epoch 104\n",
      "loss: 7.809968948364258 at epoch 104\n",
      "loss: 7.114850044250488 at epoch 104\n",
      "loss: 7.401632308959961 at epoch 104\n",
      "loss: 6.933823108673096 at epoch 104\n",
      "loss: 8.590158462524414 at epoch 104\n",
      "loss: 10.01285171508789 at epoch 104\n",
      "loss: 7.7545061111450195 at epoch 105\n",
      "loss: 7.730268478393555 at epoch 105\n",
      "loss: 7.502012729644775 at epoch 105\n",
      "loss: 8.312721252441406 at epoch 105\n",
      "loss: 7.860332012176514 at epoch 105\n",
      "loss: 7.391419887542725 at epoch 105\n",
      "loss: 8.059103965759277 at epoch 105\n",
      "loss: 6.695376873016357 at epoch 105\n",
      "loss: 6.561853408813477 at epoch 106\n",
      "loss: 7.795581817626953 at epoch 106\n",
      "loss: 7.1485395431518555 at epoch 106\n",
      "loss: 8.316642761230469 at epoch 106\n",
      "loss: 7.324016571044922 at epoch 106\n",
      "loss: 8.317357063293457 at epoch 106\n",
      "loss: 8.349912643432617 at epoch 106\n",
      "loss: 7.2327046394348145 at epoch 106\n",
      "loss: 7.6566667556762695 at epoch 107\n",
      "loss: 7.0444488525390625 at epoch 107\n",
      "loss: 7.931818008422852 at epoch 107\n",
      "loss: 8.3561429977417 at epoch 107\n",
      "loss: 7.698858737945557 at epoch 107\n",
      "loss: 6.369473457336426 at epoch 107\n",
      "loss: 8.412248611450195 at epoch 107\n",
      "loss: 8.320745468139648 at epoch 107\n",
      "loss: 9.786023139953613 at epoch 108\n",
      "loss: 7.500055313110352 at epoch 108\n",
      "loss: 9.552083015441895 at epoch 108\n",
      "loss: 7.747395038604736 at epoch 108\n",
      "loss: 6.841070175170898 at epoch 108\n",
      "loss: 7.995197772979736 at epoch 108\n",
      "loss: 8.727021217346191 at epoch 108\n",
      "loss: 8.674843788146973 at epoch 108\n",
      "loss: 8.58633804321289 at epoch 109\n",
      "loss: 7.501280784606934 at epoch 109\n",
      "loss: 8.318794250488281 at epoch 109\n",
      "loss: 8.177078247070312 at epoch 109\n",
      "loss: 8.103506088256836 at epoch 109\n",
      "loss: 6.894006729125977 at epoch 109\n",
      "loss: 9.924951553344727 at epoch 109\n",
      "loss: 7.05982780456543 at epoch 109\n",
      "loss: 7.908151626586914 at epoch 110\n",
      "loss: 8.831033706665039 at epoch 110\n",
      "loss: 6.640193462371826 at epoch 110\n",
      "loss: 7.471625804901123 at epoch 110\n",
      "loss: 7.4130144119262695 at epoch 110\n",
      "loss: 8.795534133911133 at epoch 110\n",
      "loss: 8.23823356628418 at epoch 110\n",
      "loss: 8.647099494934082 at epoch 110\n",
      "loss: 8.357967376708984 at epoch 111\n",
      "loss: 8.551573753356934 at epoch 111\n",
      "loss: 7.60036563873291 at epoch 111\n",
      "loss: 6.699581623077393 at epoch 111\n",
      "loss: 7.818052291870117 at epoch 111\n",
      "loss: 7.096786975860596 at epoch 111\n",
      "loss: 8.126544952392578 at epoch 111\n",
      "loss: 7.149589538574219 at epoch 111\n",
      "loss: 7.881638526916504 at epoch 112\n",
      "loss: 7.3637375831604 at epoch 112\n",
      "loss: 6.92979621887207 at epoch 112\n",
      "loss: 8.534418106079102 at epoch 112\n",
      "loss: 7.030738830566406 at epoch 112\n",
      "loss: 8.743145942687988 at epoch 112\n",
      "loss: 7.14250373840332 at epoch 112\n",
      "loss: 8.26127815246582 at epoch 112\n",
      "loss: 7.208181381225586 at epoch 113\n",
      "loss: 7.098057270050049 at epoch 113\n",
      "loss: 7.783540725708008 at epoch 113\n",
      "loss: 7.832976818084717 at epoch 113\n",
      "loss: 6.84597110748291 at epoch 113\n",
      "loss: 8.102783203125 at epoch 113\n",
      "loss: 8.559673309326172 at epoch 113\n",
      "loss: 7.147228717803955 at epoch 113\n",
      "loss: 7.585271835327148 at epoch 114\n",
      "loss: 8.752968788146973 at epoch 114\n",
      "loss: 7.116162300109863 at epoch 114\n",
      "loss: 6.36321496963501 at epoch 114\n",
      "loss: 8.56088638305664 at epoch 114\n",
      "loss: 8.864276885986328 at epoch 114\n",
      "loss: 7.979785919189453 at epoch 114\n",
      "loss: 8.413415908813477 at epoch 114\n",
      "loss: 6.429064750671387 at epoch 115\n",
      "loss: 7.833327293395996 at epoch 115\n",
      "loss: 8.569648742675781 at epoch 115\n",
      "loss: 8.910593032836914 at epoch 115\n",
      "loss: 6.900935173034668 at epoch 115\n",
      "loss: 8.292394638061523 at epoch 115\n",
      "loss: 8.597636222839355 at epoch 115\n",
      "loss: 7.033501148223877 at epoch 115\n",
      "loss: 6.353026390075684 at epoch 116\n",
      "loss: 6.867606163024902 at epoch 116\n",
      "loss: 7.799983024597168 at epoch 116\n",
      "loss: 9.492573738098145 at epoch 116\n",
      "loss: 8.679609298706055 at epoch 116\n",
      "loss: 7.521993637084961 at epoch 116\n",
      "loss: 7.451807975769043 at epoch 116\n",
      "loss: 6.512203693389893 at epoch 116\n",
      "loss: 8.120308876037598 at epoch 117\n",
      "loss: 8.899908065795898 at epoch 117\n",
      "loss: 7.003470420837402 at epoch 117\n",
      "loss: 8.619890213012695 at epoch 117\n",
      "loss: 7.946175575256348 at epoch 117\n",
      "loss: 7.300457000732422 at epoch 117\n",
      "loss: 8.29142951965332 at epoch 117\n",
      "loss: 6.994290828704834 at epoch 117\n",
      "loss: 6.420069217681885 at epoch 118\n",
      "loss: 7.015497207641602 at epoch 118\n",
      "loss: 8.4044189453125 at epoch 118\n",
      "loss: 7.2769880294799805 at epoch 118\n",
      "loss: 7.431222915649414 at epoch 118\n",
      "loss: 8.737285614013672 at epoch 118\n",
      "loss: 8.531999588012695 at epoch 118\n",
      "loss: 8.480504035949707 at epoch 118\n",
      "loss: 8.132354736328125 at epoch 119\n",
      "loss: 7.0200018882751465 at epoch 119\n",
      "loss: 8.286394119262695 at epoch 119\n",
      "loss: 9.308161735534668 at epoch 119\n",
      "loss: 6.397627353668213 at epoch 119\n",
      "loss: 7.417999267578125 at epoch 119\n",
      "loss: 8.738999366760254 at epoch 119\n",
      "loss: 8.254451751708984 at epoch 119\n",
      "loss: 8.736663818359375 at epoch 120\n",
      "loss: 7.1441779136657715 at epoch 120\n",
      "loss: 8.094623565673828 at epoch 120\n",
      "loss: 7.664250373840332 at epoch 120\n",
      "loss: 7.049087047576904 at epoch 120\n",
      "loss: 7.6364312171936035 at epoch 120\n",
      "loss: 5.836208820343018 at epoch 120\n",
      "loss: 8.579265594482422 at epoch 120\n",
      "loss: 7.738651275634766 at epoch 121\n",
      "loss: 7.789333820343018 at epoch 121\n",
      "loss: 8.410538673400879 at epoch 121\n",
      "loss: 7.740677356719971 at epoch 121\n",
      "loss: 7.769680500030518 at epoch 121\n",
      "loss: 7.134057998657227 at epoch 121\n",
      "loss: 8.54483413696289 at epoch 121\n",
      "loss: 7.933446884155273 at epoch 121\n",
      "loss: 8.49013614654541 at epoch 122\n",
      "loss: 8.93243408203125 at epoch 122\n",
      "loss: 7.555668354034424 at epoch 122\n",
      "loss: 5.8582305908203125 at epoch 122\n",
      "loss: 7.8458709716796875 at epoch 122\n",
      "loss: 8.315778732299805 at epoch 122\n",
      "loss: 7.48566198348999 at epoch 122\n",
      "loss: 8.42005729675293 at epoch 122\n",
      "loss: 7.163023948669434 at epoch 123\n",
      "loss: 7.674637317657471 at epoch 123\n",
      "loss: 8.24614429473877 at epoch 123\n",
      "loss: 7.135076522827148 at epoch 123\n",
      "loss: 8.516478538513184 at epoch 123\n",
      "loss: 8.799932479858398 at epoch 123\n",
      "loss: 7.120116710662842 at epoch 123\n",
      "loss: 7.8013505935668945 at epoch 123\n",
      "loss: 8.512625694274902 at epoch 124\n",
      "loss: 6.935208797454834 at epoch 124\n",
      "loss: 8.341169357299805 at epoch 124\n",
      "loss: 7.585841655731201 at epoch 124\n",
      "loss: 7.763082504272461 at epoch 124\n",
      "loss: 6.675784111022949 at epoch 124\n",
      "loss: 8.167218208312988 at epoch 124\n",
      "loss: 8.886489868164062 at epoch 124\n",
      "loss: 8.305127143859863 at epoch 125\n",
      "loss: 6.96377420425415 at epoch 125\n",
      "loss: 8.498013496398926 at epoch 125\n",
      "loss: 8.529611587524414 at epoch 125\n",
      "loss: 8.406736373901367 at epoch 125\n",
      "loss: 6.659553527832031 at epoch 125\n",
      "loss: 7.184579849243164 at epoch 125\n",
      "loss: 7.869488716125488 at epoch 125\n",
      "loss: 8.13761043548584 at epoch 126\n",
      "loss: 6.951120853424072 at epoch 126\n",
      "loss: 7.7963786125183105 at epoch 126\n",
      "loss: 8.80984878540039 at epoch 126\n",
      "loss: 8.468396186828613 at epoch 126\n",
      "loss: 7.21284818649292 at epoch 126\n",
      "loss: 6.977759838104248 at epoch 126\n",
      "loss: 7.889912128448486 at epoch 126\n",
      "loss: 7.83283805847168 at epoch 127\n",
      "loss: 7.664792060852051 at epoch 127\n",
      "loss: 7.5680413246154785 at epoch 127\n",
      "loss: 7.6421356201171875 at epoch 127\n",
      "loss: 7.184421062469482 at epoch 127\n",
      "loss: 8.277324676513672 at epoch 127\n",
      "loss: 8.131141662597656 at epoch 127\n",
      "loss: 7.4365081787109375 at epoch 127\n",
      "loss: 7.75925350189209 at epoch 128\n",
      "loss: 6.962180137634277 at epoch 128\n",
      "loss: 7.3674516677856445 at epoch 128\n",
      "loss: 8.566877365112305 at epoch 128\n",
      "loss: 6.982630729675293 at epoch 128\n",
      "loss: 8.070246696472168 at epoch 128\n",
      "loss: 8.97068977355957 at epoch 128\n",
      "loss: 8.314377784729004 at epoch 128\n",
      "loss: 8.51778793334961 at epoch 129\n",
      "loss: 7.297275543212891 at epoch 129\n",
      "loss: 8.856790542602539 at epoch 129\n",
      "loss: 8.127287864685059 at epoch 129\n",
      "loss: 7.35305118560791 at epoch 129\n",
      "loss: 7.017005443572998 at epoch 129\n",
      "loss: 8.742508888244629 at epoch 129\n",
      "loss: 6.206270217895508 at epoch 129\n",
      "loss: 6.291886329650879 at epoch 130\n",
      "loss: 7.92122745513916 at epoch 130\n",
      "loss: 7.234106063842773 at epoch 130\n",
      "loss: 8.11939811706543 at epoch 130\n",
      "loss: 7.991153717041016 at epoch 130\n",
      "loss: 8.88791275024414 at epoch 130\n",
      "loss: 7.739649772644043 at epoch 130\n",
      "loss: 9.957310676574707 at epoch 130\n",
      "loss: 7.559954643249512 at epoch 131\n",
      "loss: 8.541805267333984 at epoch 131\n",
      "loss: 8.30423355102539 at epoch 131\n",
      "loss: 7.725016117095947 at epoch 131\n",
      "loss: 8.634849548339844 at epoch 131\n",
      "loss: 6.406044960021973 at epoch 131\n",
      "loss: 9.62651252746582 at epoch 131\n",
      "loss: 7.36759614944458 at epoch 131\n",
      "loss: 8.44941520690918 at epoch 132\n",
      "loss: 8.729316711425781 at epoch 132\n",
      "loss: 7.52533483505249 at epoch 132\n",
      "loss: 7.895529747009277 at epoch 132\n",
      "loss: 8.35336685180664 at epoch 132\n",
      "loss: 8.464444160461426 at epoch 132\n",
      "loss: 9.506093978881836 at epoch 132\n",
      "loss: 8.001504898071289 at epoch 132\n",
      "loss: 8.207100868225098 at epoch 133\n",
      "loss: 7.564626693725586 at epoch 133\n",
      "loss: 7.712099552154541 at epoch 133\n",
      "loss: 8.138391494750977 at epoch 133\n",
      "loss: 7.4208598136901855 at epoch 133\n",
      "loss: 8.6381196975708 at epoch 133\n",
      "loss: 7.1528520584106445 at epoch 133\n",
      "loss: 6.31970739364624 at epoch 133\n",
      "loss: 9.591123580932617 at epoch 134\n",
      "loss: 7.718145370483398 at epoch 134\n",
      "loss: 7.267312526702881 at epoch 134\n",
      "loss: 6.773392677307129 at epoch 134\n",
      "loss: 7.166945934295654 at epoch 134\n",
      "loss: 7.3694844245910645 at epoch 134\n",
      "loss: 6.793532371520996 at epoch 134\n",
      "loss: 8.101544380187988 at epoch 134\n",
      "loss: 8.627439498901367 at epoch 135\n",
      "loss: 7.329578399658203 at epoch 135\n",
      "loss: 8.164185523986816 at epoch 135\n",
      "loss: 8.166949272155762 at epoch 135\n",
      "loss: 8.547622680664062 at epoch 135\n",
      "loss: 6.561003684997559 at epoch 135\n",
      "loss: 9.167107582092285 at epoch 135\n",
      "loss: 7.9932780265808105 at epoch 135\n",
      "loss: 6.874413967132568 at epoch 136\n",
      "loss: 6.272250175476074 at epoch 136\n",
      "loss: 9.29713249206543 at epoch 136\n",
      "loss: 7.1986589431762695 at epoch 136\n",
      "loss: 7.538001537322998 at epoch 136\n",
      "loss: 8.661240577697754 at epoch 136\n",
      "loss: 8.851536750793457 at epoch 136\n",
      "loss: 5.8356475830078125 at epoch 136\n",
      "loss: 8.298202514648438 at epoch 137\n",
      "loss: 7.739065170288086 at epoch 137\n",
      "loss: 7.5562849044799805 at epoch 137\n",
      "loss: 8.577533721923828 at epoch 137\n",
      "loss: 7.562754154205322 at epoch 137\n",
      "loss: 7.430667400360107 at epoch 137\n",
      "loss: 7.195889472961426 at epoch 137\n",
      "loss: 8.08017349243164 at epoch 137\n",
      "loss: 9.315255165100098 at epoch 138\n",
      "loss: 8.157703399658203 at epoch 138\n",
      "loss: 7.35392951965332 at epoch 138\n",
      "loss: 6.052045822143555 at epoch 138\n",
      "loss: 9.145929336547852 at epoch 138\n",
      "loss: 7.6324310302734375 at epoch 138\n",
      "loss: 8.26076889038086 at epoch 138\n",
      "loss: 6.929775714874268 at epoch 138\n",
      "loss: 7.599617958068848 at epoch 139\n",
      "loss: 8.300990104675293 at epoch 139\n",
      "loss: 7.4535698890686035 at epoch 139\n",
      "loss: 6.193655967712402 at epoch 139\n",
      "loss: 8.004137992858887 at epoch 139\n",
      "loss: 6.845293045043945 at epoch 139\n",
      "loss: 9.843164443969727 at epoch 139\n",
      "loss: 8.829614639282227 at epoch 139\n",
      "loss: 9.786288261413574 at epoch 140\n",
      "loss: 7.311785697937012 at epoch 140\n",
      "loss: 7.013749599456787 at epoch 140\n",
      "loss: 6.7797770500183105 at epoch 140\n",
      "loss: 7.488587379455566 at epoch 140\n",
      "loss: 7.802558898925781 at epoch 140\n",
      "loss: 8.253989219665527 at epoch 140\n",
      "loss: 7.4912333488464355 at epoch 140\n",
      "loss: 7.9589715003967285 at epoch 141\n",
      "loss: 11.057280540466309 at epoch 141\n",
      "loss: 9.331526756286621 at epoch 141\n",
      "loss: 10.467958450317383 at epoch 141\n",
      "loss: 8.094569206237793 at epoch 141\n",
      "loss: 7.1265411376953125 at epoch 141\n",
      "loss: 9.298407554626465 at epoch 141\n",
      "loss: 6.54065465927124 at epoch 141\n",
      "loss: 7.560786247253418 at epoch 142\n",
      "loss: 6.787008285522461 at epoch 142\n",
      "loss: 8.455934524536133 at epoch 142\n",
      "loss: 9.72305679321289 at epoch 142\n",
      "loss: 8.15134048461914 at epoch 142\n",
      "loss: 7.00398063659668 at epoch 142\n",
      "loss: 7.965682029724121 at epoch 142\n",
      "loss: 8.851201057434082 at epoch 142\n",
      "loss: 9.901065826416016 at epoch 143\n",
      "loss: 7.5923333168029785 at epoch 143\n",
      "loss: 7.235374450683594 at epoch 143\n",
      "loss: 7.698703765869141 at epoch 143\n",
      "loss: 7.387850284576416 at epoch 143\n",
      "loss: 9.185664176940918 at epoch 143\n",
      "loss: 8.240872383117676 at epoch 143\n",
      "loss: 6.39481258392334 at epoch 143\n",
      "loss: 8.413614273071289 at epoch 144\n",
      "loss: 8.277173042297363 at epoch 144\n",
      "loss: 7.535989761352539 at epoch 144\n",
      "loss: 7.582625865936279 at epoch 144\n",
      "loss: 6.7964982986450195 at epoch 144\n",
      "loss: 7.079639911651611 at epoch 144\n",
      "loss: 6.836294174194336 at epoch 144\n",
      "loss: 8.951003074645996 at epoch 144\n",
      "loss: 7.638481140136719 at epoch 145\n",
      "loss: 7.438846588134766 at epoch 145\n",
      "loss: 6.813398361206055 at epoch 145\n",
      "loss: 7.398383140563965 at epoch 145\n",
      "loss: 7.61358642578125 at epoch 145\n",
      "loss: 8.501599311828613 at epoch 145\n",
      "loss: 8.407721519470215 at epoch 145\n",
      "loss: 7.671411514282227 at epoch 145\n",
      "loss: 8.515583992004395 at epoch 146\n",
      "loss: 7.977248191833496 at epoch 146\n",
      "loss: 7.274996280670166 at epoch 146\n",
      "loss: 7.284733295440674 at epoch 146\n",
      "loss: 5.9681243896484375 at epoch 146\n",
      "loss: 8.61116886138916 at epoch 146\n",
      "loss: 9.535332679748535 at epoch 146\n",
      "loss: 7.739351272583008 at epoch 146\n",
      "loss: 6.639280796051025 at epoch 147\n",
      "loss: 8.869486808776855 at epoch 147\n",
      "loss: 8.265408515930176 at epoch 147\n",
      "loss: 7.881695747375488 at epoch 147\n",
      "loss: 8.007034301757812 at epoch 147\n",
      "loss: 8.038222312927246 at epoch 147\n",
      "loss: 7.479546070098877 at epoch 147\n",
      "loss: 8.33902645111084 at epoch 147\n",
      "loss: 7.61029052734375 at epoch 148\n",
      "loss: 8.131897926330566 at epoch 148\n",
      "loss: 8.982426643371582 at epoch 148\n",
      "loss: 9.27672004699707 at epoch 148\n",
      "loss: 6.781737327575684 at epoch 148\n",
      "loss: 8.151485443115234 at epoch 148\n",
      "loss: 7.252585411071777 at epoch 148\n",
      "loss: 10.17623519897461 at epoch 148\n",
      "loss: 8.267797470092773 at epoch 149\n",
      "loss: 7.569976806640625 at epoch 149\n",
      "loss: 8.627847671508789 at epoch 149\n",
      "loss: 8.3456392288208 at epoch 149\n",
      "loss: 8.130030632019043 at epoch 149\n",
      "loss: 6.756807327270508 at epoch 149\n",
      "loss: 8.061848640441895 at epoch 149\n",
      "loss: 7.629340648651123 at epoch 149\n",
      "loss: 9.044710159301758 at epoch 150\n",
      "loss: 9.607337951660156 at epoch 150\n",
      "loss: 6.284028053283691 at epoch 150\n",
      "loss: 6.982382774353027 at epoch 150\n",
      "loss: 8.27133846282959 at epoch 150\n",
      "loss: 6.852069854736328 at epoch 150\n",
      "loss: 8.438194274902344 at epoch 150\n",
      "loss: 7.852434158325195 at epoch 150\n",
      "loss: 7.304998397827148 at epoch 151\n",
      "loss: 9.796025276184082 at epoch 151\n",
      "loss: 7.5013933181762695 at epoch 151\n",
      "loss: 7.994846343994141 at epoch 151\n",
      "loss: 7.50792121887207 at epoch 151\n",
      "loss: 8.434266090393066 at epoch 151\n",
      "loss: 6.909185409545898 at epoch 151\n",
      "loss: 8.215360641479492 at epoch 151\n",
      "loss: 7.5058441162109375 at epoch 152\n",
      "loss: 7.644850730895996 at epoch 152\n",
      "loss: 8.425209045410156 at epoch 152\n",
      "loss: 7.993962287902832 at epoch 152\n",
      "loss: 7.92125940322876 at epoch 152\n",
      "loss: 7.995256423950195 at epoch 152\n",
      "loss: 7.616812705993652 at epoch 152\n",
      "loss: 8.078634262084961 at epoch 152\n",
      "loss: 6.5898847579956055 at epoch 153\n",
      "loss: 9.5337495803833 at epoch 153\n",
      "loss: 7.609103202819824 at epoch 153\n",
      "loss: 7.658379077911377 at epoch 153\n",
      "loss: 8.26236629486084 at epoch 153\n",
      "loss: 6.823291301727295 at epoch 153\n",
      "loss: 8.017297744750977 at epoch 153\n",
      "loss: 8.674318313598633 at epoch 153\n",
      "loss: 6.73667049407959 at epoch 154\n",
      "loss: 8.608481407165527 at epoch 154\n",
      "loss: 7.391162395477295 at epoch 154\n",
      "loss: 7.7878265380859375 at epoch 154\n",
      "loss: 8.653525352478027 at epoch 154\n",
      "loss: 7.654141426086426 at epoch 154\n",
      "loss: 8.093854904174805 at epoch 154\n",
      "loss: 8.581153869628906 at epoch 154\n",
      "loss: 7.40738582611084 at epoch 155\n",
      "loss: 8.597845077514648 at epoch 155\n",
      "loss: 9.015630722045898 at epoch 155\n",
      "loss: 8.34423828125 at epoch 155\n",
      "loss: 7.490660667419434 at epoch 155\n",
      "loss: 8.896096229553223 at epoch 155\n",
      "loss: 7.587899684906006 at epoch 155\n",
      "loss: 7.434786319732666 at epoch 155\n",
      "loss: 7.322939872741699 at epoch 156\n",
      "loss: 7.5587477684021 at epoch 156\n",
      "loss: 6.807271480560303 at epoch 156\n",
      "loss: 9.756054878234863 at epoch 156\n",
      "loss: 7.947507858276367 at epoch 156\n",
      "loss: 9.168365478515625 at epoch 156\n",
      "loss: 7.273690223693848 at epoch 156\n",
      "loss: 7.927077293395996 at epoch 156\n",
      "loss: 8.39745807647705 at epoch 157\n",
      "loss: 9.472555160522461 at epoch 157\n",
      "loss: 7.547875881195068 at epoch 157\n",
      "loss: 10.27500057220459 at epoch 157\n",
      "loss: 7.622522830963135 at epoch 157\n",
      "loss: 6.828375339508057 at epoch 157\n",
      "loss: 7.594489574432373 at epoch 157\n",
      "loss: 7.400243759155273 at epoch 157\n",
      "loss: 9.823936462402344 at epoch 158\n",
      "loss: 8.151762008666992 at epoch 158\n",
      "loss: 8.549592971801758 at epoch 158\n",
      "loss: 7.400609016418457 at epoch 158\n",
      "loss: 8.32921314239502 at epoch 158\n",
      "loss: 8.604336738586426 at epoch 158\n",
      "loss: 8.757339477539062 at epoch 158\n",
      "loss: 8.849773406982422 at epoch 158\n",
      "loss: 9.678104400634766 at epoch 159\n",
      "loss: 8.227217674255371 at epoch 159\n",
      "loss: 10.021889686584473 at epoch 159\n",
      "loss: 6.80485725402832 at epoch 159\n",
      "loss: 7.643171310424805 at epoch 159\n",
      "loss: 9.223297119140625 at epoch 159\n",
      "loss: 7.132901668548584 at epoch 159\n",
      "loss: 9.061455726623535 at epoch 159\n",
      "loss: 9.089054107666016 at epoch 160\n",
      "loss: 9.392723083496094 at epoch 160\n",
      "loss: 8.264372825622559 at epoch 160\n",
      "loss: 7.779825687408447 at epoch 160\n",
      "loss: 7.0484538078308105 at epoch 160\n",
      "loss: 8.932537078857422 at epoch 160\n",
      "loss: 7.344864845275879 at epoch 160\n",
      "loss: 8.67868709564209 at epoch 160\n",
      "loss: 7.692288875579834 at epoch 161\n",
      "loss: 7.883647918701172 at epoch 161\n",
      "loss: 7.540248870849609 at epoch 161\n",
      "loss: 8.715812683105469 at epoch 161\n",
      "loss: 8.058804512023926 at epoch 161\n",
      "loss: 8.81367015838623 at epoch 161\n",
      "loss: 7.568032264709473 at epoch 161\n",
      "loss: 9.439512252807617 at epoch 161\n",
      "loss: 7.413821697235107 at epoch 162\n",
      "loss: 9.771004676818848 at epoch 162\n",
      "loss: 8.679295539855957 at epoch 162\n",
      "loss: 11.084768295288086 at epoch 162\n",
      "loss: 8.4246244430542 at epoch 162\n",
      "loss: 7.213510990142822 at epoch 162\n",
      "loss: 9.571479797363281 at epoch 162\n",
      "loss: 7.496850967407227 at epoch 162\n",
      "loss: 7.33463716506958 at epoch 163\n",
      "loss: 6.423618793487549 at epoch 163\n",
      "loss: 9.910682678222656 at epoch 163\n",
      "loss: 8.575108528137207 at epoch 163\n",
      "loss: 8.608254432678223 at epoch 163\n",
      "loss: 7.538631439208984 at epoch 163\n",
      "loss: 7.701331615447998 at epoch 163\n",
      "loss: 8.384678840637207 at epoch 163\n",
      "loss: 7.182551383972168 at epoch 164\n",
      "loss: 7.280463218688965 at epoch 164\n",
      "loss: 7.269650459289551 at epoch 164\n",
      "loss: 8.645389556884766 at epoch 164\n",
      "loss: 6.297630310058594 at epoch 164\n",
      "loss: 6.933281898498535 at epoch 164\n",
      "loss: 10.196080207824707 at epoch 164\n",
      "loss: 7.455838680267334 at epoch 164\n",
      "loss: 6.935635566711426 at epoch 165\n",
      "loss: 8.88829231262207 at epoch 165\n",
      "loss: 7.722902297973633 at epoch 165\n",
      "loss: 8.903280258178711 at epoch 165\n",
      "loss: 8.816938400268555 at epoch 165\n",
      "loss: 6.92262601852417 at epoch 165\n",
      "loss: 7.497830867767334 at epoch 165\n",
      "loss: 9.287765502929688 at epoch 165\n",
      "loss: 8.806859016418457 at epoch 166\n",
      "loss: 8.976415634155273 at epoch 166\n",
      "loss: 6.617451190948486 at epoch 166\n",
      "loss: 8.724230766296387 at epoch 166\n",
      "loss: 7.763719081878662 at epoch 166\n",
      "loss: 8.26632022857666 at epoch 166\n",
      "loss: 11.95676040649414 at epoch 166\n",
      "loss: 7.571685791015625 at epoch 166\n",
      "loss: 7.720526695251465 at epoch 167\n",
      "loss: 9.204936981201172 at epoch 167\n",
      "loss: 8.495646476745605 at epoch 167\n",
      "loss: 6.616501808166504 at epoch 167\n",
      "loss: 10.749926567077637 at epoch 167\n",
      "loss: 6.963639259338379 at epoch 167\n",
      "loss: 8.034963607788086 at epoch 167\n",
      "loss: 9.204071044921875 at epoch 167\n",
      "loss: 8.175952911376953 at epoch 168\n",
      "loss: 7.827844619750977 at epoch 168\n",
      "loss: 8.453558921813965 at epoch 168\n",
      "loss: 7.504620552062988 at epoch 168\n",
      "loss: 8.86202621459961 at epoch 168\n",
      "loss: 8.876875877380371 at epoch 168\n",
      "loss: 10.061765670776367 at epoch 168\n",
      "loss: 7.861918926239014 at epoch 168\n",
      "loss: 8.606941223144531 at epoch 169\n",
      "loss: 9.307161331176758 at epoch 169\n",
      "loss: 7.7288312911987305 at epoch 169\n",
      "loss: 7.987666130065918 at epoch 169\n",
      "loss: 10.205000877380371 at epoch 169\n",
      "loss: 8.084996223449707 at epoch 169\n",
      "loss: 7.040802001953125 at epoch 169\n",
      "loss: 8.128959655761719 at epoch 169\n",
      "loss: 7.296785354614258 at epoch 170\n",
      "loss: 7.281315326690674 at epoch 170\n",
      "loss: 7.737668514251709 at epoch 170\n",
      "loss: 7.246859073638916 at epoch 170\n",
      "loss: 8.586092948913574 at epoch 170\n",
      "loss: 6.633464336395264 at epoch 170\n",
      "loss: 8.313213348388672 at epoch 170\n",
      "loss: 8.080690383911133 at epoch 170\n",
      "loss: 7.994149208068848 at epoch 171\n",
      "loss: 7.436330318450928 at epoch 171\n",
      "loss: 7.750270843505859 at epoch 171\n",
      "loss: 6.761259078979492 at epoch 171\n",
      "loss: 7.641973972320557 at epoch 171\n",
      "loss: 6.191296577453613 at epoch 171\n",
      "loss: 8.510703086853027 at epoch 171\n",
      "loss: 8.469117164611816 at epoch 171\n",
      "loss: 7.11272668838501 at epoch 172\n",
      "loss: 8.245933532714844 at epoch 172\n",
      "loss: 7.352254390716553 at epoch 172\n",
      "loss: 7.603268623352051 at epoch 172\n",
      "loss: 7.471365928649902 at epoch 172\n",
      "loss: 7.259832382202148 at epoch 172\n",
      "loss: 6.589662551879883 at epoch 172\n",
      "loss: 7.92970085144043 at epoch 172\n",
      "loss: 7.0705671310424805 at epoch 173\n",
      "loss: 9.25602912902832 at epoch 173\n",
      "loss: 7.126472473144531 at epoch 173\n",
      "loss: 7.200450897216797 at epoch 173\n",
      "loss: 7.764064788818359 at epoch 173\n",
      "loss: 7.664950847625732 at epoch 173\n",
      "loss: 8.826454162597656 at epoch 173\n",
      "loss: 7.547457218170166 at epoch 173\n",
      "loss: 8.435266494750977 at epoch 174\n",
      "loss: 8.854599952697754 at epoch 174\n",
      "loss: 6.7502641677856445 at epoch 174\n",
      "loss: 8.145076751708984 at epoch 174\n",
      "loss: 7.582616806030273 at epoch 174\n",
      "loss: 7.147995948791504 at epoch 174\n",
      "loss: 8.184804916381836 at epoch 174\n",
      "loss: 7.838921070098877 at epoch 174\n",
      "loss: 7.161565780639648 at epoch 175\n",
      "loss: 9.400558471679688 at epoch 175\n",
      "loss: 8.180222511291504 at epoch 175\n",
      "loss: 9.251080513000488 at epoch 175\n",
      "loss: 7.3162641525268555 at epoch 175\n",
      "loss: 7.344043731689453 at epoch 175\n",
      "loss: 6.964571475982666 at epoch 175\n",
      "loss: 7.816198348999023 at epoch 175\n",
      "loss: 7.883865833282471 at epoch 176\n",
      "loss: 7.593844890594482 at epoch 176\n",
      "loss: 7.166024684906006 at epoch 176\n",
      "loss: 8.131742477416992 at epoch 176\n",
      "loss: 7.182985305786133 at epoch 176\n",
      "loss: 6.861520767211914 at epoch 176\n",
      "loss: 8.459924697875977 at epoch 176\n",
      "loss: 6.555191993713379 at epoch 176\n",
      "loss: 9.393235206604004 at epoch 177\n",
      "loss: 7.795515060424805 at epoch 177\n",
      "loss: 6.948159217834473 at epoch 177\n",
      "loss: 7.869402885437012 at epoch 177\n",
      "loss: 7.6871185302734375 at epoch 177\n",
      "loss: 6.841740608215332 at epoch 177\n",
      "loss: 8.968084335327148 at epoch 177\n",
      "loss: 7.5344343185424805 at epoch 177\n",
      "loss: 7.42325496673584 at epoch 178\n",
      "loss: 8.440164566040039 at epoch 178\n",
      "loss: 8.194024085998535 at epoch 178\n",
      "loss: 7.410508155822754 at epoch 178\n",
      "loss: 7.122199058532715 at epoch 178\n",
      "loss: 7.336568832397461 at epoch 178\n",
      "loss: 8.462133407592773 at epoch 178\n",
      "loss: 7.1439738273620605 at epoch 178\n",
      "loss: 7.905421257019043 at epoch 179\n",
      "loss: 7.9513702392578125 at epoch 179\n",
      "loss: 8.51417350769043 at epoch 179\n",
      "loss: 7.99765157699585 at epoch 179\n",
      "loss: 9.232771873474121 at epoch 179\n",
      "loss: 7.823892116546631 at epoch 179\n",
      "loss: 8.269760131835938 at epoch 179\n",
      "loss: 7.60172176361084 at epoch 179\n",
      "loss: 7.829775810241699 at epoch 180\n",
      "loss: 8.064330101013184 at epoch 180\n",
      "loss: 8.40002727508545 at epoch 180\n",
      "loss: 6.031660556793213 at epoch 180\n",
      "loss: 5.740176677703857 at epoch 180\n",
      "loss: 9.247626304626465 at epoch 180\n",
      "loss: 8.123268127441406 at epoch 180\n",
      "loss: 10.194782257080078 at epoch 180\n",
      "loss: 6.763966083526611 at epoch 181\n",
      "loss: 10.169568061828613 at epoch 181\n",
      "loss: 8.241312026977539 at epoch 181\n",
      "loss: 7.987637996673584 at epoch 181\n",
      "loss: 9.487751007080078 at epoch 181\n",
      "loss: 9.374231338500977 at epoch 181\n",
      "loss: 7.735518455505371 at epoch 181\n",
      "loss: 10.69497299194336 at epoch 181\n",
      "loss: 8.48913860321045 at epoch 182\n",
      "loss: 8.626261711120605 at epoch 182\n",
      "loss: 10.58177375793457 at epoch 182\n",
      "loss: 9.447525024414062 at epoch 182\n",
      "loss: 6.445801734924316 at epoch 182\n",
      "loss: 9.254549026489258 at epoch 182\n",
      "loss: 8.344850540161133 at epoch 182\n",
      "loss: 6.690577507019043 at epoch 182\n",
      "loss: 11.285317420959473 at epoch 183\n",
      "loss: 8.82209587097168 at epoch 183\n",
      "loss: 7.250905513763428 at epoch 183\n",
      "loss: 9.743943214416504 at epoch 183\n",
      "loss: 8.599231719970703 at epoch 183\n",
      "loss: 7.307163715362549 at epoch 183\n",
      "loss: 8.070401191711426 at epoch 183\n",
      "loss: 8.3367919921875 at epoch 183\n",
      "loss: 8.21799373626709 at epoch 184\n",
      "loss: 6.842207908630371 at epoch 184\n",
      "loss: 8.413069725036621 at epoch 184\n",
      "loss: 7.615964889526367 at epoch 184\n",
      "loss: 7.365075588226318 at epoch 184\n",
      "loss: 8.292263984680176 at epoch 184\n",
      "loss: 7.319178581237793 at epoch 184\n",
      "loss: 7.594525337219238 at epoch 184\n",
      "loss: 7.777573585510254 at epoch 185\n",
      "loss: 9.234110832214355 at epoch 185\n",
      "loss: 6.476388931274414 at epoch 185\n",
      "loss: 7.455724239349365 at epoch 185\n",
      "loss: 7.365789413452148 at epoch 185\n",
      "loss: 8.063435554504395 at epoch 185\n",
      "loss: 9.240864753723145 at epoch 185\n",
      "loss: 7.464559555053711 at epoch 185\n",
      "loss: 9.442654609680176 at epoch 186\n",
      "loss: 7.449948310852051 at epoch 186\n",
      "loss: 7.415155410766602 at epoch 186\n",
      "loss: 8.183914184570312 at epoch 186\n",
      "loss: 8.048166275024414 at epoch 186\n",
      "loss: 6.899268627166748 at epoch 186\n",
      "loss: 6.6184611320495605 at epoch 186\n",
      "loss: 8.554228782653809 at epoch 186\n",
      "loss: 8.434019088745117 at epoch 187\n",
      "loss: 6.1963911056518555 at epoch 187\n",
      "loss: 8.499343872070312 at epoch 187\n",
      "loss: 8.916277885437012 at epoch 187\n",
      "loss: 8.803411483764648 at epoch 187\n",
      "loss: 8.81473445892334 at epoch 187\n",
      "loss: 8.297956466674805 at epoch 187\n",
      "loss: 7.910860061645508 at epoch 187\n",
      "loss: 8.417057037353516 at epoch 188\n",
      "loss: 7.789640426635742 at epoch 188\n",
      "loss: 8.14522933959961 at epoch 188\n",
      "loss: 8.754969596862793 at epoch 188\n",
      "loss: 7.553996562957764 at epoch 188\n",
      "loss: 7.9533371925354 at epoch 188\n",
      "loss: 7.525116443634033 at epoch 188\n",
      "loss: 8.89646053314209 at epoch 188\n",
      "loss: 8.374476432800293 at epoch 189\n",
      "loss: 6.257946968078613 at epoch 189\n",
      "loss: 9.239825248718262 at epoch 189\n",
      "loss: 7.867443084716797 at epoch 189\n",
      "loss: 7.928565979003906 at epoch 189\n",
      "loss: 9.548867225646973 at epoch 189\n",
      "loss: 7.49990701675415 at epoch 189\n",
      "loss: 7.5131659507751465 at epoch 189\n",
      "loss: 7.1296916007995605 at epoch 190\n",
      "loss: 8.336256980895996 at epoch 190\n",
      "loss: 7.013690948486328 at epoch 190\n",
      "loss: 8.287364959716797 at epoch 190\n",
      "loss: 8.122016906738281 at epoch 190\n",
      "loss: 6.984622955322266 at epoch 190\n",
      "loss: 8.019757270812988 at epoch 190\n",
      "loss: 8.454835891723633 at epoch 190\n",
      "loss: 7.247645378112793 at epoch 191\n",
      "loss: 7.950533866882324 at epoch 191\n",
      "loss: 8.560981750488281 at epoch 191\n",
      "loss: 7.11474609375 at epoch 191\n",
      "loss: 8.373316764831543 at epoch 191\n",
      "loss: 7.9761061668396 at epoch 191\n",
      "loss: 7.178768634796143 at epoch 191\n",
      "loss: 7.816405773162842 at epoch 191\n",
      "loss: 8.159860610961914 at epoch 192\n",
      "loss: 7.009004592895508 at epoch 192\n",
      "loss: 7.652499198913574 at epoch 192\n",
      "loss: 7.864480495452881 at epoch 192\n",
      "loss: 7.863574981689453 at epoch 192\n",
      "loss: 8.123834609985352 at epoch 192\n",
      "loss: 6.553038120269775 at epoch 192\n",
      "loss: 7.357019424438477 at epoch 192\n",
      "loss: 8.179121971130371 at epoch 193\n",
      "loss: 7.391143798828125 at epoch 193\n",
      "loss: 7.0778985023498535 at epoch 193\n",
      "loss: 8.553239822387695 at epoch 193\n",
      "loss: 7.381789207458496 at epoch 193\n",
      "loss: 7.359110355377197 at epoch 193\n",
      "loss: 7.11288595199585 at epoch 193\n",
      "loss: 7.500150203704834 at epoch 193\n",
      "loss: 8.303781509399414 at epoch 194\n",
      "loss: 9.536797523498535 at epoch 194\n",
      "loss: 7.399514198303223 at epoch 194\n",
      "loss: 9.02865982055664 at epoch 194\n",
      "loss: 9.71511173248291 at epoch 194\n",
      "loss: 7.864877223968506 at epoch 194\n",
      "loss: 7.227652549743652 at epoch 194\n",
      "loss: 8.35617446899414 at epoch 194\n",
      "loss: 8.12903118133545 at epoch 195\n",
      "loss: 7.719998836517334 at epoch 195\n",
      "loss: 9.316838264465332 at epoch 195\n",
      "loss: 7.795384407043457 at epoch 195\n",
      "loss: 6.997903347015381 at epoch 195\n",
      "loss: 8.421667098999023 at epoch 195\n",
      "loss: 8.680807113647461 at epoch 195\n",
      "loss: 8.90558910369873 at epoch 195\n",
      "loss: 8.516390800476074 at epoch 196\n",
      "loss: 6.150568962097168 at epoch 196\n",
      "loss: 6.329797744750977 at epoch 196\n",
      "loss: 9.991037368774414 at epoch 196\n",
      "loss: 8.725366592407227 at epoch 196\n",
      "loss: 7.739716529846191 at epoch 196\n",
      "loss: 9.556086540222168 at epoch 196\n",
      "loss: 6.4952497482299805 at epoch 196\n",
      "loss: 7.9928460121154785 at epoch 197\n",
      "loss: 6.777169227600098 at epoch 197\n",
      "loss: 8.302133560180664 at epoch 197\n",
      "loss: 7.533109664916992 at epoch 197\n",
      "loss: 8.294426918029785 at epoch 197\n",
      "loss: 8.16723918914795 at epoch 197\n",
      "loss: 9.135577201843262 at epoch 197\n",
      "loss: 8.455650329589844 at epoch 197\n",
      "loss: 7.646949291229248 at epoch 198\n",
      "loss: 7.375697135925293 at epoch 198\n",
      "loss: 7.666207790374756 at epoch 198\n",
      "loss: 7.932652473449707 at epoch 198\n",
      "loss: 7.271658897399902 at epoch 198\n",
      "loss: 9.208091735839844 at epoch 198\n",
      "loss: 7.821521282196045 at epoch 198\n",
      "loss: 8.80629825592041 at epoch 198\n",
      "loss: 8.728198051452637 at epoch 199\n",
      "loss: 7.542020797729492 at epoch 199\n",
      "loss: 7.079913139343262 at epoch 199\n",
      "loss: 8.812180519104004 at epoch 199\n",
      "loss: 7.096111297607422 at epoch 199\n",
      "loss: 8.182833671569824 at epoch 199\n",
      "loss: 7.17999792098999 at epoch 199\n",
      "loss: 7.833439350128174 at epoch 199\n",
      "faculty_vectors: [[0.09500923 0.00487923 0.18147199 0.11610997 0.01592281 0.14233466\n",
      "  0.1295956  0.12687137 0.16180247 0.02600268]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.09500923 0.00487923 0.18147199 0.11610997 0.01592281 0.14233466\n",
      "  0.1295956  0.12687137 0.16180247 0.02600268]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.09500923 0.00487923 0.18147199 0.11610997 0.01592281 0.14233466\n",
      "  0.1295956  0.12687137 0.16180247 0.02600268]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.09500923 0.00487923 0.18147199 0.11610997 0.01592281 0.14233466\n",
      "  0.1295956  0.12687137 0.16180247 0.02600268]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.09500923 0.00487923 0.18147199 0.11610997 0.01592281 0.14233466\n",
      "  0.1295956  0.12687137 0.16180247 0.02600268]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.09500923 0.00487923 0.18147199 0.11610997 0.01592281 0.14233466\n",
      "  0.1295956  0.12687137 0.16180247 0.02600268]\n",
      " [0.09500923 0.00487923 0.18147199 0.11610997 0.01592281 0.14233466\n",
      "  0.1295956  0.12687137 0.16180247 0.02600268]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.09500923 0.00487923 0.18147199 0.11610997 0.01592281 0.14233466\n",
      "  0.1295956  0.12687137 0.16180247 0.02600268]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.09500923 0.00487923 0.18147199 0.11610997 0.01592281 0.14233466\n",
      "  0.1295956  0.12687137 0.16180247 0.02600268]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.09500923 0.00487923 0.18147199 0.11610997 0.01592281 0.14233466\n",
      "  0.1295956  0.12687137 0.16180247 0.02600268]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.09500923 0.00487923 0.18147199 0.11610997 0.01592281 0.14233466\n",
      "  0.1295956  0.12687137 0.16180247 0.02600268]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.12492317 0.01551887 0.10069949 0.19701844 0.04589312 0.12386088\n",
      "  0.16544505 0.13733659 0.07572964 0.01357477]\n",
      " [0.09500923 0.00487923 0.18147199 0.11610997 0.01592281 0.14233466\n",
      "  0.1295956  0.12687137 0.16180247 0.02600268]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.09500923 0.00487923 0.18147199 0.11610997 0.01592281 0.14233466\n",
      "  0.1295956  0.12687137 0.16180247 0.02600268]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.09500923 0.00487923 0.18147199 0.11610997 0.01592281 0.14233466\n",
      "  0.1295956  0.12687137 0.16180247 0.02600268]\n",
      " [0.14275804 0.0409986  0.1709147  0.10477968 0.09018954 0.07758378\n",
      "  0.01505044 0.15977153 0.11905473 0.07889897]\n",
      " [0.22253402 0.11939366 0.01154676 0.01036126 0.13165714 0.07443267\n",
      "  0.19734432 0.10345783 0.00689458 0.12237776]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]\n",
      " [0.20631951 0.13576789 0.06337426 0.05668758 0.00289986 0.0515502\n",
      "  0.20877754 0.06269554 0.18248942 0.02943819]]\n",
      "student_features: [[ 42.13033083  82.89643453  87.52661073  40.          84.54827819\n",
      "   69.42505763  90.16001916  56.9942101   74.83171803  58.60531747]\n",
      " [ 70.62195116  65.40128787  61.82716082  98.65301101  51.78343704\n",
      "   84.26912807  61.01745448  42.22628901  69.58757936  40.        ]\n",
      " [100.          89.43515494  40.          55.11145468 100.\n",
      "   52.49597572  40.          40.          68.34560882  74.98564048]\n",
      " [ 53.71561465  76.19037698  65.78173429  88.33794098  50.87161367\n",
      "   45.01653781 100.          91.90769813  40.         100.        ]\n",
      " [ 40.         100.          57.22087458  89.08378006  82.57792669\n",
      "   49.56370387  90.96240476  74.56355454  46.10671286  40.        ]\n",
      " [ 53.3905453   63.38840427  40.          51.12182391  63.16269236\n",
      "   40.          47.28476859  44.7175185   61.77274004  40.        ]\n",
      " [ 97.03756401  89.66657701  68.13755085  70.80345887  80.69286775\n",
      "   40.          90.36792938  69.95292919  85.30780387  85.73819378]\n",
      " [ 68.10733089 100.          63.40477158  60.09105754  76.05311844\n",
      "   45.77416035  65.72802711  62.98361816  88.53743522  87.43590015]\n",
      " [ 40.88477056  74.16591306  97.58398516  46.10240823  92.48016051\n",
      "   92.23903577  42.02961266 100.          83.70367334  80.52034879]\n",
      " [ 52.97466068  40.          46.95120765  69.46730729  75.06690644\n",
      "   94.42969399  59.00012547  95.97618961  53.93464625  76.01728357]\n",
      " [ 48.67397043  74.23014278  52.55344482  40.          68.10554675\n",
      "   48.93581612  43.22090733  40.07906576  84.28531969  95.61400531]\n",
      " [ 70.04055191  63.50367522  62.98987239  68.44040271  59.62594312\n",
      "   74.88135895  96.53507232  64.1941181   46.96849255  40.        ]\n",
      " [ 63.52742823  49.37193858  79.33434579  40.          61.23396365\n",
      "   66.86536973  59.92741043 100.         100.          90.01924321]\n",
      " [ 50.45919604 100.          89.56209793  80.9912358  100.\n",
      "   40.          90.35230794  83.82136517  94.48310216  94.05059652]\n",
      " [ 53.88782279  40.          40.          49.9745131   92.83090546\n",
      "   51.62922632  91.37619206  79.38069576  72.45023202  50.92724311]\n",
      " [100.          87.04171751  85.87308739  40.          59.0103825\n",
      "   50.41393414  79.08448397  40.          97.98587707  43.53585499]\n",
      " [ 81.32349425  47.57238362  85.03032825  40.          94.38768336\n",
      "  100.         100.          46.53023855  94.54936422  61.06024671]\n",
      " [ 75.35010635  84.00077698  85.30340527  76.86245744 100.\n",
      "   52.07458961  40.          90.01100434  84.91576551  65.96517729]\n",
      " [ 46.39606805 100.          49.73240365  40.          61.95026634\n",
      "   88.90056704  86.10143321  91.31703392  83.34214518 100.        ]\n",
      " [ 46.90636795  59.88080519  90.31068645  97.01564676 100.\n",
      "   93.83763276  86.51755993  80.29841345  84.37689484 100.        ]\n",
      " [ 40.         100.          78.99243414  92.44606209  89.71884457\n",
      "   40.          88.00626043  92.64432989  65.06112504  90.06683372]\n",
      " [ 72.80895656  91.78602865 100.          66.36736606 100.\n",
      "   55.35550696  45.77482071  87.61450702 100.         100.        ]\n",
      " [ 67.64979238  64.31087059  72.69875843  56.0409192   68.2674826\n",
      "   71.45174293  80.52082895  91.54029732  75.09255105  54.82884931]\n",
      " [100.          64.51759176  48.96171857  40.          40.\n",
      "   40.          99.93148226  96.23410616  94.10854307  56.94416098]\n",
      " [100.          99.68086064  66.09373887  68.83722088  50.55561159\n",
      "   70.40841203  45.58833923  40.          81.96205105  40.        ]\n",
      " [ 77.14797287  79.42728129  72.10334524  40.          62.52846865\n",
      "   84.27262369  40.          90.14002976  40.          99.13933669]\n",
      " [ 45.17949121  84.20526281  63.15893129  63.98751065  95.78915764\n",
      "   55.34575472  57.48979688  69.73520907 100.          91.82017924]\n",
      " [ 74.88913754  55.78700947  40.          53.9688239   86.31975763\n",
      "   86.90856785  57.80110394  55.59153326  95.0287363   84.10991466]\n",
      " [ 92.81404296  40.45211286  69.83750679  49.00064287 100.\n",
      "   75.39705338  70.04468788  65.47163448  56.25865545  40.        ]\n",
      " [ 61.25229447  55.67619305  40.         100.          40.\n",
      "   48.07966011  87.72251587  45.13939917  47.49446767  40.        ]\n",
      " [100.         100.          94.49832792  54.06669597 100.\n",
      "   79.74377637  66.79310436  85.42071559 100.          49.95889055]\n",
      " [ 50.90133252  45.39539829  54.4383368   71.7255118   89.07663399\n",
      "   77.99723485  60.27612769  40.         100.          40.        ]\n",
      " [ 43.32238544  58.55756597  67.05640916 100.         100.\n",
      "   55.68534261  49.67928303  79.36478675 100.          78.96749061]\n",
      " [ 65.82698081  57.77998555  85.6967365  100.          76.81237162\n",
      "   42.69648145  68.36524589  40.          50.12658991  40.        ]\n",
      " [ 79.57234051  50.50986318  65.88153483  40.          40.\n",
      "   77.9486201   69.84784731  71.83059436  40.         100.        ]\n",
      " [ 98.10340305  40.          88.89841792  55.44888235  40.\n",
      "   75.0432546   63.20374437  54.72623175  62.33485468  75.54978543]\n",
      " [100.          84.72189071  51.82214486  70.52523879  80.41742269\n",
      "   40.          94.24267157  91.63739089  85.79263365  52.74698001]\n",
      " [ 51.26547073  59.68917153  97.03433991  56.44750503  60.40867055\n",
      "   67.95215283  87.46919524  83.36331583  83.36988417  40.        ]\n",
      " [ 61.11208038  40.          71.03585366  40.          44.22756941\n",
      "   60.03221515  40.          86.85261907  52.20963599  74.65161232]\n",
      " [ 71.3834841   76.84008514  59.72112229  89.26282344  40.\n",
      "   94.93300722  87.26229095  40.         100.          59.55128243]\n",
      " [ 88.08740981  40.          40.          85.59977724  40.\n",
      "   41.99778488  86.58306035  76.33562223  76.31351542  45.35490045]\n",
      " [ 92.27434313  63.70343016  69.16230103  77.25010154  50.38835686\n",
      "   60.47184948  41.52450801  81.14500659  97.23907293  83.49964567]\n",
      " [ 68.52100687  97.17788778 100.          40.          93.89907923\n",
      "   77.74698     80.9600552  100.          60.54435445  70.03065056]\n",
      " [100.          56.35642821  56.87336589  42.72194798  74.44035824\n",
      "   77.66560676  68.51033887  86.3290165   65.49772336  40.        ]\n",
      " [ 60.13924383  95.56761506  87.72331903  50.42558795  65.7378081\n",
      "   97.20531783 100.          53.62177769  40.          84.32682146]\n",
      " [ 42.76499797 100.          58.6011107  100.          51.38973237\n",
      "   73.06739714  60.11878728  70.1740788   40.          77.56697228]\n",
      " [ 66.10243355  70.69466582  59.92096411  82.74245254  52.71956015\n",
      "   42.04414871  74.77442492  71.38655759  42.09835889  82.13271373]\n",
      " [100.          63.44343869  93.24370001  40.          54.42184092\n",
      "   40.          40.          42.74079149  40.          80.72223824]\n",
      " [ 76.42831346  40.          87.27834753  40.          40.\n",
      "   59.84562936  62.82402282  47.71855165  44.12331429  68.10225216]\n",
      " [ 66.84854294  65.00219898  83.72122427  73.95260097  45.93169567\n",
      "   96.31342419  63.36232594  54.55346138  44.72226134  52.92480417]\n",
      " [ 40.          91.6045052  100.          91.80483619  84.65163948\n",
      "   65.89932277  51.96856577  84.91189604  78.95396665 100.        ]\n",
      " [ 81.73709689 100.          40.          89.6588449   70.5529521\n",
      "   54.17852155  76.67614533  40.          53.88953385  74.54969636]\n",
      " [ 60.57768206  55.19829863  40.          88.75987687 100.\n",
      "   96.10469667  97.571521   100.          83.3734196   40.        ]\n",
      " [ 40.          77.47102154  40.         100.          67.39804737\n",
      "   73.94325928  40.          82.11509074  79.36037821  94.27908493]\n",
      " [ 51.1451013   71.5318584   83.97464487  40.          87.60932135\n",
      "   63.90663153  54.3944639  100.          91.08096929  92.10623614]\n",
      " [ 40.          79.73854713  50.2999347   79.26568046  59.02637448\n",
      "   82.96229421  72.46587449 100.         100.          99.69384982]\n",
      " [ 90.83586529 100.          78.44625681  88.18532556  85.79791714\n",
      "   58.77758771  75.61649678  74.57110146  40.          86.46031615]\n",
      " [ 72.00266372  45.00579166  53.7218993   51.14584333  53.89379944\n",
      "   77.02264442  88.2452028   40.          66.70423564  77.47573859]\n",
      " [ 69.59618101  50.48829755 100.          87.12957694  77.6257371\n",
      "   52.67830726 100.          88.8784415   40.          97.05429224]\n",
      " [ 99.32787682  72.83453744  56.39294481  56.26226406  81.0593397\n",
      "   42.88665308  72.24468055  82.8880326   56.92663078  55.50479404]\n",
      " [ 73.31117454  70.5599988   40.          91.41380853  68.47399033\n",
      "   40.          40.04813566  47.51854226  75.43682799  62.19610114]\n",
      " [ 40.          74.72933058  72.4494571  100.          40.\n",
      "   40.          91.88026541  71.26279098  40.         100.        ]\n",
      " [ 40.          90.42691878  94.85319651  51.28429597  82.32755168\n",
      "   60.6497563   54.23986124  68.0039696   84.3403313   64.75223567]\n",
      " [ 61.46185044  98.13481972  40.          69.29060071 100.\n",
      "   89.53022275  73.51345122  52.31484635  83.48308501  40.        ]\n",
      " [ 51.96146782  40.          41.39118396  68.12290947  40.\n",
      "   72.85189308  40.          94.9894676   40.          45.43121826]\n",
      " [ 82.244466    73.84017174  58.2397889   40.         100.\n",
      "   59.66706237  53.08225758  80.36841383  74.46575494  61.70877085]\n",
      " [ 40.53658631  68.07933208  64.08861844  82.72183969  55.18569267\n",
      "   60.82813763  65.00003619  78.35295596  60.9783936   40.        ]\n",
      " [ 59.96596157  59.89239151  41.90642947 100.          60.85161506\n",
      "  100.          52.9115044   46.3818887   78.10752382  86.744444  ]\n",
      " [ 75.85678399  40.41828799  67.91655831  86.1292203   53.69272541\n",
      "   66.80906955 100.          45.3662484   85.3980253   42.98655778]\n",
      " [ 83.97201447  71.05152403  81.49343664  69.74729102  40.\n",
      "  100.          40.          73.02530711  40.         100.        ]\n",
      " [ 58.26782119  84.50395071  84.26793742  74.28478201  72.35310622\n",
      "   58.25541953  50.77444586  76.38340458  58.22790381  74.1271993 ]\n",
      " [ 89.7465395   78.23975075  85.78886529  72.122631    91.43381748\n",
      "   92.69685962 100.          75.03025392  59.30742221  67.06788771]\n",
      " [ 60.27129353  81.81033645  91.03658637  41.31638683  92.07634071\n",
      "   60.87036404  63.23123398 100.          60.62300744 100.        ]\n",
      " [ 71.44358747  99.62546568  42.60909249  74.82126968  49.40433628\n",
      "   52.00887992  56.16427443  40.          76.91766185  62.79075152]\n",
      " [ 95.29912399  54.60408647  80.99780761  69.15478283  48.92472926\n",
      "  100.          64.85791644  46.06001401  97.27027139  68.66783739]\n",
      " [ 40.          87.27964425  55.89378855  53.06730955  64.32109247\n",
      "   89.67593751  40.          82.66492573  40.         100.        ]\n",
      " [ 73.00602036 100.          97.86923237  90.65100312  70.30560874\n",
      "   53.51495757 100.          75.12343425  40.         100.        ]\n",
      " [ 99.29133621  57.6373733   40.          85.48688538  88.26210247\n",
      "   65.24321992 100.          40.          88.79598951  40.        ]\n",
      " [ 83.40572143  65.39174538  84.87090731  69.83433221  82.79954498\n",
      "   71.57027489  89.30555281  70.29984407 100.         100.        ]\n",
      " [ 97.31939724  67.74759824  65.2949683   84.3062581   65.53236263\n",
      "   40.         100.         100.          40.          60.92903054]\n",
      " [ 59.6935618  100.          45.31454753  69.09795435  65.17072527\n",
      "  100.          40.          40.          47.51720623  83.59163391]\n",
      " [ 81.62156726 100.          68.78915278  80.04417582  96.11806934\n",
      "   82.28036081  65.049631    77.56486945  84.52137767  91.01567602]\n",
      " [ 85.67338178  47.58671398  96.44265279 100.          81.0958744\n",
      "   78.77193383  58.96472608 100.          69.85045187  86.90941152]\n",
      " [ 40.          93.75822506  40.68100805  79.02708976  51.83432452\n",
      "   79.05387477  40.          82.9661785   69.88329887  84.02477453]\n",
      " [ 47.75994791  69.36766656 100.          66.1053786  100.\n",
      "   46.33945818  97.01559305  86.19168045  83.42561595  61.94673707]\n",
      " [ 40.          86.52225485 100.          40.          89.60038784\n",
      "   63.94908703 100.          59.77828449  64.70869807  71.68963527]\n",
      " [100.          67.59664652  89.74997618 100.          40.\n",
      "   93.7936637   86.54346894  93.86930836 100.          87.11553579]\n",
      " [ 97.61770747  58.71964876  54.14839557  67.25283217  55.23817643\n",
      "   87.42369178  82.41954283  88.30395234  87.12141155  40.        ]\n",
      " [ 40.          40.          74.693223    98.01558816  90.17418095\n",
      "   40.          64.70834486  69.18393459  53.78650066  77.59496377]\n",
      " [ 40.          91.63884274  74.78668761  74.91312279  40.\n",
      "   98.28586328  73.45846635  58.18693092  76.02051535  62.9042827 ]\n",
      " [ 96.69336651  66.14751368  62.67217185  51.39576899  44.26080731\n",
      "   64.83222782  72.39492929  67.24266632  79.28634083  77.64682085]\n",
      " [ 67.29276797  68.42260219  86.94813568  84.26759127  43.46586666\n",
      "   79.09536238  80.64637345  53.48184635  52.22912413  40.        ]\n",
      " [100.          46.97100161  58.93051465  59.20079134  61.00496223\n",
      "  100.          82.73212674  94.23128015  74.09129772  78.93544171]\n",
      " [ 69.1569511   40.          40.          57.94941389  40.\n",
      "   40.          98.12657076  40.          54.78209311  40.        ]\n",
      " [ 73.96139525 100.          83.36942288  87.7161889   53.26376125\n",
      "   61.40226551  70.52579664  78.09863574  52.86087045  99.288079  ]\n",
      " [ 40.          69.81089872  96.3684103  100.          61.91527645\n",
      "   93.987424    40.          51.21459176  92.12418976  52.6969322 ]\n",
      " [ 40.          40.          49.49601022  45.25760729  80.23359673\n",
      "   57.47946836  40.          81.07614538  60.98794366 100.        ]\n",
      " [ 72.68022065  89.56812342  61.47628454  80.22566399 100.\n",
      "  100.          73.86650999  58.08616409  55.09346897  40.        ]\n",
      " [ 69.80496378 100.          51.71217739  63.93252909  40.\n",
      "   54.23059402  65.14263729  96.31641675  77.82129499 100.        ]\n",
      " [ 85.13296263  40.          62.21369568  40.          89.34581849\n",
      "   40.          65.3638502   41.11315437 100.          86.77772426]]\n",
      "\n",
      "Results:\n",
      "Mean grade across all applicants: 73.61\n",
      "\n",
      "Percentage of students accepted to desired faculty: 14.00%\n",
      "\n",
      "Detailed results for first 5 applicants:\n",
      "\n",
      "Applicant 0:\n",
      "Desired faculty: 1.0\n",
      "Assigned faculty: 0\n",
      "Final grade: 68.30\n",
      "\n",
      "Applicant 1:\n",
      "Desired faculty: 1.0\n",
      "Assigned faculty: 3\n",
      "Final grade: 69.68\n",
      "\n",
      "Applicant 2:\n",
      "Desired faculty: 0.0\n",
      "Assigned faculty: 2\n",
      "Final grade: 72.69\n",
      "\n",
      "Applicant 3:\n",
      "Desired faculty: 2.0\n",
      "Assigned faculty: 3\n",
      "Final grade: 70.02\n",
      "\n",
      "Applicant 4:\n",
      "Desired faculty: 4.0\n",
      "Assigned faculty: 3\n",
      "Final grade: 73.92\n"
     ]
    }
   ],
   "source": [
    "run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_desired_faculty_stats(assigned_faculties, desired_faculties):\n",
    "        total_students = len(desired_faculties)\n",
    "        matches = sum(assigned == desired for assigned, desired in zip(assigned_faculties, desired_faculties))\n",
    "        percentage = (matches / total_students) * 100\n",
    "        return matches, percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_iteration_example():\n",
    "    # Create environment\n",
    "    env = UniversityEnvironment()\n",
    "    feature_cols = [f\"feature_{i}\" for i in range(env.n_features)]\n",
    "    \n",
    "    # Iteration -1: Initial University Training\n",
    "    print(\"\\n=== Iteration -1: Initial University Training ===\")\n",
    "    past_df = env.generate_past_applicants(1000)\n",
    "    trained_model = env.train_university_model(past_df)\n",
    "    \n",
    "    # Generate students that will be used in iterations 0\n",
    "    iteration0_applicants_df = env.generate_current_applicants(1000)\n",
    "    original_features = iteration0_applicants_df[feature_cols].values\n",
    "    \n",
    "    # Iteration 0: Pure Assignment\n",
    "    print(\"\\n=== Iteration 0: Pure Assignment ===\")\n",
    "    # Assign faculties using original features\n",
    "    iteration0_faculties = env.assign_applicants_to_faculties(\n",
    "        trained_model,\n",
    "        original_features\n",
    "    )\n",
    "    \n",
    "    # Get real grades for these assignments\n",
    "    iteration0_grades = env.recommend(original_features, iteration0_faculties)\n",
    "    \n",
    "    # Create training data for students from iteration 0\n",
    "    iteration0_df = pd.DataFrame(original_features, columns=feature_cols)\n",
    "    iteration0_df['assigned_faculty'] = iteration0_faculties\n",
    "    iteration0_df['final_grade'] = iteration0_grades\n",
    "    \n",
    "    # Iteration 1: Student Learning\n",
    "    print(\"\\n=== Iteration 1: Student Learning ===\")\n",
    "    iteration1_applicants_df = env.generate_current_applicants(1000)\n",
    "    modified_features = []\n",
    "    \n",
    "    applicant_model = env.train_applicant_model(iteration0_df)\n",
    "    \n",
    "    for idx in range(len(iteration1_applicants_df)):\n",
    "        student_features = iteration1_applicants_df.iloc[idx][feature_cols].values\n",
    "        desired_faculty = iteration1_applicants_df.iloc[idx]['desired_faculty']\n",
    "        \n",
    "        # Now students learn from iteration0 data instead of past_df\n",
    "        _, modified_student_features = env.choose_supplier_for_applicant(\n",
    "            student_features,\n",
    "            desired_faculty,\n",
    "            applicant_model\n",
    "        )\n",
    "        modified_features.append(modified_student_features)\n",
    "    \n",
    "    modified_features = np.array(modified_features)\n",
    "    \n",
    "    # Get final assignments and grades using modified features\n",
    "    final_faculties = env.assign_applicants_to_faculties(\n",
    "        trained_model,\n",
    "        modified_features\n",
    "    )\n",
    "    \n",
    "    # Calculate final grades using original features\n",
    "    final_grades = env.recommend(original_features, final_faculties)\n",
    "    \n",
    "    desired_faculties = iteration1_applicants_df['desired_faculty'].values\n",
    "    \n",
    "    # Calculate stats for both iterations\n",
    "    # iter0_matches, iter0_percentage = calculate_desired_faculty_stats(iteration0_faculties, desired_faculties)\n",
    "    final_matches, final_percentage = calculate_desired_faculty_stats(final_faculties, desired_faculties)\n",
    "\n",
    "    \n",
    "    # # Print comparison of results\n",
    "    # print(\"\\nResults Comparison:\")\n",
    "    # print(\"\\nIteration 0 (No Gaming):\")\n",
    "    # print(f\"Mean grade: {np.mean(iteration0_grades):.2f}\")\n",
    "    # print(f\"Faculty distribution: {np.bincount(iteration0_faculties)}\")\n",
    "    # print(f\"Students who got desired faculty: {iter0_matches} ({iter0_percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nIteration 1 (With Gaming):\")\n",
    "    print(f\"Mean grade: {np.mean(final_grades):.2f}\")\n",
    "    print(f\"Faculty distribution: {np.bincount(final_faculties)}\")\n",
    "    print(f\"Students who got desired faculty: {final_matches} ({final_percentage:.1f}%)\")\n",
    "    # Print detailed results for first 5 applicants\n",
    "    print(\"\\nDetailed results for first 5 applicants:\")\n",
    "    for i in range(5):\n",
    "        desired_faculty = iteration1_applicants_df.iloc[i]['desired_faculty']\n",
    "        print(f\"\\nApplicant {i}:\")\n",
    "        print(f\"Desired faculty: {desired_faculty}\")\n",
    "        print(f\"Iteration 0 faculty: {iteration0_faculties[i]}\")\n",
    "        print(f\"Iteration 0 grade: {iteration0_grades[i]:.2f}\")\n",
    "        print(f\"Final faculty: {final_faculties[i]}\")\n",
    "        print(f\"Final grade: {final_grades[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration -1: Initial University Training ===\n",
      "loss: 4415.513671875 at epoch 0\n",
      "loss: 2951.92236328125 at epoch 0\n",
      "loss: 1845.8995361328125 at epoch 0\n",
      "loss: 961.4121704101562 at epoch 0\n",
      "loss: 361.37567138671875 at epoch 0\n",
      "loss: 125.92678833007812 at epoch 0\n",
      "loss: 336.9335632324219 at epoch 0\n",
      "loss: 655.2412719726562 at epoch 0\n",
      "loss: 732.822265625 at epoch 1\n",
      "loss: 646.0856323242188 at epoch 1\n",
      "loss: 421.03765869140625 at epoch 1\n",
      "loss: 173.06597900390625 at epoch 1\n",
      "loss: 117.80741882324219 at epoch 1\n",
      "loss: 84.39306640625 at epoch 1\n",
      "loss: 136.38082885742188 at epoch 1\n",
      "loss: 193.75717163085938 at epoch 1\n",
      "loss: 255.619384765625 at epoch 2\n",
      "loss: 253.7826385498047 at epoch 2\n",
      "loss: 254.40704345703125 at epoch 2\n",
      "loss: 229.77069091796875 at epoch 2\n",
      "loss: 167.79165649414062 at epoch 2\n",
      "loss: 112.71031188964844 at epoch 2\n",
      "loss: 69.82736206054688 at epoch 2\n",
      "loss: 44.206748962402344 at epoch 2\n",
      "loss: 37.600955963134766 at epoch 3\n",
      "loss: 54.909915924072266 at epoch 3\n",
      "loss: 81.50520324707031 at epoch 3\n",
      "loss: 98.37754821777344 at epoch 3\n",
      "loss: 110.24971008300781 at epoch 3\n",
      "loss: 108.92909240722656 at epoch 3\n",
      "loss: 90.61883544921875 at epoch 3\n",
      "loss: 55.20938491821289 at epoch 3\n",
      "loss: 39.79424285888672 at epoch 4\n",
      "loss: 26.94550323486328 at epoch 4\n",
      "loss: 32.61757278442383 at epoch 4\n",
      "loss: 40.952640533447266 at epoch 4\n",
      "loss: 61.54380798339844 at epoch 4\n",
      "loss: 59.90861511230469 at epoch 4\n",
      "loss: 59.225250244140625 at epoch 4\n",
      "loss: 49.813377380371094 at epoch 4\n",
      "loss: 52.64738845825195 at epoch 5\n",
      "loss: 37.397117614746094 at epoch 5\n",
      "loss: 37.22673797607422 at epoch 5\n",
      "loss: 26.65561294555664 at epoch 5\n",
      "loss: 23.794971466064453 at epoch 5\n",
      "loss: 31.450664520263672 at epoch 5\n",
      "loss: 28.175796508789062 at epoch 5\n",
      "loss: 38.95681381225586 at epoch 5\n",
      "loss: 37.8720588684082 at epoch 6\n",
      "loss: 29.709333419799805 at epoch 6\n",
      "loss: 36.81840515136719 at epoch 6\n",
      "loss: 24.981307983398438 at epoch 6\n",
      "loss: 27.598548889160156 at epoch 6\n",
      "loss: 24.057933807373047 at epoch 6\n",
      "loss: 20.345264434814453 at epoch 6\n",
      "loss: 22.595947265625 at epoch 6\n",
      "loss: 24.637495040893555 at epoch 7\n",
      "loss: 25.05693817138672 at epoch 7\n",
      "loss: 30.546939849853516 at epoch 7\n",
      "loss: 25.430908203125 at epoch 7\n",
      "loss: 22.712007522583008 at epoch 7\n",
      "loss: 27.319046020507812 at epoch 7\n",
      "loss: 26.248554229736328 at epoch 7\n",
      "loss: 25.39808464050293 at epoch 7\n",
      "loss: 24.074460983276367 at epoch 8\n",
      "loss: 21.80138397216797 at epoch 8\n",
      "loss: 25.469648361206055 at epoch 8\n",
      "loss: 22.818452835083008 at epoch 8\n",
      "loss: 17.632709503173828 at epoch 8\n",
      "loss: 18.43686294555664 at epoch 8\n",
      "loss: 22.361909866333008 at epoch 8\n",
      "loss: 25.04609489440918 at epoch 8\n",
      "loss: 23.27602767944336 at epoch 9\n",
      "loss: 21.97247314453125 at epoch 9\n",
      "loss: 20.855541229248047 at epoch 9\n",
      "loss: 20.603601455688477 at epoch 9\n",
      "loss: 19.166269302368164 at epoch 9\n",
      "loss: 15.624002456665039 at epoch 9\n",
      "loss: 18.8499698638916 at epoch 9\n",
      "loss: 19.729755401611328 at epoch 9\n",
      "loss: 15.58750057220459 at epoch 10\n",
      "loss: 19.685813903808594 at epoch 10\n",
      "loss: 21.26018524169922 at epoch 10\n",
      "loss: 19.697736740112305 at epoch 10\n",
      "loss: 18.55299949645996 at epoch 10\n",
      "loss: 16.08501625061035 at epoch 10\n",
      "loss: 19.47929573059082 at epoch 10\n",
      "loss: 17.206369400024414 at epoch 10\n",
      "loss: 20.258010864257812 at epoch 11\n",
      "loss: 13.926673889160156 at epoch 11\n",
      "loss: 17.45372200012207 at epoch 11\n",
      "loss: 13.749014854431152 at epoch 11\n",
      "loss: 16.98725128173828 at epoch 11\n",
      "loss: 17.36271858215332 at epoch 11\n",
      "loss: 14.272147178649902 at epoch 11\n",
      "loss: 21.14339828491211 at epoch 11\n",
      "loss: 17.72957420349121 at epoch 12\n",
      "loss: 15.124628067016602 at epoch 12\n",
      "loss: 15.45565414428711 at epoch 12\n",
      "loss: 14.328800201416016 at epoch 12\n",
      "loss: 16.720523834228516 at epoch 12\n",
      "loss: 14.266759872436523 at epoch 12\n",
      "loss: 15.660383224487305 at epoch 12\n",
      "loss: 14.910757064819336 at epoch 12\n",
      "loss: 16.48992919921875 at epoch 13\n",
      "loss: 14.899402618408203 at epoch 13\n",
      "loss: 12.5783052444458 at epoch 13\n",
      "loss: 15.541162490844727 at epoch 13\n",
      "loss: 14.207961082458496 at epoch 13\n",
      "loss: 16.995702743530273 at epoch 13\n",
      "loss: 15.893446922302246 at epoch 13\n",
      "loss: 11.415740966796875 at epoch 13\n",
      "loss: 12.271025657653809 at epoch 14\n",
      "loss: 14.103760719299316 at epoch 14\n",
      "loss: 14.427742004394531 at epoch 14\n",
      "loss: 14.919981956481934 at epoch 14\n",
      "loss: 13.35551643371582 at epoch 14\n",
      "loss: 12.827095031738281 at epoch 14\n",
      "loss: 15.93350601196289 at epoch 14\n",
      "loss: 14.234419822692871 at epoch 14\n",
      "loss: 11.387758255004883 at epoch 15\n",
      "loss: 10.536996841430664 at epoch 15\n",
      "loss: 14.270407676696777 at epoch 15\n",
      "loss: 14.275634765625 at epoch 15\n",
      "loss: 13.675312042236328 at epoch 15\n",
      "loss: 13.444331169128418 at epoch 15\n",
      "loss: 14.983430862426758 at epoch 15\n",
      "loss: 12.809980392456055 at epoch 15\n",
      "loss: 12.773096084594727 at epoch 16\n",
      "loss: 12.33639144897461 at epoch 16\n",
      "loss: 11.573038101196289 at epoch 16\n",
      "loss: 12.626175880432129 at epoch 16\n",
      "loss: 15.10189437866211 at epoch 16\n",
      "loss: 11.935233116149902 at epoch 16\n",
      "loss: 10.718375205993652 at epoch 16\n",
      "loss: 13.734905242919922 at epoch 16\n",
      "loss: 11.876028060913086 at epoch 17\n",
      "loss: 10.062581062316895 at epoch 17\n",
      "loss: 13.65990161895752 at epoch 17\n",
      "loss: 11.134661674499512 at epoch 17\n",
      "loss: 11.305882453918457 at epoch 17\n",
      "loss: 10.98682689666748 at epoch 17\n",
      "loss: 11.408849716186523 at epoch 17\n",
      "loss: 15.554380416870117 at epoch 17\n",
      "loss: 13.836183547973633 at epoch 18\n",
      "loss: 10.257862091064453 at epoch 18\n",
      "loss: 11.715757369995117 at epoch 18\n",
      "loss: 9.342818260192871 at epoch 18\n",
      "loss: 11.377975463867188 at epoch 18\n",
      "loss: 12.04137134552002 at epoch 18\n",
      "loss: 12.785737037658691 at epoch 18\n",
      "loss: 10.977829933166504 at epoch 18\n",
      "loss: 10.030961036682129 at epoch 19\n",
      "loss: 10.561272621154785 at epoch 19\n",
      "loss: 10.465517044067383 at epoch 19\n",
      "loss: 12.314268112182617 at epoch 19\n",
      "loss: 9.933279037475586 at epoch 19\n",
      "loss: 11.688101768493652 at epoch 19\n",
      "loss: 13.051985740661621 at epoch 19\n",
      "loss: 11.623101234436035 at epoch 19\n",
      "loss: 10.679879188537598 at epoch 20\n",
      "loss: 11.49774169921875 at epoch 20\n",
      "loss: 10.190948486328125 at epoch 20\n",
      "loss: 12.085450172424316 at epoch 20\n",
      "loss: 11.432934761047363 at epoch 20\n",
      "loss: 10.423409461975098 at epoch 20\n",
      "loss: 11.824718475341797 at epoch 20\n",
      "loss: 10.163284301757812 at epoch 20\n",
      "loss: 10.322344779968262 at epoch 21\n",
      "loss: 11.992558479309082 at epoch 21\n",
      "loss: 11.03369426727295 at epoch 21\n",
      "loss: 11.425329208374023 at epoch 21\n",
      "loss: 9.555521965026855 at epoch 21\n",
      "loss: 9.601716995239258 at epoch 21\n",
      "loss: 11.390547752380371 at epoch 21\n",
      "loss: 10.187122344970703 at epoch 21\n",
      "loss: 13.174070358276367 at epoch 22\n",
      "loss: 11.362567901611328 at epoch 22\n",
      "loss: 10.24635124206543 at epoch 22\n",
      "loss: 7.5698323249816895 at epoch 22\n",
      "loss: 10.24306869506836 at epoch 22\n",
      "loss: 8.222613334655762 at epoch 22\n",
      "loss: 11.294914245605469 at epoch 22\n",
      "loss: 10.89608383178711 at epoch 22\n",
      "loss: 10.43563175201416 at epoch 23\n",
      "loss: 10.61558723449707 at epoch 23\n",
      "loss: 7.6865739822387695 at epoch 23\n",
      "loss: 9.253626823425293 at epoch 23\n",
      "loss: 8.979055404663086 at epoch 23\n",
      "loss: 12.254043579101562 at epoch 23\n",
      "loss: 11.822843551635742 at epoch 23\n",
      "loss: 9.799023628234863 at epoch 23\n",
      "loss: 10.325825691223145 at epoch 24\n",
      "loss: 10.711024284362793 at epoch 24\n",
      "loss: 10.106817245483398 at epoch 24\n",
      "loss: 9.879058837890625 at epoch 24\n",
      "loss: 9.876398086547852 at epoch 24\n",
      "loss: 10.316179275512695 at epoch 24\n",
      "loss: 8.396052360534668 at epoch 24\n",
      "loss: 10.760926246643066 at epoch 24\n",
      "loss: 10.424687385559082 at epoch 25\n",
      "loss: 9.35722827911377 at epoch 25\n",
      "loss: 10.739286422729492 at epoch 25\n",
      "loss: 10.30445384979248 at epoch 25\n",
      "loss: 10.239359855651855 at epoch 25\n",
      "loss: 9.787464141845703 at epoch 25\n",
      "loss: 7.991798400878906 at epoch 25\n",
      "loss: 8.175992965698242 at epoch 25\n",
      "loss: 9.207683563232422 at epoch 26\n",
      "loss: 11.197317123413086 at epoch 26\n",
      "loss: 10.09286880493164 at epoch 26\n",
      "loss: 8.81116771697998 at epoch 26\n",
      "loss: 8.94583511352539 at epoch 26\n",
      "loss: 8.627635955810547 at epoch 26\n",
      "loss: 9.512779235839844 at epoch 26\n",
      "loss: 10.709014892578125 at epoch 26\n",
      "loss: 8.722505569458008 at epoch 27\n",
      "loss: 8.875133514404297 at epoch 27\n",
      "loss: 9.720184326171875 at epoch 27\n",
      "loss: 9.873433113098145 at epoch 27\n",
      "loss: 10.9410982131958 at epoch 27\n",
      "loss: 10.551740646362305 at epoch 27\n",
      "loss: 9.37945556640625 at epoch 27\n",
      "loss: 8.215370178222656 at epoch 27\n",
      "loss: 10.401012420654297 at epoch 28\n",
      "loss: 8.295461654663086 at epoch 28\n",
      "loss: 9.542752265930176 at epoch 28\n",
      "loss: 10.452825546264648 at epoch 28\n",
      "loss: 9.090080261230469 at epoch 28\n",
      "loss: 9.249946594238281 at epoch 28\n",
      "loss: 11.142715454101562 at epoch 28\n",
      "loss: 10.275313377380371 at epoch 28\n",
      "loss: 8.603306770324707 at epoch 29\n",
      "loss: 9.016133308410645 at epoch 29\n",
      "loss: 9.974933624267578 at epoch 29\n",
      "loss: 10.01181411743164 at epoch 29\n",
      "loss: 9.822586059570312 at epoch 29\n",
      "loss: 9.251640319824219 at epoch 29\n",
      "loss: 8.022278785705566 at epoch 29\n",
      "loss: 10.863298416137695 at epoch 29\n",
      "loss: 9.285256385803223 at epoch 30\n",
      "loss: 10.179399490356445 at epoch 30\n",
      "loss: 8.691692352294922 at epoch 30\n",
      "loss: 8.084404945373535 at epoch 30\n",
      "loss: 10.130058288574219 at epoch 30\n",
      "loss: 9.545758247375488 at epoch 30\n",
      "loss: 8.693603515625 at epoch 30\n",
      "loss: 10.282150268554688 at epoch 30\n",
      "loss: 9.560766220092773 at epoch 31\n",
      "loss: 9.2503023147583 at epoch 31\n",
      "loss: 10.310770034790039 at epoch 31\n",
      "loss: 8.748591423034668 at epoch 31\n",
      "loss: 8.894420623779297 at epoch 31\n",
      "loss: 8.3516845703125 at epoch 31\n",
      "loss: 9.577569961547852 at epoch 31\n",
      "loss: 11.499924659729004 at epoch 31\n",
      "loss: 8.169522285461426 at epoch 32\n",
      "loss: 9.054938316345215 at epoch 32\n",
      "loss: 9.254648208618164 at epoch 32\n",
      "loss: 9.818861961364746 at epoch 32\n",
      "loss: 9.593904495239258 at epoch 32\n",
      "loss: 9.623858451843262 at epoch 32\n",
      "loss: 8.501880645751953 at epoch 32\n",
      "loss: 9.387507438659668 at epoch 32\n",
      "loss: 9.518267631530762 at epoch 33\n",
      "loss: 8.46473217010498 at epoch 33\n",
      "loss: 8.137212753295898 at epoch 33\n",
      "loss: 9.240606307983398 at epoch 33\n",
      "loss: 8.987228393554688 at epoch 33\n",
      "loss: 8.534404754638672 at epoch 33\n",
      "loss: 9.079834938049316 at epoch 33\n",
      "loss: 9.240926742553711 at epoch 33\n",
      "loss: 9.872313499450684 at epoch 34\n",
      "loss: 9.915759086608887 at epoch 34\n",
      "loss: 8.750389099121094 at epoch 34\n",
      "loss: 7.481539726257324 at epoch 34\n",
      "loss: 9.667158126831055 at epoch 34\n",
      "loss: 8.832568168640137 at epoch 34\n",
      "loss: 9.893126487731934 at epoch 34\n",
      "loss: 8.46634292602539 at epoch 34\n",
      "loss: 8.474392890930176 at epoch 35\n",
      "loss: 8.20331859588623 at epoch 35\n",
      "loss: 10.02530574798584 at epoch 35\n",
      "loss: 8.78430461883545 at epoch 35\n",
      "loss: 8.98639965057373 at epoch 35\n",
      "loss: 9.53676986694336 at epoch 35\n",
      "loss: 9.234541893005371 at epoch 35\n",
      "loss: 8.42928695678711 at epoch 35\n",
      "loss: 8.364297866821289 at epoch 36\n",
      "loss: 11.03745174407959 at epoch 36\n",
      "loss: 10.60940933227539 at epoch 36\n",
      "loss: 7.101473331451416 at epoch 36\n",
      "loss: 8.214517593383789 at epoch 36\n",
      "loss: 10.229534149169922 at epoch 36\n",
      "loss: 9.048565864562988 at epoch 36\n",
      "loss: 8.295915603637695 at epoch 36\n",
      "loss: 8.285791397094727 at epoch 37\n",
      "loss: 10.530912399291992 at epoch 37\n",
      "loss: 7.795282363891602 at epoch 37\n",
      "loss: 7.071564674377441 at epoch 37\n",
      "loss: 10.392565727233887 at epoch 37\n",
      "loss: 10.407550811767578 at epoch 37\n",
      "loss: 9.275771141052246 at epoch 37\n",
      "loss: 9.007801055908203 at epoch 37\n",
      "loss: 7.796260833740234 at epoch 38\n",
      "loss: 8.90111255645752 at epoch 38\n",
      "loss: 8.513567924499512 at epoch 38\n",
      "loss: 11.23470401763916 at epoch 38\n",
      "loss: 9.738024711608887 at epoch 38\n",
      "loss: 8.244962692260742 at epoch 38\n",
      "loss: 9.278144836425781 at epoch 38\n",
      "loss: 8.952549934387207 at epoch 38\n",
      "loss: 9.550299644470215 at epoch 39\n",
      "loss: 8.572311401367188 at epoch 39\n",
      "loss: 10.766672134399414 at epoch 39\n",
      "loss: 8.967610359191895 at epoch 39\n",
      "loss: 9.891671180725098 at epoch 39\n",
      "loss: 9.766749382019043 at epoch 39\n",
      "loss: 8.901662826538086 at epoch 39\n",
      "loss: 7.458402633666992 at epoch 39\n",
      "loss: 7.6350250244140625 at epoch 40\n",
      "loss: 7.578764915466309 at epoch 40\n",
      "loss: 9.269258499145508 at epoch 40\n",
      "loss: 8.798025131225586 at epoch 40\n",
      "loss: 8.45071029663086 at epoch 40\n",
      "loss: 9.44194221496582 at epoch 40\n",
      "loss: 8.303726196289062 at epoch 40\n",
      "loss: 10.900680541992188 at epoch 40\n",
      "loss: 8.340655326843262 at epoch 41\n",
      "loss: 8.415034294128418 at epoch 41\n",
      "loss: 9.422340393066406 at epoch 41\n",
      "loss: 8.885847091674805 at epoch 41\n",
      "loss: 8.329642295837402 at epoch 41\n",
      "loss: 9.228143692016602 at epoch 41\n",
      "loss: 8.517450332641602 at epoch 41\n",
      "loss: 9.717461585998535 at epoch 41\n",
      "loss: 8.827932357788086 at epoch 42\n",
      "loss: 8.37305736541748 at epoch 42\n",
      "loss: 9.201820373535156 at epoch 42\n",
      "loss: 9.601218223571777 at epoch 42\n",
      "loss: 8.462798118591309 at epoch 42\n",
      "loss: 8.014972686767578 at epoch 42\n",
      "loss: 7.779473304748535 at epoch 42\n",
      "loss: 8.735111236572266 at epoch 42\n",
      "loss: 8.864102363586426 at epoch 43\n",
      "loss: 8.346019744873047 at epoch 43\n",
      "loss: 8.235713005065918 at epoch 43\n",
      "loss: 8.515629768371582 at epoch 43\n",
      "loss: 8.931180953979492 at epoch 43\n",
      "loss: 7.588129043579102 at epoch 43\n",
      "loss: 9.421506881713867 at epoch 43\n",
      "loss: 8.328551292419434 at epoch 43\n",
      "loss: 9.032176971435547 at epoch 44\n",
      "loss: 8.122045516967773 at epoch 44\n",
      "loss: 9.526153564453125 at epoch 44\n",
      "loss: 9.401971817016602 at epoch 44\n",
      "loss: 9.307758331298828 at epoch 44\n",
      "loss: 6.69523286819458 at epoch 44\n",
      "loss: 8.518223762512207 at epoch 44\n",
      "loss: 8.874670028686523 at epoch 44\n",
      "loss: 8.881299018859863 at epoch 45\n",
      "loss: 8.328075408935547 at epoch 45\n",
      "loss: 8.505256652832031 at epoch 45\n",
      "loss: 7.764874458312988 at epoch 45\n",
      "loss: 8.754179000854492 at epoch 45\n",
      "loss: 7.9045867919921875 at epoch 45\n",
      "loss: 8.469916343688965 at epoch 45\n",
      "loss: 9.594430923461914 at epoch 45\n",
      "loss: 9.146925926208496 at epoch 46\n",
      "loss: 7.558026313781738 at epoch 46\n",
      "loss: 7.692381858825684 at epoch 46\n",
      "loss: 8.923320770263672 at epoch 46\n",
      "loss: 9.531316757202148 at epoch 46\n",
      "loss: 7.708909511566162 at epoch 46\n",
      "loss: 9.922028541564941 at epoch 46\n",
      "loss: 7.3882975578308105 at epoch 46\n",
      "loss: 8.405620574951172 at epoch 47\n",
      "loss: 8.30907917022705 at epoch 47\n",
      "loss: 9.008158683776855 at epoch 47\n",
      "loss: 8.29544448852539 at epoch 47\n",
      "loss: 8.965767860412598 at epoch 47\n",
      "loss: 7.819867134094238 at epoch 47\n",
      "loss: 7.017887592315674 at epoch 47\n",
      "loss: 9.416894912719727 at epoch 47\n",
      "loss: 8.752689361572266 at epoch 48\n",
      "loss: 7.841693878173828 at epoch 48\n",
      "loss: 8.474486351013184 at epoch 48\n",
      "loss: 8.145129203796387 at epoch 48\n",
      "loss: 8.903188705444336 at epoch 48\n",
      "loss: 9.04930305480957 at epoch 48\n",
      "loss: 8.004308700561523 at epoch 48\n",
      "loss: 7.438979148864746 at epoch 48\n",
      "loss: 8.997798919677734 at epoch 49\n",
      "loss: 8.248405456542969 at epoch 49\n",
      "loss: 7.103701591491699 at epoch 49\n",
      "loss: 7.406985759735107 at epoch 49\n",
      "loss: 8.299734115600586 at epoch 49\n",
      "loss: 9.97259521484375 at epoch 49\n",
      "loss: 9.265047073364258 at epoch 49\n",
      "loss: 8.17189884185791 at epoch 49\n",
      "loss: 8.64956283569336 at epoch 50\n",
      "loss: 8.876014709472656 at epoch 50\n",
      "loss: 8.561294555664062 at epoch 50\n",
      "loss: 8.104225158691406 at epoch 50\n",
      "loss: 7.11337423324585 at epoch 50\n",
      "loss: 8.521282196044922 at epoch 50\n",
      "loss: 9.150747299194336 at epoch 50\n",
      "loss: 8.978590965270996 at epoch 50\n",
      "loss: 8.304075241088867 at epoch 51\n",
      "loss: 8.086743354797363 at epoch 51\n",
      "loss: 9.008131980895996 at epoch 51\n",
      "loss: 8.77171516418457 at epoch 51\n",
      "loss: 7.446541786193848 at epoch 51\n",
      "loss: 8.501127243041992 at epoch 51\n",
      "loss: 7.684704780578613 at epoch 51\n",
      "loss: 10.15890884399414 at epoch 51\n",
      "loss: 7.536321640014648 at epoch 52\n",
      "loss: 9.082317352294922 at epoch 52\n",
      "loss: 7.698543548583984 at epoch 52\n",
      "loss: 8.433107376098633 at epoch 52\n",
      "loss: 9.26427173614502 at epoch 52\n",
      "loss: 8.295605659484863 at epoch 52\n",
      "loss: 8.208903312683105 at epoch 52\n",
      "loss: 8.080509185791016 at epoch 52\n",
      "loss: 7.5614752769470215 at epoch 53\n",
      "loss: 8.237833023071289 at epoch 53\n",
      "loss: 8.159432411193848 at epoch 53\n",
      "loss: 8.280699729919434 at epoch 53\n",
      "loss: 8.248919486999512 at epoch 53\n",
      "loss: 9.234539031982422 at epoch 53\n",
      "loss: 9.451923370361328 at epoch 53\n",
      "loss: 8.948213577270508 at epoch 53\n",
      "loss: 8.923486709594727 at epoch 54\n",
      "loss: 9.314427375793457 at epoch 54\n",
      "loss: 8.63801383972168 at epoch 54\n",
      "loss: 8.96204948425293 at epoch 54\n",
      "loss: 7.59397029876709 at epoch 54\n",
      "loss: 7.5223188400268555 at epoch 54\n",
      "loss: 7.007397651672363 at epoch 54\n",
      "loss: 9.148345947265625 at epoch 54\n",
      "loss: 7.529925346374512 at epoch 55\n",
      "loss: 7.718255996704102 at epoch 55\n",
      "loss: 9.539901733398438 at epoch 55\n",
      "loss: 7.788780212402344 at epoch 55\n",
      "loss: 9.060011863708496 at epoch 55\n",
      "loss: 9.28017520904541 at epoch 55\n",
      "loss: 9.278234481811523 at epoch 55\n",
      "loss: 10.766636848449707 at epoch 55\n",
      "loss: 8.569114685058594 at epoch 56\n",
      "loss: 8.615818977355957 at epoch 56\n",
      "loss: 8.559164047241211 at epoch 56\n",
      "loss: 8.991133689880371 at epoch 56\n",
      "loss: 8.341519355773926 at epoch 56\n",
      "loss: 7.948202133178711 at epoch 56\n",
      "loss: 8.607162475585938 at epoch 56\n",
      "loss: 8.776750564575195 at epoch 56\n",
      "loss: 9.517501831054688 at epoch 57\n",
      "loss: 7.805208683013916 at epoch 57\n",
      "loss: 8.196290016174316 at epoch 57\n",
      "loss: 9.17006778717041 at epoch 57\n",
      "loss: 8.989838600158691 at epoch 57\n",
      "loss: 8.87628173828125 at epoch 57\n",
      "loss: 8.673693656921387 at epoch 57\n",
      "loss: 7.842002868652344 at epoch 57\n",
      "loss: 8.74026870727539 at epoch 58\n",
      "loss: 8.361598014831543 at epoch 58\n",
      "loss: 8.53593635559082 at epoch 58\n",
      "loss: 8.27083683013916 at epoch 58\n",
      "loss: 9.907228469848633 at epoch 58\n",
      "loss: 8.394968032836914 at epoch 58\n",
      "loss: 8.150004386901855 at epoch 58\n",
      "loss: 8.539249420166016 at epoch 58\n",
      "loss: 10.299873352050781 at epoch 59\n",
      "loss: 9.381103515625 at epoch 59\n",
      "loss: 6.453821182250977 at epoch 59\n",
      "loss: 8.608530044555664 at epoch 59\n",
      "loss: 9.492045402526855 at epoch 59\n",
      "loss: 8.097271919250488 at epoch 59\n",
      "loss: 8.01123046875 at epoch 59\n",
      "loss: 8.67884635925293 at epoch 59\n",
      "loss: 9.005033493041992 at epoch 60\n",
      "loss: 8.175254821777344 at epoch 60\n",
      "loss: 9.550448417663574 at epoch 60\n",
      "loss: 8.135967254638672 at epoch 60\n",
      "loss: 10.510490417480469 at epoch 60\n",
      "loss: 7.127347946166992 at epoch 60\n",
      "loss: 8.6975736618042 at epoch 60\n",
      "loss: 7.7752251625061035 at epoch 60\n",
      "loss: 10.399871826171875 at epoch 61\n",
      "loss: 7.5197930335998535 at epoch 61\n",
      "loss: 10.357431411743164 at epoch 61\n",
      "loss: 7.932192325592041 at epoch 61\n",
      "loss: 7.663261890411377 at epoch 61\n",
      "loss: 8.098548889160156 at epoch 61\n",
      "loss: 8.235198020935059 at epoch 61\n",
      "loss: 10.283722877502441 at epoch 61\n",
      "loss: 8.411014556884766 at epoch 62\n",
      "loss: 8.254902839660645 at epoch 62\n",
      "loss: 8.119536399841309 at epoch 62\n",
      "loss: 8.579850196838379 at epoch 62\n",
      "loss: 8.358537673950195 at epoch 62\n",
      "loss: 8.170120239257812 at epoch 62\n",
      "loss: 8.165217399597168 at epoch 62\n",
      "loss: 9.87373161315918 at epoch 62\n",
      "loss: 8.645756721496582 at epoch 63\n",
      "loss: 8.322992324829102 at epoch 63\n",
      "loss: 8.084848403930664 at epoch 63\n",
      "loss: 8.024845123291016 at epoch 63\n",
      "loss: 9.480666160583496 at epoch 63\n",
      "loss: 9.036041259765625 at epoch 63\n",
      "loss: 7.373000621795654 at epoch 63\n",
      "loss: 7.97746467590332 at epoch 63\n",
      "loss: 7.760859489440918 at epoch 64\n",
      "loss: 7.9019389152526855 at epoch 64\n",
      "loss: 7.387693405151367 at epoch 64\n",
      "loss: 8.001243591308594 at epoch 64\n",
      "loss: 8.4004487991333 at epoch 64\n",
      "loss: 9.798908233642578 at epoch 64\n",
      "loss: 7.987720966339111 at epoch 64\n",
      "loss: 10.639032363891602 at epoch 64\n",
      "loss: 7.585699081420898 at epoch 65\n",
      "loss: 8.552940368652344 at epoch 65\n",
      "loss: 7.6675310134887695 at epoch 65\n",
      "loss: 8.89612102508545 at epoch 65\n",
      "loss: 8.549883842468262 at epoch 65\n",
      "loss: 9.277938842773438 at epoch 65\n",
      "loss: 7.246160984039307 at epoch 65\n",
      "loss: 9.147531509399414 at epoch 65\n",
      "loss: 7.530714988708496 at epoch 66\n",
      "loss: 7.975217819213867 at epoch 66\n",
      "loss: 8.546760559082031 at epoch 66\n",
      "loss: 8.166692733764648 at epoch 66\n",
      "loss: 8.197816848754883 at epoch 66\n",
      "loss: 8.337355613708496 at epoch 66\n",
      "loss: 8.978264808654785 at epoch 66\n",
      "loss: 8.073563575744629 at epoch 66\n",
      "loss: 8.677109718322754 at epoch 67\n",
      "loss: 7.7810163497924805 at epoch 67\n",
      "loss: 8.642980575561523 at epoch 67\n",
      "loss: 8.115551948547363 at epoch 67\n",
      "loss: 7.896212100982666 at epoch 67\n",
      "loss: 8.618154525756836 at epoch 67\n",
      "loss: 7.668402194976807 at epoch 67\n",
      "loss: 8.023799896240234 at epoch 67\n",
      "loss: 8.203238487243652 at epoch 68\n",
      "loss: 8.969060897827148 at epoch 68\n",
      "loss: 9.251279830932617 at epoch 68\n",
      "loss: 8.239323616027832 at epoch 68\n",
      "loss: 8.189108848571777 at epoch 68\n",
      "loss: 7.606225967407227 at epoch 68\n",
      "loss: 8.370752334594727 at epoch 68\n",
      "loss: 7.219111442565918 at epoch 68\n",
      "loss: 8.123422622680664 at epoch 69\n",
      "loss: 8.277697563171387 at epoch 69\n",
      "loss: 9.678852081298828 at epoch 69\n",
      "loss: 9.156731605529785 at epoch 69\n",
      "loss: 8.61696720123291 at epoch 69\n",
      "loss: 8.32844066619873 at epoch 69\n",
      "loss: 7.7561750411987305 at epoch 69\n",
      "loss: 7.860378742218018 at epoch 69\n",
      "loss: 9.087443351745605 at epoch 70\n",
      "loss: 8.313005447387695 at epoch 70\n",
      "loss: 8.855504035949707 at epoch 70\n",
      "loss: 9.082626342773438 at epoch 70\n",
      "loss: 7.793731212615967 at epoch 70\n",
      "loss: 8.179646492004395 at epoch 70\n",
      "loss: 8.064669609069824 at epoch 70\n",
      "loss: 7.429690361022949 at epoch 70\n",
      "loss: 6.938133239746094 at epoch 71\n",
      "loss: 9.69246768951416 at epoch 71\n",
      "loss: 9.41238784790039 at epoch 71\n",
      "loss: 7.604748725891113 at epoch 71\n",
      "loss: 8.403334617614746 at epoch 71\n",
      "loss: 8.075687408447266 at epoch 71\n",
      "loss: 7.315554618835449 at epoch 71\n",
      "loss: 9.454955101013184 at epoch 71\n",
      "loss: 9.716440200805664 at epoch 72\n",
      "loss: 9.921140670776367 at epoch 72\n",
      "loss: 8.42095947265625 at epoch 72\n",
      "loss: 9.086795806884766 at epoch 72\n",
      "loss: 10.385858535766602 at epoch 72\n",
      "loss: 7.7179975509643555 at epoch 72\n",
      "loss: 6.75050687789917 at epoch 72\n",
      "loss: 8.483367919921875 at epoch 72\n",
      "loss: 9.08551025390625 at epoch 73\n",
      "loss: 11.138689994812012 at epoch 73\n",
      "loss: 7.077566623687744 at epoch 73\n",
      "loss: 8.071557998657227 at epoch 73\n",
      "loss: 8.577095031738281 at epoch 73\n",
      "loss: 8.189886093139648 at epoch 73\n",
      "loss: 9.218320846557617 at epoch 73\n",
      "loss: 8.309189796447754 at epoch 73\n",
      "loss: 7.879562854766846 at epoch 74\n",
      "loss: 9.35267448425293 at epoch 74\n",
      "loss: 9.337757110595703 at epoch 74\n",
      "loss: 7.876338005065918 at epoch 74\n",
      "loss: 9.556257247924805 at epoch 74\n",
      "loss: 8.154962539672852 at epoch 74\n",
      "loss: 6.995185852050781 at epoch 74\n",
      "loss: 7.67801570892334 at epoch 74\n",
      "loss: 8.517854690551758 at epoch 75\n",
      "loss: 7.5905303955078125 at epoch 75\n",
      "loss: 8.343034744262695 at epoch 75\n",
      "loss: 8.92570972442627 at epoch 75\n",
      "loss: 8.152015686035156 at epoch 75\n",
      "loss: 7.881496429443359 at epoch 75\n",
      "loss: 8.40148639678955 at epoch 75\n",
      "loss: 7.230226993560791 at epoch 75\n",
      "loss: 7.871223449707031 at epoch 76\n",
      "loss: 7.932387351989746 at epoch 76\n",
      "loss: 8.2898588180542 at epoch 76\n",
      "loss: 8.609280586242676 at epoch 76\n",
      "loss: 9.328042984008789 at epoch 76\n",
      "loss: 9.52008056640625 at epoch 76\n",
      "loss: 7.2732930183410645 at epoch 76\n",
      "loss: 8.18037223815918 at epoch 76\n",
      "loss: 9.21509075164795 at epoch 77\n",
      "loss: 8.169608116149902 at epoch 77\n",
      "loss: 8.943609237670898 at epoch 77\n",
      "loss: 9.067770004272461 at epoch 77\n",
      "loss: 8.730634689331055 at epoch 77\n",
      "loss: 5.9798808097839355 at epoch 77\n",
      "loss: 8.272443771362305 at epoch 77\n",
      "loss: 8.195006370544434 at epoch 77\n",
      "loss: 8.606893539428711 at epoch 78\n",
      "loss: 7.58099889755249 at epoch 78\n",
      "loss: 9.019542694091797 at epoch 78\n",
      "loss: 10.255138397216797 at epoch 78\n",
      "loss: 8.41728687286377 at epoch 78\n",
      "loss: 8.67727279663086 at epoch 78\n",
      "loss: 9.85411262512207 at epoch 78\n",
      "loss: 7.936861038208008 at epoch 78\n",
      "loss: 8.662336349487305 at epoch 79\n",
      "loss: 7.033520698547363 at epoch 79\n",
      "loss: 8.624394416809082 at epoch 79\n",
      "loss: 10.94924545288086 at epoch 79\n",
      "loss: 8.486557960510254 at epoch 79\n",
      "loss: 8.099430084228516 at epoch 79\n",
      "loss: 9.358589172363281 at epoch 79\n",
      "loss: 8.607484817504883 at epoch 79\n",
      "loss: 8.806070327758789 at epoch 80\n",
      "loss: 7.884393215179443 at epoch 80\n",
      "loss: 7.3084797859191895 at epoch 80\n",
      "loss: 8.832221984863281 at epoch 80\n",
      "loss: 8.382648468017578 at epoch 80\n",
      "loss: 8.371770858764648 at epoch 80\n",
      "loss: 8.416325569152832 at epoch 80\n",
      "loss: 9.900224685668945 at epoch 80\n",
      "loss: 6.676647663116455 at epoch 81\n",
      "loss: 10.008747100830078 at epoch 81\n",
      "loss: 7.476823806762695 at epoch 81\n",
      "loss: 9.153890609741211 at epoch 81\n",
      "loss: 9.748392105102539 at epoch 81\n",
      "loss: 6.93015718460083 at epoch 81\n",
      "loss: 7.650444507598877 at epoch 81\n",
      "loss: 9.136615753173828 at epoch 81\n",
      "loss: 10.117587089538574 at epoch 82\n",
      "loss: 7.884176731109619 at epoch 82\n",
      "loss: 7.577300548553467 at epoch 82\n",
      "loss: 8.002815246582031 at epoch 82\n",
      "loss: 8.880599975585938 at epoch 82\n",
      "loss: 8.315995216369629 at epoch 82\n",
      "loss: 9.162092208862305 at epoch 82\n",
      "loss: 7.529287338256836 at epoch 82\n",
      "loss: 7.104068279266357 at epoch 83\n",
      "loss: 9.194737434387207 at epoch 83\n",
      "loss: 8.553474426269531 at epoch 83\n",
      "loss: 9.044666290283203 at epoch 83\n",
      "loss: 9.394235610961914 at epoch 83\n",
      "loss: 8.694089889526367 at epoch 83\n",
      "loss: 8.141251564025879 at epoch 83\n",
      "loss: 8.596504211425781 at epoch 83\n",
      "loss: 8.745681762695312 at epoch 84\n",
      "loss: 9.126716613769531 at epoch 84\n",
      "loss: 7.307160377502441 at epoch 84\n",
      "loss: 9.930298805236816 at epoch 84\n",
      "loss: 8.996210098266602 at epoch 84\n",
      "loss: 8.567936897277832 at epoch 84\n",
      "loss: 8.655336380004883 at epoch 84\n",
      "loss: 9.96261978149414 at epoch 84\n",
      "loss: 6.756298065185547 at epoch 85\n",
      "loss: 8.296996116638184 at epoch 85\n",
      "loss: 8.189179420471191 at epoch 85\n",
      "loss: 8.235503196716309 at epoch 85\n",
      "loss: 9.106073379516602 at epoch 85\n",
      "loss: 8.565563201904297 at epoch 85\n",
      "loss: 9.310625076293945 at epoch 85\n",
      "loss: 9.27401351928711 at epoch 85\n",
      "loss: 7.539663791656494 at epoch 86\n",
      "loss: 8.760009765625 at epoch 86\n",
      "loss: 7.691908836364746 at epoch 86\n",
      "loss: 8.781356811523438 at epoch 86\n",
      "loss: 9.006752014160156 at epoch 86\n",
      "loss: 8.401496887207031 at epoch 86\n",
      "loss: 8.78312873840332 at epoch 86\n",
      "loss: 7.503222942352295 at epoch 86\n",
      "loss: 8.260175704956055 at epoch 87\n",
      "loss: 9.026936531066895 at epoch 87\n",
      "loss: 8.702609062194824 at epoch 87\n",
      "loss: 7.962343215942383 at epoch 87\n",
      "loss: 8.215462684631348 at epoch 87\n",
      "loss: 8.3823823928833 at epoch 87\n",
      "loss: 8.291658401489258 at epoch 87\n",
      "loss: 8.085275650024414 at epoch 87\n",
      "loss: 8.198511123657227 at epoch 88\n",
      "loss: 6.91335916519165 at epoch 88\n",
      "loss: 7.429288387298584 at epoch 88\n",
      "loss: 8.165401458740234 at epoch 88\n",
      "loss: 7.968112468719482 at epoch 88\n",
      "loss: 9.725830078125 at epoch 88\n",
      "loss: 8.662322998046875 at epoch 88\n",
      "loss: 8.281390190124512 at epoch 88\n",
      "loss: 8.009572982788086 at epoch 89\n",
      "loss: 7.504437446594238 at epoch 89\n",
      "loss: 8.498801231384277 at epoch 89\n",
      "loss: 9.505366325378418 at epoch 89\n",
      "loss: 8.365485191345215 at epoch 89\n",
      "loss: 9.394304275512695 at epoch 89\n",
      "loss: 9.445222854614258 at epoch 89\n",
      "loss: 9.244908332824707 at epoch 89\n",
      "loss: 7.2968339920043945 at epoch 90\n",
      "loss: 9.240864753723145 at epoch 90\n",
      "loss: 9.105864524841309 at epoch 90\n",
      "loss: 8.26317310333252 at epoch 90\n",
      "loss: 8.2412748336792 at epoch 90\n",
      "loss: 8.729022979736328 at epoch 90\n",
      "loss: 7.832948684692383 at epoch 90\n",
      "loss: 9.32663345336914 at epoch 90\n",
      "loss: 8.322669982910156 at epoch 91\n",
      "loss: 8.702849388122559 at epoch 91\n",
      "loss: 8.720442771911621 at epoch 91\n",
      "loss: 7.3097734451293945 at epoch 91\n",
      "loss: 9.1051607131958 at epoch 91\n",
      "loss: 8.339418411254883 at epoch 91\n",
      "loss: 8.893499374389648 at epoch 91\n",
      "loss: 7.930254936218262 at epoch 91\n",
      "loss: 8.742592811584473 at epoch 92\n",
      "loss: 8.139021873474121 at epoch 92\n",
      "loss: 8.330435752868652 at epoch 92\n",
      "loss: 7.964137077331543 at epoch 92\n",
      "loss: 8.709532737731934 at epoch 92\n",
      "loss: 8.311811447143555 at epoch 92\n",
      "loss: 8.389363288879395 at epoch 92\n",
      "loss: 7.783408164978027 at epoch 92\n",
      "loss: 8.04941463470459 at epoch 93\n",
      "loss: 9.007835388183594 at epoch 93\n",
      "loss: 7.98610782623291 at epoch 93\n",
      "loss: 8.732522010803223 at epoch 93\n",
      "loss: 9.283561706542969 at epoch 93\n",
      "loss: 8.964138984680176 at epoch 93\n",
      "loss: 8.135076522827148 at epoch 93\n",
      "loss: 7.408647537231445 at epoch 93\n",
      "loss: 9.960855484008789 at epoch 94\n",
      "loss: 8.194831848144531 at epoch 94\n",
      "loss: 8.358118057250977 at epoch 94\n",
      "loss: 8.567276954650879 at epoch 94\n",
      "loss: 8.664787292480469 at epoch 94\n",
      "loss: 8.567458152770996 at epoch 94\n",
      "loss: 7.469148635864258 at epoch 94\n",
      "loss: 9.399552345275879 at epoch 94\n",
      "loss: 8.855082511901855 at epoch 95\n",
      "loss: 8.405069351196289 at epoch 95\n",
      "loss: 7.185810565948486 at epoch 95\n",
      "loss: 8.842705726623535 at epoch 95\n",
      "loss: 7.373684406280518 at epoch 95\n",
      "loss: 8.940733909606934 at epoch 95\n",
      "loss: 9.772778511047363 at epoch 95\n",
      "loss: 8.401803016662598 at epoch 95\n",
      "loss: 9.340137481689453 at epoch 96\n",
      "loss: 10.098550796508789 at epoch 96\n",
      "loss: 8.7941255569458 at epoch 96\n",
      "loss: 9.164477348327637 at epoch 96\n",
      "loss: 7.201204776763916 at epoch 96\n",
      "loss: 8.662710189819336 at epoch 96\n",
      "loss: 7.6167802810668945 at epoch 96\n",
      "loss: 7.49542760848999 at epoch 96\n",
      "loss: 8.626739501953125 at epoch 97\n",
      "loss: 8.972492218017578 at epoch 97\n",
      "loss: 8.508769035339355 at epoch 97\n",
      "loss: 7.6748433113098145 at epoch 97\n",
      "loss: 7.881892681121826 at epoch 97\n",
      "loss: 8.879631042480469 at epoch 97\n",
      "loss: 9.84201431274414 at epoch 97\n",
      "loss: 8.154824256896973 at epoch 97\n",
      "loss: 8.215706825256348 at epoch 98\n",
      "loss: 10.062439918518066 at epoch 98\n",
      "loss: 8.815081596374512 at epoch 98\n",
      "loss: 8.385702133178711 at epoch 98\n",
      "loss: 8.370442390441895 at epoch 98\n",
      "loss: 10.241372108459473 at epoch 98\n",
      "loss: 8.421513557434082 at epoch 98\n",
      "loss: 8.702348709106445 at epoch 98\n",
      "loss: 8.461577415466309 at epoch 99\n",
      "loss: 8.24168586730957 at epoch 99\n",
      "loss: 8.986231803894043 at epoch 99\n",
      "loss: 9.30222225189209 at epoch 99\n",
      "loss: 7.978850841522217 at epoch 99\n",
      "loss: 8.23257064819336 at epoch 99\n",
      "loss: 8.758417129516602 at epoch 99\n",
      "loss: 8.08079719543457 at epoch 99\n",
      "loss: 8.540270805358887 at epoch 100\n",
      "loss: 7.868047714233398 at epoch 100\n",
      "loss: 8.69429874420166 at epoch 100\n",
      "loss: 7.972254753112793 at epoch 100\n",
      "loss: 9.450055122375488 at epoch 100\n",
      "loss: 9.286962509155273 at epoch 100\n",
      "loss: 8.230060577392578 at epoch 100\n",
      "loss: 7.345142841339111 at epoch 100\n",
      "loss: 7.796619415283203 at epoch 101\n",
      "loss: 9.713776588439941 at epoch 101\n",
      "loss: 8.214152336120605 at epoch 101\n",
      "loss: 8.785470008850098 at epoch 101\n",
      "loss: 8.143414497375488 at epoch 101\n",
      "loss: 10.227276802062988 at epoch 101\n",
      "loss: 7.691952705383301 at epoch 101\n",
      "loss: 7.86614990234375 at epoch 101\n",
      "loss: 9.30169677734375 at epoch 102\n",
      "loss: 8.713000297546387 at epoch 102\n",
      "loss: 8.792884826660156 at epoch 102\n",
      "loss: 8.59756088256836 at epoch 102\n",
      "loss: 7.931303024291992 at epoch 102\n",
      "loss: 8.270233154296875 at epoch 102\n",
      "loss: 7.918951034545898 at epoch 102\n",
      "loss: 9.050989151000977 at epoch 102\n",
      "loss: 8.492695808410645 at epoch 103\n",
      "loss: 7.677532196044922 at epoch 103\n",
      "loss: 8.374375343322754 at epoch 103\n",
      "loss: 7.854196071624756 at epoch 103\n",
      "loss: 8.127251625061035 at epoch 103\n",
      "loss: 9.577731132507324 at epoch 103\n",
      "loss: 8.990893363952637 at epoch 103\n",
      "loss: 7.8800177574157715 at epoch 103\n",
      "loss: 7.358221530914307 at epoch 104\n",
      "loss: 8.711443901062012 at epoch 104\n",
      "loss: 7.51033878326416 at epoch 104\n",
      "loss: 9.912307739257812 at epoch 104\n",
      "loss: 7.4572248458862305 at epoch 104\n",
      "loss: 9.007977485656738 at epoch 104\n",
      "loss: 6.889832019805908 at epoch 104\n",
      "loss: 7.831665992736816 at epoch 104\n",
      "loss: 7.191345691680908 at epoch 105\n",
      "loss: 7.375377655029297 at epoch 105\n",
      "loss: 8.42990779876709 at epoch 105\n",
      "loss: 9.570591926574707 at epoch 105\n",
      "loss: 9.469213485717773 at epoch 105\n",
      "loss: 7.6469926834106445 at epoch 105\n",
      "loss: 9.277473449707031 at epoch 105\n",
      "loss: 8.0829496383667 at epoch 105\n",
      "loss: 9.592931747436523 at epoch 106\n",
      "loss: 9.686177253723145 at epoch 106\n",
      "loss: 8.588680267333984 at epoch 106\n",
      "loss: 8.977547645568848 at epoch 106\n",
      "loss: 8.48143482208252 at epoch 106\n",
      "loss: 8.036616325378418 at epoch 106\n",
      "loss: 9.774724960327148 at epoch 106\n",
      "loss: 9.948678016662598 at epoch 106\n",
      "loss: 8.614737510681152 at epoch 107\n",
      "loss: 8.739927291870117 at epoch 107\n",
      "loss: 8.914514541625977 at epoch 107\n",
      "loss: 9.414600372314453 at epoch 107\n",
      "loss: 8.357333183288574 at epoch 107\n",
      "loss: 7.757775783538818 at epoch 107\n",
      "loss: 8.726229667663574 at epoch 107\n",
      "loss: 9.919971466064453 at epoch 107\n",
      "loss: 9.067687034606934 at epoch 108\n",
      "loss: 8.132650375366211 at epoch 108\n",
      "loss: 7.709817886352539 at epoch 108\n",
      "loss: 8.50974178314209 at epoch 108\n",
      "loss: 9.141456604003906 at epoch 108\n",
      "loss: 10.368605613708496 at epoch 108\n",
      "loss: 8.108627319335938 at epoch 108\n",
      "loss: 8.754476547241211 at epoch 108\n",
      "loss: 9.875356674194336 at epoch 109\n",
      "loss: 8.90261459350586 at epoch 109\n",
      "loss: 9.501137733459473 at epoch 109\n",
      "loss: 7.774532794952393 at epoch 109\n",
      "loss: 7.751231670379639 at epoch 109\n",
      "loss: 9.859830856323242 at epoch 109\n",
      "loss: 10.559793472290039 at epoch 109\n",
      "loss: 7.996671199798584 at epoch 109\n",
      "loss: 7.926357746124268 at epoch 110\n",
      "loss: 9.837594985961914 at epoch 110\n",
      "loss: 10.286921501159668 at epoch 110\n",
      "loss: 9.121275901794434 at epoch 110\n",
      "loss: 9.187530517578125 at epoch 110\n",
      "loss: 8.392467498779297 at epoch 110\n",
      "loss: 8.587251663208008 at epoch 110\n",
      "loss: 9.250231742858887 at epoch 110\n",
      "loss: 9.158056259155273 at epoch 111\n",
      "loss: 8.55179214477539 at epoch 111\n",
      "loss: 8.52807331085205 at epoch 111\n",
      "loss: 6.808512210845947 at epoch 111\n",
      "loss: 8.181672096252441 at epoch 111\n",
      "loss: 8.59276294708252 at epoch 111\n",
      "loss: 9.233343124389648 at epoch 111\n",
      "loss: 7.462009906768799 at epoch 111\n",
      "loss: 7.445908546447754 at epoch 112\n",
      "loss: 8.689821243286133 at epoch 112\n",
      "loss: 8.850165367126465 at epoch 112\n",
      "loss: 7.164369106292725 at epoch 112\n",
      "loss: 8.076250076293945 at epoch 112\n",
      "loss: 8.283265113830566 at epoch 112\n",
      "loss: 8.525118827819824 at epoch 112\n",
      "loss: 10.321752548217773 at epoch 112\n",
      "loss: 8.882323265075684 at epoch 113\n",
      "loss: 6.283266067504883 at epoch 113\n",
      "loss: 9.409782409667969 at epoch 113\n",
      "loss: 8.814046859741211 at epoch 113\n",
      "loss: 8.627055168151855 at epoch 113\n",
      "loss: 7.965889930725098 at epoch 113\n",
      "loss: 7.759312629699707 at epoch 113\n",
      "loss: 10.87766170501709 at epoch 113\n",
      "loss: 9.32838249206543 at epoch 114\n",
      "loss: 8.64423942565918 at epoch 114\n",
      "loss: 7.435574531555176 at epoch 114\n",
      "loss: 8.388879776000977 at epoch 114\n",
      "loss: 10.326675415039062 at epoch 114\n",
      "loss: 9.875052452087402 at epoch 114\n",
      "loss: 8.284296035766602 at epoch 114\n",
      "loss: 8.548233985900879 at epoch 114\n",
      "loss: 9.812390327453613 at epoch 115\n",
      "loss: 8.853951454162598 at epoch 115\n",
      "loss: 8.743314743041992 at epoch 115\n",
      "loss: 9.682304382324219 at epoch 115\n",
      "loss: 6.734825611114502 at epoch 115\n",
      "loss: 8.853365898132324 at epoch 115\n",
      "loss: 10.159466743469238 at epoch 115\n",
      "loss: 8.694671630859375 at epoch 115\n",
      "loss: 10.048200607299805 at epoch 116\n",
      "loss: 8.528300285339355 at epoch 116\n",
      "loss: 9.102142333984375 at epoch 116\n",
      "loss: 9.34909439086914 at epoch 116\n",
      "loss: 10.039544105529785 at epoch 116\n",
      "loss: 8.716781616210938 at epoch 116\n",
      "loss: 7.173668384552002 at epoch 116\n",
      "loss: 8.74315071105957 at epoch 116\n",
      "loss: 9.547080993652344 at epoch 117\n",
      "loss: 9.616720199584961 at epoch 117\n",
      "loss: 7.841053009033203 at epoch 117\n",
      "loss: 8.232832908630371 at epoch 117\n",
      "loss: 9.325841903686523 at epoch 117\n",
      "loss: 8.827754974365234 at epoch 117\n",
      "loss: 8.045148849487305 at epoch 117\n",
      "loss: 9.909550666809082 at epoch 117\n",
      "loss: 9.462199211120605 at epoch 118\n",
      "loss: 9.934576034545898 at epoch 118\n",
      "loss: 8.118453979492188 at epoch 118\n",
      "loss: 8.423749923706055 at epoch 118\n",
      "loss: 9.350342750549316 at epoch 118\n",
      "loss: 11.140315055847168 at epoch 118\n",
      "loss: 9.196418762207031 at epoch 118\n",
      "loss: 8.523255348205566 at epoch 118\n",
      "loss: 9.948558807373047 at epoch 119\n",
      "loss: 8.798158645629883 at epoch 119\n",
      "loss: 9.254700660705566 at epoch 119\n",
      "loss: 8.82265567779541 at epoch 119\n",
      "loss: 7.483432292938232 at epoch 119\n",
      "loss: 7.215000629425049 at epoch 119\n",
      "loss: 9.743393898010254 at epoch 119\n",
      "loss: 10.720584869384766 at epoch 119\n",
      "loss: 8.818862915039062 at epoch 120\n",
      "loss: 9.14721965789795 at epoch 120\n",
      "loss: 8.157397270202637 at epoch 120\n",
      "loss: 7.02899694442749 at epoch 120\n",
      "loss: 10.896390914916992 at epoch 120\n",
      "loss: 9.165040969848633 at epoch 120\n",
      "loss: 8.872915267944336 at epoch 120\n",
      "loss: 8.049556732177734 at epoch 120\n",
      "loss: 8.611605644226074 at epoch 121\n",
      "loss: 9.246583938598633 at epoch 121\n",
      "loss: 9.10095500946045 at epoch 121\n",
      "loss: 8.055994987487793 at epoch 121\n",
      "loss: 8.921483039855957 at epoch 121\n",
      "loss: 8.279427528381348 at epoch 121\n",
      "loss: 7.552748203277588 at epoch 121\n",
      "loss: 9.015477180480957 at epoch 121\n",
      "loss: 8.701138496398926 at epoch 122\n",
      "loss: 7.707326412200928 at epoch 122\n",
      "loss: 7.889965057373047 at epoch 122\n",
      "loss: 7.213322639465332 at epoch 122\n",
      "loss: 8.87088680267334 at epoch 122\n",
      "loss: 8.37582778930664 at epoch 122\n",
      "loss: 8.690818786621094 at epoch 122\n",
      "loss: 8.492277145385742 at epoch 122\n",
      "loss: 9.15621280670166 at epoch 123\n",
      "loss: 8.358552932739258 at epoch 123\n",
      "loss: 8.916635513305664 at epoch 123\n",
      "loss: 7.990860939025879 at epoch 123\n",
      "loss: 8.166130065917969 at epoch 123\n",
      "loss: 8.258091926574707 at epoch 123\n",
      "loss: 7.780586242675781 at epoch 123\n",
      "loss: 9.069812774658203 at epoch 123\n",
      "loss: 8.982398986816406 at epoch 124\n",
      "loss: 7.991614818572998 at epoch 124\n",
      "loss: 8.708663940429688 at epoch 124\n",
      "loss: 9.321615219116211 at epoch 124\n",
      "loss: 8.669647216796875 at epoch 124\n",
      "loss: 8.422586441040039 at epoch 124\n",
      "loss: 9.305727005004883 at epoch 124\n",
      "loss: 7.9057207107543945 at epoch 124\n",
      "loss: 7.603027820587158 at epoch 125\n",
      "loss: 10.442848205566406 at epoch 125\n",
      "loss: 7.146778106689453 at epoch 125\n",
      "loss: 7.80978536605835 at epoch 125\n",
      "loss: 8.839950561523438 at epoch 125\n",
      "loss: 9.22336196899414 at epoch 125\n",
      "loss: 7.747699737548828 at epoch 125\n",
      "loss: 8.909923553466797 at epoch 125\n",
      "loss: 9.143954277038574 at epoch 126\n",
      "loss: 8.64509105682373 at epoch 126\n",
      "loss: 9.40010929107666 at epoch 126\n",
      "loss: 7.544614791870117 at epoch 126\n",
      "loss: 8.552507400512695 at epoch 126\n",
      "loss: 8.7381010055542 at epoch 126\n",
      "loss: 7.507408142089844 at epoch 126\n",
      "loss: 7.852847576141357 at epoch 126\n",
      "loss: 8.543024063110352 at epoch 127\n",
      "loss: 8.695089340209961 at epoch 127\n",
      "loss: 8.590537071228027 at epoch 127\n",
      "loss: 7.78600549697876 at epoch 127\n",
      "loss: 7.943661689758301 at epoch 127\n",
      "loss: 9.396888732910156 at epoch 127\n",
      "loss: 8.523273468017578 at epoch 127\n",
      "loss: 8.448277473449707 at epoch 127\n",
      "loss: 8.337197303771973 at epoch 128\n",
      "loss: 8.79170036315918 at epoch 128\n",
      "loss: 8.201424598693848 at epoch 128\n",
      "loss: 9.911277770996094 at epoch 128\n",
      "loss: 8.231344223022461 at epoch 128\n",
      "loss: 7.62852144241333 at epoch 128\n",
      "loss: 6.5509538650512695 at epoch 128\n",
      "loss: 10.149109840393066 at epoch 128\n",
      "loss: 9.54948902130127 at epoch 129\n",
      "loss: 10.986101150512695 at epoch 129\n",
      "loss: 7.910724639892578 at epoch 129\n",
      "loss: 8.522636413574219 at epoch 129\n",
      "loss: 7.687831878662109 at epoch 129\n",
      "loss: 9.334144592285156 at epoch 129\n",
      "loss: 9.550984382629395 at epoch 129\n",
      "loss: 8.171940803527832 at epoch 129\n",
      "loss: 9.818387985229492 at epoch 130\n",
      "loss: 9.722900390625 at epoch 130\n",
      "loss: 10.904420852661133 at epoch 130\n",
      "loss: 11.184281349182129 at epoch 130\n",
      "loss: 7.323192119598389 at epoch 130\n",
      "loss: 8.767515182495117 at epoch 130\n",
      "loss: 10.392485618591309 at epoch 130\n",
      "loss: 11.058773040771484 at epoch 130\n",
      "loss: 8.566161155700684 at epoch 131\n",
      "loss: 10.29741096496582 at epoch 131\n",
      "loss: 9.888280868530273 at epoch 131\n",
      "loss: 9.446024894714355 at epoch 131\n",
      "loss: 11.069226264953613 at epoch 131\n",
      "loss: 9.705391883850098 at epoch 131\n",
      "loss: 8.468973159790039 at epoch 131\n",
      "loss: 9.971906661987305 at epoch 131\n",
      "loss: 9.08105182647705 at epoch 132\n",
      "loss: 10.775416374206543 at epoch 132\n",
      "loss: 11.285877227783203 at epoch 132\n",
      "loss: 7.045004844665527 at epoch 132\n",
      "loss: 8.578245162963867 at epoch 132\n",
      "loss: 13.042839050292969 at epoch 132\n",
      "loss: 8.40919303894043 at epoch 132\n",
      "loss: 10.543432235717773 at epoch 132\n",
      "loss: 8.659883499145508 at epoch 133\n",
      "loss: 8.986862182617188 at epoch 133\n",
      "loss: 9.774771690368652 at epoch 133\n",
      "loss: 9.134278297424316 at epoch 133\n",
      "loss: 8.957189559936523 at epoch 133\n",
      "loss: 7.844841957092285 at epoch 133\n",
      "loss: 8.47951602935791 at epoch 133\n",
      "loss: 11.19460391998291 at epoch 133\n",
      "loss: 7.153853416442871 at epoch 134\n",
      "loss: 8.361467361450195 at epoch 134\n",
      "loss: 8.490461349487305 at epoch 134\n",
      "loss: 9.792112350463867 at epoch 134\n",
      "loss: 10.003286361694336 at epoch 134\n",
      "loss: 8.63376522064209 at epoch 134\n",
      "loss: 10.758063316345215 at epoch 134\n",
      "loss: 9.98398208618164 at epoch 134\n",
      "loss: 8.670137405395508 at epoch 135\n",
      "loss: 8.407204627990723 at epoch 135\n",
      "loss: 8.6295804977417 at epoch 135\n",
      "loss: 9.205597877502441 at epoch 135\n",
      "loss: 9.316242218017578 at epoch 135\n",
      "loss: 10.959087371826172 at epoch 135\n",
      "loss: 9.299606323242188 at epoch 135\n",
      "loss: 9.322821617126465 at epoch 135\n",
      "loss: 9.865893363952637 at epoch 136\n",
      "loss: 9.790706634521484 at epoch 136\n",
      "loss: 8.920312881469727 at epoch 136\n",
      "loss: 8.481231689453125 at epoch 136\n",
      "loss: 8.625839233398438 at epoch 136\n",
      "loss: 8.277424812316895 at epoch 136\n",
      "loss: 8.25385570526123 at epoch 136\n",
      "loss: 7.205173015594482 at epoch 136\n",
      "loss: 7.892207145690918 at epoch 137\n",
      "loss: 8.162463188171387 at epoch 137\n",
      "loss: 8.77664852142334 at epoch 137\n",
      "loss: 8.52653694152832 at epoch 137\n",
      "loss: 8.139376640319824 at epoch 137\n",
      "loss: 8.440694808959961 at epoch 137\n",
      "loss: 8.04318618774414 at epoch 137\n",
      "loss: 8.70920181274414 at epoch 137\n",
      "loss: 8.610763549804688 at epoch 138\n",
      "loss: 7.81745719909668 at epoch 138\n",
      "loss: 9.163867950439453 at epoch 138\n",
      "loss: 8.487935066223145 at epoch 138\n",
      "loss: 9.130480766296387 at epoch 138\n",
      "loss: 9.409236907958984 at epoch 138\n",
      "loss: 9.93901538848877 at epoch 138\n",
      "loss: 9.381110191345215 at epoch 138\n",
      "loss: 8.772221565246582 at epoch 139\n",
      "loss: 9.007179260253906 at epoch 139\n",
      "loss: 8.709270477294922 at epoch 139\n",
      "loss: 9.421486854553223 at epoch 139\n",
      "loss: 9.814035415649414 at epoch 139\n",
      "loss: 9.467192649841309 at epoch 139\n",
      "loss: 8.625255584716797 at epoch 139\n",
      "loss: 8.111385345458984 at epoch 139\n",
      "loss: 9.391634941101074 at epoch 140\n",
      "loss: 7.9949116706848145 at epoch 140\n",
      "loss: 7.836931228637695 at epoch 140\n",
      "loss: 10.001564979553223 at epoch 140\n",
      "loss: 10.478595733642578 at epoch 140\n",
      "loss: 9.025842666625977 at epoch 140\n",
      "loss: 8.624347686767578 at epoch 140\n",
      "loss: 8.977155685424805 at epoch 140\n",
      "loss: 10.463277816772461 at epoch 141\n",
      "loss: 9.448129653930664 at epoch 141\n",
      "loss: 11.682130813598633 at epoch 141\n",
      "loss: 7.827880859375 at epoch 141\n",
      "loss: 7.330831527709961 at epoch 141\n",
      "loss: 9.894786834716797 at epoch 141\n",
      "loss: 8.954143524169922 at epoch 141\n",
      "loss: 8.35844612121582 at epoch 141\n",
      "loss: 8.768756866455078 at epoch 142\n",
      "loss: 8.409345626831055 at epoch 142\n",
      "loss: 9.368034362792969 at epoch 142\n",
      "loss: 8.466282844543457 at epoch 142\n",
      "loss: 8.628951072692871 at epoch 142\n",
      "loss: 9.85118579864502 at epoch 142\n",
      "loss: 8.31232738494873 at epoch 142\n",
      "loss: 8.839420318603516 at epoch 142\n",
      "loss: 8.350699424743652 at epoch 143\n",
      "loss: 9.589188575744629 at epoch 143\n",
      "loss: 8.562128067016602 at epoch 143\n",
      "loss: 10.55749225616455 at epoch 143\n",
      "loss: 8.012258529663086 at epoch 143\n",
      "loss: 7.41854190826416 at epoch 143\n",
      "loss: 10.600691795349121 at epoch 143\n",
      "loss: 8.785711288452148 at epoch 143\n",
      "loss: 8.387300491333008 at epoch 144\n",
      "loss: 8.840631484985352 at epoch 144\n",
      "loss: 7.161004066467285 at epoch 144\n",
      "loss: 7.785293102264404 at epoch 144\n",
      "loss: 10.379422187805176 at epoch 144\n",
      "loss: 7.4161696434021 at epoch 144\n",
      "loss: 8.188047409057617 at epoch 144\n",
      "loss: 11.050948143005371 at epoch 144\n",
      "loss: 7.9050469398498535 at epoch 145\n",
      "loss: 8.66628646850586 at epoch 145\n",
      "loss: 8.321962356567383 at epoch 145\n",
      "loss: 8.42589282989502 at epoch 145\n",
      "loss: 7.467198371887207 at epoch 145\n",
      "loss: 7.492922306060791 at epoch 145\n",
      "loss: 8.024012565612793 at epoch 145\n",
      "loss: 9.419292449951172 at epoch 145\n",
      "loss: 9.121094703674316 at epoch 146\n",
      "loss: 9.434317588806152 at epoch 146\n",
      "loss: 8.863765716552734 at epoch 146\n",
      "loss: 8.676502227783203 at epoch 146\n",
      "loss: 8.33604907989502 at epoch 146\n",
      "loss: 9.635249137878418 at epoch 146\n",
      "loss: 9.835212707519531 at epoch 146\n",
      "loss: 8.362944602966309 at epoch 146\n",
      "loss: 9.529767036437988 at epoch 147\n",
      "loss: 11.667808532714844 at epoch 147\n",
      "loss: 8.895730018615723 at epoch 147\n",
      "loss: 7.613896369934082 at epoch 147\n",
      "loss: 8.013334274291992 at epoch 147\n",
      "loss: 9.963236808776855 at epoch 147\n",
      "loss: 9.178285598754883 at epoch 147\n",
      "loss: 9.365775108337402 at epoch 147\n",
      "loss: 8.637950897216797 at epoch 148\n",
      "loss: 7.987239360809326 at epoch 148\n",
      "loss: 9.41954517364502 at epoch 148\n",
      "loss: 8.570708274841309 at epoch 148\n",
      "loss: 8.5593900680542 at epoch 148\n",
      "loss: 7.634221076965332 at epoch 148\n",
      "loss: 10.11768627166748 at epoch 148\n",
      "loss: 8.842848777770996 at epoch 148\n",
      "loss: 7.826117038726807 at epoch 149\n",
      "loss: 7.947662353515625 at epoch 149\n",
      "loss: 8.811990737915039 at epoch 149\n",
      "loss: 8.47439193725586 at epoch 149\n",
      "loss: 9.127710342407227 at epoch 149\n",
      "loss: 8.84335708618164 at epoch 149\n",
      "loss: 9.146978378295898 at epoch 149\n",
      "loss: 7.879112243652344 at epoch 149\n",
      "loss: 7.701588153839111 at epoch 150\n",
      "loss: 9.759859085083008 at epoch 150\n",
      "loss: 8.241046905517578 at epoch 150\n",
      "loss: 7.586397171020508 at epoch 150\n",
      "loss: 8.119585037231445 at epoch 150\n",
      "loss: 8.65483570098877 at epoch 150\n",
      "loss: 8.96121597290039 at epoch 150\n",
      "loss: 8.540389060974121 at epoch 150\n",
      "loss: 9.113309860229492 at epoch 151\n",
      "loss: 8.972808837890625 at epoch 151\n",
      "loss: 8.658529281616211 at epoch 151\n",
      "loss: 9.097430229187012 at epoch 151\n",
      "loss: 6.896886348724365 at epoch 151\n",
      "loss: 8.545248031616211 at epoch 151\n",
      "loss: 9.641902923583984 at epoch 151\n",
      "loss: 9.381775856018066 at epoch 151\n",
      "loss: 8.157120704650879 at epoch 152\n",
      "loss: 9.3335599899292 at epoch 152\n",
      "loss: 9.13906478881836 at epoch 152\n",
      "loss: 8.269746780395508 at epoch 152\n",
      "loss: 9.636091232299805 at epoch 152\n",
      "loss: 9.778650283813477 at epoch 152\n",
      "loss: 10.007518768310547 at epoch 152\n",
      "loss: 9.279862403869629 at epoch 152\n",
      "loss: 10.568229675292969 at epoch 153\n",
      "loss: 8.446470260620117 at epoch 153\n",
      "loss: 13.023101806640625 at epoch 153\n",
      "loss: 8.410639762878418 at epoch 153\n",
      "loss: 9.246395111083984 at epoch 153\n",
      "loss: 9.491048812866211 at epoch 153\n",
      "loss: 9.187790870666504 at epoch 153\n",
      "loss: 8.832759857177734 at epoch 153\n",
      "loss: 9.983993530273438 at epoch 154\n",
      "loss: 10.0997314453125 at epoch 154\n",
      "loss: 10.496647834777832 at epoch 154\n",
      "loss: 9.83952522277832 at epoch 154\n",
      "loss: 9.573055267333984 at epoch 154\n",
      "loss: 11.499666213989258 at epoch 154\n",
      "loss: 8.744903564453125 at epoch 154\n",
      "loss: 10.094636917114258 at epoch 154\n",
      "loss: 10.91641616821289 at epoch 155\n",
      "loss: 10.473657608032227 at epoch 155\n",
      "loss: 7.948850154876709 at epoch 155\n",
      "loss: 10.218865394592285 at epoch 155\n",
      "loss: 9.99637508392334 at epoch 155\n",
      "loss: 10.699698448181152 at epoch 155\n",
      "loss: 7.576257705688477 at epoch 155\n",
      "loss: 10.477896690368652 at epoch 155\n",
      "loss: 10.296422958374023 at epoch 156\n",
      "loss: 9.880231857299805 at epoch 156\n",
      "loss: 10.798999786376953 at epoch 156\n",
      "loss: 8.128782272338867 at epoch 156\n",
      "loss: 9.773565292358398 at epoch 156\n",
      "loss: 8.998379707336426 at epoch 156\n",
      "loss: 8.114435195922852 at epoch 156\n",
      "loss: 10.323854446411133 at epoch 156\n",
      "loss: 10.979679107666016 at epoch 157\n",
      "loss: 7.878292083740234 at epoch 157\n",
      "loss: 7.911616325378418 at epoch 157\n",
      "loss: 12.790567398071289 at epoch 157\n",
      "loss: 10.71496868133545 at epoch 157\n",
      "loss: 8.242759704589844 at epoch 157\n",
      "loss: 14.003887176513672 at epoch 157\n",
      "loss: 15.652042388916016 at epoch 157\n",
      "loss: 8.91172981262207 at epoch 158\n",
      "loss: 10.377632141113281 at epoch 158\n",
      "loss: 14.799554824829102 at epoch 158\n",
      "loss: 9.45173454284668 at epoch 158\n",
      "loss: 10.038297653198242 at epoch 158\n",
      "loss: 10.535476684570312 at epoch 158\n",
      "loss: 8.790144920349121 at epoch 158\n",
      "loss: 10.366515159606934 at epoch 158\n",
      "loss: 9.976300239562988 at epoch 159\n",
      "loss: 8.933648109436035 at epoch 159\n",
      "loss: 8.613177299499512 at epoch 159\n",
      "loss: 9.785385131835938 at epoch 159\n",
      "loss: 10.449210166931152 at epoch 159\n",
      "loss: 7.886815547943115 at epoch 159\n",
      "loss: 8.457643508911133 at epoch 159\n",
      "loss: 8.74357795715332 at epoch 159\n",
      "loss: 9.40372371673584 at epoch 160\n",
      "loss: 9.036434173583984 at epoch 160\n",
      "loss: 8.63431167602539 at epoch 160\n",
      "loss: 8.522953987121582 at epoch 160\n",
      "loss: 7.589278697967529 at epoch 160\n",
      "loss: 10.679248809814453 at epoch 160\n",
      "loss: 8.21353530883789 at epoch 160\n",
      "loss: 6.846887588500977 at epoch 160\n",
      "loss: 10.321920394897461 at epoch 161\n",
      "loss: 7.818257808685303 at epoch 161\n",
      "loss: 8.777664184570312 at epoch 161\n",
      "loss: 8.51378345489502 at epoch 161\n",
      "loss: 9.894277572631836 at epoch 161\n",
      "loss: 7.970481872558594 at epoch 161\n",
      "loss: 8.242850303649902 at epoch 161\n",
      "loss: 6.778778553009033 at epoch 161\n",
      "loss: 8.958882331848145 at epoch 162\n",
      "loss: 8.9967679977417 at epoch 162\n",
      "loss: 8.70842456817627 at epoch 162\n",
      "loss: 8.22948169708252 at epoch 162\n",
      "loss: 9.828164100646973 at epoch 162\n",
      "loss: 8.246737480163574 at epoch 162\n",
      "loss: 8.347984313964844 at epoch 162\n",
      "loss: 6.802164077758789 at epoch 162\n",
      "loss: 9.40594482421875 at epoch 163\n",
      "loss: 8.426898956298828 at epoch 163\n",
      "loss: 9.437240600585938 at epoch 163\n",
      "loss: 9.710128784179688 at epoch 163\n",
      "loss: 8.978172302246094 at epoch 163\n",
      "loss: 8.41609001159668 at epoch 163\n",
      "loss: 10.452186584472656 at epoch 163\n",
      "loss: 7.462947368621826 at epoch 163\n",
      "loss: 8.401516914367676 at epoch 164\n",
      "loss: 9.089752197265625 at epoch 164\n",
      "loss: 10.153594017028809 at epoch 164\n",
      "loss: 7.9828081130981445 at epoch 164\n",
      "loss: 9.27195930480957 at epoch 164\n",
      "loss: 10.051774024963379 at epoch 164\n",
      "loss: 10.266410827636719 at epoch 164\n",
      "loss: 8.97722053527832 at epoch 164\n",
      "loss: 8.241191864013672 at epoch 165\n",
      "loss: 9.156518936157227 at epoch 165\n",
      "loss: 9.704294204711914 at epoch 165\n",
      "loss: 7.960499286651611 at epoch 165\n",
      "loss: 8.377793312072754 at epoch 165\n",
      "loss: 7.767319679260254 at epoch 165\n",
      "loss: 7.88485860824585 at epoch 165\n",
      "loss: 7.95814323425293 at epoch 165\n",
      "loss: 8.326420783996582 at epoch 166\n",
      "loss: 8.321897506713867 at epoch 166\n",
      "loss: 8.780559539794922 at epoch 166\n",
      "loss: 9.482170104980469 at epoch 166\n",
      "loss: 8.027444839477539 at epoch 166\n",
      "loss: 8.231036186218262 at epoch 166\n",
      "loss: 8.824460983276367 at epoch 166\n",
      "loss: 8.38252067565918 at epoch 166\n",
      "loss: 9.928425788879395 at epoch 167\n",
      "loss: 8.385445594787598 at epoch 167\n",
      "loss: 11.496830940246582 at epoch 167\n",
      "loss: 8.84458065032959 at epoch 167\n",
      "loss: 6.807875156402588 at epoch 167\n",
      "loss: 9.51823616027832 at epoch 167\n",
      "loss: 9.479833602905273 at epoch 167\n",
      "loss: 10.016366958618164 at epoch 167\n",
      "loss: 8.582868576049805 at epoch 168\n",
      "loss: 8.531115531921387 at epoch 168\n",
      "loss: 9.268543243408203 at epoch 168\n",
      "loss: 7.883227348327637 at epoch 168\n",
      "loss: 9.523941040039062 at epoch 168\n",
      "loss: 9.147626876831055 at epoch 168\n",
      "loss: 9.05461597442627 at epoch 168\n",
      "loss: 8.751286506652832 at epoch 168\n",
      "loss: 9.722670555114746 at epoch 169\n",
      "loss: 8.496652603149414 at epoch 169\n",
      "loss: 7.359243392944336 at epoch 169\n",
      "loss: 9.538022994995117 at epoch 169\n",
      "loss: 7.515134811401367 at epoch 169\n",
      "loss: 7.519413471221924 at epoch 169\n",
      "loss: 9.095810890197754 at epoch 169\n",
      "loss: 8.701897621154785 at epoch 169\n",
      "loss: 9.348047256469727 at epoch 170\n",
      "loss: 8.467111587524414 at epoch 170\n",
      "loss: 9.57369613647461 at epoch 170\n",
      "loss: 8.379594802856445 at epoch 170\n",
      "loss: 9.042659759521484 at epoch 170\n",
      "loss: 9.130776405334473 at epoch 170\n",
      "loss: 8.623937606811523 at epoch 170\n",
      "loss: 7.6732964515686035 at epoch 170\n",
      "loss: 8.992809295654297 at epoch 171\n",
      "loss: 8.075007438659668 at epoch 171\n",
      "loss: 8.8580322265625 at epoch 171\n",
      "loss: 8.462352752685547 at epoch 171\n",
      "loss: 8.730300903320312 at epoch 171\n",
      "loss: 9.494026184082031 at epoch 171\n",
      "loss: 9.062087059020996 at epoch 171\n",
      "loss: 7.922337055206299 at epoch 171\n",
      "loss: 8.871642112731934 at epoch 172\n",
      "loss: 7.5947184562683105 at epoch 172\n",
      "loss: 8.79835319519043 at epoch 172\n",
      "loss: 8.341910362243652 at epoch 172\n",
      "loss: 10.491747856140137 at epoch 172\n",
      "loss: 7.671404838562012 at epoch 172\n",
      "loss: 9.764880180358887 at epoch 172\n",
      "loss: 8.767274856567383 at epoch 172\n",
      "loss: 9.252092361450195 at epoch 173\n",
      "loss: 10.248151779174805 at epoch 173\n",
      "loss: 8.178194046020508 at epoch 173\n",
      "loss: 7.933940410614014 at epoch 173\n",
      "loss: 8.888533592224121 at epoch 173\n",
      "loss: 7.825057029724121 at epoch 173\n",
      "loss: 8.730626106262207 at epoch 173\n",
      "loss: 8.122849464416504 at epoch 173\n",
      "loss: 10.250518798828125 at epoch 174\n",
      "loss: 9.410896301269531 at epoch 174\n",
      "loss: 7.161684989929199 at epoch 174\n",
      "loss: 8.161051750183105 at epoch 174\n",
      "loss: 10.040075302124023 at epoch 174\n",
      "loss: 10.255638122558594 at epoch 174\n",
      "loss: 7.672739028930664 at epoch 174\n",
      "loss: 8.126442909240723 at epoch 174\n",
      "loss: 8.502490043640137 at epoch 175\n",
      "loss: 8.630143165588379 at epoch 175\n",
      "loss: 10.393722534179688 at epoch 175\n",
      "loss: 9.300626754760742 at epoch 175\n",
      "loss: 8.899786949157715 at epoch 175\n",
      "loss: 7.4696736335754395 at epoch 175\n",
      "loss: 8.333962440490723 at epoch 175\n",
      "loss: 9.005573272705078 at epoch 175\n",
      "loss: 7.475722789764404 at epoch 176\n",
      "loss: 9.31355094909668 at epoch 176\n",
      "loss: 8.366806030273438 at epoch 176\n",
      "loss: 8.057884216308594 at epoch 176\n",
      "loss: 7.719000816345215 at epoch 176\n",
      "loss: 9.138545036315918 at epoch 176\n",
      "loss: 10.107341766357422 at epoch 176\n",
      "loss: 9.67495059967041 at epoch 176\n",
      "loss: 8.762866973876953 at epoch 177\n",
      "loss: 8.092780113220215 at epoch 177\n",
      "loss: 8.218615531921387 at epoch 177\n",
      "loss: 7.05892276763916 at epoch 177\n",
      "loss: 8.104631423950195 at epoch 177\n",
      "loss: 9.390023231506348 at epoch 177\n",
      "loss: 8.531179428100586 at epoch 177\n",
      "loss: 8.268052101135254 at epoch 177\n",
      "loss: 8.187700271606445 at epoch 178\n",
      "loss: 7.939785480499268 at epoch 178\n",
      "loss: 7.581820011138916 at epoch 178\n",
      "loss: 8.097984313964844 at epoch 178\n",
      "loss: 8.752836227416992 at epoch 178\n",
      "loss: 8.715705871582031 at epoch 178\n",
      "loss: 7.971155643463135 at epoch 178\n",
      "loss: 7.744139671325684 at epoch 178\n",
      "loss: 8.337801933288574 at epoch 179\n",
      "loss: 9.21509838104248 at epoch 179\n",
      "loss: 8.554779052734375 at epoch 179\n",
      "loss: 7.896056652069092 at epoch 179\n",
      "loss: 9.32081413269043 at epoch 179\n",
      "loss: 8.755605697631836 at epoch 179\n",
      "loss: 9.143611907958984 at epoch 179\n",
      "loss: 7.9468488693237305 at epoch 179\n",
      "loss: 8.747187614440918 at epoch 180\n",
      "loss: 9.452611923217773 at epoch 180\n",
      "loss: 7.991745948791504 at epoch 180\n",
      "loss: 8.512619972229004 at epoch 180\n",
      "loss: 7.700092792510986 at epoch 180\n",
      "loss: 8.541295051574707 at epoch 180\n",
      "loss: 8.72795581817627 at epoch 180\n",
      "loss: 9.106345176696777 at epoch 180\n",
      "loss: 8.216604232788086 at epoch 181\n",
      "loss: 8.204804420471191 at epoch 181\n",
      "loss: 9.767667770385742 at epoch 181\n",
      "loss: 8.032720565795898 at epoch 181\n",
      "loss: 8.304057121276855 at epoch 181\n",
      "loss: 11.88877010345459 at epoch 181\n",
      "loss: 8.06963062286377 at epoch 181\n",
      "loss: 7.779268741607666 at epoch 181\n",
      "loss: 9.165639877319336 at epoch 182\n",
      "loss: 8.627769470214844 at epoch 182\n",
      "loss: 8.914443969726562 at epoch 182\n",
      "loss: 11.469595909118652 at epoch 182\n",
      "loss: 10.333653450012207 at epoch 182\n",
      "loss: 7.934140682220459 at epoch 182\n",
      "loss: 11.491308212280273 at epoch 182\n",
      "loss: 8.145485877990723 at epoch 182\n",
      "loss: 9.066682815551758 at epoch 183\n",
      "loss: 8.476463317871094 at epoch 183\n",
      "loss: 10.455958366394043 at epoch 183\n",
      "loss: 8.981700897216797 at epoch 183\n",
      "loss: 8.38382625579834 at epoch 183\n",
      "loss: 8.566605567932129 at epoch 183\n",
      "loss: 9.761456489562988 at epoch 183\n",
      "loss: 7.902987957000732 at epoch 183\n",
      "loss: 10.8349609375 at epoch 184\n",
      "loss: 8.229610443115234 at epoch 184\n",
      "loss: 8.344895362854004 at epoch 184\n",
      "loss: 9.722764015197754 at epoch 184\n",
      "loss: 11.63045597076416 at epoch 184\n",
      "loss: 8.368008613586426 at epoch 184\n",
      "loss: 9.461008071899414 at epoch 184\n",
      "loss: 9.142271995544434 at epoch 184\n",
      "loss: 8.825017929077148 at epoch 185\n",
      "loss: 8.274081230163574 at epoch 185\n",
      "loss: 8.72811508178711 at epoch 185\n",
      "loss: 8.14220142364502 at epoch 185\n",
      "loss: 8.676379203796387 at epoch 185\n",
      "loss: 10.418701171875 at epoch 185\n",
      "loss: 7.324225425720215 at epoch 185\n",
      "loss: 8.903575897216797 at epoch 185\n",
      "loss: 7.350430488586426 at epoch 186\n",
      "loss: 8.413402557373047 at epoch 186\n",
      "loss: 9.892606735229492 at epoch 186\n",
      "loss: 8.555956840515137 at epoch 186\n",
      "loss: 9.844038009643555 at epoch 186\n",
      "loss: 8.55373764038086 at epoch 186\n",
      "loss: 10.227288246154785 at epoch 186\n",
      "loss: 6.998751640319824 at epoch 186\n",
      "loss: 7.936513423919678 at epoch 187\n",
      "loss: 7.968410491943359 at epoch 187\n",
      "loss: 10.579276084899902 at epoch 187\n",
      "loss: 8.377469062805176 at epoch 187\n",
      "loss: 8.639389991760254 at epoch 187\n",
      "loss: 8.10611343383789 at epoch 187\n",
      "loss: 10.0606689453125 at epoch 187\n",
      "loss: 8.284405708312988 at epoch 187\n",
      "loss: 7.276607513427734 at epoch 188\n",
      "loss: 9.343656539916992 at epoch 188\n",
      "loss: 10.567615509033203 at epoch 188\n",
      "loss: 8.120645523071289 at epoch 188\n",
      "loss: 8.868084907531738 at epoch 188\n",
      "loss: 9.630071640014648 at epoch 188\n",
      "loss: 10.330486297607422 at epoch 188\n",
      "loss: 9.679140090942383 at epoch 188\n",
      "loss: 9.268688201904297 at epoch 189\n",
      "loss: 10.768779754638672 at epoch 189\n",
      "loss: 10.131792068481445 at epoch 189\n",
      "loss: 8.12412166595459 at epoch 189\n",
      "loss: 8.264253616333008 at epoch 189\n",
      "loss: 10.94499683380127 at epoch 189\n",
      "loss: 10.018190383911133 at epoch 189\n",
      "loss: 8.753561973571777 at epoch 189\n",
      "loss: 8.926265716552734 at epoch 190\n",
      "loss: 10.567310333251953 at epoch 190\n",
      "loss: 10.418801307678223 at epoch 190\n",
      "loss: 7.957545757293701 at epoch 190\n",
      "loss: 9.038248062133789 at epoch 190\n",
      "loss: 8.904038429260254 at epoch 190\n",
      "loss: 8.10065746307373 at epoch 190\n",
      "loss: 8.861019134521484 at epoch 190\n",
      "loss: 7.642877578735352 at epoch 191\n",
      "loss: 8.887253761291504 at epoch 191\n",
      "loss: 10.578426361083984 at epoch 191\n",
      "loss: 7.743750095367432 at epoch 191\n",
      "loss: 8.087833404541016 at epoch 191\n",
      "loss: 8.468503952026367 at epoch 191\n",
      "loss: 7.681552410125732 at epoch 191\n",
      "loss: 9.021855354309082 at epoch 191\n",
      "loss: 9.996498107910156 at epoch 192\n",
      "loss: 8.914992332458496 at epoch 192\n",
      "loss: 9.497054100036621 at epoch 192\n",
      "loss: 9.608750343322754 at epoch 192\n",
      "loss: 8.355975151062012 at epoch 192\n",
      "loss: 8.369242668151855 at epoch 192\n",
      "loss: 7.67225456237793 at epoch 192\n",
      "loss: 8.402838706970215 at epoch 192\n",
      "loss: 9.182616233825684 at epoch 193\n",
      "loss: 8.70589828491211 at epoch 193\n",
      "loss: 9.927839279174805 at epoch 193\n",
      "loss: 8.726317405700684 at epoch 193\n",
      "loss: 8.430313110351562 at epoch 193\n",
      "loss: 7.98313045501709 at epoch 193\n",
      "loss: 8.532424926757812 at epoch 193\n",
      "loss: 9.178335189819336 at epoch 193\n",
      "loss: 7.477535724639893 at epoch 194\n",
      "loss: 8.49262523651123 at epoch 194\n",
      "loss: 8.925280570983887 at epoch 194\n",
      "loss: 10.859209060668945 at epoch 194\n",
      "loss: 8.54024887084961 at epoch 194\n",
      "loss: 8.930310249328613 at epoch 194\n",
      "loss: 8.049164772033691 at epoch 194\n",
      "loss: 9.239913940429688 at epoch 194\n",
      "loss: 9.555719375610352 at epoch 195\n",
      "loss: 8.115487098693848 at epoch 195\n",
      "loss: 8.069108963012695 at epoch 195\n",
      "loss: 9.811927795410156 at epoch 195\n",
      "loss: 7.940494537353516 at epoch 195\n",
      "loss: 6.890094757080078 at epoch 195\n",
      "loss: 7.749879837036133 at epoch 195\n",
      "loss: 10.271993637084961 at epoch 195\n",
      "loss: 9.013065338134766 at epoch 196\n",
      "loss: 8.155905723571777 at epoch 196\n",
      "loss: 7.335387229919434 at epoch 196\n",
      "loss: 8.17992877960205 at epoch 196\n",
      "loss: 10.291072845458984 at epoch 196\n",
      "loss: 9.505058288574219 at epoch 196\n",
      "loss: 7.4012837409973145 at epoch 196\n",
      "loss: 8.280139923095703 at epoch 196\n",
      "loss: 7.7940239906311035 at epoch 197\n",
      "loss: 7.684141159057617 at epoch 197\n",
      "loss: 8.253532409667969 at epoch 197\n",
      "loss: 8.627572059631348 at epoch 197\n",
      "loss: 7.572920799255371 at epoch 197\n",
      "loss: 10.153297424316406 at epoch 197\n",
      "loss: 8.558842658996582 at epoch 197\n",
      "loss: 9.318058967590332 at epoch 197\n",
      "loss: 10.52800178527832 at epoch 198\n",
      "loss: 8.895560264587402 at epoch 198\n",
      "loss: 8.83060359954834 at epoch 198\n",
      "loss: 8.514766693115234 at epoch 198\n",
      "loss: 7.9068732261657715 at epoch 198\n",
      "loss: 9.024689674377441 at epoch 198\n",
      "loss: 8.118507385253906 at epoch 198\n",
      "loss: 9.33031177520752 at epoch 198\n",
      "loss: 10.57569408416748 at epoch 199\n",
      "loss: 8.593637466430664 at epoch 199\n",
      "loss: 7.8392863273620605 at epoch 199\n",
      "loss: 11.141989707946777 at epoch 199\n",
      "loss: 8.14212703704834 at epoch 199\n",
      "loss: 7.600384712219238 at epoch 199\n",
      "loss: 10.965269088745117 at epoch 199\n",
      "loss: 8.4864501953125 at epoch 199\n",
      "\n",
      "=== Iteration 0: Pure Assignment ===\n",
      "faculty_vectors: [[0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]]\n",
      "student_features: [[ 84.61845413  91.35264284  92.39754531  71.01904582  60.89586403\n",
      "   70.06210319  74.31994144  91.288768    67.76846361  95.77581402]\n",
      " [ 74.00766447  40.          73.03810339 100.          57.11482107\n",
      "   64.69250744  45.91662421  92.00521698  87.13765175  52.26246932]\n",
      " [ 75.58787673  78.73637625  71.66543405  55.46607965  47.04532845\n",
      "   46.26268487  85.78305825  90.76534213  69.268758    84.0931705 ]\n",
      " [ 40.          67.80305652  73.02954062  46.38359958 100.\n",
      "   70.14714595  40.          58.56592078  46.44045257  54.66770408]\n",
      " [ 50.77550384  61.98133937  74.99888775  82.04910518 100.\n",
      "   79.17374644  54.34710766  50.71616297  99.97424934  40.        ]\n",
      " [100.          92.98874062  77.7401636   69.393147    82.21710657\n",
      "   59.23432419  99.88372338  40.          60.68590607  69.99318953]\n",
      " [ 64.72237755  71.46787477  40.          40.          98.52992196\n",
      "   89.45628569  73.63316825  59.01343913  42.41772006  89.70935076]\n",
      " [ 97.16513824  91.82727849  75.24368712 100.          41.73152409\n",
      "   64.92429265  69.09490561  47.65676947  99.2741422   63.22046585]\n",
      " [ 73.56873208  51.42792401  40.          40.          56.85164849\n",
      "  100.          61.82850355  40.          42.1355724   49.57398378]\n",
      " [ 73.49321619  96.21154072  62.70646887  93.36634238  74.00050729\n",
      "   86.48110079  73.86531471  53.83966423  91.70682498  79.2697792 ]\n",
      " [ 65.92661164  58.78960969  77.94343286 100.          93.12400165\n",
      "   99.61405641  40.          53.00218843  60.11957901  40.        ]\n",
      " [ 63.71628004  95.11152835  83.49106654  55.8946345  100.\n",
      "   94.78461992  93.86240715  93.24379408  40.          51.58559784]\n",
      " [ 87.80878885  50.89985299  40.          48.19827352  49.60542495\n",
      "   95.81278791 100.          62.71177451  78.03125628 100.        ]\n",
      " [ 82.76095318  79.76216001  65.59682822  72.66100447  80.46673791\n",
      "   40.16234623  61.53172941  46.4647612   67.54132376 100.        ]\n",
      " [ 93.5675656   52.08303855  40.          71.9020371   89.06158967\n",
      "   96.66817499 100.          56.26456897  80.54080694  75.39620165]\n",
      " [ 70.78134173 100.          99.00558812  87.6721703   85.57683426\n",
      "   40.          64.67998279 100.          92.0965641   90.55772362]\n",
      " [100.          40.84202169  48.34979668  67.62614423  86.18220371\n",
      "   82.57080584  62.78085205  57.17390201  77.32229104  63.87048326]\n",
      " [ 40.          40.          54.59356868  40.          75.18928774\n",
      "   56.65211791  40.         100.          69.19079661  62.39221226]\n",
      " [ 40.          40.          40.          69.62224312  80.75107206\n",
      "   40.          49.58905236  40.          71.09052742  68.4714313 ]\n",
      " [ 48.66465616 100.          40.         100.          63.15682836\n",
      "  100.          76.13591529  54.24798815  76.5176318   80.50790011]\n",
      " [ 72.60729253  68.45105873  76.5981102   40.          66.58895497\n",
      "   67.89295041  85.41914874  40.          67.73883367  40.        ]\n",
      " [ 48.87899748 100.          57.21509061  81.97723319 100.\n",
      "   88.5183361   70.70359672  82.26359483  59.64205646  60.21249312]\n",
      " [ 55.87127636  89.37875909  72.96175584  61.18328706  52.00066992\n",
      "   53.04117003  73.68063228  43.05948796 100.          75.28276803]\n",
      " [100.          40.          89.80390362  93.16542661 100.\n",
      "   77.00202389  80.39801995  89.78466852  54.86817116  41.98083614]\n",
      " [ 56.09741093  40.          95.52773993  50.18342488 100.\n",
      "   40.          97.0226332   75.21242    100.          44.65865855]\n",
      " [ 64.73723979  48.69368899  75.72514479  46.17989932  81.77943836\n",
      "   50.26541281  40.         100.          40.          69.42905835]\n",
      " [100.          66.33727076  79.99297744  89.98833197  47.4860802\n",
      "   51.79865091  94.68602347  89.14290433  42.5776541   69.43491215]\n",
      " [ 50.26182375  60.73155832  73.25639497 100.          80.62897902\n",
      "   40.          40.          92.99309677  40.          69.77599226]\n",
      " [ 40.          40.          69.24484614  45.71336967  46.20621856\n",
      "   64.0115027   89.49406286  99.11264485  89.34444011  49.25023103]\n",
      " [ 76.59845676  85.5699106   66.86253612  52.67217511  60.73987866\n",
      "   79.70667064  93.58700774  91.90117451  60.64499898  71.62957287]\n",
      " [ 65.74527881  59.2695485  100.          65.33449794  40.\n",
      "   73.48064606 100.          47.29175807  89.78002393  82.76945159]\n",
      " [100.          56.33182501  70.02700145  41.55993426  40.\n",
      "   41.14515898  75.88581027 100.          47.92219515  58.14509843]\n",
      " [ 89.1456465   74.25764253  95.99434061  78.51180156  51.54841077\n",
      "   58.15216669  54.77679554  66.55157248  47.08701959  87.9379949 ]\n",
      " [ 45.5190952  100.          71.91042718  74.8234997   76.78495607\n",
      "   91.26062544  47.10796785  77.20419542  70.85560237  76.10371441]\n",
      " [ 40.          73.5570972   90.73868065  65.70015476  50.42935657\n",
      "   94.17186765 100.         100.          40.         100.        ]\n",
      " [ 79.55517687  65.77162827  85.52393595  74.17770591  40.\n",
      "   62.60503502  57.7139171   49.44284728  50.35593091  48.80200385]\n",
      " [ 40.          68.31612368  40.          80.52235093  60.07281323\n",
      "   57.53363618  65.93152917  78.3071343   40.          62.29791503]\n",
      " [ 55.0584441   93.14987782  57.44861604  67.97433365 100.\n",
      "   40.         100.          90.7031937  100.          82.97726474]\n",
      " [ 66.54346394  78.58754913  57.8304291  100.          78.57929193\n",
      "   40.          70.1224765   67.11747989  69.48880788 100.        ]\n",
      " [ 71.58589862  49.0022444   50.09776102  40.          45.72758284\n",
      "   45.78876399  68.238485   100.          40.          71.57413637]\n",
      " [ 65.45869267  41.02434426  56.64728467  93.55522884  56.48050185\n",
      "   78.38661147 100.          43.74870791 100.          90.2141261 ]\n",
      " [ 64.13951342  68.77382778  91.8860268  100.          92.17802855\n",
      "  100.          40.          91.89357994  60.21207334  48.7051747 ]\n",
      " [ 83.25579158 100.          80.02794194 100.          82.93799143\n",
      "  100.         100.          98.93212748  58.85776627  70.50058479]\n",
      " [ 82.32486984  66.86502537  46.17221642  53.13801166  73.98906036\n",
      "   41.85133753  40.          55.45499991  96.31385642  83.68454169]\n",
      " [ 40.          59.93871385  72.20861718  40.          57.70132175\n",
      "   61.8707027   49.93686269  68.31246725  66.34110923  40.        ]\n",
      " [ 99.21629875  40.          70.60786333 100.          76.28071557\n",
      "   53.25751239  40.         100.          41.35958298  71.71676794]\n",
      " [ 76.56480368 100.          44.07170689  75.47279448  40.90567806\n",
      "   81.54740499  71.59381153  56.84314802  59.02923332  40.        ]\n",
      " [ 69.64916892  69.21783328  57.42469462  46.00828337  78.02549078\n",
      "   91.54011126  40.          86.95476509  62.938461    59.29185255]\n",
      " [ 76.88549748 100.          40.47755506  90.42692464  66.27120155\n",
      "  100.          80.58960489  65.581809    75.19263416  49.6372305 ]\n",
      " [ 40.          43.57638382  90.4483717   65.9605415   40.\n",
      "   57.20428385  51.1137973   52.81241398  91.99534431  64.0422889 ]\n",
      " [ 50.6940453   40.          51.83102636  97.88805452  84.7425806\n",
      "   75.30653016  66.84596411  40.          70.09223151  88.38976218]\n",
      " [ 54.84743156  40.          89.02192329  76.21287083  55.8857874\n",
      "   69.68968378  85.96767963  84.09545702  73.96637149  44.543224  ]\n",
      " [ 78.82587292  75.3036614  100.         100.          70.38859372\n",
      "  100.          76.36191592  86.69536474  72.40879199 100.        ]\n",
      " [ 60.40027132  40.          63.71186877  50.73970059  71.21900929\n",
      "   67.19286726  65.42743952  81.49758431  79.5834907   66.98714105]\n",
      " [ 98.2681082   56.24225537 100.          90.22545307  84.36598515\n",
      "   57.51747591  67.24602352  91.38580283 100.          78.6260286 ]\n",
      " [ 91.11204456  40.          40.          40.          64.72317986\n",
      "   68.53634299  57.08352484  66.23186257  40.          52.00392868]\n",
      " [100.          63.72614066  74.73269454  76.77814277  54.23181728\n",
      "   69.98185191  62.35004912  40.          55.42319207  63.65497477]\n",
      " [100.          53.89165584  78.3626459   61.55105528  65.97556641\n",
      "  100.         100.          50.34534168  90.41742568  93.99484804]\n",
      " [ 61.54719582  98.40116655  40.01752782  67.98111875  81.47220082\n",
      "   40.          78.69838109  89.15867661  40.          84.12327883]\n",
      " [ 40.         100.          85.53651607 100.          89.50181342\n",
      "   99.23072341  53.03566809  91.94068653  98.45900181  96.71099179]\n",
      " [ 76.18325319  89.4413036   92.94494417  81.6923632   86.17300894\n",
      "   69.18953645  94.68658279  96.62242111  51.46434237  59.74227792]\n",
      " [ 84.53173971  46.27283086  40.         100.          74.45239072\n",
      "   72.07469945 100.          57.88445242  91.75457638 100.        ]\n",
      " [ 98.68619774  71.68859002  50.7751799   91.33413881  40.\n",
      "   47.98136405  57.38029493  89.99028148  96.63491898  53.4746838 ]\n",
      " [ 71.98414992  56.45621063  44.94297955  75.39166346  46.70991946\n",
      "   44.1795635   48.29824384  64.77332033  83.73365815  63.21285097]\n",
      " [ 94.09892398  40.          40.          99.15938734  50.82474856\n",
      "   40.          56.01246471  88.89325251  79.35974821  78.55557533]\n",
      " [ 60.46947062 100.          96.38178696  92.83911974 100.\n",
      "   64.23252607  64.12072864  78.68311653  63.65713224  93.60197845]\n",
      " [ 84.89855635 100.          40.         100.          87.98872142\n",
      "   95.86107772  82.06480557  89.46876114  95.59519843  57.78801818]\n",
      " [ 40.47664815  76.7402503   79.68915864  40.          48.65079976\n",
      "   72.83498329 100.          79.18500262  45.66387455  84.62567534]\n",
      " [ 92.23178657  74.91806876  40.          44.35876143  40.\n",
      "   71.17576478  88.516289    57.76720189  81.81763639  60.58863662]\n",
      " [ 60.97044162  64.63621196  87.74056003  85.04545107  40.\n",
      "   45.51515699  48.46492408  40.          54.12254044  52.56517004]\n",
      " [ 85.40872194  70.44333865  80.99767103  91.06783969  86.23373505\n",
      "   63.88192866 100.          73.97344812  44.18807042  40.        ]\n",
      " [ 72.01723359  40.          74.2332979  100.          57.86307574\n",
      "   76.37148418  66.26994926  79.46002311  85.20886871  63.34462473]\n",
      " [100.          40.          63.98391649  94.69392774 100.\n",
      "   77.12749811  92.91405106  45.35910706  70.91981569  98.36665083]\n",
      " [100.          40.         100.         100.          49.34682929\n",
      "   58.03963916  40.          58.55538319  44.59220904  65.9210438 ]\n",
      " [ 47.78194257  71.15857335  85.92961555  75.27865913  53.22172338\n",
      "   75.56649158  59.07694613  40.          92.94109191  40.        ]\n",
      " [ 40.          93.00557445  41.5118509   86.14949708  89.17529596\n",
      "   79.85936772  68.82950877  95.39412228  90.13060235  87.72041448]\n",
      " [ 92.47209348  90.49946366  73.4106111   87.01710609 100.\n",
      "   80.73207078  56.83134038  88.89544259  67.51331652  77.49829276]\n",
      " [ 90.86120203  84.96489498  40.          40.          99.11562004\n",
      "   65.18687723 100.          40.          60.4392017   94.1714819 ]\n",
      " [ 70.60144458  76.17286544  40.          40.          43.64476798\n",
      "   40.         100.          88.28240023  66.01564655 100.        ]\n",
      " [ 83.83455275  40.38663755 100.          68.17780276  40.\n",
      "   57.33694487  40.          64.11938952  51.55757741  90.57005904]\n",
      " [ 73.37896722  53.20884485  81.32988069  98.11141202  56.38104198\n",
      "  100.          96.84940124 100.         100.          92.20825529]\n",
      " [ 46.77565117  70.19919685  55.40051459  70.37817291  40.\n",
      "   98.14136365  83.08951692 100.          64.78727505  54.00980699]\n",
      " [ 86.55006732  95.75970138 100.          79.05421606  71.67863241\n",
      "   93.97207405  70.82091524  68.64854135  87.6739928   40.        ]\n",
      " [ 66.36984382  91.90907045  90.04331181  55.27799771  66.4279366\n",
      "   70.51873955 100.          83.35311649  65.90467361  85.84983994]\n",
      " [ 75.91160422 100.          64.90858001  66.40159968  40.\n",
      "  100.          50.52661472  65.31101154 100.          44.02946875]\n",
      " [ 66.11444325  98.90759176  69.8124073   45.39319637  79.25086718\n",
      "   57.70204434  87.25254155  71.26442024  97.04626004  71.69661943]\n",
      " [ 44.13195351  40.          72.33187288  50.50129212  40.\n",
      "   56.59437788  40.          54.06079527  73.65035551 100.        ]\n",
      " [ 55.56189194  58.78354754  75.81831248  49.84691887  40.\n",
      "   68.71894544  40.          67.93247844  55.08332829  66.85558723]\n",
      " [ 79.39050947  77.8348638   57.40642314  62.97805026  65.49739445\n",
      "   75.61810999  51.80730851 100.          68.24343406  73.27434582]\n",
      " [ 44.04052339  56.00373846  72.1116636  100.          82.01581284\n",
      "   87.76463323 100.          58.93975578  66.12688069  40.        ]\n",
      " [ 40.          66.68750557  77.2820855   65.0940996   90.25173273\n",
      "   98.24750319  89.98167272  60.50252724  40.          40.        ]\n",
      " [ 76.4293528   68.76636502  62.84063199  47.52222662  67.24221816\n",
      "   48.17845087  60.96693514 100.         100.          40.        ]\n",
      " [ 95.96306209  83.90955344  58.82028297  44.67105348  61.27462015\n",
      "   65.7127185   51.02938105  40.         100.          49.63064646]\n",
      " [ 98.09745855  52.48463161 100.          68.01907927 100.\n",
      "   60.21813959  53.73998321  89.76721371  96.06952885  68.50616617]\n",
      " [ 40.          80.03183332  68.50137366  82.77728805  50.86045969\n",
      "   83.56431139  65.81261179  64.3965796   40.          61.67629181]\n",
      " [ 40.          63.48715985  64.34964704  46.07842804 100.\n",
      "   80.22945289  60.25257722  93.17962412  46.98241007  49.02446714]\n",
      " [ 74.44717705  95.63669567  84.33989655  80.19290582  61.9663379\n",
      "   49.96026967  75.80448533  81.6504805   49.31956502  70.05760176]\n",
      " [ 87.54422282  72.46551101  64.39068319  77.05627524  79.28042153\n",
      "  100.          78.18804506  69.94050178  57.71777363  59.13832891]\n",
      " [ 44.49801338  40.          78.33230567  49.48974001  65.78953437\n",
      "  100.          94.25521783  56.5630999   96.25794353 100.        ]\n",
      " [ 44.6807331   40.          40.          99.61115897  40.\n",
      "  100.          95.48208099  40.          73.54523715  66.15840367]]\n",
      "\n",
      "=== Iteration 1: Student Learning ===\n",
      "loss: 1.7247910499572754 at epoch 0 at applicants training\n",
      "loss: 1.8054792881011963 at epoch 1 at applicants training\n",
      "loss: 1.7272982597351074 at epoch 2 at applicants training\n",
      "loss: 1.7247474193572998 at epoch 3 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 4 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 5 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 6 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 7 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 8 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 9 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 10 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 11 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 12 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 13 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 14 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 15 at applicants training\n",
      "loss: 1.7248305082321167 at epoch 16 at applicants training\n",
      "loss: 1.7248034477233887 at epoch 17 at applicants training\n",
      "loss: 1.7241359949111938 at epoch 18 at applicants training\n",
      "loss: 1.7086515426635742 at epoch 19 at applicants training\n",
      "loss: 1.5739108324050903 at epoch 20 at applicants training\n",
      "loss: 1.6362318992614746 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.5482748746871948 at epoch 0 at applicants training\n",
      "loss: 1.805759310722351 at epoch 1 at applicants training\n",
      "loss: 1.8015872240066528 at epoch 2 at applicants training\n",
      "loss: 1.7605290412902832 at epoch 3 at applicants training\n",
      "loss: 1.6896787881851196 at epoch 4 at applicants training\n",
      "loss: 1.7118706703186035 at epoch 5 at applicants training\n",
      "loss: 1.7047529220581055 at epoch 6 at applicants training\n",
      "loss: 1.6625748872756958 at epoch 7 at applicants training\n",
      "loss: 1.5699800252914429 at epoch 8 at applicants training\n",
      "loss: 1.533622145652771 at epoch 9 at applicants training\n",
      "loss: 1.5339258909225464 at epoch 10 at applicants training\n",
      "loss: 1.5352271795272827 at epoch 11 at applicants training\n",
      "loss: 1.5327272415161133 at epoch 12 at applicants training\n",
      "loss: 1.5333198308944702 at epoch 13 at applicants training\n",
      "loss: 1.5322635173797607 at epoch 14 at applicants training\n",
      "loss: 1.5322461128234863 at epoch 15 at applicants training\n",
      "loss: 1.5313198566436768 at epoch 16 at applicants training\n",
      "loss: 1.531065821647644 at epoch 17 at applicants training\n",
      "loss: 1.5308887958526611 at epoch 18 at applicants training\n",
      "loss: 1.530591607093811 at epoch 19 at applicants training\n",
      "loss: 1.5300637483596802 at epoch 20 at applicants training\n",
      "loss: 1.5301721096038818 at epoch 21 at applicants training\n",
      "loss: 1.5297874212265015 at epoch 22 at applicants training\n",
      "loss: 1.53009033203125 at epoch 23 at applicants training\n",
      "loss: 1.5296162366867065 at epoch 24 at applicants training\n",
      "loss: 1.529994249343872 at epoch 25 at applicants training\n",
      "loss: 1.529564380645752 at epoch 26 at applicants training\n",
      "loss: 1.5297267436981201 at epoch 27 at applicants training\n",
      "loss: 1.5296131372451782 at epoch 28 at applicants training\n",
      "loss: 1.5295239686965942 at epoch 29 at applicants training\n",
      "loss: 1.5295612812042236 at epoch 30 at applicants training\n",
      "loss: 1.5294163227081299 at epoch 31 at applicants training\n",
      "loss: 1.5294499397277832 at epoch 32 at applicants training\n",
      "loss: 1.5291810035705566 at epoch 33 at applicants training\n",
      "loss: 1.5291709899902344 at epoch 34 at applicants training\n",
      "loss: 1.5288574695587158 at epoch 35 at applicants training\n",
      "loss: 1.528865098953247 at epoch 36 at applicants training\n",
      "loss: 1.5286095142364502 at epoch 37 at applicants training\n",
      "loss: 1.528733730316162 at epoch 38 at applicants training\n",
      "loss: 1.5287034511566162 at epoch 39 at applicants training\n",
      "loss: 1.528438925743103 at epoch 40 at applicants training\n",
      "loss: 1.5285967588424683 at epoch 41 at applicants training\n",
      "loss: 1.5288059711456299 at epoch 42 at applicants training\n",
      "loss: 1.5282433032989502 at epoch 43 at applicants training\n",
      "loss: 1.5281708240509033 at epoch 44 at applicants training\n",
      "loss: 1.5284420251846313 at epoch 45 at applicants training\n",
      "loss: 1.5281404256820679 at epoch 46 at applicants training\n",
      "loss: 1.5278531312942505 at epoch 47 at applicants training\n",
      "loss: 1.5279676914215088 at epoch 48 at applicants training\n",
      "loss: 1.5280321836471558 at epoch 49 at applicants training\n",
      "loss: 1.5277775526046753 at epoch 50 at applicants training\n",
      "loss: 1.527543306350708 at epoch 51 at applicants training\n",
      "loss: 1.5275460481643677 at epoch 52 at applicants training\n",
      "loss: 1.5276905298233032 at epoch 53 at applicants training\n",
      "loss: 1.5277047157287598 at epoch 54 at applicants training\n",
      "loss: 1.5277220010757446 at epoch 55 at applicants training\n",
      "loss: 1.5272334814071655 at epoch 56 at applicants training\n",
      "loss: 1.5271071195602417 at epoch 57 at applicants training\n",
      "loss: 1.5272727012634277 at epoch 58 at applicants training\n",
      "loss: 1.5271891355514526 at epoch 59 at applicants training\n",
      "loss: 1.5270676612854004 at epoch 60 at applicants training\n",
      "loss: 1.5267348289489746 at epoch 61 at applicants training\n",
      "loss: 1.5266140699386597 at epoch 62 at applicants training\n",
      "loss: 1.5267410278320312 at epoch 63 at applicants training\n",
      "loss: 1.5270090103149414 at epoch 64 at applicants training\n",
      "loss: 1.5271679162979126 at epoch 65 at applicants training\n",
      "loss: 1.5268558263778687 at epoch 66 at applicants training\n",
      "loss: 1.5264198780059814 at epoch 67 at applicants training\n",
      "loss: 1.5261297225952148 at epoch 68 at applicants training\n",
      "loss: 1.5261982679367065 at epoch 69 at applicants training\n",
      "loss: 1.5265220403671265 at epoch 70 at applicants training\n",
      "loss: 1.5268003940582275 at epoch 71 at applicants training\n",
      "loss: 1.526841640472412 at epoch 72 at applicants training\n",
      "loss: 1.5261492729187012 at epoch 73 at applicants training\n",
      "loss: 1.5257755517959595 at epoch 74 at applicants training\n",
      "loss: 1.5255905389785767 at epoch 75 at applicants training\n",
      "loss: 1.5256221294403076 at epoch 76 at applicants training\n",
      "loss: 1.5260940790176392 at epoch 77 at applicants training\n",
      "loss: 1.5266616344451904 at epoch 78 at applicants training\n",
      "loss: 1.527807593345642 at epoch 79 at applicants training\n",
      "loss: 1.525589942932129 at epoch 80 at applicants training\n",
      "loss: 1.5252602100372314 at epoch 81 at applicants training\n",
      "loss: 1.5263323783874512 at epoch 82 at applicants training\n",
      "loss: 1.5260084867477417 at epoch 83 at applicants training\n",
      "loss: 1.525831699371338 at epoch 84 at applicants training\n",
      "loss: 1.5248042345046997 at epoch 85 at applicants training\n",
      "loss: 1.5253901481628418 at epoch 86 at applicants training\n",
      "loss: 1.5269486904144287 at epoch 87 at applicants training\n",
      "loss: 1.5245180130004883 at epoch 88 at applicants training\n",
      "loss: 1.524830937385559 at epoch 89 at applicants training\n",
      "loss: 1.5267139673233032 at epoch 90 at applicants training\n",
      "loss: 1.5238641500473022 at epoch 91 at applicants training\n",
      "loss: 1.5254571437835693 at epoch 92 at applicants training\n",
      "loss: 1.5282585620880127 at epoch 93 at applicants training\n",
      "loss: 1.5232583284378052 at epoch 94 at applicants training\n",
      "loss: 1.526497483253479 at epoch 95 at applicants training\n",
      "loss: 1.5278749465942383 at epoch 96 at applicants training\n",
      "loss: 1.5227689743041992 at epoch 97 at applicants training\n",
      "loss: 1.5298885107040405 at epoch 98 at applicants training\n",
      "loss: 1.526951551437378 at epoch 99 at applicants training\n",
      "loss: 1.6702942848205566 at epoch 0 at applicants training\n",
      "loss: 1.7570022344589233 at epoch 1 at applicants training\n",
      "loss: 1.614568829536438 at epoch 2 at applicants training\n",
      "loss: 1.5847489833831787 at epoch 3 at applicants training\n",
      "loss: 1.6470683813095093 at epoch 4 at applicants training\n",
      "loss: 1.6357895135879517 at epoch 5 at applicants training\n",
      "loss: 1.565218448638916 at epoch 6 at applicants training\n",
      "loss: 1.5400822162628174 at epoch 7 at applicants training\n",
      "loss: 1.5875396728515625 at epoch 8 at applicants training\n",
      "loss: 1.5318201780319214 at epoch 9 at applicants training\n",
      "loss: 1.5374411344528198 at epoch 10 at applicants training\n",
      "loss: 1.5338349342346191 at epoch 11 at applicants training\n",
      "loss: 1.508459448814392 at epoch 12 at applicants training\n",
      "loss: 1.5064479112625122 at epoch 13 at applicants training\n",
      "loss: 1.5027334690093994 at epoch 14 at applicants training\n",
      "loss: 1.496631145477295 at epoch 15 at applicants training\n",
      "loss: 1.4994378089904785 at epoch 16 at applicants training\n",
      "loss: 1.4880883693695068 at epoch 17 at applicants training\n",
      "loss: 1.4876208305358887 at epoch 18 at applicants training\n",
      "loss: 1.4882304668426514 at epoch 19 at applicants training\n",
      "loss: 1.4768496751785278 at epoch 20 at applicants training\n",
      "loss: 1.4805335998535156 at epoch 21 at applicants training\n",
      "loss: 1.4700627326965332 at epoch 22 at applicants training\n",
      "loss: 1.4726852178573608 at epoch 23 at applicants training\n",
      "loss: 1.483984351158142 at epoch 24 at applicants training\n",
      "loss: 1.4839369058609009 at epoch 25 at applicants training\n",
      "loss: 1.4850231409072876 at epoch 26 at applicants training\n",
      "loss: 1.4784225225448608 at epoch 27 at applicants training\n",
      "loss: 1.4689216613769531 at epoch 28 at applicants training\n",
      "loss: 1.4781584739685059 at epoch 29 at applicants training\n",
      "loss: 1.4659645557403564 at epoch 30 at applicants training\n",
      "loss: 1.466161847114563 at epoch 31 at applicants training\n",
      "loss: 1.4685419797897339 at epoch 32 at applicants training\n",
      "loss: 1.4631969928741455 at epoch 33 at applicants training\n",
      "loss: 1.4623360633850098 at epoch 34 at applicants training\n",
      "loss: 1.4629870653152466 at epoch 35 at applicants training\n",
      "loss: 1.4571046829223633 at epoch 36 at applicants training\n",
      "loss: 1.4598970413208008 at epoch 37 at applicants training\n",
      "loss: 1.458245038986206 at epoch 38 at applicants training\n",
      "loss: 1.4546395540237427 at epoch 39 at applicants training\n",
      "loss: 1.4588056802749634 at epoch 40 at applicants training\n",
      "loss: 1.453969120979309 at epoch 41 at applicants training\n",
      "loss: 1.4561920166015625 at epoch 42 at applicants training\n",
      "loss: 1.455264925956726 at epoch 43 at applicants training\n",
      "loss: 1.452986240386963 at epoch 44 at applicants training\n",
      "loss: 1.4533101320266724 at epoch 45 at applicants training\n",
      "loss: 1.454086184501648 at epoch 46 at applicants training\n",
      "loss: 1.4522215127944946 at epoch 47 at applicants training\n",
      "loss: 1.4520573616027832 at epoch 48 at applicants training\n",
      "loss: 1.4532463550567627 at epoch 49 at applicants training\n",
      "loss: 1.4520787000656128 at epoch 50 at applicants training\n",
      "loss: 1.4516856670379639 at epoch 51 at applicants training\n",
      "loss: 1.4521279335021973 at epoch 52 at applicants training\n",
      "loss: 1.4523051977157593 at epoch 53 at applicants training\n",
      "loss: 1.4517196416854858 at epoch 54 at applicants training\n",
      "loss: 1.4512290954589844 at epoch 55 at applicants training\n",
      "loss: 1.451837420463562 at epoch 56 at applicants training\n",
      "loss: 1.4509004354476929 at epoch 57 at applicants training\n",
      "loss: 1.4509875774383545 at epoch 58 at applicants training\n",
      "loss: 1.451077938079834 at epoch 59 at applicants training\n",
      "loss: 1.450619101524353 at epoch 60 at applicants training\n",
      "loss: 1.4508975744247437 at epoch 61 at applicants training\n",
      "loss: 1.4504921436309814 at epoch 62 at applicants training\n",
      "loss: 1.4507423639297485 at epoch 63 at applicants training\n",
      "loss: 1.4504185914993286 at epoch 64 at applicants training\n",
      "loss: 1.4506340026855469 at epoch 65 at applicants training\n",
      "loss: 1.45015287399292 at epoch 66 at applicants training\n",
      "loss: 1.4503753185272217 at epoch 67 at applicants training\n",
      "loss: 1.450028896331787 at epoch 68 at applicants training\n",
      "loss: 1.450270414352417 at epoch 69 at applicants training\n",
      "loss: 1.4500035047531128 at epoch 70 at applicants training\n",
      "loss: 1.4501125812530518 at epoch 71 at applicants training\n",
      "loss: 1.4499595165252686 at epoch 72 at applicants training\n",
      "loss: 1.450000286102295 at epoch 73 at applicants training\n",
      "loss: 1.4499577283859253 at epoch 74 at applicants training\n",
      "loss: 1.4498891830444336 at epoch 75 at applicants training\n",
      "loss: 1.4499114751815796 at epoch 76 at applicants training\n",
      "loss: 1.4497570991516113 at epoch 77 at applicants training\n",
      "loss: 1.4498162269592285 at epoch 78 at applicants training\n",
      "loss: 1.4496721029281616 at epoch 79 at applicants training\n",
      "loss: 1.4496934413909912 at epoch 80 at applicants training\n",
      "loss: 1.449690341949463 at epoch 81 at applicants training\n",
      "loss: 1.4495915174484253 at epoch 82 at applicants training\n",
      "loss: 1.449644923210144 at epoch 83 at applicants training\n",
      "loss: 1.4496208429336548 at epoch 84 at applicants training\n",
      "loss: 1.4495090246200562 at epoch 85 at applicants training\n",
      "loss: 1.4495331048965454 at epoch 86 at applicants training\n",
      "loss: 1.4495460987091064 at epoch 87 at applicants training\n",
      "loss: 1.449469804763794 at epoch 88 at applicants training\n",
      "loss: 1.4494975805282593 at epoch 89 at applicants training\n",
      "loss: 1.4494880437850952 at epoch 90 at applicants training\n",
      "loss: 1.4494062662124634 at epoch 91 at applicants training\n",
      "loss: 1.44942307472229 at epoch 92 at applicants training\n",
      "loss: 1.4494448900222778 at epoch 93 at applicants training\n",
      "loss: 1.4493869543075562 at epoch 94 at applicants training\n",
      "loss: 1.4493862390518188 at epoch 95 at applicants training\n",
      "loss: 1.449402928352356 at epoch 96 at applicants training\n",
      "loss: 1.4493439197540283 at epoch 97 at applicants training\n",
      "loss: 1.449311375617981 at epoch 98 at applicants training\n",
      "loss: 1.4493341445922852 at epoch 99 at applicants training\n",
      "loss: 1.6673263311386108 at epoch 0 at applicants training\n",
      "loss: 1.7843632698059082 at epoch 1 at applicants training\n",
      "loss: 1.7723714113235474 at epoch 2 at applicants training\n",
      "loss: 1.7550132274627686 at epoch 3 at applicants training\n",
      "loss: 1.7343146800994873 at epoch 4 at applicants training\n",
      "loss: 1.664272665977478 at epoch 5 at applicants training\n",
      "loss: 1.6846612691879272 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 9 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 10 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 11 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 12 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 13 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 14 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 15 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 16 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 17 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 18 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 19 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 20 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 21 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 22 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.6847710609436035 at epoch 0 at applicants training\n",
      "loss: 1.6999140977859497 at epoch 1 at applicants training\n",
      "loss: 1.6841650009155273 at epoch 2 at applicants training\n",
      "loss: 1.6820425987243652 at epoch 3 at applicants training\n",
      "loss: 1.6642487049102783 at epoch 4 at applicants training\n",
      "loss: 1.6186028718948364 at epoch 5 at applicants training\n",
      "loss: 1.58864164352417 at epoch 6 at applicants training\n",
      "loss: 1.5736498832702637 at epoch 7 at applicants training\n",
      "loss: 1.5308233499526978 at epoch 8 at applicants training\n",
      "loss: 1.5250580310821533 at epoch 9 at applicants training\n",
      "loss: 1.5012695789337158 at epoch 10 at applicants training\n",
      "loss: 1.4884941577911377 at epoch 11 at applicants training\n",
      "loss: 1.477329969406128 at epoch 12 at applicants training\n",
      "loss: 1.463302493095398 at epoch 13 at applicants training\n",
      "loss: 1.4550837278366089 at epoch 14 at applicants training\n",
      "loss: 1.4498556852340698 at epoch 15 at applicants training\n",
      "loss: 1.4428868293762207 at epoch 16 at applicants training\n",
      "loss: 1.4447922706604004 at epoch 17 at applicants training\n",
      "loss: 1.4389817714691162 at epoch 18 at applicants training\n",
      "loss: 1.4389050006866455 at epoch 19 at applicants training\n",
      "loss: 1.436310887336731 at epoch 20 at applicants training\n",
      "loss: 1.431652545928955 at epoch 21 at applicants training\n",
      "loss: 1.4309875965118408 at epoch 22 at applicants training\n",
      "loss: 1.4284765720367432 at epoch 23 at applicants training\n",
      "loss: 1.4250082969665527 at epoch 24 at applicants training\n",
      "loss: 1.4212504625320435 at epoch 25 at applicants training\n",
      "loss: 1.4226384162902832 at epoch 26 at applicants training\n",
      "loss: 1.4173176288604736 at epoch 27 at applicants training\n",
      "loss: 1.418136715888977 at epoch 28 at applicants training\n",
      "loss: 1.4150623083114624 at epoch 29 at applicants training\n",
      "loss: 1.4113311767578125 at epoch 30 at applicants training\n",
      "loss: 1.4130629301071167 at epoch 31 at applicants training\n",
      "loss: 1.4093109369277954 at epoch 32 at applicants training\n",
      "loss: 1.4100075960159302 at epoch 33 at applicants training\n",
      "loss: 1.404981255531311 at epoch 34 at applicants training\n",
      "loss: 1.4084997177124023 at epoch 35 at applicants training\n",
      "loss: 1.4038265943527222 at epoch 36 at applicants training\n",
      "loss: 1.4057270288467407 at epoch 37 at applicants training\n",
      "loss: 1.4029361009597778 at epoch 38 at applicants training\n",
      "loss: 1.404268503189087 at epoch 39 at applicants training\n",
      "loss: 1.403077244758606 at epoch 40 at applicants training\n",
      "loss: 1.4028637409210205 at epoch 41 at applicants training\n",
      "loss: 1.4036296606063843 at epoch 42 at applicants training\n",
      "loss: 1.4031034708023071 at epoch 43 at applicants training\n",
      "loss: 1.4025286436080933 at epoch 44 at applicants training\n",
      "loss: 1.4029344320297241 at epoch 45 at applicants training\n",
      "loss: 1.4029442071914673 at epoch 46 at applicants training\n",
      "loss: 1.4022525548934937 at epoch 47 at applicants training\n",
      "loss: 1.4022210836410522 at epoch 48 at applicants training\n",
      "loss: 1.4024485349655151 at epoch 49 at applicants training\n",
      "loss: 1.4019558429718018 at epoch 50 at applicants training\n",
      "loss: 1.401595950126648 at epoch 51 at applicants training\n",
      "loss: 1.401719570159912 at epoch 52 at applicants training\n",
      "loss: 1.401476263999939 at epoch 53 at applicants training\n",
      "loss: 1.400986671447754 at epoch 54 at applicants training\n",
      "loss: 1.4010194540023804 at epoch 55 at applicants training\n",
      "loss: 1.4008111953735352 at epoch 56 at applicants training\n",
      "loss: 1.4004464149475098 at epoch 57 at applicants training\n",
      "loss: 1.4005475044250488 at epoch 58 at applicants training\n",
      "loss: 1.4002070426940918 at epoch 59 at applicants training\n",
      "loss: 1.4001539945602417 at epoch 60 at applicants training\n",
      "loss: 1.4000978469848633 at epoch 61 at applicants training\n",
      "loss: 1.3998547792434692 at epoch 62 at applicants training\n",
      "loss: 1.399922490119934 at epoch 63 at applicants training\n",
      "loss: 1.399631142616272 at epoch 64 at applicants training\n",
      "loss: 1.399708867073059 at epoch 65 at applicants training\n",
      "loss: 1.3994861841201782 at epoch 66 at applicants training\n",
      "loss: 1.399397850036621 at epoch 67 at applicants training\n",
      "loss: 1.399266004562378 at epoch 68 at applicants training\n",
      "loss: 1.3991079330444336 at epoch 69 at applicants training\n",
      "loss: 1.3989757299423218 at epoch 70 at applicants training\n",
      "loss: 1.3987791538238525 at epoch 71 at applicants training\n",
      "loss: 1.3987326622009277 at epoch 72 at applicants training\n",
      "loss: 1.3985131978988647 at epoch 73 at applicants training\n",
      "loss: 1.3984798192977905 at epoch 74 at applicants training\n",
      "loss: 1.3983666896820068 at epoch 75 at applicants training\n",
      "loss: 1.3982877731323242 at epoch 76 at applicants training\n",
      "loss: 1.398175835609436 at epoch 77 at applicants training\n",
      "loss: 1.3981280326843262 at epoch 78 at applicants training\n",
      "loss: 1.398166298866272 at epoch 79 at applicants training\n",
      "loss: 1.3980718851089478 at epoch 80 at applicants training\n",
      "loss: 1.3980549573898315 at epoch 81 at applicants training\n",
      "loss: 1.3979356288909912 at epoch 82 at applicants training\n",
      "loss: 1.397926688194275 at epoch 83 at applicants training\n",
      "loss: 1.3978804349899292 at epoch 84 at applicants training\n",
      "loss: 1.3977771997451782 at epoch 85 at applicants training\n",
      "loss: 1.397741436958313 at epoch 86 at applicants training\n",
      "loss: 1.3976469039916992 at epoch 87 at applicants training\n",
      "loss: 1.3976272344589233 at epoch 88 at applicants training\n",
      "loss: 1.3975335359573364 at epoch 89 at applicants training\n",
      "loss: 1.3974803686141968 at epoch 90 at applicants training\n",
      "loss: 1.3974751234054565 at epoch 91 at applicants training\n",
      "loss: 1.3974064588546753 at epoch 92 at applicants training\n",
      "loss: 1.3973441123962402 at epoch 93 at applicants training\n",
      "loss: 1.3973286151885986 at epoch 94 at applicants training\n",
      "loss: 1.3973257541656494 at epoch 95 at applicants training\n",
      "loss: 1.3972561359405518 at epoch 96 at applicants training\n",
      "loss: 1.3972538709640503 at epoch 97 at applicants training\n",
      "loss: 1.3972582817077637 at epoch 98 at applicants training\n",
      "loss: 1.3972283601760864 at epoch 99 at applicants training\n",
      "loss: 1.7037544250488281 at epoch 0 at applicants training\n",
      "loss: 1.6216737031936646 at epoch 1 at applicants training\n",
      "loss: 1.6259033679962158 at epoch 2 at applicants training\n",
      "loss: 1.5699982643127441 at epoch 3 at applicants training\n",
      "loss: 1.5861124992370605 at epoch 4 at applicants training\n",
      "loss: 1.5555291175842285 at epoch 5 at applicants training\n",
      "loss: 1.5257811546325684 at epoch 6 at applicants training\n",
      "loss: 1.569593071937561 at epoch 7 at applicants training\n",
      "loss: 1.5249546766281128 at epoch 8 at applicants training\n",
      "loss: 1.507315993309021 at epoch 9 at applicants training\n",
      "loss: 1.5120409727096558 at epoch 10 at applicants training\n",
      "loss: 1.5221877098083496 at epoch 11 at applicants training\n",
      "loss: 1.5093539953231812 at epoch 12 at applicants training\n",
      "loss: 1.4790656566619873 at epoch 13 at applicants training\n",
      "loss: 1.4928628206253052 at epoch 14 at applicants training\n",
      "loss: 1.486030101776123 at epoch 15 at applicants training\n",
      "loss: 1.480534553527832 at epoch 16 at applicants training\n",
      "loss: 1.4662444591522217 at epoch 17 at applicants training\n",
      "loss: 1.4499058723449707 at epoch 18 at applicants training\n",
      "loss: 1.4429081678390503 at epoch 19 at applicants training\n",
      "loss: 1.4246302843093872 at epoch 20 at applicants training\n",
      "loss: 1.3859949111938477 at epoch 21 at applicants training\n",
      "loss: 1.4033623933792114 at epoch 22 at applicants training\n",
      "loss: 1.348697543144226 at epoch 23 at applicants training\n",
      "loss: 1.3486523628234863 at epoch 24 at applicants training\n",
      "loss: 1.3445804119110107 at epoch 25 at applicants training\n",
      "loss: 1.3089579343795776 at epoch 26 at applicants training\n",
      "loss: 1.3033486604690552 at epoch 27 at applicants training\n",
      "loss: 1.2856990098953247 at epoch 28 at applicants training\n",
      "loss: 1.2780203819274902 at epoch 29 at applicants training\n",
      "loss: 1.277897834777832 at epoch 30 at applicants training\n",
      "loss: 1.2486944198608398 at epoch 31 at applicants training\n",
      "loss: 1.2508119344711304 at epoch 32 at applicants training\n",
      "loss: 1.2515912055969238 at epoch 33 at applicants training\n",
      "loss: 1.2284245491027832 at epoch 34 at applicants training\n",
      "loss: 1.232936143875122 at epoch 35 at applicants training\n",
      "loss: 1.2256561517715454 at epoch 36 at applicants training\n",
      "loss: 1.2162046432495117 at epoch 37 at applicants training\n",
      "loss: 1.2081092596054077 at epoch 38 at applicants training\n",
      "loss: 1.2064288854599 at epoch 39 at applicants training\n",
      "loss: 1.1976238489151 at epoch 40 at applicants training\n",
      "loss: 1.1984717845916748 at epoch 41 at applicants training\n",
      "loss: 1.188944697380066 at epoch 42 at applicants training\n",
      "loss: 1.1973459720611572 at epoch 43 at applicants training\n",
      "loss: 1.186840295791626 at epoch 44 at applicants training\n",
      "loss: 1.1931346654891968 at epoch 45 at applicants training\n",
      "loss: 1.1842701435089111 at epoch 46 at applicants training\n",
      "loss: 1.19087553024292 at epoch 47 at applicants training\n",
      "loss: 1.1824288368225098 at epoch 48 at applicants training\n",
      "loss: 1.1871447563171387 at epoch 49 at applicants training\n",
      "loss: 1.1803789138793945 at epoch 50 at applicants training\n",
      "loss: 1.1834584474563599 at epoch 51 at applicants training\n",
      "loss: 1.1789755821228027 at epoch 52 at applicants training\n",
      "loss: 1.1803582906723022 at epoch 53 at applicants training\n",
      "loss: 1.1782042980194092 at epoch 54 at applicants training\n",
      "loss: 1.177892804145813 at epoch 55 at applicants training\n",
      "loss: 1.1779415607452393 at epoch 56 at applicants training\n",
      "loss: 1.1762522459030151 at epoch 57 at applicants training\n",
      "loss: 1.1774235963821411 at epoch 58 at applicants training\n",
      "loss: 1.175408124923706 at epoch 59 at applicants training\n",
      "loss: 1.176851749420166 at epoch 60 at applicants training\n",
      "loss: 1.175123929977417 at epoch 61 at applicants training\n",
      "loss: 1.1759839057922363 at epoch 62 at applicants training\n",
      "loss: 1.1750794649124146 at epoch 63 at applicants training\n",
      "loss: 1.1751415729522705 at epoch 64 at applicants training\n",
      "loss: 1.174820899963379 at epoch 65 at applicants training\n",
      "loss: 1.174438238143921 at epoch 66 at applicants training\n",
      "loss: 1.1744637489318848 at epoch 67 at applicants training\n",
      "loss: 1.1738612651824951 at epoch 68 at applicants training\n",
      "loss: 1.173944115638733 at epoch 69 at applicants training\n",
      "loss: 1.1735265254974365 at epoch 70 at applicants training\n",
      "loss: 1.1733475923538208 at epoch 71 at applicants training\n",
      "loss: 1.1732351779937744 at epoch 72 at applicants training\n",
      "loss: 1.1728808879852295 at epoch 73 at applicants training\n",
      "loss: 1.1729369163513184 at epoch 74 at applicants training\n",
      "loss: 1.1725952625274658 at epoch 75 at applicants training\n",
      "loss: 1.1726244688034058 at epoch 76 at applicants training\n",
      "loss: 1.172457218170166 at epoch 77 at applicants training\n",
      "loss: 1.172334909439087 at epoch 78 at applicants training\n",
      "loss: 1.17237389087677 at epoch 79 at applicants training\n",
      "loss: 1.1721363067626953 at epoch 80 at applicants training\n",
      "loss: 1.1722345352172852 at epoch 81 at applicants training\n",
      "loss: 1.172041416168213 at epoch 82 at applicants training\n",
      "loss: 1.1720430850982666 at epoch 83 at applicants training\n",
      "loss: 1.171962022781372 at epoch 84 at applicants training\n",
      "loss: 1.1718635559082031 at epoch 85 at applicants training\n",
      "loss: 1.1718382835388184 at epoch 86 at applicants training\n",
      "loss: 1.1717395782470703 at epoch 87 at applicants training\n",
      "loss: 1.1716850996017456 at epoch 88 at applicants training\n",
      "loss: 1.1716428995132446 at epoch 89 at applicants training\n",
      "loss: 1.171546220779419 at epoch 90 at applicants training\n",
      "loss: 1.1715389490127563 at epoch 91 at applicants training\n",
      "loss: 1.1714425086975098 at epoch 92 at applicants training\n",
      "loss: 1.1714200973510742 at epoch 93 at applicants training\n",
      "loss: 1.1713606119155884 at epoch 94 at applicants training\n",
      "loss: 1.1713050603866577 at epoch 95 at applicants training\n",
      "loss: 1.17127525806427 at epoch 96 at applicants training\n",
      "loss: 1.171208381652832 at epoch 97 at applicants training\n",
      "loss: 1.1711761951446533 at epoch 98 at applicants training\n",
      "loss: 1.1711267232894897 at epoch 99 at applicants training\n",
      "loss: 1.802543044090271 at epoch 0 at applicants training\n",
      "loss: 1.6672134399414062 at epoch 1 at applicants training\n",
      "loss: 1.6538209915161133 at epoch 2 at applicants training\n",
      "loss: 1.6339349746704102 at epoch 3 at applicants training\n",
      "loss: 1.6069552898406982 at epoch 4 at applicants training\n",
      "loss: 1.6319537162780762 at epoch 5 at applicants training\n",
      "loss: 1.5980256795883179 at epoch 6 at applicants training\n",
      "loss: 1.5696827173233032 at epoch 7 at applicants training\n",
      "loss: 1.6337158679962158 at epoch 8 at applicants training\n",
      "loss: 1.5393213033676147 at epoch 9 at applicants training\n",
      "loss: 1.548102855682373 at epoch 10 at applicants training\n",
      "loss: 1.5585238933563232 at epoch 11 at applicants training\n",
      "loss: 1.5106946229934692 at epoch 12 at applicants training\n",
      "loss: 1.5205334424972534 at epoch 13 at applicants training\n",
      "loss: 1.5247833728790283 at epoch 14 at applicants training\n",
      "loss: 1.4821934700012207 at epoch 15 at applicants training\n",
      "loss: 1.497803807258606 at epoch 16 at applicants training\n",
      "loss: 1.508013129234314 at epoch 17 at applicants training\n",
      "loss: 1.4957365989685059 at epoch 18 at applicants training\n",
      "loss: 1.4764631986618042 at epoch 19 at applicants training\n",
      "loss: 1.4882229566574097 at epoch 20 at applicants training\n",
      "loss: 1.4844075441360474 at epoch 21 at applicants training\n",
      "loss: 1.4715216159820557 at epoch 22 at applicants training\n",
      "loss: 1.4800735712051392 at epoch 23 at applicants training\n",
      "loss: 1.4786314964294434 at epoch 24 at applicants training\n",
      "loss: 1.4659724235534668 at epoch 25 at applicants training\n",
      "loss: 1.470484733581543 at epoch 26 at applicants training\n",
      "loss: 1.4734493494033813 at epoch 27 at applicants training\n",
      "loss: 1.458560824394226 at epoch 28 at applicants training\n",
      "loss: 1.467899203300476 at epoch 29 at applicants training\n",
      "loss: 1.4674748182296753 at epoch 30 at applicants training\n",
      "loss: 1.4566093683242798 at epoch 31 at applicants training\n",
      "loss: 1.4666050672531128 at epoch 32 at applicants training\n",
      "loss: 1.458571195602417 at epoch 33 at applicants training\n",
      "loss: 1.4593042135238647 at epoch 34 at applicants training\n",
      "loss: 1.46342134475708 at epoch 35 at applicants training\n",
      "loss: 1.4565973281860352 at epoch 36 at applicants training\n",
      "loss: 1.4569237232208252 at epoch 37 at applicants training\n",
      "loss: 1.4586254358291626 at epoch 38 at applicants training\n",
      "loss: 1.4539556503295898 at epoch 39 at applicants training\n",
      "loss: 1.4574787616729736 at epoch 40 at applicants training\n",
      "loss: 1.4564599990844727 at epoch 41 at applicants training\n",
      "loss: 1.4532208442687988 at epoch 42 at applicants training\n",
      "loss: 1.4559953212738037 at epoch 43 at applicants training\n",
      "loss: 1.4540793895721436 at epoch 44 at applicants training\n",
      "loss: 1.4531373977661133 at epoch 45 at applicants training\n",
      "loss: 1.4545962810516357 at epoch 46 at applicants training\n",
      "loss: 1.4537853002548218 at epoch 47 at applicants training\n",
      "loss: 1.4523853063583374 at epoch 48 at applicants training\n",
      "loss: 1.453240156173706 at epoch 49 at applicants training\n",
      "loss: 1.45326566696167 at epoch 50 at applicants training\n",
      "loss: 1.451982021331787 at epoch 51 at applicants training\n",
      "loss: 1.4522649049758911 at epoch 52 at applicants training\n",
      "loss: 1.4526556730270386 at epoch 53 at applicants training\n",
      "loss: 1.4516106843948364 at epoch 54 at applicants training\n",
      "loss: 1.4515752792358398 at epoch 55 at applicants training\n",
      "loss: 1.4519619941711426 at epoch 56 at applicants training\n",
      "loss: 1.4511762857437134 at epoch 57 at applicants training\n",
      "loss: 1.4511291980743408 at epoch 58 at applicants training\n",
      "loss: 1.4513978958129883 at epoch 59 at applicants training\n",
      "loss: 1.4507038593292236 at epoch 60 at applicants training\n",
      "loss: 1.4508012533187866 at epoch 61 at applicants training\n",
      "loss: 1.45078444480896 at epoch 62 at applicants training\n",
      "loss: 1.4502694606781006 at epoch 63 at applicants training\n",
      "loss: 1.4505140781402588 at epoch 64 at applicants training\n",
      "loss: 1.4502003192901611 at epoch 65 at applicants training\n",
      "loss: 1.449985146522522 at epoch 66 at applicants training\n",
      "loss: 1.4501020908355713 at epoch 67 at applicants training\n",
      "loss: 1.449683427810669 at epoch 68 at applicants training\n",
      "loss: 1.4497859477996826 at epoch 69 at applicants training\n",
      "loss: 1.4495326280593872 at epoch 70 at applicants training\n",
      "loss: 1.4493716955184937 at epoch 71 at applicants training\n",
      "loss: 1.4493343830108643 at epoch 72 at applicants training\n",
      "loss: 1.4490333795547485 at epoch 73 at applicants training\n",
      "loss: 1.4490633010864258 at epoch 74 at applicants training\n",
      "loss: 1.4488035440444946 at epoch 75 at applicants training\n",
      "loss: 1.4488223791122437 at epoch 76 at applicants training\n",
      "loss: 1.4486193656921387 at epoch 77 at applicants training\n",
      "loss: 1.4485913515090942 at epoch 78 at applicants training\n",
      "loss: 1.4484546184539795 at epoch 79 at applicants training\n",
      "loss: 1.4483997821807861 at epoch 80 at applicants training\n",
      "loss: 1.448290228843689 at epoch 81 at applicants training\n",
      "loss: 1.4482758045196533 at epoch 82 at applicants training\n",
      "loss: 1.44816255569458 at epoch 83 at applicants training\n",
      "loss: 1.4481511116027832 at epoch 84 at applicants training\n",
      "loss: 1.448004961013794 at epoch 85 at applicants training\n",
      "loss: 1.4479773044586182 at epoch 86 at applicants training\n",
      "loss: 1.44780695438385 at epoch 87 at applicants training\n",
      "loss: 1.4477620124816895 at epoch 88 at applicants training\n",
      "loss: 1.4476051330566406 at epoch 89 at applicants training\n",
      "loss: 1.4475674629211426 at epoch 90 at applicants training\n",
      "loss: 1.4474685192108154 at epoch 91 at applicants training\n",
      "loss: 1.4474220275878906 at epoch 92 at applicants training\n",
      "loss: 1.447370171546936 at epoch 93 at applicants training\n",
      "loss: 1.447320818901062 at epoch 94 at applicants training\n",
      "loss: 1.4472975730895996 at epoch 95 at applicants training\n",
      "loss: 1.4472382068634033 at epoch 96 at applicants training\n",
      "loss: 1.4472236633300781 at epoch 97 at applicants training\n",
      "loss: 1.4471663236618042 at epoch 98 at applicants training\n",
      "loss: 1.447145938873291 at epoch 99 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 0 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 1 at applicants training\n",
      "loss: 1.6591079235076904 at epoch 2 at applicants training\n",
      "loss: 1.593800663948059 at epoch 3 at applicants training\n",
      "loss: 1.6535381078720093 at epoch 4 at applicants training\n",
      "loss: 1.6616090536117554 at epoch 5 at applicants training\n",
      "loss: 1.6606141328811646 at epoch 6 at applicants training\n",
      "loss: 1.6544986963272095 at epoch 7 at applicants training\n",
      "loss: 1.649590015411377 at epoch 8 at applicants training\n",
      "loss: 1.6141718626022339 at epoch 9 at applicants training\n",
      "loss: 1.5764431953430176 at epoch 10 at applicants training\n",
      "loss: 1.586394190788269 at epoch 11 at applicants training\n",
      "loss: 1.5964709520339966 at epoch 12 at applicants training\n",
      "loss: 1.5789762735366821 at epoch 13 at applicants training\n",
      "loss: 1.5666999816894531 at epoch 14 at applicants training\n",
      "loss: 1.5684884786605835 at epoch 15 at applicants training\n",
      "loss: 1.5720105171203613 at epoch 16 at applicants training\n",
      "loss: 1.569973111152649 at epoch 17 at applicants training\n",
      "loss: 1.5599082708358765 at epoch 18 at applicants training\n",
      "loss: 1.5354180335998535 at epoch 19 at applicants training\n",
      "loss: 1.5600429773330688 at epoch 20 at applicants training\n",
      "loss: 1.5641119480133057 at epoch 21 at applicants training\n",
      "loss: 1.5412367582321167 at epoch 22 at applicants training\n",
      "loss: 1.5332187414169312 at epoch 23 at applicants training\n",
      "loss: 1.5360515117645264 at epoch 24 at applicants training\n",
      "loss: 1.5393120050430298 at epoch 25 at applicants training\n",
      "loss: 1.5321743488311768 at epoch 26 at applicants training\n",
      "loss: 1.5263416767120361 at epoch 27 at applicants training\n",
      "loss: 1.5153344869613647 at epoch 28 at applicants training\n",
      "loss: 1.5056672096252441 at epoch 29 at applicants training\n",
      "loss: 1.509821891784668 at epoch 30 at applicants training\n",
      "loss: 1.5029999017715454 at epoch 31 at applicants training\n",
      "loss: 1.496464729309082 at epoch 32 at applicants training\n",
      "loss: 1.483925461769104 at epoch 33 at applicants training\n",
      "loss: 1.4689702987670898 at epoch 34 at applicants training\n",
      "loss: 1.467474341392517 at epoch 35 at applicants training\n",
      "loss: 1.4547853469848633 at epoch 36 at applicants training\n",
      "loss: 1.4496597051620483 at epoch 37 at applicants training\n",
      "loss: 1.452296257019043 at epoch 38 at applicants training\n",
      "loss: 1.450104832649231 at epoch 39 at applicants training\n",
      "loss: 1.4498534202575684 at epoch 40 at applicants training\n",
      "loss: 1.4450998306274414 at epoch 41 at applicants training\n",
      "loss: 1.4422030448913574 at epoch 42 at applicants training\n",
      "loss: 1.433742642402649 at epoch 43 at applicants training\n",
      "loss: 1.4211846590042114 at epoch 44 at applicants training\n",
      "loss: 1.4814828634262085 at epoch 45 at applicants training\n",
      "loss: 1.4786485433578491 at epoch 46 at applicants training\n",
      "loss: 1.444339394569397 at epoch 47 at applicants training\n",
      "loss: 1.4152626991271973 at epoch 48 at applicants training\n",
      "loss: 1.4278370141983032 at epoch 49 at applicants training\n",
      "loss: 1.3947181701660156 at epoch 50 at applicants training\n",
      "loss: 1.4091349840164185 at epoch 51 at applicants training\n",
      "loss: 1.397456407546997 at epoch 52 at applicants training\n",
      "loss: 1.3778263330459595 at epoch 53 at applicants training\n",
      "loss: 1.3733505010604858 at epoch 54 at applicants training\n",
      "loss: 1.3737698793411255 at epoch 55 at applicants training\n",
      "loss: 1.373087763786316 at epoch 56 at applicants training\n",
      "loss: 1.3610217571258545 at epoch 57 at applicants training\n",
      "loss: 1.3515465259552002 at epoch 58 at applicants training\n",
      "loss: 1.3700305223464966 at epoch 59 at applicants training\n",
      "loss: 1.3426610231399536 at epoch 60 at applicants training\n",
      "loss: 1.3548223972320557 at epoch 61 at applicants training\n",
      "loss: 1.3562684059143066 at epoch 62 at applicants training\n",
      "loss: 1.3432585000991821 at epoch 63 at applicants training\n",
      "loss: 1.3534692525863647 at epoch 64 at applicants training\n",
      "loss: 1.3260284662246704 at epoch 65 at applicants training\n",
      "loss: 1.325423002243042 at epoch 66 at applicants training\n",
      "loss: 1.3262592554092407 at epoch 67 at applicants training\n",
      "loss: 1.3212156295776367 at epoch 68 at applicants training\n",
      "loss: 1.3261102437973022 at epoch 69 at applicants training\n",
      "loss: 1.3116450309753418 at epoch 70 at applicants training\n",
      "loss: 1.3156991004943848 at epoch 71 at applicants training\n",
      "loss: 1.3044593334197998 at epoch 72 at applicants training\n",
      "loss: 1.300811767578125 at epoch 73 at applicants training\n",
      "loss: 1.3008090257644653 at epoch 74 at applicants training\n",
      "loss: 1.2949790954589844 at epoch 75 at applicants training\n",
      "loss: 1.2953667640686035 at epoch 76 at applicants training\n",
      "loss: 1.2886544466018677 at epoch 77 at applicants training\n",
      "loss: 1.2915011644363403 at epoch 78 at applicants training\n",
      "loss: 1.2905534505844116 at epoch 79 at applicants training\n",
      "loss: 1.283216118812561 at epoch 80 at applicants training\n",
      "loss: 1.2921512126922607 at epoch 81 at applicants training\n",
      "loss: 1.2769384384155273 at epoch 82 at applicants training\n",
      "loss: 1.2841023206710815 at epoch 83 at applicants training\n",
      "loss: 1.2805262804031372 at epoch 84 at applicants training\n",
      "loss: 1.278648018836975 at epoch 85 at applicants training\n",
      "loss: 1.2770859003067017 at epoch 86 at applicants training\n",
      "loss: 1.2758002281188965 at epoch 87 at applicants training\n",
      "loss: 1.275354266166687 at epoch 88 at applicants training\n",
      "loss: 1.2756351232528687 at epoch 89 at applicants training\n",
      "loss: 1.2735350131988525 at epoch 90 at applicants training\n",
      "loss: 1.2753280401229858 at epoch 91 at applicants training\n",
      "loss: 1.273128867149353 at epoch 92 at applicants training\n",
      "loss: 1.274531602859497 at epoch 93 at applicants training\n",
      "loss: 1.2721728086471558 at epoch 94 at applicants training\n",
      "loss: 1.275174617767334 at epoch 95 at applicants training\n",
      "loss: 1.271683692932129 at epoch 96 at applicants training\n",
      "loss: 1.273443579673767 at epoch 97 at applicants training\n",
      "loss: 1.2718148231506348 at epoch 98 at applicants training\n",
      "loss: 1.2719495296478271 at epoch 99 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 0 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 1 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 2 at applicants training\n",
      "loss: 1.815234661102295 at epoch 3 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 4 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 5 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 6 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 7 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 8 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 9 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 10 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 11 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 12 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 13 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 14 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 15 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 16 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 17 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 18 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 19 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 20 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 21 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 22 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 23 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 24 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 25 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 26 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 27 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 28 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 29 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 30 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 31 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 32 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 33 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 34 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 35 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 36 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 37 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 38 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 39 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 40 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 41 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 42 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 43 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 44 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 45 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 46 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 47 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 48 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 49 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 50 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 51 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 52 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 53 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 54 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 55 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 56 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 57 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 58 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 59 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 60 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 61 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 62 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 63 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 64 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 65 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 66 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 67 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 68 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 69 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 70 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 71 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 72 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 73 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 74 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 75 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 76 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 77 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 78 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 79 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 80 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 81 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 82 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 83 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 84 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 85 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 86 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 87 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 88 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 89 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 90 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 91 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 92 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 93 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 94 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 95 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 96 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 97 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 98 at applicants training\n",
      "loss: 1.8148325681686401 at epoch 99 at applicants training\n",
      "loss: 1.68288254737854 at epoch 0 at applicants training\n",
      "loss: 1.674831748008728 at epoch 1 at applicants training\n",
      "loss: 1.6748324632644653 at epoch 2 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 3 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 4 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 5 at applicants training\n",
      "loss: 1.6748318672180176 at epoch 6 at applicants training\n",
      "loss: 1.6748247146606445 at epoch 7 at applicants training\n",
      "loss: 1.6741573810577393 at epoch 8 at applicants training\n",
      "loss: 1.6419719457626343 at epoch 9 at applicants training\n",
      "loss: 1.557875156402588 at epoch 10 at applicants training\n",
      "loss: 1.6249598264694214 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.724804401397705 at epoch 0 at applicants training\n",
      "loss: 1.724725365638733 at epoch 1 at applicants training\n",
      "loss: 1.7248317003250122 at epoch 2 at applicants training\n",
      "loss: 1.724830985069275 at epoch 3 at applicants training\n",
      "loss: 1.7248226404190063 at epoch 4 at applicants training\n",
      "loss: 1.7247236967086792 at epoch 5 at applicants training\n",
      "loss: 1.7233555316925049 at epoch 6 at applicants training\n",
      "loss: 1.7382986545562744 at epoch 7 at applicants training\n",
      "loss: 1.7222981452941895 at epoch 8 at applicants training\n",
      "loss: 1.6726548671722412 at epoch 9 at applicants training\n",
      "loss: 1.641699194908142 at epoch 10 at applicants training\n",
      "loss: 1.6065986156463623 at epoch 11 at applicants training\n",
      "loss: 1.5745428800582886 at epoch 12 at applicants training\n",
      "loss: 1.607854962348938 at epoch 13 at applicants training\n",
      "loss: 1.5636131763458252 at epoch 14 at applicants training\n",
      "loss: 1.585446834564209 at epoch 15 at applicants training\n",
      "loss: 1.5860291719436646 at epoch 16 at applicants training\n",
      "loss: 1.5648047924041748 at epoch 17 at applicants training\n",
      "loss: 1.5525702238082886 at epoch 18 at applicants training\n",
      "loss: 1.5434046983718872 at epoch 19 at applicants training\n",
      "loss: 1.5284913778305054 at epoch 20 at applicants training\n",
      "loss: 1.5219496488571167 at epoch 21 at applicants training\n",
      "loss: 1.5141209363937378 at epoch 22 at applicants training\n",
      "loss: 1.5118485689163208 at epoch 23 at applicants training\n",
      "loss: 1.5105432271957397 at epoch 24 at applicants training\n",
      "loss: 1.508466362953186 at epoch 25 at applicants training\n",
      "loss: 1.5061646699905396 at epoch 26 at applicants training\n",
      "loss: 1.498689889907837 at epoch 27 at applicants training\n",
      "loss: 1.4818629026412964 at epoch 28 at applicants training\n",
      "loss: 1.4842917919158936 at epoch 29 at applicants training\n",
      "loss: 1.4911946058273315 at epoch 30 at applicants training\n",
      "loss: 1.4909467697143555 at epoch 31 at applicants training\n",
      "loss: 1.4904626607894897 at epoch 32 at applicants training\n",
      "loss: 1.4902361631393433 at epoch 33 at applicants training\n",
      "loss: 1.4852356910705566 at epoch 34 at applicants training\n",
      "loss: 1.4611912965774536 at epoch 35 at applicants training\n",
      "loss: 1.4812928438186646 at epoch 36 at applicants training\n",
      "loss: 1.462011694908142 at epoch 37 at applicants training\n",
      "loss: 1.4599946737289429 at epoch 38 at applicants training\n",
      "loss: 1.4744083881378174 at epoch 39 at applicants training\n",
      "loss: 1.4535974264144897 at epoch 40 at applicants training\n",
      "loss: 1.4540992975234985 at epoch 41 at applicants training\n",
      "loss: 1.4627768993377686 at epoch 42 at applicants training\n",
      "loss: 1.4553645849227905 at epoch 43 at applicants training\n",
      "loss: 1.4458962678909302 at epoch 44 at applicants training\n",
      "loss: 1.460575819015503 at epoch 45 at applicants training\n",
      "loss: 1.4445369243621826 at epoch 46 at applicants training\n",
      "loss: 1.449733018875122 at epoch 47 at applicants training\n",
      "loss: 1.4528350830078125 at epoch 48 at applicants training\n",
      "loss: 1.4490286111831665 at epoch 49 at applicants training\n",
      "loss: 1.4437488317489624 at epoch 50 at applicants training\n",
      "loss: 1.4537513256072998 at epoch 51 at applicants training\n",
      "loss: 1.4440594911575317 at epoch 52 at applicants training\n",
      "loss: 1.451594352722168 at epoch 53 at applicants training\n",
      "loss: 1.4507864713668823 at epoch 54 at applicants training\n",
      "loss: 1.4424914121627808 at epoch 55 at applicants training\n",
      "loss: 1.447928786277771 at epoch 56 at applicants training\n",
      "loss: 1.4447226524353027 at epoch 57 at applicants training\n",
      "loss: 1.4421194791793823 at epoch 58 at applicants training\n",
      "loss: 1.4466845989227295 at epoch 59 at applicants training\n",
      "loss: 1.4431155920028687 at epoch 60 at applicants training\n",
      "loss: 1.442380428314209 at epoch 61 at applicants training\n",
      "loss: 1.4450433254241943 at epoch 62 at applicants training\n",
      "loss: 1.4428939819335938 at epoch 63 at applicants training\n",
      "loss: 1.4418938159942627 at epoch 64 at applicants training\n",
      "loss: 1.4433971643447876 at epoch 65 at applicants training\n",
      "loss: 1.4430769681930542 at epoch 66 at applicants training\n",
      "loss: 1.4418491125106812 at epoch 67 at applicants training\n",
      "loss: 1.4422175884246826 at epoch 68 at applicants training\n",
      "loss: 1.4429932832717896 at epoch 69 at applicants training\n",
      "loss: 1.4418994188308716 at epoch 70 at applicants training\n",
      "loss: 1.4415925741195679 at epoch 71 at applicants training\n",
      "loss: 1.4422235488891602 at epoch 72 at applicants training\n",
      "loss: 1.4419406652450562 at epoch 73 at applicants training\n",
      "loss: 1.4412667751312256 at epoch 74 at applicants training\n",
      "loss: 1.4415584802627563 at epoch 75 at applicants training\n",
      "loss: 1.4417335987091064 at epoch 76 at applicants training\n",
      "loss: 1.441141963005066 at epoch 77 at applicants training\n",
      "loss: 1.4409290552139282 at epoch 78 at applicants training\n",
      "loss: 1.4413577318191528 at epoch 79 at applicants training\n",
      "loss: 1.440565586090088 at epoch 80 at applicants training\n",
      "loss: 1.4409399032592773 at epoch 81 at applicants training\n",
      "loss: 1.440679669380188 at epoch 82 at applicants training\n",
      "loss: 1.4404006004333496 at epoch 83 at applicants training\n",
      "loss: 1.4405854940414429 at epoch 84 at applicants training\n",
      "loss: 1.4402068853378296 at epoch 85 at applicants training\n",
      "loss: 1.4403971433639526 at epoch 86 at applicants training\n",
      "loss: 1.4399877786636353 at epoch 87 at applicants training\n",
      "loss: 1.4402590990066528 at epoch 88 at applicants training\n",
      "loss: 1.4399418830871582 at epoch 89 at applicants training\n",
      "loss: 1.4400867223739624 at epoch 90 at applicants training\n",
      "loss: 1.4398759603500366 at epoch 91 at applicants training\n",
      "loss: 1.4399387836456299 at epoch 92 at applicants training\n",
      "loss: 1.43987238407135 at epoch 93 at applicants training\n",
      "loss: 1.4397828578948975 at epoch 94 at applicants training\n",
      "loss: 1.4398493766784668 at epoch 95 at applicants training\n",
      "loss: 1.4396363496780396 at epoch 96 at applicants training\n",
      "loss: 1.4397611618041992 at epoch 97 at applicants training\n",
      "loss: 1.439568042755127 at epoch 98 at applicants training\n",
      "loss: 1.439592719078064 at epoch 99 at applicants training\n",
      "loss: 1.8143646717071533 at epoch 0 at applicants training\n",
      "loss: 1.6477586030960083 at epoch 1 at applicants training\n",
      "loss: 1.6199284791946411 at epoch 2 at applicants training\n",
      "loss: 1.6222003698349 at epoch 3 at applicants training\n",
      "loss: 1.6152061223983765 at epoch 4 at applicants training\n",
      "loss: 1.605851411819458 at epoch 5 at applicants training\n",
      "loss: 1.6142414808273315 at epoch 6 at applicants training\n",
      "loss: 1.6231268644332886 at epoch 7 at applicants training\n",
      "loss: 1.6248223781585693 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.7020984888076782 at epoch 0 at applicants training\n",
      "loss: 1.6848251819610596 at epoch 1 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 2 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 3 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 4 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 5 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 9 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 10 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 11 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 12 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 13 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 14 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 15 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 16 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 17 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 18 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 19 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 20 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 21 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 22 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.6393420696258545 at epoch 0 at applicants training\n",
      "loss: 1.546980857849121 at epoch 1 at applicants training\n",
      "loss: 1.55899977684021 at epoch 2 at applicants training\n",
      "loss: 1.5333654880523682 at epoch 3 at applicants training\n",
      "loss: 1.5125126838684082 at epoch 4 at applicants training\n",
      "loss: 1.4805876016616821 at epoch 5 at applicants training\n",
      "loss: 1.4730769395828247 at epoch 6 at applicants training\n",
      "loss: 1.4051601886749268 at epoch 7 at applicants training\n",
      "loss: 1.5341668128967285 at epoch 8 at applicants training\n",
      "loss: 1.4566630125045776 at epoch 9 at applicants training\n",
      "loss: 1.4071791172027588 at epoch 10 at applicants training\n",
      "loss: 1.4394590854644775 at epoch 11 at applicants training\n",
      "loss: 1.4628264904022217 at epoch 12 at applicants training\n",
      "loss: 1.4360219240188599 at epoch 13 at applicants training\n",
      "loss: 1.3976396322250366 at epoch 14 at applicants training\n",
      "loss: 1.3794327974319458 at epoch 15 at applicants training\n",
      "loss: 1.400657296180725 at epoch 16 at applicants training\n",
      "loss: 1.40053129196167 at epoch 17 at applicants training\n",
      "loss: 1.3805863857269287 at epoch 18 at applicants training\n",
      "loss: 1.365214228630066 at epoch 19 at applicants training\n",
      "loss: 1.363845944404602 at epoch 20 at applicants training\n",
      "loss: 1.3724490404129028 at epoch 21 at applicants training\n",
      "loss: 1.3512144088745117 at epoch 22 at applicants training\n",
      "loss: 1.3459763526916504 at epoch 23 at applicants training\n",
      "loss: 1.345018744468689 at epoch 24 at applicants training\n",
      "loss: 1.3432469367980957 at epoch 25 at applicants training\n",
      "loss: 1.325872540473938 at epoch 26 at applicants training\n",
      "loss: 1.3228976726531982 at epoch 27 at applicants training\n",
      "loss: 1.3220970630645752 at epoch 28 at applicants training\n",
      "loss: 1.3054641485214233 at epoch 29 at applicants training\n",
      "loss: 1.3043508529663086 at epoch 30 at applicants training\n",
      "loss: 1.3005658388137817 at epoch 31 at applicants training\n",
      "loss: 1.2913743257522583 at epoch 32 at applicants training\n",
      "loss: 1.2833064794540405 at epoch 33 at applicants training\n",
      "loss: 1.2829667329788208 at epoch 34 at applicants training\n",
      "loss: 1.2712745666503906 at epoch 35 at applicants training\n",
      "loss: 1.2713762521743774 at epoch 36 at applicants training\n",
      "loss: 1.266707181930542 at epoch 37 at applicants training\n",
      "loss: 1.2613701820373535 at epoch 38 at applicants training\n",
      "loss: 1.2639079093933105 at epoch 39 at applicants training\n",
      "loss: 1.2591066360473633 at epoch 40 at applicants training\n",
      "loss: 1.2580585479736328 at epoch 41 at applicants training\n",
      "loss: 1.2605199813842773 at epoch 42 at applicants training\n",
      "loss: 1.2551634311676025 at epoch 43 at applicants training\n",
      "loss: 1.2557169198989868 at epoch 44 at applicants training\n",
      "loss: 1.2524774074554443 at epoch 45 at applicants training\n",
      "loss: 1.2506335973739624 at epoch 46 at applicants training\n",
      "loss: 1.2499374151229858 at epoch 47 at applicants training\n",
      "loss: 1.2450414896011353 at epoch 48 at applicants training\n",
      "loss: 1.2447742223739624 at epoch 49 at applicants training\n",
      "loss: 1.2401419878005981 at epoch 50 at applicants training\n",
      "loss: 1.2395790815353394 at epoch 51 at applicants training\n",
      "loss: 1.2369084358215332 at epoch 52 at applicants training\n",
      "loss: 1.2349727153778076 at epoch 53 at applicants training\n",
      "loss: 1.2338802814483643 at epoch 54 at applicants training\n",
      "loss: 1.2310742139816284 at epoch 55 at applicants training\n",
      "loss: 1.2306418418884277 at epoch 56 at applicants training\n",
      "loss: 1.227868676185608 at epoch 57 at applicants training\n",
      "loss: 1.2276790142059326 at epoch 58 at applicants training\n",
      "loss: 1.2256836891174316 at epoch 59 at applicants training\n",
      "loss: 1.2255457639694214 at epoch 60 at applicants training\n",
      "loss: 1.2240806818008423 at epoch 61 at applicants training\n",
      "loss: 1.2235628366470337 at epoch 62 at applicants training\n",
      "loss: 1.2230459451675415 at epoch 63 at applicants training\n",
      "loss: 1.222475290298462 at epoch 64 at applicants training\n",
      "loss: 1.2222331762313843 at epoch 65 at applicants training\n",
      "loss: 1.2215096950531006 at epoch 66 at applicants training\n",
      "loss: 1.221502661705017 at epoch 67 at applicants training\n",
      "loss: 1.2206989526748657 at epoch 68 at applicants training\n",
      "loss: 1.220664620399475 at epoch 69 at applicants training\n",
      "loss: 1.2201014757156372 at epoch 70 at applicants training\n",
      "loss: 1.2194310426712036 at epoch 71 at applicants training\n",
      "loss: 1.219123125076294 at epoch 72 at applicants training\n",
      "loss: 1.2185739278793335 at epoch 73 at applicants training\n",
      "loss: 1.2184381484985352 at epoch 74 at applicants training\n",
      "loss: 1.2179816961288452 at epoch 75 at applicants training\n",
      "loss: 1.2179616689682007 at epoch 76 at applicants training\n",
      "loss: 1.2175984382629395 at epoch 77 at applicants training\n",
      "loss: 1.2174863815307617 at epoch 78 at applicants training\n",
      "loss: 1.2172530889511108 at epoch 79 at applicants training\n",
      "loss: 1.217004656791687 at epoch 80 at applicants training\n",
      "loss: 1.2168368101119995 at epoch 81 at applicants training\n",
      "loss: 1.216582179069519 at epoch 82 at applicants training\n",
      "loss: 1.2164779901504517 at epoch 83 at applicants training\n",
      "loss: 1.2162166833877563 at epoch 84 at applicants training\n",
      "loss: 1.2161030769348145 at epoch 85 at applicants training\n",
      "loss: 1.215997338294983 at epoch 86 at applicants training\n",
      "loss: 1.2158316373825073 at epoch 87 at applicants training\n",
      "loss: 1.2157169580459595 at epoch 88 at applicants training\n",
      "loss: 1.215546727180481 at epoch 89 at applicants training\n",
      "loss: 1.215449333190918 at epoch 90 at applicants training\n",
      "loss: 1.21531081199646 at epoch 91 at applicants training\n",
      "loss: 1.215198278427124 at epoch 92 at applicants training\n",
      "loss: 1.2151291370391846 at epoch 93 at applicants training\n",
      "loss: 1.2150168418884277 at epoch 94 at applicants training\n",
      "loss: 1.2149394750595093 at epoch 95 at applicants training\n",
      "loss: 1.2148381471633911 at epoch 96 at applicants training\n",
      "loss: 1.2147316932678223 at epoch 97 at applicants training\n",
      "loss: 1.2146469354629517 at epoch 98 at applicants training\n",
      "loss: 1.214542269706726 at epoch 99 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 0 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 1 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 2 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 3 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 4 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 5 at applicants training\n",
      "loss: 1.7240753173828125 at epoch 6 at applicants training\n",
      "loss: 1.7247300148010254 at epoch 7 at applicants training\n",
      "loss: 1.7240926027297974 at epoch 8 at applicants training\n",
      "loss: 1.7247754335403442 at epoch 9 at applicants training\n",
      "loss: 1.72483229637146 at epoch 10 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 11 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 12 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 13 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 14 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 15 at applicants training\n",
      "loss: 1.72483229637146 at epoch 16 at applicants training\n",
      "loss: 1.724830985069275 at epoch 17 at applicants training\n",
      "loss: 1.7248049974441528 at epoch 18 at applicants training\n",
      "loss: 1.7241249084472656 at epoch 19 at applicants training\n",
      "loss: 1.7125941514968872 at epoch 20 at applicants training\n",
      "loss: 1.6794118881225586 at epoch 21 at applicants training\n",
      "loss: 1.6819535493850708 at epoch 22 at applicants training\n",
      "loss: 1.684739589691162 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.727138876914978 at epoch 0 at applicants training\n",
      "loss: 1.600572109222412 at epoch 1 at applicants training\n",
      "loss: 1.562370777130127 at epoch 2 at applicants training\n",
      "loss: 1.4932374954223633 at epoch 3 at applicants training\n",
      "loss: 1.5903716087341309 at epoch 4 at applicants training\n",
      "loss: 1.5370310544967651 at epoch 5 at applicants training\n",
      "loss: 1.4647372961044312 at epoch 6 at applicants training\n",
      "loss: 1.5568913221359253 at epoch 7 at applicants training\n",
      "loss: 1.5720889568328857 at epoch 8 at applicants training\n",
      "loss: 1.5521031618118286 at epoch 9 at applicants training\n",
      "loss: 1.4975439310073853 at epoch 10 at applicants training\n",
      "loss: 1.4685195684432983 at epoch 11 at applicants training\n",
      "loss: 1.495702862739563 at epoch 12 at applicants training\n",
      "loss: 1.5295158624649048 at epoch 13 at applicants training\n",
      "loss: 1.514570951461792 at epoch 14 at applicants training\n",
      "loss: 1.4842971563339233 at epoch 15 at applicants training\n",
      "loss: 1.4554847478866577 at epoch 16 at applicants training\n",
      "loss: 1.4562880992889404 at epoch 17 at applicants training\n",
      "loss: 1.4635733366012573 at epoch 18 at applicants training\n",
      "loss: 1.4785041809082031 at epoch 19 at applicants training\n",
      "loss: 1.4546796083450317 at epoch 20 at applicants training\n",
      "loss: 1.4472050666809082 at epoch 21 at applicants training\n",
      "loss: 1.43388032913208 at epoch 22 at applicants training\n",
      "loss: 1.4634010791778564 at epoch 23 at applicants training\n",
      "loss: 1.461054801940918 at epoch 24 at applicants training\n",
      "loss: 1.4318042993545532 at epoch 25 at applicants training\n",
      "loss: 1.433287501335144 at epoch 26 at applicants training\n",
      "loss: 1.447839379310608 at epoch 27 at applicants training\n",
      "loss: 1.4554885625839233 at epoch 28 at applicants training\n",
      "loss: 1.4518344402313232 at epoch 29 at applicants training\n",
      "loss: 1.4385474920272827 at epoch 30 at applicants training\n",
      "loss: 1.42057204246521 at epoch 31 at applicants training\n",
      "loss: 1.4428385496139526 at epoch 32 at applicants training\n",
      "loss: 1.4283462762832642 at epoch 33 at applicants training\n",
      "loss: 1.4178472757339478 at epoch 34 at applicants training\n",
      "loss: 1.4314335584640503 at epoch 35 at applicants training\n",
      "loss: 1.4363658428192139 at epoch 36 at applicants training\n",
      "loss: 1.432770848274231 at epoch 37 at applicants training\n",
      "loss: 1.4202513694763184 at epoch 38 at applicants training\n",
      "loss: 1.4120086431503296 at epoch 39 at applicants training\n",
      "loss: 1.4373693466186523 at epoch 40 at applicants training\n",
      "loss: 1.4352445602416992 at epoch 41 at applicants training\n",
      "loss: 1.4098073244094849 at epoch 42 at applicants training\n",
      "loss: 1.4108957052230835 at epoch 43 at applicants training\n",
      "loss: 1.4222705364227295 at epoch 44 at applicants training\n",
      "loss: 1.4159231185913086 at epoch 45 at applicants training\n",
      "loss: 1.4102391004562378 at epoch 46 at applicants training\n",
      "loss: 1.408007025718689 at epoch 47 at applicants training\n",
      "loss: 1.4191234111785889 at epoch 48 at applicants training\n",
      "loss: 1.4072363376617432 at epoch 49 at applicants training\n",
      "loss: 1.4087591171264648 at epoch 50 at applicants training\n",
      "loss: 1.4098831415176392 at epoch 51 at applicants training\n",
      "loss: 1.4115797281265259 at epoch 52 at applicants training\n",
      "loss: 1.410322904586792 at epoch 53 at applicants training\n",
      "loss: 1.4078788757324219 at epoch 54 at applicants training\n",
      "loss: 1.4036171436309814 at epoch 55 at applicants training\n",
      "loss: 1.4064719676971436 at epoch 56 at applicants training\n",
      "loss: 1.4073634147644043 at epoch 57 at applicants training\n",
      "loss: 1.402035117149353 at epoch 58 at applicants training\n",
      "loss: 1.40342116355896 at epoch 59 at applicants training\n",
      "loss: 1.4052727222442627 at epoch 60 at applicants training\n",
      "loss: 1.4020766019821167 at epoch 61 at applicants training\n",
      "loss: 1.4013891220092773 at epoch 62 at applicants training\n",
      "loss: 1.4028215408325195 at epoch 63 at applicants training\n",
      "loss: 1.4029016494750977 at epoch 64 at applicants training\n",
      "loss: 1.4014923572540283 at epoch 65 at applicants training\n",
      "loss: 1.4010658264160156 at epoch 66 at applicants training\n",
      "loss: 1.4014097452163696 at epoch 67 at applicants training\n",
      "loss: 1.4019612073898315 at epoch 68 at applicants training\n",
      "loss: 1.4013954401016235 at epoch 69 at applicants training\n",
      "loss: 1.4007883071899414 at epoch 70 at applicants training\n",
      "loss: 1.400708794593811 at epoch 71 at applicants training\n",
      "loss: 1.4007983207702637 at epoch 72 at applicants training\n",
      "loss: 1.400852918624878 at epoch 73 at applicants training\n",
      "loss: 1.400362491607666 at epoch 74 at applicants training\n",
      "loss: 1.3997890949249268 at epoch 75 at applicants training\n",
      "loss: 1.3996772766113281 at epoch 76 at applicants training\n",
      "loss: 1.3995548486709595 at epoch 77 at applicants training\n",
      "loss: 1.399134635925293 at epoch 78 at applicants training\n",
      "loss: 1.3989028930664062 at epoch 79 at applicants training\n",
      "loss: 1.3989589214324951 at epoch 80 at applicants training\n",
      "loss: 1.3984466791152954 at epoch 81 at applicants training\n",
      "loss: 1.398625135421753 at epoch 82 at applicants training\n",
      "loss: 1.3981090784072876 at epoch 83 at applicants training\n",
      "loss: 1.3982828855514526 at epoch 84 at applicants training\n",
      "loss: 1.3977354764938354 at epoch 85 at applicants training\n",
      "loss: 1.3978962898254395 at epoch 86 at applicants training\n",
      "loss: 1.3974473476409912 at epoch 87 at applicants training\n",
      "loss: 1.3975387811660767 at epoch 88 at applicants training\n",
      "loss: 1.397493600845337 at epoch 89 at applicants training\n",
      "loss: 1.3972495794296265 at epoch 90 at applicants training\n",
      "loss: 1.397615909576416 at epoch 91 at applicants training\n",
      "loss: 1.397394061088562 at epoch 92 at applicants training\n",
      "loss: 1.3973950147628784 at epoch 93 at applicants training\n",
      "loss: 1.3974229097366333 at epoch 94 at applicants training\n",
      "loss: 1.3971072435379028 at epoch 95 at applicants training\n",
      "loss: 1.3972004652023315 at epoch 96 at applicants training\n",
      "loss: 1.3971797227859497 at epoch 97 at applicants training\n",
      "loss: 1.3970319032669067 at epoch 98 at applicants training\n",
      "loss: 1.3971073627471924 at epoch 99 at applicants training\n",
      "loss: 1.674751877784729 at epoch 0 at applicants training\n",
      "loss: 1.6585605144500732 at epoch 1 at applicants training\n",
      "loss: 1.6173012256622314 at epoch 2 at applicants training\n",
      "loss: 1.6060776710510254 at epoch 3 at applicants training\n",
      "loss: 1.6836951971054077 at epoch 4 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 5 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 9 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 10 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 11 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 12 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 13 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 14 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 15 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 16 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 17 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 18 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 19 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 20 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 21 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 22 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.6712113618850708 at epoch 0 at applicants training\n",
      "loss: 1.590306043624878 at epoch 1 at applicants training\n",
      "loss: 1.5358624458312988 at epoch 2 at applicants training\n",
      "loss: 1.557849407196045 at epoch 3 at applicants training\n",
      "loss: 1.4836562871932983 at epoch 4 at applicants training\n",
      "loss: 1.4877794981002808 at epoch 5 at applicants training\n",
      "loss: 1.4734675884246826 at epoch 6 at applicants training\n",
      "loss: 1.476553201675415 at epoch 7 at applicants training\n",
      "loss: 1.462631106376648 at epoch 8 at applicants training\n",
      "loss: 1.4378132820129395 at epoch 9 at applicants training\n",
      "loss: 1.4361110925674438 at epoch 10 at applicants training\n",
      "loss: 1.4037506580352783 at epoch 11 at applicants training\n",
      "loss: 1.3892035484313965 at epoch 12 at applicants training\n",
      "loss: 1.3680751323699951 at epoch 13 at applicants training\n",
      "loss: 1.3477814197540283 at epoch 14 at applicants training\n",
      "loss: 1.3308403491973877 at epoch 15 at applicants training\n",
      "loss: 1.3249382972717285 at epoch 16 at applicants training\n",
      "loss: 1.2937122583389282 at epoch 17 at applicants training\n",
      "loss: 1.2863348722457886 at epoch 18 at applicants training\n",
      "loss: 1.271234393119812 at epoch 19 at applicants training\n",
      "loss: 1.2519545555114746 at epoch 20 at applicants training\n",
      "loss: 1.2442364692687988 at epoch 21 at applicants training\n",
      "loss: 1.2362473011016846 at epoch 22 at applicants training\n",
      "loss: 1.2276031970977783 at epoch 23 at applicants training\n",
      "loss: 1.226191520690918 at epoch 24 at applicants training\n",
      "loss: 1.224807858467102 at epoch 25 at applicants training\n",
      "loss: 1.2244206666946411 at epoch 26 at applicants training\n",
      "loss: 1.2175486087799072 at epoch 27 at applicants training\n",
      "loss: 1.2115720510482788 at epoch 28 at applicants training\n",
      "loss: 1.2103171348571777 at epoch 29 at applicants training\n",
      "loss: 1.2057417631149292 at epoch 30 at applicants training\n",
      "loss: 1.202864408493042 at epoch 31 at applicants training\n",
      "loss: 1.2014691829681396 at epoch 32 at applicants training\n",
      "loss: 1.1998751163482666 at epoch 33 at applicants training\n",
      "loss: 1.197831392288208 at epoch 34 at applicants training\n",
      "loss: 1.1954267024993896 at epoch 35 at applicants training\n",
      "loss: 1.195700764656067 at epoch 36 at applicants training\n",
      "loss: 1.1946626901626587 at epoch 37 at applicants training\n",
      "loss: 1.1926196813583374 at epoch 38 at applicants training\n",
      "loss: 1.192787528038025 at epoch 39 at applicants training\n",
      "loss: 1.1922955513000488 at epoch 40 at applicants training\n",
      "loss: 1.1907377243041992 at epoch 41 at applicants training\n",
      "loss: 1.1913583278656006 at epoch 42 at applicants training\n",
      "loss: 1.190300464630127 at epoch 43 at applicants training\n",
      "loss: 1.189958095550537 at epoch 44 at applicants training\n",
      "loss: 1.1898385286331177 at epoch 45 at applicants training\n",
      "loss: 1.188966989517212 at epoch 46 at applicants training\n",
      "loss: 1.1890876293182373 at epoch 47 at applicants training\n",
      "loss: 1.1880028247833252 at epoch 48 at applicants training\n",
      "loss: 1.1879816055297852 at epoch 49 at applicants training\n",
      "loss: 1.186966896057129 at epoch 50 at applicants training\n",
      "loss: 1.1870250701904297 at epoch 51 at applicants training\n",
      "loss: 1.186474084854126 at epoch 52 at applicants training\n",
      "loss: 1.1865184307098389 at epoch 53 at applicants training\n",
      "loss: 1.1859930753707886 at epoch 54 at applicants training\n",
      "loss: 1.1857246160507202 at epoch 55 at applicants training\n",
      "loss: 1.1849989891052246 at epoch 56 at applicants training\n",
      "loss: 1.1843278408050537 at epoch 57 at applicants training\n",
      "loss: 1.1834526062011719 at epoch 58 at applicants training\n",
      "loss: 1.1824020147323608 at epoch 59 at applicants training\n",
      "loss: 1.181463599205017 at epoch 60 at applicants training\n",
      "loss: 1.1803077459335327 at epoch 61 at applicants training\n",
      "loss: 1.1795042753219604 at epoch 62 at applicants training\n",
      "loss: 1.1784173250198364 at epoch 63 at applicants training\n",
      "loss: 1.177729606628418 at epoch 64 at applicants training\n",
      "loss: 1.1769206523895264 at epoch 65 at applicants training\n",
      "loss: 1.1764154434204102 at epoch 66 at applicants training\n",
      "loss: 1.1758818626403809 at epoch 67 at applicants training\n",
      "loss: 1.1755623817443848 at epoch 68 at applicants training\n",
      "loss: 1.1753383874893188 at epoch 69 at applicants training\n",
      "loss: 1.1751455068588257 at epoch 70 at applicants training\n",
      "loss: 1.1750568151474 at epoch 71 at applicants training\n",
      "loss: 1.1747944355010986 at epoch 72 at applicants training\n",
      "loss: 1.1746163368225098 at epoch 73 at applicants training\n",
      "loss: 1.174262285232544 at epoch 74 at applicants training\n",
      "loss: 1.1738938093185425 at epoch 75 at applicants training\n",
      "loss: 1.173604965209961 at epoch 76 at applicants training\n",
      "loss: 1.173283338546753 at epoch 77 at applicants training\n",
      "loss: 1.1730345487594604 at epoch 78 at applicants training\n",
      "loss: 1.1729788780212402 at epoch 79 at applicants training\n",
      "loss: 1.172913670539856 at epoch 80 at applicants training\n",
      "loss: 1.1727951765060425 at epoch 81 at applicants training\n",
      "loss: 1.1726034879684448 at epoch 82 at applicants training\n",
      "loss: 1.1723718643188477 at epoch 83 at applicants training\n",
      "loss: 1.1720898151397705 at epoch 84 at applicants training\n",
      "loss: 1.1718484163284302 at epoch 85 at applicants training\n",
      "loss: 1.171685814857483 at epoch 86 at applicants training\n",
      "loss: 1.1715949773788452 at epoch 87 at applicants training\n",
      "loss: 1.171517014503479 at epoch 88 at applicants training\n",
      "loss: 1.1714389324188232 at epoch 89 at applicants training\n",
      "loss: 1.1713473796844482 at epoch 90 at applicants training\n",
      "loss: 1.1712234020233154 at epoch 91 at applicants training\n",
      "loss: 1.1710726022720337 at epoch 92 at applicants training\n",
      "loss: 1.1709344387054443 at epoch 93 at applicants training\n",
      "loss: 1.1708152294158936 at epoch 94 at applicants training\n",
      "loss: 1.1707035303115845 at epoch 95 at applicants training\n",
      "loss: 1.1706085205078125 at epoch 96 at applicants training\n",
      "loss: 1.1705204248428345 at epoch 97 at applicants training\n",
      "loss: 1.1704272031784058 at epoch 98 at applicants training\n",
      "loss: 1.1703195571899414 at epoch 99 at applicants training\n",
      "loss: 1.7485630512237549 at epoch 0 at applicants training\n",
      "loss: 1.5839107036590576 at epoch 1 at applicants training\n",
      "loss: 1.6247966289520264 at epoch 2 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 3 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.735664963722229 at epoch 0 at applicants training\n",
      "loss: 1.6656394004821777 at epoch 1 at applicants training\n",
      "loss: 1.649806261062622 at epoch 2 at applicants training\n",
      "loss: 1.6552678346633911 at epoch 3 at applicants training\n",
      "loss: 1.659733772277832 at epoch 4 at applicants training\n",
      "loss: 1.6520752906799316 at epoch 5 at applicants training\n",
      "loss: 1.6302573680877686 at epoch 6 at applicants training\n",
      "loss: 1.5870444774627686 at epoch 7 at applicants training\n",
      "loss: 1.6022664308547974 at epoch 8 at applicants training\n",
      "loss: 1.614139437675476 at epoch 9 at applicants training\n",
      "loss: 1.5794490575790405 at epoch 10 at applicants training\n",
      "loss: 1.5222883224487305 at epoch 11 at applicants training\n",
      "loss: 1.5474494695663452 at epoch 12 at applicants training\n",
      "loss: 1.5162957906723022 at epoch 13 at applicants training\n",
      "loss: 1.5104765892028809 at epoch 14 at applicants training\n",
      "loss: 1.521023154258728 at epoch 15 at applicants training\n",
      "loss: 1.509323000907898 at epoch 16 at applicants training\n",
      "loss: 1.512742519378662 at epoch 17 at applicants training\n",
      "loss: 1.4914982318878174 at epoch 18 at applicants training\n",
      "loss: 1.4921391010284424 at epoch 19 at applicants training\n",
      "loss: 1.495078682899475 at epoch 20 at applicants training\n",
      "loss: 1.4890799522399902 at epoch 21 at applicants training\n",
      "loss: 1.4872599840164185 at epoch 22 at applicants training\n",
      "loss: 1.4860018491744995 at epoch 23 at applicants training\n",
      "loss: 1.4799102544784546 at epoch 24 at applicants training\n",
      "loss: 1.4803141355514526 at epoch 25 at applicants training\n",
      "loss: 1.476875901222229 at epoch 26 at applicants training\n",
      "loss: 1.4747370481491089 at epoch 27 at applicants training\n",
      "loss: 1.4754917621612549 at epoch 28 at applicants training\n",
      "loss: 1.4726392030715942 at epoch 29 at applicants training\n",
      "loss: 1.4716691970825195 at epoch 30 at applicants training\n",
      "loss: 1.4707146883010864 at epoch 31 at applicants training\n",
      "loss: 1.467474102973938 at epoch 32 at applicants training\n",
      "loss: 1.4662010669708252 at epoch 33 at applicants training\n",
      "loss: 1.4661015272140503 at epoch 34 at applicants training\n",
      "loss: 1.4638748168945312 at epoch 35 at applicants training\n",
      "loss: 1.4626291990280151 at epoch 36 at applicants training\n",
      "loss: 1.4629237651824951 at epoch 37 at applicants training\n",
      "loss: 1.460734248161316 at epoch 38 at applicants training\n",
      "loss: 1.4592602252960205 at epoch 39 at applicants training\n",
      "loss: 1.4585551023483276 at epoch 40 at applicants training\n",
      "loss: 1.4568012952804565 at epoch 41 at applicants training\n",
      "loss: 1.4556201696395874 at epoch 42 at applicants training\n",
      "loss: 1.4556699991226196 at epoch 43 at applicants training\n",
      "loss: 1.4551186561584473 at epoch 44 at applicants training\n",
      "loss: 1.4551945924758911 at epoch 45 at applicants training\n",
      "loss: 1.4543852806091309 at epoch 46 at applicants training\n",
      "loss: 1.4541813135147095 at epoch 47 at applicants training\n",
      "loss: 1.4534639120101929 at epoch 48 at applicants training\n",
      "loss: 1.4531210660934448 at epoch 49 at applicants training\n",
      "loss: 1.4525755643844604 at epoch 50 at applicants training\n",
      "loss: 1.4522017240524292 at epoch 51 at applicants training\n",
      "loss: 1.4518957138061523 at epoch 52 at applicants training\n",
      "loss: 1.4516475200653076 at epoch 53 at applicants training\n",
      "loss: 1.4516384601593018 at epoch 54 at applicants training\n",
      "loss: 1.4514127969741821 at epoch 55 at applicants training\n",
      "loss: 1.45139741897583 at epoch 56 at applicants training\n",
      "loss: 1.4510295391082764 at epoch 57 at applicants training\n",
      "loss: 1.4508614540100098 at epoch 58 at applicants training\n",
      "loss: 1.450423002243042 at epoch 59 at applicants training\n",
      "loss: 1.4502681493759155 at epoch 60 at applicants training\n",
      "loss: 1.4500340223312378 at epoch 61 at applicants training\n",
      "loss: 1.4500253200531006 at epoch 62 at applicants training\n",
      "loss: 1.4498960971832275 at epoch 63 at applicants training\n",
      "loss: 1.4499164819717407 at epoch 64 at applicants training\n",
      "loss: 1.44978928565979 at epoch 65 at applicants training\n",
      "loss: 1.449759840965271 at epoch 66 at applicants training\n",
      "loss: 1.449625849723816 at epoch 67 at applicants training\n",
      "loss: 1.4495954513549805 at epoch 68 at applicants training\n",
      "loss: 1.4495346546173096 at epoch 69 at applicants training\n",
      "loss: 1.4494765996932983 at epoch 70 at applicants training\n",
      "loss: 1.4493883848190308 at epoch 71 at applicants training\n",
      "loss: 1.4492868185043335 at epoch 72 at applicants training\n",
      "loss: 1.4492007493972778 at epoch 73 at applicants training\n",
      "loss: 1.4491122961044312 at epoch 74 at applicants training\n",
      "loss: 1.4490684270858765 at epoch 75 at applicants training\n",
      "loss: 1.4489978551864624 at epoch 76 at applicants training\n",
      "loss: 1.4489561319351196 at epoch 77 at applicants training\n",
      "loss: 1.4488600492477417 at epoch 78 at applicants training\n",
      "loss: 1.44879949092865 at epoch 79 at applicants training\n",
      "loss: 1.4487295150756836 at epoch 80 at applicants training\n",
      "loss: 1.4486829042434692 at epoch 81 at applicants training\n",
      "loss: 1.4486281871795654 at epoch 82 at applicants training\n",
      "loss: 1.4485652446746826 at epoch 83 at applicants training\n",
      "loss: 1.4485077857971191 at epoch 84 at applicants training\n",
      "loss: 1.4484329223632812 at epoch 85 at applicants training\n",
      "loss: 1.4483768939971924 at epoch 86 at applicants training\n",
      "loss: 1.4483195543289185 at epoch 87 at applicants training\n",
      "loss: 1.4482640027999878 at epoch 88 at applicants training\n",
      "loss: 1.4482084512710571 at epoch 89 at applicants training\n",
      "loss: 1.4481406211853027 at epoch 90 at applicants training\n",
      "loss: 1.4480819702148438 at epoch 91 at applicants training\n",
      "loss: 1.448026418685913 at epoch 92 at applicants training\n",
      "loss: 1.4479690790176392 at epoch 93 at applicants training\n",
      "loss: 1.4479159116744995 at epoch 94 at applicants training\n",
      "loss: 1.4478545188903809 at epoch 95 at applicants training\n",
      "loss: 1.447794795036316 at epoch 96 at applicants training\n",
      "loss: 1.4477298259735107 at epoch 97 at applicants training\n",
      "loss: 1.4476609230041504 at epoch 98 at applicants training\n",
      "loss: 1.4475990533828735 at epoch 99 at applicants training\n",
      "loss: 1.694939136505127 at epoch 0 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 1 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 2 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 3 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.7114334106445312 at epoch 0 at applicants training\n",
      "loss: 1.6744331121444702 at epoch 1 at applicants training\n",
      "loss: 1.67171049118042 at epoch 2 at applicants training\n",
      "loss: 1.6176817417144775 at epoch 3 at applicants training\n",
      "loss: 1.7067042589187622 at epoch 4 at applicants training\n",
      "loss: 1.631345272064209 at epoch 5 at applicants training\n",
      "loss: 1.618723750114441 at epoch 6 at applicants training\n",
      "loss: 1.6532973051071167 at epoch 7 at applicants training\n",
      "loss: 1.6527230739593506 at epoch 8 at applicants training\n",
      "loss: 1.6386291980743408 at epoch 9 at applicants training\n",
      "loss: 1.6033457517623901 at epoch 10 at applicants training\n",
      "loss: 1.5883804559707642 at epoch 11 at applicants training\n",
      "loss: 1.609362006187439 at epoch 12 at applicants training\n",
      "loss: 1.5624123811721802 at epoch 13 at applicants training\n",
      "loss: 1.604716181755066 at epoch 14 at applicants training\n",
      "loss: 1.606916069984436 at epoch 15 at applicants training\n",
      "loss: 1.621897578239441 at epoch 16 at applicants training\n",
      "loss: 1.6238834857940674 at epoch 17 at applicants training\n",
      "loss: 1.623530387878418 at epoch 18 at applicants training\n",
      "loss: 1.6169445514678955 at epoch 19 at applicants training\n",
      "loss: 1.6048426628112793 at epoch 20 at applicants training\n",
      "loss: 1.604793906211853 at epoch 21 at applicants training\n",
      "loss: 1.5898218154907227 at epoch 22 at applicants training\n",
      "loss: 1.5498770475387573 at epoch 23 at applicants training\n",
      "loss: 1.5565459728240967 at epoch 24 at applicants training\n",
      "loss: 1.5690734386444092 at epoch 25 at applicants training\n",
      "loss: 1.5723272562026978 at epoch 26 at applicants training\n",
      "loss: 1.5668678283691406 at epoch 27 at applicants training\n",
      "loss: 1.58384108543396 at epoch 28 at applicants training\n",
      "loss: 1.57501220703125 at epoch 29 at applicants training\n",
      "loss: 1.5652064085006714 at epoch 30 at applicants training\n",
      "loss: 1.5705868005752563 at epoch 31 at applicants training\n",
      "loss: 1.5700864791870117 at epoch 32 at applicants training\n",
      "loss: 1.565376877784729 at epoch 33 at applicants training\n",
      "loss: 1.5546443462371826 at epoch 34 at applicants training\n",
      "loss: 1.557420015335083 at epoch 35 at applicants training\n",
      "loss: 1.5614521503448486 at epoch 36 at applicants training\n",
      "loss: 1.5540802478790283 at epoch 37 at applicants training\n",
      "loss: 1.5513291358947754 at epoch 38 at applicants training\n",
      "loss: 1.544959306716919 at epoch 39 at applicants training\n",
      "loss: 1.5478113889694214 at epoch 40 at applicants training\n",
      "loss: 1.545356273651123 at epoch 41 at applicants training\n",
      "loss: 1.5426585674285889 at epoch 42 at applicants training\n",
      "loss: 1.5420455932617188 at epoch 43 at applicants training\n",
      "loss: 1.5430552959442139 at epoch 44 at applicants training\n",
      "loss: 1.5421884059906006 at epoch 45 at applicants training\n",
      "loss: 1.5440902709960938 at epoch 46 at applicants training\n",
      "loss: 1.5422042608261108 at epoch 47 at applicants training\n",
      "loss: 1.5421555042266846 at epoch 48 at applicants training\n",
      "loss: 1.5425587892532349 at epoch 49 at applicants training\n",
      "loss: 1.5424047708511353 at epoch 50 at applicants training\n",
      "loss: 1.5416728258132935 at epoch 51 at applicants training\n",
      "loss: 1.535975694656372 at epoch 52 at applicants training\n",
      "loss: 1.505330204963684 at epoch 53 at applicants training\n",
      "loss: 1.5026519298553467 at epoch 54 at applicants training\n",
      "loss: 1.4874677658081055 at epoch 55 at applicants training\n",
      "loss: 1.4732043743133545 at epoch 56 at applicants training\n",
      "loss: 1.4907792806625366 at epoch 57 at applicants training\n",
      "loss: 1.5095961093902588 at epoch 58 at applicants training\n",
      "loss: 1.4855895042419434 at epoch 59 at applicants training\n",
      "loss: 1.4549216032028198 at epoch 60 at applicants training\n",
      "loss: 1.4620057344436646 at epoch 61 at applicants training\n",
      "loss: 1.5006768703460693 at epoch 62 at applicants training\n",
      "loss: 1.5014338493347168 at epoch 63 at applicants training\n",
      "loss: 1.5170992612838745 at epoch 64 at applicants training\n",
      "loss: 1.5038076639175415 at epoch 65 at applicants training\n",
      "loss: 1.501287579536438 at epoch 66 at applicants training\n",
      "loss: 1.4945155382156372 at epoch 67 at applicants training\n",
      "loss: 1.4412635564804077 at epoch 68 at applicants training\n",
      "loss: 1.4521801471710205 at epoch 69 at applicants training\n",
      "loss: 1.465748906135559 at epoch 70 at applicants training\n",
      "loss: 1.4724653959274292 at epoch 71 at applicants training\n",
      "loss: 1.450209140777588 at epoch 72 at applicants training\n",
      "loss: 1.426236629486084 at epoch 73 at applicants training\n",
      "loss: 1.4675158262252808 at epoch 74 at applicants training\n",
      "loss: 1.4879045486450195 at epoch 75 at applicants training\n",
      "loss: 1.5001182556152344 at epoch 76 at applicants training\n",
      "loss: 1.4814963340759277 at epoch 77 at applicants training\n",
      "loss: 1.4626226425170898 at epoch 78 at applicants training\n",
      "loss: 1.420844316482544 at epoch 79 at applicants training\n",
      "loss: 1.4333235025405884 at epoch 80 at applicants training\n",
      "loss: 1.4585374593734741 at epoch 81 at applicants training\n",
      "loss: 1.4605706930160522 at epoch 82 at applicants training\n",
      "loss: 1.44024658203125 at epoch 83 at applicants training\n",
      "loss: 1.4220855236053467 at epoch 84 at applicants training\n",
      "loss: 1.4172056913375854 at epoch 85 at applicants training\n",
      "loss: 1.431624174118042 at epoch 86 at applicants training\n",
      "loss: 1.4159026145935059 at epoch 87 at applicants training\n",
      "loss: 1.4161568880081177 at epoch 88 at applicants training\n",
      "loss: 1.4238871335983276 at epoch 89 at applicants training\n",
      "loss: 1.4254753589630127 at epoch 90 at applicants training\n",
      "loss: 1.4241200685501099 at epoch 91 at applicants training\n",
      "loss: 1.4230860471725464 at epoch 92 at applicants training\n",
      "loss: 1.4200217723846436 at epoch 93 at applicants training\n",
      "loss: 1.4037381410598755 at epoch 94 at applicants training\n",
      "loss: 1.4095383882522583 at epoch 95 at applicants training\n",
      "loss: 1.4077228307724 at epoch 96 at applicants training\n",
      "loss: 1.4038324356079102 at epoch 97 at applicants training\n",
      "loss: 1.406876564025879 at epoch 98 at applicants training\n",
      "loss: 1.4076013565063477 at epoch 99 at applicants training\n",
      "loss: 1.7216218709945679 at epoch 0 at applicants training\n",
      "loss: 1.7031749486923218 at epoch 1 at applicants training\n",
      "loss: 1.6458183526992798 at epoch 2 at applicants training\n",
      "loss: 1.653378963470459 at epoch 3 at applicants training\n",
      "loss: 1.6220953464508057 at epoch 4 at applicants training\n",
      "loss: 1.633059024810791 at epoch 5 at applicants training\n",
      "loss: 1.5820684432983398 at epoch 6 at applicants training\n",
      "loss: 1.583767056465149 at epoch 7 at applicants training\n",
      "loss: 1.5452873706817627 at epoch 8 at applicants training\n",
      "loss: 1.534756064414978 at epoch 9 at applicants training\n",
      "loss: 1.5171815156936646 at epoch 10 at applicants training\n",
      "loss: 1.4893685579299927 at epoch 11 at applicants training\n",
      "loss: 1.50858473777771 at epoch 12 at applicants training\n",
      "loss: 1.4815051555633545 at epoch 13 at applicants training\n",
      "loss: 1.4602857828140259 at epoch 14 at applicants training\n",
      "loss: 1.4696240425109863 at epoch 15 at applicants training\n",
      "loss: 1.4522281885147095 at epoch 16 at applicants training\n",
      "loss: 1.4213016033172607 at epoch 17 at applicants training\n",
      "loss: 1.4341938495635986 at epoch 18 at applicants training\n",
      "loss: 1.4163154363632202 at epoch 19 at applicants training\n",
      "loss: 1.4159376621246338 at epoch 20 at applicants training\n",
      "loss: 1.4002964496612549 at epoch 21 at applicants training\n",
      "loss: 1.3930913209915161 at epoch 22 at applicants training\n",
      "loss: 1.3977563381195068 at epoch 23 at applicants training\n",
      "loss: 1.3866597414016724 at epoch 24 at applicants training\n",
      "loss: 1.3822346925735474 at epoch 25 at applicants training\n",
      "loss: 1.3731634616851807 at epoch 26 at applicants training\n",
      "loss: 1.3793259859085083 at epoch 27 at applicants training\n",
      "loss: 1.3729732036590576 at epoch 28 at applicants training\n",
      "loss: 1.371046543121338 at epoch 29 at applicants training\n",
      "loss: 1.3666120767593384 at epoch 30 at applicants training\n",
      "loss: 1.3728632926940918 at epoch 31 at applicants training\n",
      "loss: 1.3664746284484863 at epoch 32 at applicants training\n",
      "loss: 1.3669025897979736 at epoch 33 at applicants training\n",
      "loss: 1.3626524209976196 at epoch 34 at applicants training\n",
      "loss: 1.3655858039855957 at epoch 35 at applicants training\n",
      "loss: 1.3653641939163208 at epoch 36 at applicants training\n",
      "loss: 1.3644694089889526 at epoch 37 at applicants training\n",
      "loss: 1.3625590801239014 at epoch 38 at applicants training\n",
      "loss: 1.3624868392944336 at epoch 39 at applicants training\n",
      "loss: 1.3621625900268555 at epoch 40 at applicants training\n",
      "loss: 1.3630471229553223 at epoch 41 at applicants training\n",
      "loss: 1.36112380027771 at epoch 42 at applicants training\n",
      "loss: 1.360701084136963 at epoch 43 at applicants training\n",
      "loss: 1.3601478338241577 at epoch 44 at applicants training\n",
      "loss: 1.3607922792434692 at epoch 45 at applicants training\n",
      "loss: 1.360346794128418 at epoch 46 at applicants training\n",
      "loss: 1.3599748611450195 at epoch 47 at applicants training\n",
      "loss: 1.359474778175354 at epoch 48 at applicants training\n",
      "loss: 1.3589565753936768 at epoch 49 at applicants training\n",
      "loss: 1.3592785596847534 at epoch 50 at applicants training\n",
      "loss: 1.3592356443405151 at epoch 51 at applicants training\n",
      "loss: 1.358888864517212 at epoch 52 at applicants training\n",
      "loss: 1.3582507371902466 at epoch 53 at applicants training\n",
      "loss: 1.358086109161377 at epoch 54 at applicants training\n",
      "loss: 1.3580467700958252 at epoch 55 at applicants training\n",
      "loss: 1.3578437566757202 at epoch 56 at applicants training\n",
      "loss: 1.3575118780136108 at epoch 57 at applicants training\n",
      "loss: 1.3570640087127686 at epoch 58 at applicants training\n",
      "loss: 1.3567636013031006 at epoch 59 at applicants training\n",
      "loss: 1.356698751449585 at epoch 60 at applicants training\n",
      "loss: 1.3564202785491943 at epoch 61 at applicants training\n",
      "loss: 1.3558831214904785 at epoch 62 at applicants training\n",
      "loss: 1.3555288314819336 at epoch 63 at applicants training\n",
      "loss: 1.3553614616394043 at epoch 64 at applicants training\n",
      "loss: 1.3550934791564941 at epoch 65 at applicants training\n",
      "loss: 1.3548269271850586 at epoch 66 at applicants training\n",
      "loss: 1.354819655418396 at epoch 67 at applicants training\n",
      "loss: 1.3547297716140747 at epoch 68 at applicants training\n",
      "loss: 1.354541301727295 at epoch 69 at applicants training\n",
      "loss: 1.3544658422470093 at epoch 70 at applicants training\n",
      "loss: 1.3542556762695312 at epoch 71 at applicants training\n",
      "loss: 1.35398268699646 at epoch 72 at applicants training\n",
      "loss: 1.3538626432418823 at epoch 73 at applicants training\n",
      "loss: 1.3536344766616821 at epoch 74 at applicants training\n",
      "loss: 1.353561520576477 at epoch 75 at applicants training\n",
      "loss: 1.3535277843475342 at epoch 76 at applicants training\n",
      "loss: 1.3533904552459717 at epoch 77 at applicants training\n",
      "loss: 1.353346347808838 at epoch 78 at applicants training\n",
      "loss: 1.3531934022903442 at epoch 79 at applicants training\n",
      "loss: 1.3530436754226685 at epoch 80 at applicants training\n",
      "loss: 1.3529173135757446 at epoch 81 at applicants training\n",
      "loss: 1.3527377843856812 at epoch 82 at applicants training\n",
      "loss: 1.3526517152786255 at epoch 83 at applicants training\n",
      "loss: 1.3525245189666748 at epoch 84 at applicants training\n",
      "loss: 1.352447509765625 at epoch 85 at applicants training\n",
      "loss: 1.3523412942886353 at epoch 86 at applicants training\n",
      "loss: 1.3522204160690308 at epoch 87 at applicants training\n",
      "loss: 1.352104902267456 at epoch 88 at applicants training\n",
      "loss: 1.3519668579101562 at epoch 89 at applicants training\n",
      "loss: 1.3518637418746948 at epoch 90 at applicants training\n",
      "loss: 1.3517321348190308 at epoch 91 at applicants training\n",
      "loss: 1.3516420125961304 at epoch 92 at applicants training\n",
      "loss: 1.351523756980896 at epoch 93 at applicants training\n",
      "loss: 1.3514304161071777 at epoch 94 at applicants training\n",
      "loss: 1.3513035774230957 at epoch 95 at applicants training\n",
      "loss: 1.3511946201324463 at epoch 96 at applicants training\n",
      "loss: 1.3510593175888062 at epoch 97 at applicants training\n",
      "loss: 1.3509331941604614 at epoch 98 at applicants training\n",
      "loss: 1.3507757186889648 at epoch 99 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 0 at applicants training\n",
      "loss: 1.6655977964401245 at epoch 1 at applicants training\n",
      "loss: 1.574109673500061 at epoch 2 at applicants training\n",
      "loss: 1.5964913368225098 at epoch 3 at applicants training\n",
      "loss: 1.5536538362503052 at epoch 4 at applicants training\n",
      "loss: 1.4930508136749268 at epoch 5 at applicants training\n",
      "loss: 1.5013465881347656 at epoch 6 at applicants training\n",
      "loss: 1.4591037034988403 at epoch 7 at applicants training\n",
      "loss: 1.490179419517517 at epoch 8 at applicants training\n",
      "loss: 1.4322127103805542 at epoch 9 at applicants training\n",
      "loss: 1.4513680934906006 at epoch 10 at applicants training\n",
      "loss: 1.4419703483581543 at epoch 11 at applicants training\n",
      "loss: 1.4185833930969238 at epoch 12 at applicants training\n",
      "loss: 1.4581011533737183 at epoch 13 at applicants training\n",
      "loss: 1.412617802619934 at epoch 14 at applicants training\n",
      "loss: 1.4309042692184448 at epoch 15 at applicants training\n",
      "loss: 1.4346956014633179 at epoch 16 at applicants training\n",
      "loss: 1.4106336832046509 at epoch 17 at applicants training\n",
      "loss: 1.4241403341293335 at epoch 18 at applicants training\n",
      "loss: 1.419846773147583 at epoch 19 at applicants training\n",
      "loss: 1.402663230895996 at epoch 20 at applicants training\n",
      "loss: 1.4276154041290283 at epoch 21 at applicants training\n",
      "loss: 1.4051512479782104 at epoch 22 at applicants training\n",
      "loss: 1.4047430753707886 at epoch 23 at applicants training\n",
      "loss: 1.4199436902999878 at epoch 24 at applicants training\n",
      "loss: 1.4048341512680054 at epoch 25 at applicants training\n",
      "loss: 1.401511788368225 at epoch 26 at applicants training\n",
      "loss: 1.4126954078674316 at epoch 27 at applicants training\n",
      "loss: 1.4025168418884277 at epoch 28 at applicants training\n",
      "loss: 1.401526689529419 at epoch 29 at applicants training\n",
      "loss: 1.4060416221618652 at epoch 30 at applicants training\n",
      "loss: 1.405623435974121 at epoch 31 at applicants training\n",
      "loss: 1.401286005973816 at epoch 32 at applicants training\n",
      "loss: 1.4012125730514526 at epoch 33 at applicants training\n",
      "loss: 1.4049203395843506 at epoch 34 at applicants training\n",
      "loss: 1.4016717672348022 at epoch 35 at applicants training\n",
      "loss: 1.40016508102417 at epoch 36 at applicants training\n",
      "loss: 1.4022160768508911 at epoch 37 at applicants training\n",
      "loss: 1.402848243713379 at epoch 38 at applicants training\n",
      "loss: 1.4013618230819702 at epoch 39 at applicants training\n",
      "loss: 1.3997362852096558 at epoch 40 at applicants training\n",
      "loss: 1.4015138149261475 at epoch 41 at applicants training\n",
      "loss: 1.4013910293579102 at epoch 42 at applicants training\n",
      "loss: 1.3995579481124878 at epoch 43 at applicants training\n",
      "loss: 1.4006913900375366 at epoch 44 at applicants training\n",
      "loss: 1.4011327028274536 at epoch 45 at applicants training\n",
      "loss: 1.3997379541397095 at epoch 46 at applicants training\n",
      "loss: 1.399884819984436 at epoch 47 at applicants training\n",
      "loss: 1.4006634950637817 at epoch 48 at applicants training\n",
      "loss: 1.3993093967437744 at epoch 49 at applicants training\n",
      "loss: 1.3998541831970215 at epoch 50 at applicants training\n",
      "loss: 1.400122880935669 at epoch 51 at applicants training\n",
      "loss: 1.3990446329116821 at epoch 52 at applicants training\n",
      "loss: 1.3998392820358276 at epoch 53 at applicants training\n",
      "loss: 1.3995420932769775 at epoch 54 at applicants training\n",
      "loss: 1.3990861177444458 at epoch 55 at applicants training\n",
      "loss: 1.3996754884719849 at epoch 56 at applicants training\n",
      "loss: 1.39899480342865 at epoch 57 at applicants training\n",
      "loss: 1.3993234634399414 at epoch 58 at applicants training\n",
      "loss: 1.3992795944213867 at epoch 59 at applicants training\n",
      "loss: 1.3988995552062988 at epoch 60 at applicants training\n",
      "loss: 1.3992812633514404 at epoch 61 at applicants training\n",
      "loss: 1.3988335132598877 at epoch 62 at applicants training\n",
      "loss: 1.399034023284912 at epoch 63 at applicants training\n",
      "loss: 1.3989332914352417 at epoch 64 at applicants training\n",
      "loss: 1.3987550735473633 at epoch 65 at applicants training\n",
      "loss: 1.3989239931106567 at epoch 66 at applicants training\n",
      "loss: 1.3986140489578247 at epoch 67 at applicants training\n",
      "loss: 1.3988032341003418 at epoch 68 at applicants training\n",
      "loss: 1.3985806703567505 at epoch 69 at applicants training\n",
      "loss: 1.3986210823059082 at epoch 70 at applicants training\n",
      "loss: 1.3985488414764404 at epoch 71 at applicants training\n",
      "loss: 1.3984472751617432 at epoch 72 at applicants training\n",
      "loss: 1.398483157157898 at epoch 73 at applicants training\n",
      "loss: 1.398319125175476 at epoch 74 at applicants training\n",
      "loss: 1.3983949422836304 at epoch 75 at applicants training\n",
      "loss: 1.3982468843460083 at epoch 76 at applicants training\n",
      "loss: 1.3983244895935059 at epoch 77 at applicants training\n",
      "loss: 1.3982057571411133 at epoch 78 at applicants training\n",
      "loss: 1.398264765739441 at epoch 79 at applicants training\n",
      "loss: 1.3981748819351196 at epoch 80 at applicants training\n",
      "loss: 1.3982168436050415 at epoch 81 at applicants training\n",
      "loss: 1.3981366157531738 at epoch 82 at applicants training\n",
      "loss: 1.3981688022613525 at epoch 83 at applicants training\n",
      "loss: 1.3980989456176758 at epoch 84 at applicants training\n",
      "loss: 1.398128628730774 at epoch 85 at applicants training\n",
      "loss: 1.3980629444122314 at epoch 86 at applicants training\n",
      "loss: 1.39809250831604 at epoch 87 at applicants training\n",
      "loss: 1.3980357646942139 at epoch 88 at applicants training\n",
      "loss: 1.3980597257614136 at epoch 89 at applicants training\n",
      "loss: 1.3980144262313843 at epoch 90 at applicants training\n",
      "loss: 1.398024559020996 at epoch 91 at applicants training\n",
      "loss: 1.3979915380477905 at epoch 92 at applicants training\n",
      "loss: 1.3979849815368652 at epoch 93 at applicants training\n",
      "loss: 1.397964358329773 at epoch 94 at applicants training\n",
      "loss: 1.3979432582855225 at epoch 95 at applicants training\n",
      "loss: 1.3979357481002808 at epoch 96 at applicants training\n",
      "loss: 1.3979077339172363 at epoch 97 at applicants training\n",
      "loss: 1.3979079723358154 at epoch 98 at applicants training\n",
      "loss: 1.3978811502456665 at epoch 99 at applicants training\n",
      "loss: 1.6743031740188599 at epoch 0 at applicants training\n",
      "loss: 1.7002918720245361 at epoch 1 at applicants training\n",
      "loss: 1.599564790725708 at epoch 2 at applicants training\n",
      "loss: 1.6136441230773926 at epoch 3 at applicants training\n",
      "loss: 1.5617495775222778 at epoch 4 at applicants training\n",
      "loss: 1.574174165725708 at epoch 5 at applicants training\n",
      "loss: 1.6132742166519165 at epoch 6 at applicants training\n",
      "loss: 1.6247951984405518 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.6240971088409424 at epoch 0 at applicants training\n",
      "loss: 1.5735398530960083 at epoch 1 at applicants training\n",
      "loss: 1.588232398033142 at epoch 2 at applicants training\n",
      "loss: 1.5249823331832886 at epoch 3 at applicants training\n",
      "loss: 1.540337085723877 at epoch 4 at applicants training\n",
      "loss: 1.4901108741760254 at epoch 5 at applicants training\n",
      "loss: 1.493222951889038 at epoch 6 at applicants training\n",
      "loss: 1.5005738735198975 at epoch 7 at applicants training\n",
      "loss: 1.4683433771133423 at epoch 8 at applicants training\n",
      "loss: 1.4577767848968506 at epoch 9 at applicants training\n",
      "loss: 1.4785152673721313 at epoch 10 at applicants training\n",
      "loss: 1.4517024755477905 at epoch 11 at applicants training\n",
      "loss: 1.4409186840057373 at epoch 12 at applicants training\n",
      "loss: 1.4510908126831055 at epoch 13 at applicants training\n",
      "loss: 1.4416842460632324 at epoch 14 at applicants training\n",
      "loss: 1.4191217422485352 at epoch 15 at applicants training\n",
      "loss: 1.4380097389221191 at epoch 16 at applicants training\n",
      "loss: 1.4247301816940308 at epoch 17 at applicants training\n",
      "loss: 1.4158663749694824 at epoch 18 at applicants training\n",
      "loss: 1.4274237155914307 at epoch 19 at applicants training\n",
      "loss: 1.4151506423950195 at epoch 20 at applicants training\n",
      "loss: 1.4070810079574585 at epoch 21 at applicants training\n",
      "loss: 1.415666937828064 at epoch 22 at applicants training\n",
      "loss: 1.3883212804794312 at epoch 23 at applicants training\n",
      "loss: 1.374143123626709 at epoch 24 at applicants training\n",
      "loss: 1.3813143968582153 at epoch 25 at applicants training\n",
      "loss: 1.3424746990203857 at epoch 26 at applicants training\n",
      "loss: 1.3406147956848145 at epoch 27 at applicants training\n",
      "loss: 1.3240100145339966 at epoch 28 at applicants training\n",
      "loss: 1.2983778715133667 at epoch 29 at applicants training\n",
      "loss: 1.2816858291625977 at epoch 30 at applicants training\n",
      "loss: 1.271538257598877 at epoch 31 at applicants training\n",
      "loss: 1.2479045391082764 at epoch 32 at applicants training\n",
      "loss: 1.2341816425323486 at epoch 33 at applicants training\n",
      "loss: 1.2236428260803223 at epoch 34 at applicants training\n",
      "loss: 1.2194503545761108 at epoch 35 at applicants training\n",
      "loss: 1.2145686149597168 at epoch 36 at applicants training\n",
      "loss: 1.2089412212371826 at epoch 37 at applicants training\n",
      "loss: 1.2012028694152832 at epoch 38 at applicants training\n",
      "loss: 1.1968529224395752 at epoch 39 at applicants training\n",
      "loss: 1.1924262046813965 at epoch 40 at applicants training\n",
      "loss: 1.1906687021255493 at epoch 41 at applicants training\n",
      "loss: 1.1863343715667725 at epoch 42 at applicants training\n",
      "loss: 1.1844509840011597 at epoch 43 at applicants training\n",
      "loss: 1.179479956626892 at epoch 44 at applicants training\n",
      "loss: 1.178637146949768 at epoch 45 at applicants training\n",
      "loss: 1.1700658798217773 at epoch 46 at applicants training\n",
      "loss: 1.1675400733947754 at epoch 47 at applicants training\n",
      "loss: 1.1663988828659058 at epoch 48 at applicants training\n",
      "loss: 1.1630449295043945 at epoch 49 at applicants training\n",
      "loss: 1.1625937223434448 at epoch 50 at applicants training\n",
      "loss: 1.158433198928833 at epoch 51 at applicants training\n",
      "loss: 1.156579852104187 at epoch 52 at applicants training\n",
      "loss: 1.1563200950622559 at epoch 53 at applicants training\n",
      "loss: 1.1542174816131592 at epoch 54 at applicants training\n",
      "loss: 1.1548606157302856 at epoch 55 at applicants training\n",
      "loss: 1.1532635688781738 at epoch 56 at applicants training\n",
      "loss: 1.1525259017944336 at epoch 57 at applicants training\n",
      "loss: 1.1517329216003418 at epoch 58 at applicants training\n",
      "loss: 1.1502748727798462 at epoch 59 at applicants training\n",
      "loss: 1.1504110097885132 at epoch 60 at applicants training\n",
      "loss: 1.1503443717956543 at epoch 61 at applicants training\n",
      "loss: 1.149806022644043 at epoch 62 at applicants training\n",
      "loss: 1.1491420269012451 at epoch 63 at applicants training\n",
      "loss: 1.1482008695602417 at epoch 64 at applicants training\n",
      "loss: 1.1473428010940552 at epoch 65 at applicants training\n",
      "loss: 1.1467530727386475 at epoch 66 at applicants training\n",
      "loss: 1.1461243629455566 at epoch 67 at applicants training\n",
      "loss: 1.1459676027297974 at epoch 68 at applicants training\n",
      "loss: 1.1456223726272583 at epoch 69 at applicants training\n",
      "loss: 1.1451938152313232 at epoch 70 at applicants training\n",
      "loss: 1.1448736190795898 at epoch 71 at applicants training\n",
      "loss: 1.1443047523498535 at epoch 72 at applicants training\n",
      "loss: 1.144112467765808 at epoch 73 at applicants training\n",
      "loss: 1.1438348293304443 at epoch 74 at applicants training\n",
      "loss: 1.1435258388519287 at epoch 75 at applicants training\n",
      "loss: 1.1433862447738647 at epoch 76 at applicants training\n",
      "loss: 1.143074631690979 at epoch 77 at applicants training\n",
      "loss: 1.1429071426391602 at epoch 78 at applicants training\n",
      "loss: 1.142646074295044 at epoch 79 at applicants training\n",
      "loss: 1.1423667669296265 at epoch 80 at applicants training\n",
      "loss: 1.1422581672668457 at epoch 81 at applicants training\n",
      "loss: 1.1420305967330933 at epoch 82 at applicants training\n",
      "loss: 1.1419087648391724 at epoch 83 at applicants training\n",
      "loss: 1.1417179107666016 at epoch 84 at applicants training\n",
      "loss: 1.1415610313415527 at epoch 85 at applicants training\n",
      "loss: 1.1414005756378174 at epoch 86 at applicants training\n",
      "loss: 1.1412242650985718 at epoch 87 at applicants training\n",
      "loss: 1.1411161422729492 at epoch 88 at applicants training\n",
      "loss: 1.1410101652145386 at epoch 89 at applicants training\n",
      "loss: 1.14080810546875 at epoch 90 at applicants training\n",
      "loss: 1.1406606435775757 at epoch 91 at applicants training\n",
      "loss: 1.1405662298202515 at epoch 92 at applicants training\n",
      "loss: 1.1404613256454468 at epoch 93 at applicants training\n",
      "loss: 1.1403969526290894 at epoch 94 at applicants training\n",
      "loss: 1.140316128730774 at epoch 95 at applicants training\n",
      "loss: 1.140222430229187 at epoch 96 at applicants training\n",
      "loss: 1.1401374340057373 at epoch 97 at applicants training\n",
      "loss: 1.1400532722473145 at epoch 98 at applicants training\n",
      "loss: 1.1399791240692139 at epoch 99 at applicants training\n",
      "loss: 1.798575758934021 at epoch 0 at applicants training\n",
      "loss: 1.6748335361480713 at epoch 1 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 2 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 3 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 4 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 5 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 6 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 7 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 8 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 9 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 10 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 11 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 12 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 13 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 14 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 15 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 16 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 17 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 18 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 19 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 20 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 21 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 22 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 23 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 24 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 25 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 26 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 27 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 28 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 29 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 30 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 31 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 32 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 33 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 34 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 35 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 36 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 37 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 38 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 39 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 40 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 41 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 42 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 43 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 44 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 45 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 46 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 47 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 48 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 49 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 50 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 51 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 52 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 53 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 54 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 55 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 56 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 57 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 58 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 59 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 60 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 61 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 62 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 63 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 64 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 65 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 66 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 67 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 68 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 69 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 70 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 71 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 72 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 73 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 74 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 75 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 76 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 77 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 78 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 79 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 80 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 81 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 82 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 83 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 84 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 85 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 86 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 87 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 88 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 89 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 90 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 91 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 92 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 93 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 94 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 95 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 96 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 97 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 98 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 99 at applicants training\n",
      "loss: 1.6272647380828857 at epoch 0 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 1 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 2 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 3 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.7361220121383667 at epoch 0 at applicants training\n",
      "loss: 1.7106869220733643 at epoch 1 at applicants training\n",
      "loss: 1.6942176818847656 at epoch 2 at applicants training\n",
      "loss: 1.657312035560608 at epoch 3 at applicants training\n",
      "loss: 1.6635737419128418 at epoch 4 at applicants training\n",
      "loss: 1.675146460533142 at epoch 5 at applicants training\n",
      "loss: 1.6334043741226196 at epoch 6 at applicants training\n",
      "loss: 1.5774235725402832 at epoch 7 at applicants training\n",
      "loss: 1.5779123306274414 at epoch 8 at applicants training\n",
      "loss: 1.5457658767700195 at epoch 9 at applicants training\n",
      "loss: 1.5343362092971802 at epoch 10 at applicants training\n",
      "loss: 1.5253580808639526 at epoch 11 at applicants training\n",
      "loss: 1.4982463121414185 at epoch 12 at applicants training\n",
      "loss: 1.4908630847930908 at epoch 13 at applicants training\n",
      "loss: 1.468896508216858 at epoch 14 at applicants training\n",
      "loss: 1.4834288358688354 at epoch 15 at applicants training\n",
      "loss: 1.4547711610794067 at epoch 16 at applicants training\n",
      "loss: 1.4496935606002808 at epoch 17 at applicants training\n",
      "loss: 1.4349360466003418 at epoch 18 at applicants training\n",
      "loss: 1.4351849555969238 at epoch 19 at applicants training\n",
      "loss: 1.4183725118637085 at epoch 20 at applicants training\n",
      "loss: 1.4097239971160889 at epoch 21 at applicants training\n",
      "loss: 1.390889286994934 at epoch 22 at applicants training\n",
      "loss: 1.370185375213623 at epoch 23 at applicants training\n",
      "loss: 1.3862462043762207 at epoch 24 at applicants training\n",
      "loss: 1.3599199056625366 at epoch 25 at applicants training\n",
      "loss: 1.368177890777588 at epoch 26 at applicants training\n",
      "loss: 1.3432315587997437 at epoch 27 at applicants training\n",
      "loss: 1.3592382669448853 at epoch 28 at applicants training\n",
      "loss: 1.334051251411438 at epoch 29 at applicants training\n",
      "loss: 1.359581708908081 at epoch 30 at applicants training\n",
      "loss: 1.3344407081604004 at epoch 31 at applicants training\n",
      "loss: 1.3293027877807617 at epoch 32 at applicants training\n",
      "loss: 1.33632230758667 at epoch 33 at applicants training\n",
      "loss: 1.3155173063278198 at epoch 34 at applicants training\n",
      "loss: 1.3208339214324951 at epoch 35 at applicants training\n",
      "loss: 1.3098582029342651 at epoch 36 at applicants training\n",
      "loss: 1.317156195640564 at epoch 37 at applicants training\n",
      "loss: 1.3140923976898193 at epoch 38 at applicants training\n",
      "loss: 1.3019996881484985 at epoch 39 at applicants training\n",
      "loss: 1.3086241483688354 at epoch 40 at applicants training\n",
      "loss: 1.3029427528381348 at epoch 41 at applicants training\n",
      "loss: 1.2997609376907349 at epoch 42 at applicants training\n",
      "loss: 1.294662356376648 at epoch 43 at applicants training\n",
      "loss: 1.2925647497177124 at epoch 44 at applicants training\n",
      "loss: 1.293267846107483 at epoch 45 at applicants training\n",
      "loss: 1.2868729829788208 at epoch 46 at applicants training\n",
      "loss: 1.2869670391082764 at epoch 47 at applicants training\n",
      "loss: 1.2846015691757202 at epoch 48 at applicants training\n",
      "loss: 1.286475658416748 at epoch 49 at applicants training\n",
      "loss: 1.282807469367981 at epoch 50 at applicants training\n",
      "loss: 1.2831977605819702 at epoch 51 at applicants training\n",
      "loss: 1.281645655632019 at epoch 52 at applicants training\n",
      "loss: 1.2826542854309082 at epoch 53 at applicants training\n",
      "loss: 1.281762719154358 at epoch 54 at applicants training\n",
      "loss: 1.280745267868042 at epoch 55 at applicants training\n",
      "loss: 1.2800002098083496 at epoch 56 at applicants training\n",
      "loss: 1.278679609298706 at epoch 57 at applicants training\n",
      "loss: 1.2798030376434326 at epoch 58 at applicants training\n",
      "loss: 1.2773480415344238 at epoch 59 at applicants training\n",
      "loss: 1.2767143249511719 at epoch 60 at applicants training\n",
      "loss: 1.2761626243591309 at epoch 61 at applicants training\n",
      "loss: 1.2744609117507935 at epoch 62 at applicants training\n",
      "loss: 1.2741026878356934 at epoch 63 at applicants training\n",
      "loss: 1.2735284566879272 at epoch 64 at applicants training\n",
      "loss: 1.2727471590042114 at epoch 65 at applicants training\n",
      "loss: 1.272433876991272 at epoch 66 at applicants training\n",
      "loss: 1.2724888324737549 at epoch 67 at applicants training\n",
      "loss: 1.2717325687408447 at epoch 68 at applicants training\n",
      "loss: 1.2718020677566528 at epoch 69 at applicants training\n",
      "loss: 1.271776795387268 at epoch 70 at applicants training\n",
      "loss: 1.2711913585662842 at epoch 71 at applicants training\n",
      "loss: 1.2712986469268799 at epoch 72 at applicants training\n",
      "loss: 1.271114706993103 at epoch 73 at applicants training\n",
      "loss: 1.2706819772720337 at epoch 74 at applicants training\n",
      "loss: 1.2707107067108154 at epoch 75 at applicants training\n",
      "loss: 1.2704846858978271 at epoch 76 at applicants training\n",
      "loss: 1.270165205001831 at epoch 77 at applicants training\n",
      "loss: 1.2702237367630005 at epoch 78 at applicants training\n",
      "loss: 1.2699143886566162 at epoch 79 at applicants training\n",
      "loss: 1.2696598768234253 at epoch 80 at applicants training\n",
      "loss: 1.2654883861541748 at epoch 81 at applicants training\n",
      "loss: 1.2509499788284302 at epoch 82 at applicants training\n",
      "loss: 1.2413256168365479 at epoch 83 at applicants training\n",
      "loss: 1.2569066286087036 at epoch 84 at applicants training\n",
      "loss: 1.219802737236023 at epoch 85 at applicants training\n",
      "loss: 1.2174218893051147 at epoch 86 at applicants training\n",
      "loss: 1.2166192531585693 at epoch 87 at applicants training\n",
      "loss: 1.1895473003387451 at epoch 88 at applicants training\n",
      "loss: 1.1823830604553223 at epoch 89 at applicants training\n",
      "loss: 1.1829249858856201 at epoch 90 at applicants training\n",
      "loss: 1.164772391319275 at epoch 91 at applicants training\n",
      "loss: 1.1501842737197876 at epoch 92 at applicants training\n",
      "loss: 1.1377415657043457 at epoch 93 at applicants training\n",
      "loss: 1.1243993043899536 at epoch 94 at applicants training\n",
      "loss: 1.1202630996704102 at epoch 95 at applicants training\n",
      "loss: 1.108566164970398 at epoch 96 at applicants training\n",
      "loss: 1.0878081321716309 at epoch 97 at applicants training\n",
      "loss: 1.0842328071594238 at epoch 98 at applicants training\n",
      "loss: 1.0656523704528809 at epoch 99 at applicants training\n",
      "loss: 1.624659776687622 at epoch 0 at applicants training\n",
      "loss: 1.6554282903671265 at epoch 1 at applicants training\n",
      "loss: 1.6011455059051514 at epoch 2 at applicants training\n",
      "loss: 1.6162985563278198 at epoch 3 at applicants training\n",
      "loss: 1.5747452974319458 at epoch 4 at applicants training\n",
      "loss: 1.5719382762908936 at epoch 5 at applicants training\n",
      "loss: 1.5690855979919434 at epoch 6 at applicants training\n",
      "loss: 1.5419776439666748 at epoch 7 at applicants training\n",
      "loss: 1.5261034965515137 at epoch 8 at applicants training\n",
      "loss: 1.5148053169250488 at epoch 9 at applicants training\n",
      "loss: 1.496649146080017 at epoch 10 at applicants training\n",
      "loss: 1.488118290901184 at epoch 11 at applicants training\n",
      "loss: 1.487305760383606 at epoch 12 at applicants training\n",
      "loss: 1.4638662338256836 at epoch 13 at applicants training\n",
      "loss: 1.4581314325332642 at epoch 14 at applicants training\n",
      "loss: 1.460316777229309 at epoch 15 at applicants training\n",
      "loss: 1.4508559703826904 at epoch 16 at applicants training\n",
      "loss: 1.4414411783218384 at epoch 17 at applicants training\n",
      "loss: 1.4413514137268066 at epoch 18 at applicants training\n",
      "loss: 1.4418420791625977 at epoch 19 at applicants training\n",
      "loss: 1.439470648765564 at epoch 20 at applicants training\n",
      "loss: 1.4324333667755127 at epoch 21 at applicants training\n",
      "loss: 1.431378960609436 at epoch 22 at applicants training\n",
      "loss: 1.430577278137207 at epoch 23 at applicants training\n",
      "loss: 1.4247249364852905 at epoch 24 at applicants training\n",
      "loss: 1.4245097637176514 at epoch 25 at applicants training\n",
      "loss: 1.4221904277801514 at epoch 26 at applicants training\n",
      "loss: 1.4187507629394531 at epoch 27 at applicants training\n",
      "loss: 1.4132713079452515 at epoch 28 at applicants training\n",
      "loss: 1.4106215238571167 at epoch 29 at applicants training\n",
      "loss: 1.4053741693496704 at epoch 30 at applicants training\n",
      "loss: 1.3993545770645142 at epoch 31 at applicants training\n",
      "loss: 1.3974534273147583 at epoch 32 at applicants training\n",
      "loss: 1.3990962505340576 at epoch 33 at applicants training\n",
      "loss: 1.3945056200027466 at epoch 34 at applicants training\n",
      "loss: 1.393266201019287 at epoch 35 at applicants training\n",
      "loss: 1.3926044702529907 at epoch 36 at applicants training\n",
      "loss: 1.3911951780319214 at epoch 37 at applicants training\n",
      "loss: 1.391172170639038 at epoch 38 at applicants training\n",
      "loss: 1.3906346559524536 at epoch 39 at applicants training\n",
      "loss: 1.3905526399612427 at epoch 40 at applicants training\n",
      "loss: 1.3907405138015747 at epoch 41 at applicants training\n",
      "loss: 1.3903868198394775 at epoch 42 at applicants training\n",
      "loss: 1.3908904790878296 at epoch 43 at applicants training\n",
      "loss: 1.3906419277191162 at epoch 44 at applicants training\n",
      "loss: 1.3910412788391113 at epoch 45 at applicants training\n",
      "loss: 1.390652060508728 at epoch 46 at applicants training\n",
      "loss: 1.390767216682434 at epoch 47 at applicants training\n",
      "loss: 1.3902578353881836 at epoch 48 at applicants training\n",
      "loss: 1.3902392387390137 at epoch 49 at applicants training\n",
      "loss: 1.3896894454956055 at epoch 50 at applicants training\n",
      "loss: 1.3896909952163696 at epoch 51 at applicants training\n",
      "loss: 1.3891894817352295 at epoch 52 at applicants training\n",
      "loss: 1.3892381191253662 at epoch 53 at applicants training\n",
      "loss: 1.3888227939605713 at epoch 54 at applicants training\n",
      "loss: 1.388982892036438 at epoch 55 at applicants training\n",
      "loss: 1.3887194395065308 at epoch 56 at applicants training\n",
      "loss: 1.3888697624206543 at epoch 57 at applicants training\n",
      "loss: 1.3888580799102783 at epoch 58 at applicants training\n",
      "loss: 1.3888598680496216 at epoch 59 at applicants training\n",
      "loss: 1.3889371156692505 at epoch 60 at applicants training\n",
      "loss: 1.3888344764709473 at epoch 61 at applicants training\n",
      "loss: 1.388884425163269 at epoch 62 at applicants training\n",
      "loss: 1.3887007236480713 at epoch 63 at applicants training\n",
      "loss: 1.388715386390686 at epoch 64 at applicants training\n",
      "loss: 1.388498067855835 at epoch 65 at applicants training\n",
      "loss: 1.3883442878723145 at epoch 66 at applicants training\n",
      "loss: 1.3883328437805176 at epoch 67 at applicants training\n",
      "loss: 1.3882818222045898 at epoch 68 at applicants training\n",
      "loss: 1.3881639242172241 at epoch 69 at applicants training\n",
      "loss: 1.3881322145462036 at epoch 70 at applicants training\n",
      "loss: 1.3881514072418213 at epoch 71 at applicants training\n",
      "loss: 1.3880993127822876 at epoch 72 at applicants training\n",
      "loss: 1.3880839347839355 at epoch 73 at applicants training\n",
      "loss: 1.3880410194396973 at epoch 74 at applicants training\n",
      "loss: 1.3880618810653687 at epoch 75 at applicants training\n",
      "loss: 1.387943983078003 at epoch 76 at applicants training\n",
      "loss: 1.3878556489944458 at epoch 77 at applicants training\n",
      "loss: 1.3878839015960693 at epoch 78 at applicants training\n",
      "loss: 1.3879553079605103 at epoch 79 at applicants training\n",
      "loss: 1.3879913091659546 at epoch 80 at applicants training\n",
      "loss: 1.3878997564315796 at epoch 81 at applicants training\n",
      "loss: 1.3878053426742554 at epoch 82 at applicants training\n",
      "loss: 1.3877094984054565 at epoch 83 at applicants training\n",
      "loss: 1.3876357078552246 at epoch 84 at applicants training\n",
      "loss: 1.3875930309295654 at epoch 85 at applicants training\n",
      "loss: 1.3875788450241089 at epoch 86 at applicants training\n",
      "loss: 1.3875796794891357 at epoch 87 at applicants training\n",
      "loss: 1.3875788450241089 at epoch 88 at applicants training\n",
      "loss: 1.3875787258148193 at epoch 89 at applicants training\n",
      "loss: 1.387597918510437 at epoch 90 at applicants training\n",
      "loss: 1.3877322673797607 at epoch 91 at applicants training\n",
      "loss: 1.3880223035812378 at epoch 92 at applicants training\n",
      "loss: 1.3885711431503296 at epoch 93 at applicants training\n",
      "loss: 1.388179063796997 at epoch 94 at applicants training\n",
      "loss: 1.3879045248031616 at epoch 95 at applicants training\n",
      "loss: 1.3876439332962036 at epoch 96 at applicants training\n",
      "loss: 1.3875999450683594 at epoch 97 at applicants training\n",
      "loss: 1.3875744342803955 at epoch 98 at applicants training\n",
      "loss: 1.3876339197158813 at epoch 99 at applicants training\n",
      "loss: 1.8074102401733398 at epoch 0 at applicants training\n",
      "loss: 1.6710416078567505 at epoch 1 at applicants training\n",
      "loss: 1.6411240100860596 at epoch 2 at applicants training\n",
      "loss: 1.745031714439392 at epoch 3 at applicants training\n",
      "loss: 1.6385120153427124 at epoch 4 at applicants training\n",
      "loss: 1.6539362668991089 at epoch 5 at applicants training\n",
      "loss: 1.6699029207229614 at epoch 6 at applicants training\n",
      "loss: 1.644624948501587 at epoch 7 at applicants training\n",
      "loss: 1.5798327922821045 at epoch 8 at applicants training\n",
      "loss: 1.5822279453277588 at epoch 9 at applicants training\n",
      "loss: 1.604425311088562 at epoch 10 at applicants training\n",
      "loss: 1.6033194065093994 at epoch 11 at applicants training\n",
      "loss: 1.6009793281555176 at epoch 12 at applicants training\n",
      "loss: 1.5918829441070557 at epoch 13 at applicants training\n",
      "loss: 1.5820603370666504 at epoch 14 at applicants training\n",
      "loss: 1.5777692794799805 at epoch 15 at applicants training\n",
      "loss: 1.5719233751296997 at epoch 16 at applicants training\n",
      "loss: 1.572160243988037 at epoch 17 at applicants training\n",
      "loss: 1.5684993267059326 at epoch 18 at applicants training\n",
      "loss: 1.5698546171188354 at epoch 19 at applicants training\n",
      "loss: 1.5697486400604248 at epoch 20 at applicants training\n",
      "loss: 1.569557785987854 at epoch 21 at applicants training\n",
      "loss: 1.5689908266067505 at epoch 22 at applicants training\n",
      "loss: 1.5654208660125732 at epoch 23 at applicants training\n",
      "loss: 1.5642633438110352 at epoch 24 at applicants training\n",
      "loss: 1.563747525215149 at epoch 25 at applicants training\n",
      "loss: 1.5579402446746826 at epoch 26 at applicants training\n",
      "loss: 1.5547242164611816 at epoch 27 at applicants training\n",
      "loss: 1.5501976013183594 at epoch 28 at applicants training\n",
      "loss: 1.5466252565383911 at epoch 29 at applicants training\n",
      "loss: 1.5440237522125244 at epoch 30 at applicants training\n",
      "loss: 1.5447574853897095 at epoch 31 at applicants training\n",
      "loss: 1.5472866296768188 at epoch 32 at applicants training\n",
      "loss: 1.5481449365615845 at epoch 33 at applicants training\n",
      "loss: 1.5385220050811768 at epoch 34 at applicants training\n",
      "loss: 1.5599132776260376 at epoch 35 at applicants training\n",
      "loss: 1.5475070476531982 at epoch 36 at applicants training\n",
      "loss: 1.5599684715270996 at epoch 37 at applicants training\n",
      "loss: 1.566346287727356 at epoch 38 at applicants training\n",
      "loss: 1.5635794401168823 at epoch 39 at applicants training\n",
      "loss: 1.5581374168395996 at epoch 40 at applicants training\n",
      "loss: 1.547033667564392 at epoch 41 at applicants training\n",
      "loss: 1.5299789905548096 at epoch 42 at applicants training\n",
      "loss: 1.5867842435836792 at epoch 43 at applicants training\n",
      "loss: 1.5371308326721191 at epoch 44 at applicants training\n",
      "loss: 1.5513243675231934 at epoch 45 at applicants training\n",
      "loss: 1.5598859786987305 at epoch 46 at applicants training\n",
      "loss: 1.562251329421997 at epoch 47 at applicants training\n",
      "loss: 1.5626641511917114 at epoch 48 at applicants training\n",
      "loss: 1.5617051124572754 at epoch 49 at applicants training\n",
      "loss: 1.5596303939819336 at epoch 50 at applicants training\n",
      "loss: 1.5508415699005127 at epoch 51 at applicants training\n",
      "loss: 1.5395996570587158 at epoch 52 at applicants training\n",
      "loss: 1.5318992137908936 at epoch 53 at applicants training\n",
      "loss: 1.5472511053085327 at epoch 54 at applicants training\n",
      "loss: 1.5310174226760864 at epoch 55 at applicants training\n",
      "loss: 1.5351474285125732 at epoch 56 at applicants training\n",
      "loss: 1.538393497467041 at epoch 57 at applicants training\n",
      "loss: 1.5388875007629395 at epoch 58 at applicants training\n",
      "loss: 1.5348247289657593 at epoch 59 at applicants training\n",
      "loss: 1.5316040515899658 at epoch 60 at applicants training\n",
      "loss: 1.5328948497772217 at epoch 61 at applicants training\n",
      "loss: 1.5361605882644653 at epoch 62 at applicants training\n",
      "loss: 1.5318260192871094 at epoch 63 at applicants training\n",
      "loss: 1.5304521322250366 at epoch 64 at applicants training\n",
      "loss: 1.5333037376403809 at epoch 65 at applicants training\n",
      "loss: 1.533993124961853 at epoch 66 at applicants training\n",
      "loss: 1.5313568115234375 at epoch 67 at applicants training\n",
      "loss: 1.5306881666183472 at epoch 68 at applicants training\n",
      "loss: 1.5320639610290527 at epoch 69 at applicants training\n",
      "loss: 1.5325754880905151 at epoch 70 at applicants training\n",
      "loss: 1.5318477153778076 at epoch 71 at applicants training\n",
      "loss: 1.5303599834442139 at epoch 72 at applicants training\n",
      "loss: 1.5310951471328735 at epoch 73 at applicants training\n",
      "loss: 1.5318447351455688 at epoch 74 at applicants training\n",
      "loss: 1.531869649887085 at epoch 75 at applicants training\n",
      "loss: 1.5312250852584839 at epoch 76 at applicants training\n",
      "loss: 1.5300217866897583 at epoch 77 at applicants training\n",
      "loss: 1.5309540033340454 at epoch 78 at applicants training\n",
      "loss: 1.5312353372573853 at epoch 79 at applicants training\n",
      "loss: 1.5299921035766602 at epoch 80 at applicants training\n",
      "loss: 1.53035569190979 at epoch 81 at applicants training\n",
      "loss: 1.5307681560516357 at epoch 82 at applicants training\n",
      "loss: 1.5295915603637695 at epoch 83 at applicants training\n",
      "loss: 1.5303088426589966 at epoch 84 at applicants training\n",
      "loss: 1.5301275253295898 at epoch 85 at applicants training\n",
      "loss: 1.529471755027771 at epoch 86 at applicants training\n",
      "loss: 1.5300462245941162 at epoch 87 at applicants training\n",
      "loss: 1.5290411710739136 at epoch 88 at applicants training\n",
      "loss: 1.529899001121521 at epoch 89 at applicants training\n",
      "loss: 1.529090404510498 at epoch 90 at applicants training\n",
      "loss: 1.5297507047653198 at epoch 91 at applicants training\n",
      "loss: 1.5292941331863403 at epoch 92 at applicants training\n",
      "loss: 1.5294897556304932 at epoch 93 at applicants training\n",
      "loss: 1.5294547080993652 at epoch 94 at applicants training\n",
      "loss: 1.5291286706924438 at epoch 95 at applicants training\n",
      "loss: 1.5294173955917358 at epoch 96 at applicants training\n",
      "loss: 1.5289459228515625 at epoch 97 at applicants training\n",
      "loss: 1.5292973518371582 at epoch 98 at applicants training\n",
      "loss: 1.5288394689559937 at epoch 99 at applicants training\n",
      "loss: 1.8091295957565308 at epoch 0 at applicants training\n",
      "loss: 1.7026689052581787 at epoch 1 at applicants training\n",
      "loss: 1.7247923612594604 at epoch 2 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 3 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 4 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 5 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 6 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 7 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 8 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 9 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 10 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 11 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 12 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 13 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 14 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 15 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 16 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 17 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 18 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 19 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 20 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 21 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 22 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 23 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 24 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 25 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 26 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 27 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 28 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 29 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 30 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 31 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 32 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 33 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 34 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 35 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 36 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 37 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 38 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 39 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 40 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 41 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 42 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 43 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 44 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 45 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 46 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 47 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 48 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 49 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 50 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 51 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 52 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 53 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 54 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 55 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 56 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 57 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 58 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 59 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 60 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 61 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 62 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 63 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 64 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 65 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 66 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 67 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 68 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 69 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 70 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 71 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 72 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 73 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 74 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 75 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 76 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 77 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 78 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 79 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 80 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 81 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 82 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 83 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 84 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 85 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 86 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 87 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 88 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 89 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 90 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 91 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 92 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 93 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 94 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 95 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 96 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 97 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 98 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 99 at applicants training\n",
      "loss: 1.7057913541793823 at epoch 0 at applicants training\n",
      "loss: 1.674694538116455 at epoch 1 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 2 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 3 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 4 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 5 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 6 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 7 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 8 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 9 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 10 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 11 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 12 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 13 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 14 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 15 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 16 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 17 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 18 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 19 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 20 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 21 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 22 at applicants training\n",
      "loss: 1.6748312711715698 at epoch 23 at applicants training\n",
      "loss: 1.674807071685791 at epoch 24 at applicants training\n",
      "loss: 1.6739492416381836 at epoch 25 at applicants training\n",
      "loss: 1.6656501293182373 at epoch 26 at applicants training\n",
      "loss: 1.613511323928833 at epoch 27 at applicants training\n",
      "loss: 1.6517608165740967 at epoch 28 at applicants training\n",
      "loss: 1.65641188621521 at epoch 29 at applicants training\n",
      "loss: 1.648073434829712 at epoch 30 at applicants training\n",
      "loss: 1.6295456886291504 at epoch 31 at applicants training\n",
      "loss: 1.5714280605316162 at epoch 32 at applicants training\n",
      "loss: 1.6310652494430542 at epoch 33 at applicants training\n",
      "loss: 1.6549172401428223 at epoch 34 at applicants training\n",
      "loss: 1.6347739696502686 at epoch 35 at applicants training\n",
      "loss: 1.5848345756530762 at epoch 36 at applicants training\n",
      "loss: 1.5741188526153564 at epoch 37 at applicants training\n",
      "loss: 1.5977603197097778 at epoch 38 at applicants training\n",
      "loss: 1.5845645666122437 at epoch 39 at applicants training\n",
      "loss: 1.5577107667922974 at epoch 40 at applicants training\n",
      "loss: 1.5244543552398682 at epoch 41 at applicants training\n",
      "loss: 1.5274908542633057 at epoch 42 at applicants training\n",
      "loss: 1.5289008617401123 at epoch 43 at applicants training\n",
      "loss: 1.524870753288269 at epoch 44 at applicants training\n",
      "loss: 1.5189378261566162 at epoch 45 at applicants training\n",
      "loss: 1.5169589519500732 at epoch 46 at applicants training\n",
      "loss: 1.5113345384597778 at epoch 47 at applicants training\n",
      "loss: 1.5070141553878784 at epoch 48 at applicants training\n",
      "loss: 1.496974229812622 at epoch 49 at applicants training\n",
      "loss: 1.4944202899932861 at epoch 50 at applicants training\n",
      "loss: 1.4870741367340088 at epoch 51 at applicants training\n",
      "loss: 1.4902350902557373 at epoch 52 at applicants training\n",
      "loss: 1.4742649793624878 at epoch 53 at applicants training\n",
      "loss: 1.470238447189331 at epoch 54 at applicants training\n",
      "loss: 1.4640719890594482 at epoch 55 at applicants training\n",
      "loss: 1.4633749723434448 at epoch 56 at applicants training\n",
      "loss: 1.460741400718689 at epoch 57 at applicants training\n",
      "loss: 1.4622070789337158 at epoch 58 at applicants training\n",
      "loss: 1.4594552516937256 at epoch 59 at applicants training\n",
      "loss: 1.4590823650360107 at epoch 60 at applicants training\n",
      "loss: 1.4578330516815186 at epoch 61 at applicants training\n",
      "loss: 1.4583762884140015 at epoch 62 at applicants training\n",
      "loss: 1.4581903219223022 at epoch 63 at applicants training\n",
      "loss: 1.4553561210632324 at epoch 64 at applicants training\n",
      "loss: 1.4569990634918213 at epoch 65 at applicants training\n",
      "loss: 1.4542508125305176 at epoch 66 at applicants training\n",
      "loss: 1.4558601379394531 at epoch 67 at applicants training\n",
      "loss: 1.4535551071166992 at epoch 68 at applicants training\n",
      "loss: 1.4545091390609741 at epoch 69 at applicants training\n",
      "loss: 1.4534507989883423 at epoch 70 at applicants training\n",
      "loss: 1.4530524015426636 at epoch 71 at applicants training\n",
      "loss: 1.4536492824554443 at epoch 72 at applicants training\n",
      "loss: 1.4527807235717773 at epoch 73 at applicants training\n",
      "loss: 1.4528398513793945 at epoch 74 at applicants training\n",
      "loss: 1.4530290365219116 at epoch 75 at applicants training\n",
      "loss: 1.452417254447937 at epoch 76 at applicants training\n",
      "loss: 1.4525730609893799 at epoch 77 at applicants training\n",
      "loss: 1.4525513648986816 at epoch 78 at applicants training\n",
      "loss: 1.4520841836929321 at epoch 79 at applicants training\n",
      "loss: 1.4521137475967407 at epoch 80 at applicants training\n",
      "loss: 1.4519524574279785 at epoch 81 at applicants training\n",
      "loss: 1.4516479969024658 at epoch 82 at applicants training\n",
      "loss: 1.451758623123169 at epoch 83 at applicants training\n",
      "loss: 1.451380968093872 at epoch 84 at applicants training\n",
      "loss: 1.4515076875686646 at epoch 85 at applicants training\n",
      "loss: 1.4512537717819214 at epoch 86 at applicants training\n",
      "loss: 1.4513757228851318 at epoch 87 at applicants training\n",
      "loss: 1.4511139392852783 at epoch 88 at applicants training\n",
      "loss: 1.4512813091278076 at epoch 89 at applicants training\n",
      "loss: 1.4510844945907593 at epoch 90 at applicants training\n",
      "loss: 1.4512932300567627 at epoch 91 at applicants training\n",
      "loss: 1.451117753982544 at epoch 92 at applicants training\n",
      "loss: 1.4512592554092407 at epoch 93 at applicants training\n",
      "loss: 1.4511646032333374 at epoch 94 at applicants training\n",
      "loss: 1.4511741399765015 at epoch 95 at applicants training\n",
      "loss: 1.4511594772338867 at epoch 96 at applicants training\n",
      "loss: 1.4510143995285034 at epoch 97 at applicants training\n",
      "loss: 1.4510488510131836 at epoch 98 at applicants training\n",
      "loss: 1.4509263038635254 at epoch 99 at applicants training\n",
      "loss: 1.7423466444015503 at epoch 0 at applicants training\n",
      "loss: 1.6842607259750366 at epoch 1 at applicants training\n",
      "loss: 1.6848241090774536 at epoch 2 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 3 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 4 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 5 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 9 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 10 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 11 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 12 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 13 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 14 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 15 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 16 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 17 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 18 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 19 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 20 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 21 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 22 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.7321125268936157 at epoch 0 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 1 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 2 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 3 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 4 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 5 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 6 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 7 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 8 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 9 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 10 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 11 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 12 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 13 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 14 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 15 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 16 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 17 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 18 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 19 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 20 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 21 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 22 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 23 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 24 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 25 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 26 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 27 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 28 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 29 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 30 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 31 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 32 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 33 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 34 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 35 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 36 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 37 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 38 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 39 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 40 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 41 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 42 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 43 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 44 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 45 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 46 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 47 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 48 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 49 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 50 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 51 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 52 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 53 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 54 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 55 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 56 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 57 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 58 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 59 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 60 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 61 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 62 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 63 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 64 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 65 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 66 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 67 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 68 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 69 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 70 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 71 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 72 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 73 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 74 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 75 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 76 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 77 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 78 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 79 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 80 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 81 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 82 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 83 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 84 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 85 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 86 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 87 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 88 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 89 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 90 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 91 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 92 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 93 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 94 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 95 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 96 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 97 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 98 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 99 at applicants training\n",
      "loss: 1.7426362037658691 at epoch 0 at applicants training\n",
      "loss: 1.598730206489563 at epoch 1 at applicants training\n",
      "loss: 1.5987666845321655 at epoch 2 at applicants training\n",
      "loss: 1.5043548345565796 at epoch 3 at applicants training\n",
      "loss: 1.5512007474899292 at epoch 4 at applicants training\n",
      "loss: 1.4550089836120605 at epoch 5 at applicants training\n",
      "loss: 1.5502384901046753 at epoch 6 at applicants training\n",
      "loss: 1.5503759384155273 at epoch 7 at applicants training\n",
      "loss: 1.4781231880187988 at epoch 8 at applicants training\n",
      "loss: 1.457884669303894 at epoch 9 at applicants training\n",
      "loss: 1.4604185819625854 at epoch 10 at applicants training\n",
      "loss: 1.4188852310180664 at epoch 11 at applicants training\n",
      "loss: 1.446223497390747 at epoch 12 at applicants training\n",
      "loss: 1.4335094690322876 at epoch 13 at applicants training\n",
      "loss: 1.413503885269165 at epoch 14 at applicants training\n",
      "loss: 1.4346840381622314 at epoch 15 at applicants training\n",
      "loss: 1.4211467504501343 at epoch 16 at applicants training\n",
      "loss: 1.4073021411895752 at epoch 17 at applicants training\n",
      "loss: 1.4271718263626099 at epoch 18 at applicants training\n",
      "loss: 1.4187719821929932 at epoch 19 at applicants training\n",
      "loss: 1.403383493423462 at epoch 20 at applicants training\n",
      "loss: 1.422591209411621 at epoch 21 at applicants training\n",
      "loss: 1.4152963161468506 at epoch 22 at applicants training\n",
      "loss: 1.4017857313156128 at epoch 23 at applicants training\n",
      "loss: 1.4105769395828247 at epoch 24 at applicants training\n",
      "loss: 1.411832332611084 at epoch 25 at applicants training\n",
      "loss: 1.4035612344741821 at epoch 26 at applicants training\n",
      "loss: 1.3985482454299927 at epoch 27 at applicants training\n",
      "loss: 1.4057775735855103 at epoch 28 at applicants training\n",
      "loss: 1.3953101634979248 at epoch 29 at applicants training\n",
      "loss: 1.3999754190444946 at epoch 30 at applicants training\n",
      "loss: 1.4016379117965698 at epoch 31 at applicants training\n",
      "loss: 1.3930420875549316 at epoch 32 at applicants training\n",
      "loss: 1.3972687721252441 at epoch 33 at applicants training\n",
      "loss: 1.3981595039367676 at epoch 34 at applicants training\n",
      "loss: 1.3925148248672485 at epoch 35 at applicants training\n",
      "loss: 1.396040916442871 at epoch 36 at applicants training\n",
      "loss: 1.3944662809371948 at epoch 37 at applicants training\n",
      "loss: 1.3923137187957764 at epoch 38 at applicants training\n",
      "loss: 1.3949774503707886 at epoch 39 at applicants training\n",
      "loss: 1.3939292430877686 at epoch 40 at applicants training\n",
      "loss: 1.391404390335083 at epoch 41 at applicants training\n",
      "loss: 1.3932873010635376 at epoch 42 at applicants training\n",
      "loss: 1.392280101776123 at epoch 43 at applicants training\n",
      "loss: 1.3911688327789307 at epoch 44 at applicants training\n",
      "loss: 1.392409324645996 at epoch 45 at applicants training\n",
      "loss: 1.3923513889312744 at epoch 46 at applicants training\n",
      "loss: 1.3911606073379517 at epoch 47 at applicants training\n",
      "loss: 1.3911175727844238 at epoch 48 at applicants training\n",
      "loss: 1.3919639587402344 at epoch 49 at applicants training\n",
      "loss: 1.3908039331436157 at epoch 50 at applicants training\n",
      "loss: 1.3907679319381714 at epoch 51 at applicants training\n",
      "loss: 1.391211748123169 at epoch 52 at applicants training\n",
      "loss: 1.390718698501587 at epoch 53 at applicants training\n",
      "loss: 1.390126347541809 at epoch 54 at applicants training\n",
      "loss: 1.390651822090149 at epoch 55 at applicants training\n",
      "loss: 1.3902472257614136 at epoch 56 at applicants training\n",
      "loss: 1.3898398876190186 at epoch 57 at applicants training\n",
      "loss: 1.390149712562561 at epoch 58 at applicants training\n",
      "loss: 1.3898017406463623 at epoch 59 at applicants training\n",
      "loss: 1.3894199132919312 at epoch 60 at applicants training\n",
      "loss: 1.3896883726119995 at epoch 61 at applicants training\n",
      "loss: 1.389182448387146 at epoch 62 at applicants training\n",
      "loss: 1.3891496658325195 at epoch 63 at applicants training\n",
      "loss: 1.3891422748565674 at epoch 64 at applicants training\n",
      "loss: 1.3887412548065186 at epoch 65 at applicants training\n",
      "loss: 1.3889155387878418 at epoch 66 at applicants training\n",
      "loss: 1.3886268138885498 at epoch 67 at applicants training\n",
      "loss: 1.3885972499847412 at epoch 68 at applicants training\n",
      "loss: 1.3885856866836548 at epoch 69 at applicants training\n",
      "loss: 1.3883559703826904 at epoch 70 at applicants training\n",
      "loss: 1.3884512186050415 at epoch 71 at applicants training\n",
      "loss: 1.388304591178894 at epoch 72 at applicants training\n",
      "loss: 1.388313889503479 at epoch 73 at applicants training\n",
      "loss: 1.3883042335510254 at epoch 74 at applicants training\n",
      "loss: 1.3881897926330566 at epoch 75 at applicants training\n",
      "loss: 1.388242244720459 at epoch 76 at applicants training\n",
      "loss: 1.3881237506866455 at epoch 77 at applicants training\n",
      "loss: 1.388134479522705 at epoch 78 at applicants training\n",
      "loss: 1.388085126876831 at epoch 79 at applicants training\n",
      "loss: 1.3880386352539062 at epoch 80 at applicants training\n",
      "loss: 1.388046145439148 at epoch 81 at applicants training\n",
      "loss: 1.3879727125167847 at epoch 82 at applicants training\n",
      "loss: 1.3880020380020142 at epoch 83 at applicants training\n",
      "loss: 1.3879344463348389 at epoch 84 at applicants training\n",
      "loss: 1.387946605682373 at epoch 85 at applicants training\n",
      "loss: 1.387882947921753 at epoch 86 at applicants training\n",
      "loss: 1.3878889083862305 at epoch 87 at applicants training\n",
      "loss: 1.3878365755081177 at epoch 88 at applicants training\n",
      "loss: 1.387834906578064 at epoch 89 at applicants training\n",
      "loss: 1.387792944908142 at epoch 90 at applicants training\n",
      "loss: 1.3877836465835571 at epoch 91 at applicants training\n",
      "loss: 1.3877450227737427 at epoch 92 at applicants training\n",
      "loss: 1.3877331018447876 at epoch 93 at applicants training\n",
      "loss: 1.387697458267212 at epoch 94 at applicants training\n",
      "loss: 1.3876862525939941 at epoch 95 at applicants training\n",
      "loss: 1.3876521587371826 at epoch 96 at applicants training\n",
      "loss: 1.3876429796218872 at epoch 97 at applicants training\n",
      "loss: 1.3876099586486816 at epoch 98 at applicants training\n",
      "loss: 1.387601613998413 at epoch 99 at applicants training\n",
      "loss: 1.6248327493667603 at epoch 0 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 1 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 2 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 3 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.7134395837783813 at epoch 0 at applicants training\n",
      "loss: 1.6272704601287842 at epoch 1 at applicants training\n",
      "loss: 1.6248273849487305 at epoch 2 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 3 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.6848976612091064 at epoch 0 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 1 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 2 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 3 at applicants training\n",
      "loss: 1.6848324537277222 at epoch 4 at applicants training\n",
      "loss: 1.6848270893096924 at epoch 5 at applicants training\n",
      "loss: 1.6833202838897705 at epoch 6 at applicants training\n",
      "loss: 1.5843138694763184 at epoch 7 at applicants training\n",
      "loss: 1.546919584274292 at epoch 8 at applicants training\n",
      "loss: 1.558031439781189 at epoch 9 at applicants training\n",
      "loss: 1.5257928371429443 at epoch 10 at applicants training\n",
      "loss: 1.511072039604187 at epoch 11 at applicants training\n",
      "loss: 1.5038740634918213 at epoch 12 at applicants training\n",
      "loss: 1.5045170783996582 at epoch 13 at applicants training\n",
      "loss: 1.5003836154937744 at epoch 14 at applicants training\n",
      "loss: 1.4927204847335815 at epoch 15 at applicants training\n",
      "loss: 1.4769432544708252 at epoch 16 at applicants training\n",
      "loss: 1.4884841442108154 at epoch 17 at applicants training\n",
      "loss: 1.4724726676940918 at epoch 18 at applicants training\n",
      "loss: 1.477686882019043 at epoch 19 at applicants training\n",
      "loss: 1.4779014587402344 at epoch 20 at applicants training\n",
      "loss: 1.468940258026123 at epoch 21 at applicants training\n",
      "loss: 1.467847466468811 at epoch 22 at applicants training\n",
      "loss: 1.471001148223877 at epoch 23 at applicants training\n",
      "loss: 1.4651004076004028 at epoch 24 at applicants training\n",
      "loss: 1.460015892982483 at epoch 25 at applicants training\n",
      "loss: 1.461403489112854 at epoch 26 at applicants training\n",
      "loss: 1.4547537565231323 at epoch 27 at applicants training\n",
      "loss: 1.4560065269470215 at epoch 28 at applicants training\n",
      "loss: 1.4529292583465576 at epoch 29 at applicants training\n",
      "loss: 1.4540634155273438 at epoch 30 at applicants training\n",
      "loss: 1.4508737325668335 at epoch 31 at applicants training\n",
      "loss: 1.451997995376587 at epoch 32 at applicants training\n",
      "loss: 1.4522405862808228 at epoch 33 at applicants training\n",
      "loss: 1.4515093564987183 at epoch 34 at applicants training\n",
      "loss: 1.452542781829834 at epoch 35 at applicants training\n",
      "loss: 1.4514650106430054 at epoch 36 at applicants training\n",
      "loss: 1.4513529539108276 at epoch 37 at applicants training\n",
      "loss: 1.4514743089675903 at epoch 38 at applicants training\n",
      "loss: 1.4509782791137695 at epoch 39 at applicants training\n",
      "loss: 1.4508922100067139 at epoch 40 at applicants training\n",
      "loss: 1.4510254859924316 at epoch 41 at applicants training\n",
      "loss: 1.4509929418563843 at epoch 42 at applicants training\n",
      "loss: 1.450858473777771 at epoch 43 at applicants training\n",
      "loss: 1.4507066011428833 at epoch 44 at applicants training\n",
      "loss: 1.450685739517212 at epoch 45 at applicants training\n",
      "loss: 1.4507050514221191 at epoch 46 at applicants training\n",
      "loss: 1.450513482093811 at epoch 47 at applicants training\n",
      "loss: 1.4504339694976807 at epoch 48 at applicants training\n",
      "loss: 1.45045804977417 at epoch 49 at applicants training\n",
      "loss: 1.4503531455993652 at epoch 50 at applicants training\n",
      "loss: 1.4502793550491333 at epoch 51 at applicants training\n",
      "loss: 1.450343370437622 at epoch 52 at applicants training\n",
      "loss: 1.4501796960830688 at epoch 53 at applicants training\n",
      "loss: 1.450218915939331 at epoch 54 at applicants training\n",
      "loss: 1.4501681327819824 at epoch 55 at applicants training\n",
      "loss: 1.4500863552093506 at epoch 56 at applicants training\n",
      "loss: 1.450119972229004 at epoch 57 at applicants training\n",
      "loss: 1.450020432472229 at epoch 58 at applicants training\n",
      "loss: 1.450008511543274 at epoch 59 at applicants training\n",
      "loss: 1.4499760866165161 at epoch 60 at applicants training\n",
      "loss: 1.4498951435089111 at epoch 61 at applicants training\n",
      "loss: 1.4498918056488037 at epoch 62 at applicants training\n",
      "loss: 1.449802279472351 at epoch 63 at applicants training\n",
      "loss: 1.4497636556625366 at epoch 64 at applicants training\n",
      "loss: 1.4496960639953613 at epoch 65 at applicants training\n",
      "loss: 1.4496078491210938 at epoch 66 at applicants training\n",
      "loss: 1.4495457410812378 at epoch 67 at applicants training\n",
      "loss: 1.4494301080703735 at epoch 68 at applicants training\n",
      "loss: 1.4493646621704102 at epoch 69 at applicants training\n",
      "loss: 1.4492406845092773 at epoch 70 at applicants training\n",
      "loss: 1.4491941928863525 at epoch 71 at applicants training\n",
      "loss: 1.4491039514541626 at epoch 72 at applicants training\n",
      "loss: 1.4490619897842407 at epoch 73 at applicants training\n",
      "loss: 1.448975682258606 at epoch 74 at applicants training\n",
      "loss: 1.448938012123108 at epoch 75 at applicants training\n",
      "loss: 1.4488869905471802 at epoch 76 at applicants training\n",
      "loss: 1.4488362073898315 at epoch 77 at applicants training\n",
      "loss: 1.4488272666931152 at epoch 78 at applicants training\n",
      "loss: 1.4487859010696411 at epoch 79 at applicants training\n",
      "loss: 1.4487680196762085 at epoch 80 at applicants training\n",
      "loss: 1.4487284421920776 at epoch 81 at applicants training\n",
      "loss: 1.4486862421035767 at epoch 82 at applicants training\n",
      "loss: 1.4486579895019531 at epoch 83 at applicants training\n",
      "loss: 1.4486050605773926 at epoch 84 at applicants training\n",
      "loss: 1.44856858253479 at epoch 85 at applicants training\n",
      "loss: 1.448532223701477 at epoch 86 at applicants training\n",
      "loss: 1.4484950304031372 at epoch 87 at applicants training\n",
      "loss: 1.4484578371047974 at epoch 88 at applicants training\n",
      "loss: 1.4484139680862427 at epoch 89 at applicants training\n",
      "loss: 1.4483610391616821 at epoch 90 at applicants training\n",
      "loss: 1.4483052492141724 at epoch 91 at applicants training\n",
      "loss: 1.4482393264770508 at epoch 92 at applicants training\n",
      "loss: 1.4481724500656128 at epoch 93 at applicants training\n",
      "loss: 1.4480842351913452 at epoch 94 at applicants training\n",
      "loss: 1.4479687213897705 at epoch 95 at applicants training\n",
      "loss: 1.447817087173462 at epoch 96 at applicants training\n",
      "loss: 1.4476314783096313 at epoch 97 at applicants training\n",
      "loss: 1.447441816329956 at epoch 98 at applicants training\n",
      "loss: 1.447302222251892 at epoch 99 at applicants training\n",
      "loss: 1.6289581060409546 at epoch 0 at applicants training\n",
      "loss: 1.602970838546753 at epoch 1 at applicants training\n",
      "loss: 1.6056032180786133 at epoch 2 at applicants training\n",
      "loss: 1.570662021636963 at epoch 3 at applicants training\n",
      "loss: 1.4909504652023315 at epoch 4 at applicants training\n",
      "loss: 1.5312460660934448 at epoch 5 at applicants training\n",
      "loss: 1.5830339193344116 at epoch 6 at applicants training\n",
      "loss: 1.5320677757263184 at epoch 7 at applicants training\n",
      "loss: 1.4073034524917603 at epoch 8 at applicants training\n",
      "loss: 1.4698715209960938 at epoch 9 at applicants training\n",
      "loss: 1.4746414422988892 at epoch 10 at applicants training\n",
      "loss: 1.4227921962738037 at epoch 11 at applicants training\n",
      "loss: 1.4086241722106934 at epoch 12 at applicants training\n",
      "loss: 1.3473910093307495 at epoch 13 at applicants training\n",
      "loss: 1.3307217359542847 at epoch 14 at applicants training\n",
      "loss: 1.361987590789795 at epoch 15 at applicants training\n",
      "loss: 1.336615800857544 at epoch 16 at applicants training\n",
      "loss: 1.2952991724014282 at epoch 17 at applicants training\n",
      "loss: 1.2971959114074707 at epoch 18 at applicants training\n",
      "loss: 1.317976951599121 at epoch 19 at applicants training\n",
      "loss: 1.283072829246521 at epoch 20 at applicants training\n",
      "loss: 1.2693833112716675 at epoch 21 at applicants training\n",
      "loss: 1.2596544027328491 at epoch 22 at applicants training\n",
      "loss: 1.2696417570114136 at epoch 23 at applicants training\n",
      "loss: 1.261647343635559 at epoch 24 at applicants training\n",
      "loss: 1.2395797967910767 at epoch 25 at applicants training\n",
      "loss: 1.2440730333328247 at epoch 26 at applicants training\n",
      "loss: 1.2419486045837402 at epoch 27 at applicants training\n",
      "loss: 1.2396726608276367 at epoch 28 at applicants training\n",
      "loss: 1.2257939577102661 at epoch 29 at applicants training\n",
      "loss: 1.2223410606384277 at epoch 30 at applicants training\n",
      "loss: 1.2248897552490234 at epoch 31 at applicants training\n",
      "loss: 1.2146763801574707 at epoch 32 at applicants training\n",
      "loss: 1.2142057418823242 at epoch 33 at applicants training\n",
      "loss: 1.2171002626419067 at epoch 34 at applicants training\n",
      "loss: 1.2074475288391113 at epoch 35 at applicants training\n",
      "loss: 1.2068623304367065 at epoch 36 at applicants training\n",
      "loss: 1.208609700202942 at epoch 37 at applicants training\n",
      "loss: 1.198979377746582 at epoch 38 at applicants training\n",
      "loss: 1.1996899843215942 at epoch 39 at applicants training\n",
      "loss: 1.1992154121398926 at epoch 40 at applicants training\n",
      "loss: 1.1956242322921753 at epoch 41 at applicants training\n",
      "loss: 1.1936790943145752 at epoch 42 at applicants training\n",
      "loss: 1.1945033073425293 at epoch 43 at applicants training\n",
      "loss: 1.1922096014022827 at epoch 44 at applicants training\n",
      "loss: 1.188891887664795 at epoch 45 at applicants training\n",
      "loss: 1.1875801086425781 at epoch 46 at applicants training\n",
      "loss: 1.1872022151947021 at epoch 47 at applicants training\n",
      "loss: 1.1849665641784668 at epoch 48 at applicants training\n",
      "loss: 1.1821767091751099 at epoch 49 at applicants training\n",
      "loss: 1.1819957494735718 at epoch 50 at applicants training\n",
      "loss: 1.1797373294830322 at epoch 51 at applicants training\n",
      "loss: 1.177435278892517 at epoch 52 at applicants training\n",
      "loss: 1.1772468090057373 at epoch 53 at applicants training\n",
      "loss: 1.1751606464385986 at epoch 54 at applicants training\n",
      "loss: 1.174680233001709 at epoch 55 at applicants training\n",
      "loss: 1.1745115518569946 at epoch 56 at applicants training\n",
      "loss: 1.1732922792434692 at epoch 57 at applicants training\n",
      "loss: 1.1734528541564941 at epoch 58 at applicants training\n",
      "loss: 1.173160433769226 at epoch 59 at applicants training\n",
      "loss: 1.1722829341888428 at epoch 60 at applicants training\n",
      "loss: 1.1723065376281738 at epoch 61 at applicants training\n",
      "loss: 1.1720389127731323 at epoch 62 at applicants training\n",
      "loss: 1.1713858842849731 at epoch 63 at applicants training\n",
      "loss: 1.1713569164276123 at epoch 64 at applicants training\n",
      "loss: 1.171120285987854 at epoch 65 at applicants training\n",
      "loss: 1.1706361770629883 at epoch 66 at applicants training\n",
      "loss: 1.1704788208007812 at epoch 67 at applicants training\n",
      "loss: 1.170380711555481 at epoch 68 at applicants training\n",
      "loss: 1.1700842380523682 at epoch 69 at applicants training\n",
      "loss: 1.1699912548065186 at epoch 70 at applicants training\n",
      "loss: 1.1699775457382202 at epoch 71 at applicants training\n",
      "loss: 1.1697615385055542 at epoch 72 at applicants training\n",
      "loss: 1.169564127922058 at epoch 73 at applicants training\n",
      "loss: 1.1695109605789185 at epoch 74 at applicants training\n",
      "loss: 1.1693655252456665 at epoch 75 at applicants training\n",
      "loss: 1.1692076921463013 at epoch 76 at applicants training\n",
      "loss: 1.1691642999649048 at epoch 77 at applicants training\n",
      "loss: 1.1690746545791626 at epoch 78 at applicants training\n",
      "loss: 1.1689149141311646 at epoch 79 at applicants training\n",
      "loss: 1.1688271760940552 at epoch 80 at applicants training\n",
      "loss: 1.1687544584274292 at epoch 81 at applicants training\n",
      "loss: 1.1686235666275024 at epoch 82 at applicants training\n",
      "loss: 1.168524146080017 at epoch 83 at applicants training\n",
      "loss: 1.1684342622756958 at epoch 84 at applicants training\n",
      "loss: 1.168296217918396 at epoch 85 at applicants training\n",
      "loss: 1.1681996583938599 at epoch 86 at applicants training\n",
      "loss: 1.1681382656097412 at epoch 87 at applicants training\n",
      "loss: 1.1680476665496826 at epoch 88 at applicants training\n",
      "loss: 1.1679718494415283 at epoch 89 at applicants training\n",
      "loss: 1.1679191589355469 at epoch 90 at applicants training\n",
      "loss: 1.1678578853607178 at epoch 91 at applicants training\n",
      "loss: 1.1678049564361572 at epoch 92 at applicants training\n",
      "loss: 1.1677697896957397 at epoch 93 at applicants training\n",
      "loss: 1.167717456817627 at epoch 94 at applicants training\n",
      "loss: 1.1676617860794067 at epoch 95 at applicants training\n",
      "loss: 1.1676149368286133 at epoch 96 at applicants training\n",
      "loss: 1.167565941810608 at epoch 97 at applicants training\n",
      "loss: 1.1675307750701904 at epoch 98 at applicants training\n",
      "loss: 1.1674968004226685 at epoch 99 at applicants training\n",
      "loss: 1.6600775718688965 at epoch 0 at applicants training\n",
      "loss: 1.6325042247772217 at epoch 1 at applicants training\n",
      "loss: 1.6250965595245361 at epoch 2 at applicants training\n",
      "loss: 1.6248265504837036 at epoch 3 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.6737219095230103 at epoch 0 at applicants training\n",
      "loss: 1.6366788148880005 at epoch 1 at applicants training\n",
      "loss: 1.6733880043029785 at epoch 2 at applicants training\n",
      "loss: 1.674830675125122 at epoch 3 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 4 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 5 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 6 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 7 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 8 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 9 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 10 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 11 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 12 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 13 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 14 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 15 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 16 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 17 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 18 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 19 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 20 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 21 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 22 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 23 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 24 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 25 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 26 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 27 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 28 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 29 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 30 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 31 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 32 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 33 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 34 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 35 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 36 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 37 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 38 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 39 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 40 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 41 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 42 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 43 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 44 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 45 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 46 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 47 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 48 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 49 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 50 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 51 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 52 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 53 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 54 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 55 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 56 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 57 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 58 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 59 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 60 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 61 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 62 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 63 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 64 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 65 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 66 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 67 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 68 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 69 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 70 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 71 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 72 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 73 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 74 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 75 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 76 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 77 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 78 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 79 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 80 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 81 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 82 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 83 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 84 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 85 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 86 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 87 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 88 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 89 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 90 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 91 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 92 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 93 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 94 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 95 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 96 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 97 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 98 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 99 at applicants training\n",
      "loss: 1.7457808256149292 at epoch 0 at applicants training\n",
      "loss: 1.724830985069275 at epoch 1 at applicants training\n",
      "loss: 1.7248324155807495 at epoch 2 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 3 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 4 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 5 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 6 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 7 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 8 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 9 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 10 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 11 at applicants training\n",
      "loss: 1.7248324155807495 at epoch 12 at applicants training\n",
      "loss: 1.7248324155807495 at epoch 13 at applicants training\n",
      "loss: 1.72483229637146 at epoch 14 at applicants training\n",
      "loss: 1.72483229637146 at epoch 15 at applicants training\n",
      "loss: 1.7248321771621704 at epoch 16 at applicants training\n",
      "loss: 1.7248315811157227 at epoch 17 at applicants training\n",
      "loss: 1.7248295545578003 at epoch 18 at applicants training\n",
      "loss: 1.724817156791687 at epoch 19 at applicants training\n",
      "loss: 1.7245166301727295 at epoch 20 at applicants training\n",
      "loss: 1.7211726903915405 at epoch 21 at applicants training\n",
      "loss: 1.7244064807891846 at epoch 22 at applicants training\n",
      "loss: 1.7246394157409668 at epoch 23 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 24 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 25 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 26 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 27 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 28 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 29 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 30 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 31 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 32 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 33 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 34 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 35 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 36 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 37 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 38 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 39 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 40 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 41 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 42 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 43 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 44 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 45 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 46 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 47 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 48 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 49 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 50 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 51 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 52 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 53 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 54 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 55 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 56 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 57 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 58 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 59 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 60 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 61 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 62 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 63 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 64 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 65 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 66 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 67 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 68 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 69 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 70 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 71 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 72 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 73 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 74 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 75 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 76 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 77 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 78 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 79 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 80 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 81 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 82 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 83 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 84 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 85 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 86 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 87 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 88 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 89 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 90 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 91 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 92 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 93 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 94 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 95 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 96 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 97 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 98 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 99 at applicants training\n",
      "loss: 1.7170486450195312 at epoch 0 at applicants training\n",
      "loss: 1.674517035484314 at epoch 1 at applicants training\n",
      "loss: 1.5977146625518799 at epoch 2 at applicants training\n",
      "loss: 1.6113516092300415 at epoch 3 at applicants training\n",
      "loss: 1.574641466140747 at epoch 4 at applicants training\n",
      "loss: 1.5142407417297363 at epoch 5 at applicants training\n",
      "loss: 1.5149644613265991 at epoch 6 at applicants training\n",
      "loss: 1.4832638502120972 at epoch 7 at applicants training\n",
      "loss: 1.4406622648239136 at epoch 8 at applicants training\n",
      "loss: 1.4459679126739502 at epoch 9 at applicants training\n",
      "loss: 1.422556757926941 at epoch 10 at applicants training\n",
      "loss: 1.4148293733596802 at epoch 11 at applicants training\n",
      "loss: 1.405299425125122 at epoch 12 at applicants training\n",
      "loss: 1.4274173974990845 at epoch 13 at applicants training\n",
      "loss: 1.4222760200500488 at epoch 14 at applicants training\n",
      "loss: 1.425204873085022 at epoch 15 at applicants training\n",
      "loss: 1.376634120941162 at epoch 16 at applicants training\n",
      "loss: 1.3911458253860474 at epoch 17 at applicants training\n",
      "loss: 1.445227026939392 at epoch 18 at applicants training\n",
      "loss: 1.3548709154129028 at epoch 19 at applicants training\n",
      "loss: 1.361213207244873 at epoch 20 at applicants training\n",
      "loss: 1.3876731395721436 at epoch 21 at applicants training\n",
      "loss: 1.405271291732788 at epoch 22 at applicants training\n",
      "loss: 1.3694446086883545 at epoch 23 at applicants training\n",
      "loss: 1.3785039186477661 at epoch 24 at applicants training\n",
      "loss: 1.3638863563537598 at epoch 25 at applicants training\n",
      "loss: 1.3451532125473022 at epoch 26 at applicants training\n",
      "loss: 1.3441280126571655 at epoch 27 at applicants training\n",
      "loss: 1.3536899089813232 at epoch 28 at applicants training\n",
      "loss: 1.335340142250061 at epoch 29 at applicants training\n",
      "loss: 1.3149420022964478 at epoch 30 at applicants training\n",
      "loss: 1.3210736513137817 at epoch 31 at applicants training\n",
      "loss: 1.3121790885925293 at epoch 32 at applicants training\n",
      "loss: 1.2918034791946411 at epoch 33 at applicants training\n",
      "loss: 1.2904517650604248 at epoch 34 at applicants training\n",
      "loss: 1.2815219163894653 at epoch 35 at applicants training\n",
      "loss: 1.2909654378890991 at epoch 36 at applicants training\n",
      "loss: 1.2647087574005127 at epoch 37 at applicants training\n",
      "loss: 1.271692156791687 at epoch 38 at applicants training\n",
      "loss: 1.2561287879943848 at epoch 39 at applicants training\n",
      "loss: 1.2582924365997314 at epoch 40 at applicants training\n",
      "loss: 1.2604137659072876 at epoch 41 at applicants training\n",
      "loss: 1.2529635429382324 at epoch 42 at applicants training\n",
      "loss: 1.2521017789840698 at epoch 43 at applicants training\n",
      "loss: 1.2535183429718018 at epoch 44 at applicants training\n",
      "loss: 1.250781536102295 at epoch 45 at applicants training\n",
      "loss: 1.2451547384262085 at epoch 46 at applicants training\n",
      "loss: 1.247898817062378 at epoch 47 at applicants training\n",
      "loss: 1.2453523874282837 at epoch 48 at applicants training\n",
      "loss: 1.2433849573135376 at epoch 49 at applicants training\n",
      "loss: 1.245294451713562 at epoch 50 at applicants training\n",
      "loss: 1.243804693222046 at epoch 51 at applicants training\n",
      "loss: 1.242811918258667 at epoch 52 at applicants training\n",
      "loss: 1.2440754175186157 at epoch 53 at applicants training\n",
      "loss: 1.2434576749801636 at epoch 54 at applicants training\n",
      "loss: 1.2424777746200562 at epoch 55 at applicants training\n",
      "loss: 1.2434180974960327 at epoch 56 at applicants training\n",
      "loss: 1.2434797286987305 at epoch 57 at applicants training\n",
      "loss: 1.242400050163269 at epoch 58 at applicants training\n",
      "loss: 1.2421154975891113 at epoch 59 at applicants training\n",
      "loss: 1.242750883102417 at epoch 60 at applicants training\n",
      "loss: 1.2421107292175293 at epoch 61 at applicants training\n",
      "loss: 1.2416757345199585 at epoch 62 at applicants training\n",
      "loss: 1.241885781288147 at epoch 63 at applicants training\n",
      "loss: 1.2417840957641602 at epoch 64 at applicants training\n",
      "loss: 1.2413796186447144 at epoch 65 at applicants training\n",
      "loss: 1.2412279844284058 at epoch 66 at applicants training\n",
      "loss: 1.2412363290786743 at epoch 67 at applicants training\n",
      "loss: 1.2410809993743896 at epoch 68 at applicants training\n",
      "loss: 1.240879774093628 at epoch 69 at applicants training\n",
      "loss: 1.240769386291504 at epoch 70 at applicants training\n",
      "loss: 1.2407233715057373 at epoch 71 at applicants training\n",
      "loss: 1.2405664920806885 at epoch 72 at applicants training\n",
      "loss: 1.2403759956359863 at epoch 73 at applicants training\n",
      "loss: 1.2403992414474487 at epoch 74 at applicants training\n",
      "loss: 1.2402560710906982 at epoch 75 at applicants training\n",
      "loss: 1.2400414943695068 at epoch 76 at applicants training\n",
      "loss: 1.2400882244110107 at epoch 77 at applicants training\n",
      "loss: 1.2399048805236816 at epoch 78 at applicants training\n",
      "loss: 1.2397149801254272 at epoch 79 at applicants training\n",
      "loss: 1.2397253513336182 at epoch 80 at applicants training\n",
      "loss: 1.2394763231277466 at epoch 81 at applicants training\n",
      "loss: 1.239309549331665 at epoch 82 at applicants training\n",
      "loss: 1.2392200231552124 at epoch 83 at applicants training\n",
      "loss: 1.2389204502105713 at epoch 84 at applicants training\n",
      "loss: 1.238816499710083 at epoch 85 at applicants training\n",
      "loss: 1.2386045455932617 at epoch 86 at applicants training\n",
      "loss: 1.2384105920791626 at epoch 87 at applicants training\n",
      "loss: 1.238303780555725 at epoch 88 at applicants training\n",
      "loss: 1.2380642890930176 at epoch 89 at applicants training\n",
      "loss: 1.2379355430603027 at epoch 90 at applicants training\n",
      "loss: 1.2375866174697876 at epoch 91 at applicants training\n",
      "loss: 1.237085223197937 at epoch 92 at applicants training\n",
      "loss: 1.2360213994979858 at epoch 93 at applicants training\n",
      "loss: 1.2344834804534912 at epoch 94 at applicants training\n",
      "loss: 1.2328780889511108 at epoch 95 at applicants training\n",
      "loss: 1.2316884994506836 at epoch 96 at applicants training\n",
      "loss: 1.2313538789749146 at epoch 97 at applicants training\n",
      "loss: 1.2313209772109985 at epoch 98 at applicants training\n",
      "loss: 1.2315274477005005 at epoch 99 at applicants training\n",
      "loss: 1.6965136528015137 at epoch 0 at applicants training\n",
      "loss: 1.615776538848877 at epoch 1 at applicants training\n",
      "loss: 1.6246272325515747 at epoch 2 at applicants training\n",
      "loss: 1.6248120069503784 at epoch 3 at applicants training\n",
      "loss: 1.6248297691345215 at epoch 4 at applicants training\n",
      "loss: 1.6248321533203125 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.68483304977417 at epoch 0 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 1 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 2 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 3 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 4 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 5 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 9 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 10 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 11 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 12 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 13 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 14 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 15 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 16 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 17 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 18 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 19 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 20 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 21 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 22 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 0 at applicants training\n",
      "loss: 1.623837947845459 at epoch 1 at applicants training\n",
      "loss: 1.624815821647644 at epoch 2 at applicants training\n",
      "loss: 1.6248292922973633 at epoch 3 at applicants training\n",
      "loss: 1.6248289346694946 at epoch 4 at applicants training\n",
      "loss: 1.6248260736465454 at epoch 5 at applicants training\n",
      "loss: 1.6248034238815308 at epoch 6 at applicants training\n",
      "loss: 1.6238582134246826 at epoch 7 at applicants training\n",
      "loss: 1.5906965732574463 at epoch 8 at applicants training\n",
      "loss: 1.4925915002822876 at epoch 9 at applicants training\n",
      "loss: 1.6534669399261475 at epoch 10 at applicants training\n",
      "loss: 1.549500584602356 at epoch 11 at applicants training\n",
      "loss: 1.573864459991455 at epoch 12 at applicants training\n",
      "loss: 1.5905576944351196 at epoch 13 at applicants training\n",
      "loss: 1.5786809921264648 at epoch 14 at applicants training\n",
      "loss: 1.5669822692871094 at epoch 15 at applicants training\n",
      "loss: 1.5470854043960571 at epoch 16 at applicants training\n",
      "loss: 1.5626035928726196 at epoch 17 at applicants training\n",
      "loss: 1.5436075925827026 at epoch 18 at applicants training\n",
      "loss: 1.5452466011047363 at epoch 19 at applicants training\n",
      "loss: 1.5565099716186523 at epoch 20 at applicants training\n",
      "loss: 1.559179663658142 at epoch 21 at applicants training\n",
      "loss: 1.5568366050720215 at epoch 22 at applicants training\n",
      "loss: 1.5426454544067383 at epoch 23 at applicants training\n",
      "loss: 1.5374090671539307 at epoch 24 at applicants training\n",
      "loss: 1.5486685037612915 at epoch 25 at applicants training\n",
      "loss: 1.5458258390426636 at epoch 26 at applicants training\n",
      "loss: 1.5361502170562744 at epoch 27 at applicants training\n",
      "loss: 1.540171504020691 at epoch 28 at applicants training\n",
      "loss: 1.5491299629211426 at epoch 29 at applicants training\n",
      "loss: 1.5423671007156372 at epoch 30 at applicants training\n",
      "loss: 1.5342439413070679 at epoch 31 at applicants training\n",
      "loss: 1.538571834564209 at epoch 32 at applicants training\n",
      "loss: 1.552357792854309 at epoch 33 at applicants training\n",
      "loss: 1.5345611572265625 at epoch 34 at applicants training\n",
      "loss: 1.535658359527588 at epoch 35 at applicants training\n",
      "loss: 1.541394829750061 at epoch 36 at applicants training\n",
      "loss: 1.542209267616272 at epoch 37 at applicants training\n",
      "loss: 1.538132905960083 at epoch 38 at applicants training\n",
      "loss: 1.5311998128890991 at epoch 39 at applicants training\n",
      "loss: 1.5335062742233276 at epoch 40 at applicants training\n",
      "loss: 1.5361969470977783 at epoch 41 at applicants training\n",
      "loss: 1.5184664726257324 at epoch 42 at applicants training\n",
      "loss: 1.528456449508667 at epoch 43 at applicants training\n",
      "loss: 1.528196096420288 at epoch 44 at applicants training\n",
      "loss: 1.494215488433838 at epoch 45 at applicants training\n",
      "loss: 1.4786477088928223 at epoch 46 at applicants training\n",
      "loss: 1.494184136390686 at epoch 47 at applicants training\n",
      "loss: 1.5097726583480835 at epoch 48 at applicants training\n",
      "loss: 1.4765704870224 at epoch 49 at applicants training\n",
      "loss: 1.4458576440811157 at epoch 50 at applicants training\n",
      "loss: 1.451062798500061 at epoch 51 at applicants training\n",
      "loss: 1.480689525604248 at epoch 52 at applicants training\n",
      "loss: 1.4626294374465942 at epoch 53 at applicants training\n",
      "loss: 1.4245730638504028 at epoch 54 at applicants training\n",
      "loss: 1.4282726049423218 at epoch 55 at applicants training\n",
      "loss: 1.4906589984893799 at epoch 56 at applicants training\n",
      "loss: 1.4471629858016968 at epoch 57 at applicants training\n",
      "loss: 1.4165606498718262 at epoch 58 at applicants training\n",
      "loss: 1.439569115638733 at epoch 59 at applicants training\n",
      "loss: 1.4699451923370361 at epoch 60 at applicants training\n",
      "loss: 1.4430946111679077 at epoch 61 at applicants training\n",
      "loss: 1.4023261070251465 at epoch 62 at applicants training\n",
      "loss: 1.4179930686950684 at epoch 63 at applicants training\n",
      "loss: 1.4102903604507446 at epoch 64 at applicants training\n",
      "loss: 1.4365382194519043 at epoch 65 at applicants training\n",
      "loss: 1.3900545835494995 at epoch 66 at applicants training\n",
      "loss: 1.386673927307129 at epoch 67 at applicants training\n",
      "loss: 1.3802391290664673 at epoch 68 at applicants training\n",
      "loss: 1.3890420198440552 at epoch 69 at applicants training\n",
      "loss: 1.3962374925613403 at epoch 70 at applicants training\n",
      "loss: 1.3965020179748535 at epoch 71 at applicants training\n",
      "loss: 1.3876017332077026 at epoch 72 at applicants training\n",
      "loss: 1.3720201253890991 at epoch 73 at applicants training\n",
      "loss: 1.3558778762817383 at epoch 74 at applicants training\n",
      "loss: 1.356144666671753 at epoch 75 at applicants training\n",
      "loss: 1.3624376058578491 at epoch 76 at applicants training\n",
      "loss: 1.3645427227020264 at epoch 77 at applicants training\n",
      "loss: 1.3562051057815552 at epoch 78 at applicants training\n",
      "loss: 1.3469003438949585 at epoch 79 at applicants training\n",
      "loss: 1.3440905809402466 at epoch 80 at applicants training\n",
      "loss: 1.3389806747436523 at epoch 81 at applicants training\n",
      "loss: 1.345189094543457 at epoch 82 at applicants training\n",
      "loss: 1.3392523527145386 at epoch 83 at applicants training\n",
      "loss: 1.3323948383331299 at epoch 84 at applicants training\n",
      "loss: 1.336556077003479 at epoch 85 at applicants training\n",
      "loss: 1.3371527194976807 at epoch 86 at applicants training\n",
      "loss: 1.334193468093872 at epoch 87 at applicants training\n",
      "loss: 1.3304742574691772 at epoch 88 at applicants training\n",
      "loss: 1.3301849365234375 at epoch 89 at applicants training\n",
      "loss: 1.3305801153182983 at epoch 90 at applicants training\n",
      "loss: 1.3265162706375122 at epoch 91 at applicants training\n",
      "loss: 1.3299757242202759 at epoch 92 at applicants training\n",
      "loss: 1.327452540397644 at epoch 93 at applicants training\n",
      "loss: 1.3265414237976074 at epoch 94 at applicants training\n",
      "loss: 1.3250476121902466 at epoch 95 at applicants training\n",
      "loss: 1.3250590562820435 at epoch 96 at applicants training\n",
      "loss: 1.3235960006713867 at epoch 97 at applicants training\n",
      "loss: 1.322243094444275 at epoch 98 at applicants training\n",
      "loss: 1.322240948677063 at epoch 99 at applicants training\n",
      "loss: 1.7039676904678345 at epoch 0 at applicants training\n",
      "loss: 1.6848293542861938 at epoch 1 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 2 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 3 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 4 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 5 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 9 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 10 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 11 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 12 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 13 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 14 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 15 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 16 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 17 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 18 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 19 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 20 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 21 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 22 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.6558226346969604 at epoch 0 at applicants training\n",
      "loss: 1.6253031492233276 at epoch 1 at applicants training\n",
      "loss: 1.6093248128890991 at epoch 2 at applicants training\n",
      "loss: 1.5554075241088867 at epoch 3 at applicants training\n",
      "loss: 1.573551893234253 at epoch 4 at applicants training\n",
      "loss: 1.4893165826797485 at epoch 5 at applicants training\n",
      "loss: 1.5396676063537598 at epoch 6 at applicants training\n",
      "loss: 1.522481083869934 at epoch 7 at applicants training\n",
      "loss: 1.4508975744247437 at epoch 8 at applicants training\n",
      "loss: 1.4757190942764282 at epoch 9 at applicants training\n",
      "loss: 1.4331355094909668 at epoch 10 at applicants training\n",
      "loss: 1.460862159729004 at epoch 11 at applicants training\n",
      "loss: 1.427599310874939 at epoch 12 at applicants training\n",
      "loss: 1.447969675064087 at epoch 13 at applicants training\n",
      "loss: 1.440172553062439 at epoch 14 at applicants training\n",
      "loss: 1.4254745244979858 at epoch 15 at applicants training\n",
      "loss: 1.4381581544876099 at epoch 16 at applicants training\n",
      "loss: 1.421604037284851 at epoch 17 at applicants training\n",
      "loss: 1.4312353134155273 at epoch 18 at applicants training\n",
      "loss: 1.4264168739318848 at epoch 19 at applicants training\n",
      "loss: 1.420629620552063 at epoch 20 at applicants training\n",
      "loss: 1.4266108274459839 at epoch 21 at applicants training\n",
      "loss: 1.4171868562698364 at epoch 22 at applicants training\n",
      "loss: 1.4216636419296265 at epoch 23 at applicants training\n",
      "loss: 1.4189367294311523 at epoch 24 at applicants training\n",
      "loss: 1.4153376817703247 at epoch 25 at applicants training\n",
      "loss: 1.4191012382507324 at epoch 26 at applicants training\n",
      "loss: 1.4140384197235107 at epoch 27 at applicants training\n",
      "loss: 1.415351390838623 at epoch 28 at applicants training\n",
      "loss: 1.4159623384475708 at epoch 29 at applicants training\n",
      "loss: 1.412806749343872 at epoch 30 at applicants training\n",
      "loss: 1.4148176908493042 at epoch 31 at applicants training\n",
      "loss: 1.4140902757644653 at epoch 32 at applicants training\n",
      "loss: 1.4125323295593262 at epoch 33 at applicants training\n",
      "loss: 1.4141478538513184 at epoch 34 at applicants training\n",
      "loss: 1.4131230115890503 at epoch 35 at applicants training\n",
      "loss: 1.4122143983840942 at epoch 36 at applicants training\n",
      "loss: 1.4133731126785278 at epoch 37 at applicants training\n",
      "loss: 1.4122319221496582 at epoch 38 at applicants training\n",
      "loss: 1.412013292312622 at epoch 39 at applicants training\n",
      "loss: 1.4126564264297485 at epoch 40 at applicants training\n",
      "loss: 1.411699652671814 at epoch 41 at applicants training\n",
      "loss: 1.4116216897964478 at epoch 42 at applicants training\n",
      "loss: 1.4119938611984253 at epoch 43 at applicants training\n",
      "loss: 1.4111449718475342 at epoch 44 at applicants training\n",
      "loss: 1.4110370874404907 at epoch 45 at applicants training\n",
      "loss: 1.4095540046691895 at epoch 46 at applicants training\n",
      "loss: 1.4038951396942139 at epoch 47 at applicants training\n",
      "loss: 1.4062304496765137 at epoch 48 at applicants training\n",
      "loss: 1.3979480266571045 at epoch 49 at applicants training\n",
      "loss: 1.3964762687683105 at epoch 50 at applicants training\n",
      "loss: 1.3910614252090454 at epoch 51 at applicants training\n",
      "loss: 1.3844550848007202 at epoch 52 at applicants training\n",
      "loss: 1.3717050552368164 at epoch 53 at applicants training\n",
      "loss: 1.3734447956085205 at epoch 54 at applicants training\n",
      "loss: 1.351396918296814 at epoch 55 at applicants training\n",
      "loss: 1.3397125005722046 at epoch 56 at applicants training\n",
      "loss: 1.3165761232376099 at epoch 57 at applicants training\n",
      "loss: 1.3080520629882812 at epoch 58 at applicants training\n",
      "loss: 1.304524540901184 at epoch 59 at applicants training\n",
      "loss: 1.2935552597045898 at epoch 60 at applicants training\n",
      "loss: 1.2928723096847534 at epoch 61 at applicants training\n",
      "loss: 1.2827703952789307 at epoch 62 at applicants training\n",
      "loss: 1.2808364629745483 at epoch 63 at applicants training\n",
      "loss: 1.2745554447174072 at epoch 64 at applicants training\n",
      "loss: 1.2720513343811035 at epoch 65 at applicants training\n",
      "loss: 1.2703615427017212 at epoch 66 at applicants training\n",
      "loss: 1.267925500869751 at epoch 67 at applicants training\n",
      "loss: 1.2670174837112427 at epoch 68 at applicants training\n",
      "loss: 1.2648093700408936 at epoch 69 at applicants training\n",
      "loss: 1.2632451057434082 at epoch 70 at applicants training\n",
      "loss: 1.2615545988082886 at epoch 71 at applicants training\n",
      "loss: 1.260727882385254 at epoch 72 at applicants training\n",
      "loss: 1.2574326992034912 at epoch 73 at applicants training\n",
      "loss: 1.2578041553497314 at epoch 74 at applicants training\n",
      "loss: 1.2550404071807861 at epoch 75 at applicants training\n",
      "loss: 1.2537354230880737 at epoch 76 at applicants training\n",
      "loss: 1.252738118171692 at epoch 77 at applicants training\n",
      "loss: 1.2506420612335205 at epoch 78 at applicants training\n",
      "loss: 1.2489628791809082 at epoch 79 at applicants training\n",
      "loss: 1.246983289718628 at epoch 80 at applicants training\n",
      "loss: 1.2456656694412231 at epoch 81 at applicants training\n",
      "loss: 1.2443510293960571 at epoch 82 at applicants training\n",
      "loss: 1.2428665161132812 at epoch 83 at applicants training\n",
      "loss: 1.2424217462539673 at epoch 84 at applicants training\n",
      "loss: 1.2412798404693604 at epoch 85 at applicants training\n",
      "loss: 1.2405152320861816 at epoch 86 at applicants training\n",
      "loss: 1.2399580478668213 at epoch 87 at applicants training\n",
      "loss: 1.2392163276672363 at epoch 88 at applicants training\n",
      "loss: 1.2386382818222046 at epoch 89 at applicants training\n",
      "loss: 1.2381677627563477 at epoch 90 at applicants training\n",
      "loss: 1.237654209136963 at epoch 91 at applicants training\n",
      "loss: 1.2370284795761108 at epoch 92 at applicants training\n",
      "loss: 1.236332893371582 at epoch 93 at applicants training\n",
      "loss: 1.2354623079299927 at epoch 94 at applicants training\n",
      "loss: 1.234193205833435 at epoch 95 at applicants training\n",
      "loss: 1.2332254648208618 at epoch 96 at applicants training\n",
      "loss: 1.2325302362442017 at epoch 97 at applicants training\n",
      "loss: 1.231972098350525 at epoch 98 at applicants training\n",
      "loss: 1.2317429780960083 at epoch 99 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 0 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 1 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 2 at applicants training\n",
      "loss: 1.6158497333526611 at epoch 3 at applicants training\n",
      "loss: 1.6247714757919312 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.6870744228363037 at epoch 0 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 1 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 2 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 3 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 4 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 5 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 9 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 10 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 11 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 12 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 13 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 14 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 15 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 16 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 17 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 18 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 19 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 20 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 21 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 22 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.7368345260620117 at epoch 0 at applicants training\n",
      "loss: 1.6845436096191406 at epoch 1 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 2 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 3 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 4 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 5 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 9 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 10 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 11 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 12 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 13 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 14 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 15 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 16 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 17 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 18 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 19 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 20 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 21 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 22 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.7033658027648926 at epoch 0 at applicants training\n",
      "loss: 1.6248325109481812 at epoch 1 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 2 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 3 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.6741394996643066 at epoch 0 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 1 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 2 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 3 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 4 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 5 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 9 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 10 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 11 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 12 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 13 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 14 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 15 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 16 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 17 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 18 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 19 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 20 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 21 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 22 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.6274914741516113 at epoch 0 at applicants training\n",
      "loss: 1.6248282194137573 at epoch 1 at applicants training\n",
      "loss: 1.6248325109481812 at epoch 2 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 3 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.674354910850525 at epoch 0 at applicants training\n",
      "loss: 1.674712061882019 at epoch 1 at applicants training\n",
      "loss: 1.627669334411621 at epoch 2 at applicants training\n",
      "loss: 1.5700896978378296 at epoch 3 at applicants training\n",
      "loss: 1.533941388130188 at epoch 4 at applicants training\n",
      "loss: 1.5533087253570557 at epoch 5 at applicants training\n",
      "loss: 1.6228854656219482 at epoch 6 at applicants training\n",
      "loss: 1.603684663772583 at epoch 7 at applicants training\n",
      "loss: 1.573805809020996 at epoch 8 at applicants training\n",
      "loss: 1.5574145317077637 at epoch 9 at applicants training\n",
      "loss: 1.5803395509719849 at epoch 10 at applicants training\n",
      "loss: 1.550970435142517 at epoch 11 at applicants training\n",
      "loss: 1.5714884996414185 at epoch 12 at applicants training\n",
      "loss: 1.5768938064575195 at epoch 13 at applicants training\n",
      "loss: 1.5743190050125122 at epoch 14 at applicants training\n",
      "loss: 1.564540147781372 at epoch 15 at applicants training\n",
      "loss: 1.5469383001327515 at epoch 16 at applicants training\n",
      "loss: 1.5390725135803223 at epoch 17 at applicants training\n",
      "loss: 1.5655946731567383 at epoch 18 at applicants training\n",
      "loss: 1.5213372707366943 at epoch 19 at applicants training\n",
      "loss: 1.5495914220809937 at epoch 20 at applicants training\n",
      "loss: 1.5546870231628418 at epoch 21 at applicants training\n",
      "loss: 1.5444653034210205 at epoch 22 at applicants training\n",
      "loss: 1.5153940916061401 at epoch 23 at applicants training\n",
      "loss: 1.5513737201690674 at epoch 24 at applicants training\n",
      "loss: 1.5382843017578125 at epoch 25 at applicants training\n",
      "loss: 1.5153496265411377 at epoch 26 at applicants training\n",
      "loss: 1.5316684246063232 at epoch 27 at applicants training\n",
      "loss: 1.5306289196014404 at epoch 28 at applicants training\n",
      "loss: 1.5144987106323242 at epoch 29 at applicants training\n",
      "loss: 1.5185823440551758 at epoch 30 at applicants training\n",
      "loss: 1.5138658285140991 at epoch 31 at applicants training\n",
      "loss: 1.5103073120117188 at epoch 32 at applicants training\n",
      "loss: 1.5179193019866943 at epoch 33 at applicants training\n",
      "loss: 1.5118213891983032 at epoch 34 at applicants training\n",
      "loss: 1.501401662826538 at epoch 35 at applicants training\n",
      "loss: 1.517033338546753 at epoch 36 at applicants training\n",
      "loss: 1.5000278949737549 at epoch 37 at applicants training\n",
      "loss: 1.507261037826538 at epoch 38 at applicants training\n",
      "loss: 1.5126945972442627 at epoch 39 at applicants training\n",
      "loss: 1.503177285194397 at epoch 40 at applicants training\n",
      "loss: 1.5023404359817505 at epoch 41 at applicants training\n",
      "loss: 1.5018606185913086 at epoch 42 at applicants training\n",
      "loss: 1.4960349798202515 at epoch 43 at applicants training\n",
      "loss: 1.5006502866744995 at epoch 44 at applicants training\n",
      "loss: 1.4920390844345093 at epoch 45 at applicants training\n",
      "loss: 1.4965914487838745 at epoch 46 at applicants training\n",
      "loss: 1.4926165342330933 at epoch 47 at applicants training\n",
      "loss: 1.492209792137146 at epoch 48 at applicants training\n",
      "loss: 1.4916281700134277 at epoch 49 at applicants training\n",
      "loss: 1.4905619621276855 at epoch 50 at applicants training\n",
      "loss: 1.492183804512024 at epoch 51 at applicants training\n",
      "loss: 1.4899226427078247 at epoch 52 at applicants training\n",
      "loss: 1.4883009195327759 at epoch 53 at applicants training\n",
      "loss: 1.4797286987304688 at epoch 54 at applicants training\n",
      "loss: 1.4712018966674805 at epoch 55 at applicants training\n",
      "loss: 1.429112195968628 at epoch 56 at applicants training\n",
      "loss: 1.436010479927063 at epoch 57 at applicants training\n",
      "loss: 1.3995441198349 at epoch 58 at applicants training\n",
      "loss: 1.3847894668579102 at epoch 59 at applicants training\n",
      "loss: 1.3588272333145142 at epoch 60 at applicants training\n",
      "loss: 1.3407989740371704 at epoch 61 at applicants training\n",
      "loss: 1.3082935810089111 at epoch 62 at applicants training\n",
      "loss: 1.2877920866012573 at epoch 63 at applicants training\n",
      "loss: 1.268354058265686 at epoch 64 at applicants training\n",
      "loss: 1.2216688394546509 at epoch 65 at applicants training\n",
      "loss: 1.3088105916976929 at epoch 66 at applicants training\n",
      "loss: 1.2704302072525024 at epoch 67 at applicants training\n",
      "loss: 1.199424386024475 at epoch 68 at applicants training\n",
      "loss: 1.2172771692276 at epoch 69 at applicants training\n",
      "loss: 1.2299606800079346 at epoch 70 at applicants training\n",
      "loss: 1.236934781074524 at epoch 71 at applicants training\n",
      "loss: 1.234692096710205 at epoch 72 at applicants training\n",
      "loss: 1.2303061485290527 at epoch 73 at applicants training\n",
      "loss: 1.225690245628357 at epoch 74 at applicants training\n",
      "loss: 1.2209107875823975 at epoch 75 at applicants training\n",
      "loss: 1.2096984386444092 at epoch 76 at applicants training\n",
      "loss: 1.2020537853240967 at epoch 77 at applicants training\n",
      "loss: 1.1789988279342651 at epoch 78 at applicants training\n",
      "loss: 1.1381419897079468 at epoch 79 at applicants training\n",
      "loss: 1.1819194555282593 at epoch 80 at applicants training\n",
      "loss: 1.1969482898712158 at epoch 81 at applicants training\n",
      "loss: 1.1777369976043701 at epoch 82 at applicants training\n",
      "loss: 1.155076265335083 at epoch 83 at applicants training\n",
      "loss: 1.1536223888397217 at epoch 84 at applicants training\n",
      "loss: 1.1454330682754517 at epoch 85 at applicants training\n",
      "loss: 1.1344548463821411 at epoch 86 at applicants training\n",
      "loss: 1.1303706169128418 at epoch 87 at applicants training\n",
      "loss: 1.1587198972702026 at epoch 88 at applicants training\n",
      "loss: 1.1233924627304077 at epoch 89 at applicants training\n",
      "loss: 1.1139049530029297 at epoch 90 at applicants training\n",
      "loss: 1.1608331203460693 at epoch 91 at applicants training\n",
      "loss: 1.1231094598770142 at epoch 92 at applicants training\n",
      "loss: 1.1300404071807861 at epoch 93 at applicants training\n",
      "loss: 1.103684902191162 at epoch 94 at applicants training\n",
      "loss: 1.107977271080017 at epoch 95 at applicants training\n",
      "loss: 1.117509126663208 at epoch 96 at applicants training\n",
      "loss: 1.1010524034500122 at epoch 97 at applicants training\n",
      "loss: 1.0842969417572021 at epoch 98 at applicants training\n",
      "loss: 1.0998163223266602 at epoch 99 at applicants training\n",
      "loss: 1.6833134889602661 at epoch 0 at applicants training\n",
      "loss: 1.6511485576629639 at epoch 1 at applicants training\n",
      "loss: 1.6176501512527466 at epoch 2 at applicants training\n",
      "loss: 1.610173225402832 at epoch 3 at applicants training\n",
      "loss: 1.588902235031128 at epoch 4 at applicants training\n",
      "loss: 1.5725533962249756 at epoch 5 at applicants training\n",
      "loss: 1.5881822109222412 at epoch 6 at applicants training\n",
      "loss: 1.5520014762878418 at epoch 7 at applicants training\n",
      "loss: 1.54032301902771 at epoch 8 at applicants training\n",
      "loss: 1.5395874977111816 at epoch 9 at applicants training\n",
      "loss: 1.5349291563034058 at epoch 10 at applicants training\n",
      "loss: 1.5329077243804932 at epoch 11 at applicants training\n",
      "loss: 1.5242228507995605 at epoch 12 at applicants training\n",
      "loss: 1.515078067779541 at epoch 13 at applicants training\n",
      "loss: 1.5082570314407349 at epoch 14 at applicants training\n",
      "loss: 1.5021623373031616 at epoch 15 at applicants training\n",
      "loss: 1.4981160163879395 at epoch 16 at applicants training\n",
      "loss: 1.4936513900756836 at epoch 17 at applicants training\n",
      "loss: 1.4923205375671387 at epoch 18 at applicants training\n",
      "loss: 1.4919729232788086 at epoch 19 at applicants training\n",
      "loss: 1.4918519258499146 at epoch 20 at applicants training\n",
      "loss: 1.4909051656723022 at epoch 21 at applicants training\n",
      "loss: 1.4888137578964233 at epoch 22 at applicants training\n",
      "loss: 1.4825445413589478 at epoch 23 at applicants training\n",
      "loss: 1.471553921699524 at epoch 24 at applicants training\n",
      "loss: 1.4740710258483887 at epoch 25 at applicants training\n",
      "loss: 1.4686082601547241 at epoch 26 at applicants training\n",
      "loss: 1.4573450088500977 at epoch 27 at applicants training\n",
      "loss: 1.473472237586975 at epoch 28 at applicants training\n",
      "loss: 1.4653323888778687 at epoch 29 at applicants training\n",
      "loss: 1.4750102758407593 at epoch 30 at applicants training\n",
      "loss: 1.4527279138565063 at epoch 31 at applicants training\n",
      "loss: 1.471258521080017 at epoch 32 at applicants training\n",
      "loss: 1.4550055265426636 at epoch 33 at applicants training\n",
      "loss: 1.4586857557296753 at epoch 34 at applicants training\n",
      "loss: 1.4609051942825317 at epoch 35 at applicants training\n",
      "loss: 1.4525130987167358 at epoch 36 at applicants training\n",
      "loss: 1.4642890691757202 at epoch 37 at applicants training\n",
      "loss: 1.452834129333496 at epoch 38 at applicants training\n",
      "loss: 1.4546281099319458 at epoch 39 at applicants training\n",
      "loss: 1.4590084552764893 at epoch 40 at applicants training\n",
      "loss: 1.4511468410491943 at epoch 41 at applicants training\n",
      "loss: 1.46327805519104 at epoch 42 at applicants training\n",
      "loss: 1.4571834802627563 at epoch 43 at applicants training\n",
      "loss: 1.4514775276184082 at epoch 44 at applicants training\n",
      "loss: 1.4693561792373657 at epoch 45 at applicants training\n",
      "loss: 1.4526896476745605 at epoch 46 at applicants training\n",
      "loss: 1.4520246982574463 at epoch 47 at applicants training\n",
      "loss: 1.4620267152786255 at epoch 48 at applicants training\n",
      "loss: 1.4539402723312378 at epoch 49 at applicants training\n",
      "loss: 1.4517831802368164 at epoch 50 at applicants training\n",
      "loss: 1.4583067893981934 at epoch 51 at applicants training\n",
      "loss: 1.4532617330551147 at epoch 52 at applicants training\n",
      "loss: 1.4516324996948242 at epoch 53 at applicants training\n",
      "loss: 1.4532798528671265 at epoch 54 at applicants training\n",
      "loss: 1.455235481262207 at epoch 55 at applicants training\n",
      "loss: 1.4518547058105469 at epoch 56 at applicants training\n",
      "loss: 1.4513959884643555 at epoch 57 at applicants training\n",
      "loss: 1.453116774559021 at epoch 58 at applicants training\n",
      "loss: 1.4529321193695068 at epoch 59 at applicants training\n",
      "loss: 1.4509987831115723 at epoch 60 at applicants training\n",
      "loss: 1.451094150543213 at epoch 61 at applicants training\n",
      "loss: 1.45206618309021 at epoch 62 at applicants training\n",
      "loss: 1.4520542621612549 at epoch 63 at applicants training\n",
      "loss: 1.450878620147705 at epoch 64 at applicants training\n",
      "loss: 1.450132131576538 at epoch 65 at applicants training\n",
      "loss: 1.4513875246047974 at epoch 66 at applicants training\n",
      "loss: 1.4512474536895752 at epoch 67 at applicants training\n",
      "loss: 1.4500752687454224 at epoch 68 at applicants training\n",
      "loss: 1.450563669204712 at epoch 69 at applicants training\n",
      "loss: 1.4511092901229858 at epoch 70 at applicants training\n",
      "loss: 1.4502109289169312 at epoch 71 at applicants training\n",
      "loss: 1.4500187635421753 at epoch 72 at applicants training\n",
      "loss: 1.450589656829834 at epoch 73 at applicants training\n",
      "loss: 1.4498776197433472 at epoch 74 at applicants training\n",
      "loss: 1.4496150016784668 at epoch 75 at applicants training\n",
      "loss: 1.4500499963760376 at epoch 76 at applicants training\n",
      "loss: 1.4491740465164185 at epoch 77 at applicants training\n",
      "loss: 1.4498260021209717 at epoch 78 at applicants training\n",
      "loss: 1.4492895603179932 at epoch 79 at applicants training\n",
      "loss: 1.4495501518249512 at epoch 80 at applicants training\n",
      "loss: 1.4494541883468628 at epoch 81 at applicants training\n",
      "loss: 1.4492573738098145 at epoch 82 at applicants training\n",
      "loss: 1.4493780136108398 at epoch 83 at applicants training\n",
      "loss: 1.44913911819458 at epoch 84 at applicants training\n",
      "loss: 1.4492676258087158 at epoch 85 at applicants training\n",
      "loss: 1.4491199254989624 at epoch 86 at applicants training\n",
      "loss: 1.4491784572601318 at epoch 87 at applicants training\n",
      "loss: 1.4490910768508911 at epoch 88 at applicants training\n",
      "loss: 1.449047565460205 at epoch 89 at applicants training\n",
      "loss: 1.4490725994110107 at epoch 90 at applicants training\n",
      "loss: 1.4488966464996338 at epoch 91 at applicants training\n",
      "loss: 1.4490411281585693 at epoch 92 at applicants training\n",
      "loss: 1.448744773864746 at epoch 93 at applicants training\n",
      "loss: 1.448946237564087 at epoch 94 at applicants training\n",
      "loss: 1.4487396478652954 at epoch 95 at applicants training\n",
      "loss: 1.448793888092041 at epoch 96 at applicants training\n",
      "loss: 1.448817491531372 at epoch 97 at applicants training\n",
      "loss: 1.4486687183380127 at epoch 98 at applicants training\n",
      "loss: 1.4488025903701782 at epoch 99 at applicants training\n",
      "loss: 1.6950055360794067 at epoch 0 at applicants training\n",
      "loss: 1.6807200908660889 at epoch 1 at applicants training\n",
      "loss: 1.6715072393417358 at epoch 2 at applicants training\n",
      "loss: 1.6672145128250122 at epoch 3 at applicants training\n",
      "loss: 1.6522936820983887 at epoch 4 at applicants training\n",
      "loss: 1.6371026039123535 at epoch 5 at applicants training\n",
      "loss: 1.6333595514297485 at epoch 6 at applicants training\n",
      "loss: 1.607107400894165 at epoch 7 at applicants training\n",
      "loss: 1.5998798608779907 at epoch 8 at applicants training\n",
      "loss: 1.598625898361206 at epoch 9 at applicants training\n",
      "loss: 1.5975096225738525 at epoch 10 at applicants training\n",
      "loss: 1.6000040769577026 at epoch 11 at applicants training\n",
      "loss: 1.6001975536346436 at epoch 12 at applicants training\n",
      "loss: 1.6009904146194458 at epoch 13 at applicants training\n",
      "loss: 1.6009058952331543 at epoch 14 at applicants training\n",
      "loss: 1.600570559501648 at epoch 15 at applicants training\n",
      "loss: 1.60079026222229 at epoch 16 at applicants training\n",
      "loss: 1.6003143787384033 at epoch 17 at applicants training\n",
      "loss: 1.5999466180801392 at epoch 18 at applicants training\n",
      "loss: 1.6000274419784546 at epoch 19 at applicants training\n",
      "loss: 1.5996475219726562 at epoch 20 at applicants training\n",
      "loss: 1.5994999408721924 at epoch 21 at applicants training\n",
      "loss: 1.5995521545410156 at epoch 22 at applicants training\n",
      "loss: 1.599371314048767 at epoch 23 at applicants training\n",
      "loss: 1.5991675853729248 at epoch 24 at applicants training\n",
      "loss: 1.5993075370788574 at epoch 25 at applicants training\n",
      "loss: 1.5993051528930664 at epoch 26 at applicants training\n",
      "loss: 1.5990666151046753 at epoch 27 at applicants training\n",
      "loss: 1.5988898277282715 at epoch 28 at applicants training\n",
      "loss: 1.5987335443496704 at epoch 29 at applicants training\n",
      "loss: 1.5982482433319092 at epoch 30 at applicants training\n",
      "loss: 1.5981674194335938 at epoch 31 at applicants training\n",
      "loss: 1.59775710105896 at epoch 32 at applicants training\n",
      "loss: 1.597460150718689 at epoch 33 at applicants training\n",
      "loss: 1.5976612567901611 at epoch 34 at applicants training\n",
      "loss: 1.5984729528427124 at epoch 35 at applicants training\n",
      "loss: 1.5981590747833252 at epoch 36 at applicants training\n",
      "loss: 1.5978097915649414 at epoch 37 at applicants training\n",
      "loss: 1.5975810289382935 at epoch 38 at applicants training\n",
      "loss: 1.5974557399749756 at epoch 39 at applicants training\n",
      "loss: 1.597335696220398 at epoch 40 at applicants training\n",
      "loss: 1.5972039699554443 at epoch 41 at applicants training\n",
      "loss: 1.5971719026565552 at epoch 42 at applicants training\n",
      "loss: 1.5977038145065308 at epoch 43 at applicants training\n",
      "loss: 1.5981329679489136 at epoch 44 at applicants training\n",
      "loss: 1.596582293510437 at epoch 45 at applicants training\n",
      "loss: 1.5960506200790405 at epoch 46 at applicants training\n",
      "loss: 1.5966192483901978 at epoch 47 at applicants training\n",
      "loss: 1.5986872911453247 at epoch 48 at applicants training\n",
      "loss: 1.5962849855422974 at epoch 49 at applicants training\n",
      "loss: 1.6001354455947876 at epoch 50 at applicants training\n",
      "loss: 1.5969668626785278 at epoch 51 at applicants training\n",
      "loss: 1.5998512506484985 at epoch 52 at applicants training\n",
      "loss: 1.5992779731750488 at epoch 53 at applicants training\n",
      "loss: 1.5968769788742065 at epoch 54 at applicants training\n",
      "loss: 1.599511981010437 at epoch 55 at applicants training\n",
      "loss: 1.5966895818710327 at epoch 56 at applicants training\n",
      "loss: 1.5987260341644287 at epoch 57 at applicants training\n",
      "loss: 1.5969144105911255 at epoch 58 at applicants training\n",
      "loss: 1.5980943441390991 at epoch 59 at applicants training\n",
      "loss: 1.5970503091812134 at epoch 60 at applicants training\n",
      "loss: 1.5971794128417969 at epoch 61 at applicants training\n",
      "loss: 1.596897840499878 at epoch 62 at applicants training\n",
      "loss: 1.5969805717468262 at epoch 63 at applicants training\n",
      "loss: 1.5966103076934814 at epoch 64 at applicants training\n",
      "loss: 1.597015380859375 at epoch 65 at applicants training\n",
      "loss: 1.5953047275543213 at epoch 66 at applicants training\n",
      "loss: 1.5972234010696411 at epoch 67 at applicants training\n",
      "loss: 1.595048189163208 at epoch 68 at applicants training\n",
      "loss: 1.5945777893066406 at epoch 69 at applicants training\n",
      "loss: 1.5964328050613403 at epoch 70 at applicants training\n",
      "loss: 1.598008632659912 at epoch 71 at applicants training\n",
      "loss: 1.5947178602218628 at epoch 72 at applicants training\n",
      "loss: 1.6023833751678467 at epoch 73 at applicants training\n",
      "loss: 1.6019008159637451 at epoch 74 at applicants training\n",
      "loss: 1.5939403772354126 at epoch 75 at applicants training\n",
      "loss: 1.6032832860946655 at epoch 76 at applicants training\n",
      "loss: 1.6011581420898438 at epoch 77 at applicants training\n",
      "loss: 1.597922921180725 at epoch 78 at applicants training\n",
      "loss: 1.5992534160614014 at epoch 79 at applicants training\n",
      "loss: 1.5949386358261108 at epoch 80 at applicants training\n",
      "loss: 1.600331425666809 at epoch 81 at applicants training\n",
      "loss: 1.5979242324829102 at epoch 82 at applicants training\n",
      "loss: 1.5975147485733032 at epoch 83 at applicants training\n",
      "loss: 1.5972929000854492 at epoch 84 at applicants training\n",
      "loss: 1.5973985195159912 at epoch 85 at applicants training\n",
      "loss: 1.5998663902282715 at epoch 86 at applicants training\n",
      "loss: 1.5950475931167603 at epoch 87 at applicants training\n",
      "loss: 1.6005702018737793 at epoch 88 at applicants training\n",
      "loss: 1.5946277379989624 at epoch 89 at applicants training\n",
      "loss: 1.5978305339813232 at epoch 90 at applicants training\n",
      "loss: 1.5956413745880127 at epoch 91 at applicants training\n",
      "loss: 1.5965827703475952 at epoch 92 at applicants training\n",
      "loss: 1.5967168807983398 at epoch 93 at applicants training\n",
      "loss: 1.5947067737579346 at epoch 94 at applicants training\n",
      "loss: 1.5955467224121094 at epoch 95 at applicants training\n",
      "loss: 1.595167636871338 at epoch 96 at applicants training\n",
      "loss: 1.5947319269180298 at epoch 97 at applicants training\n",
      "loss: 1.595260739326477 at epoch 98 at applicants training\n",
      "loss: 1.5933237075805664 at epoch 99 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 0 at applicants training\n",
      "loss: 1.7258367538452148 at epoch 1 at applicants training\n",
      "loss: 1.724825143814087 at epoch 2 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 3 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 4 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 5 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 6 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 7 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 8 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 9 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 10 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 11 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 12 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 13 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 14 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 15 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 16 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 17 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 18 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 19 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 20 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 21 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 22 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 23 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 24 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 25 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 26 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 27 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 28 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 29 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 30 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 31 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 32 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 33 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 34 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 35 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 36 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 37 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 38 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 39 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 40 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 41 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 42 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 43 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 44 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 45 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 46 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 47 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 48 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 49 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 50 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 51 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 52 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 53 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 54 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 55 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 56 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 57 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 58 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 59 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 60 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 61 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 62 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 63 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 64 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 65 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 66 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 67 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 68 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 69 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 70 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 71 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 72 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 73 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 74 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 75 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 76 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 77 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 78 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 79 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 80 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 81 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 82 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 83 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 84 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 85 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 86 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 87 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 88 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 89 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 90 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 91 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 92 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 93 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 94 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 95 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 96 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 97 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 98 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 99 at applicants training\n",
      "loss: 1.6524441242218018 at epoch 0 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 1 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 2 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 3 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.610092282295227 at epoch 0 at applicants training\n",
      "loss: 1.6738990545272827 at epoch 1 at applicants training\n",
      "loss: 1.6848193407058716 at epoch 2 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 3 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 4 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 5 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 9 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 10 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 11 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 12 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 13 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 14 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 15 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 16 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 17 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 18 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 19 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 20 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 21 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 22 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.798048973083496 at epoch 0 at applicants training\n",
      "loss: 1.6989210844039917 at epoch 1 at applicants training\n",
      "loss: 1.6121301651000977 at epoch 2 at applicants training\n",
      "loss: 1.6132458448410034 at epoch 3 at applicants training\n",
      "loss: 1.581276297569275 at epoch 4 at applicants training\n",
      "loss: 1.523837685585022 at epoch 5 at applicants training\n",
      "loss: 1.5432301759719849 at epoch 6 at applicants training\n",
      "loss: 1.5018339157104492 at epoch 7 at applicants training\n",
      "loss: 1.501381278038025 at epoch 8 at applicants training\n",
      "loss: 1.5171924829483032 at epoch 9 at applicants training\n",
      "loss: 1.5075595378875732 at epoch 10 at applicants training\n",
      "loss: 1.4774086475372314 at epoch 11 at applicants training\n",
      "loss: 1.468367099761963 at epoch 12 at applicants training\n",
      "loss: 1.471930980682373 at epoch 13 at applicants training\n",
      "loss: 1.4786725044250488 at epoch 14 at applicants training\n",
      "loss: 1.4706103801727295 at epoch 15 at applicants training\n",
      "loss: 1.452256441116333 at epoch 16 at applicants training\n",
      "loss: 1.4426722526550293 at epoch 17 at applicants training\n",
      "loss: 1.4444057941436768 at epoch 18 at applicants training\n",
      "loss: 1.4323140382766724 at epoch 19 at applicants training\n",
      "loss: 1.4399603605270386 at epoch 20 at applicants training\n",
      "loss: 1.4209901094436646 at epoch 21 at applicants training\n",
      "loss: 1.4230949878692627 at epoch 22 at applicants training\n",
      "loss: 1.4161813259124756 at epoch 23 at applicants training\n",
      "loss: 1.4158521890640259 at epoch 24 at applicants training\n",
      "loss: 1.413670301437378 at epoch 25 at applicants training\n",
      "loss: 1.4141740798950195 at epoch 26 at applicants training\n",
      "loss: 1.4126704931259155 at epoch 27 at applicants training\n",
      "loss: 1.4136919975280762 at epoch 28 at applicants training\n",
      "loss: 1.4122896194458008 at epoch 29 at applicants training\n",
      "loss: 1.411625623703003 at epoch 30 at applicants training\n",
      "loss: 1.4108150005340576 at epoch 31 at applicants training\n",
      "loss: 1.408113718032837 at epoch 32 at applicants training\n",
      "loss: 1.4038985967636108 at epoch 33 at applicants training\n",
      "loss: 1.3995743989944458 at epoch 34 at applicants training\n",
      "loss: 1.3962311744689941 at epoch 35 at applicants training\n",
      "loss: 1.3942493200302124 at epoch 36 at applicants training\n",
      "loss: 1.393275260925293 at epoch 37 at applicants training\n",
      "loss: 1.3929072618484497 at epoch 38 at applicants training\n",
      "loss: 1.3927741050720215 at epoch 39 at applicants training\n",
      "loss: 1.3928364515304565 at epoch 40 at applicants training\n",
      "loss: 1.3929287195205688 at epoch 41 at applicants training\n",
      "loss: 1.3929492235183716 at epoch 42 at applicants training\n",
      "loss: 1.3928641080856323 at epoch 43 at applicants training\n",
      "loss: 1.3926827907562256 at epoch 44 at applicants training\n",
      "loss: 1.392427921295166 at epoch 45 at applicants training\n",
      "loss: 1.392115831375122 at epoch 46 at applicants training\n",
      "loss: 1.3917728662490845 at epoch 47 at applicants training\n",
      "loss: 1.3914542198181152 at epoch 48 at applicants training\n",
      "loss: 1.3912079334259033 at epoch 49 at applicants training\n",
      "loss: 1.391038179397583 at epoch 50 at applicants training\n",
      "loss: 1.3909660577774048 at epoch 51 at applicants training\n",
      "loss: 1.3909008502960205 at epoch 52 at applicants training\n",
      "loss: 1.3907831907272339 at epoch 53 at applicants training\n",
      "loss: 1.3906217813491821 at epoch 54 at applicants training\n",
      "loss: 1.390413522720337 at epoch 55 at applicants training\n",
      "loss: 1.3901487588882446 at epoch 56 at applicants training\n",
      "loss: 1.3898515701293945 at epoch 57 at applicants training\n",
      "loss: 1.3895859718322754 at epoch 58 at applicants training\n",
      "loss: 1.3894532918930054 at epoch 59 at applicants training\n",
      "loss: 1.389472484588623 at epoch 60 at applicants training\n",
      "loss: 1.3895652294158936 at epoch 61 at applicants training\n",
      "loss: 1.3896496295928955 at epoch 62 at applicants training\n",
      "loss: 1.3896793127059937 at epoch 63 at applicants training\n",
      "loss: 1.3896484375 at epoch 64 at applicants training\n",
      "loss: 1.3895790576934814 at epoch 65 at applicants training\n",
      "loss: 1.38950514793396 at epoch 66 at applicants training\n",
      "loss: 1.3894567489624023 at epoch 67 at applicants training\n",
      "loss: 1.3894375562667847 at epoch 68 at applicants training\n",
      "loss: 1.3894298076629639 at epoch 69 at applicants training\n",
      "loss: 1.3894108533859253 at epoch 70 at applicants training\n",
      "loss: 1.3893665075302124 at epoch 71 at applicants training\n",
      "loss: 1.3892936706542969 at epoch 72 at applicants training\n",
      "loss: 1.389201045036316 at epoch 73 at applicants training\n",
      "loss: 1.3891044855117798 at epoch 74 at applicants training\n",
      "loss: 1.3890475034713745 at epoch 75 at applicants training\n",
      "loss: 1.388993263244629 at epoch 76 at applicants training\n",
      "loss: 1.3889403343200684 at epoch 77 at applicants training\n",
      "loss: 1.3888802528381348 at epoch 78 at applicants training\n",
      "loss: 1.388814091682434 at epoch 79 at applicants training\n",
      "loss: 1.3887466192245483 at epoch 80 at applicants training\n",
      "loss: 1.3886843919754028 at epoch 81 at applicants training\n",
      "loss: 1.3886340856552124 at epoch 82 at applicants training\n",
      "loss: 1.3885977268218994 at epoch 83 at applicants training\n",
      "loss: 1.3885682821273804 at epoch 84 at applicants training\n",
      "loss: 1.3885350227355957 at epoch 85 at applicants training\n",
      "loss: 1.3884930610656738 at epoch 86 at applicants training\n",
      "loss: 1.3884433507919312 at epoch 87 at applicants training\n",
      "loss: 1.3883894681930542 at epoch 88 at applicants training\n",
      "loss: 1.3883373737335205 at epoch 89 at applicants training\n",
      "loss: 1.3882848024368286 at epoch 90 at applicants training\n",
      "loss: 1.3882265090942383 at epoch 91 at applicants training\n",
      "loss: 1.3881638050079346 at epoch 92 at applicants training\n",
      "loss: 1.3881016969680786 at epoch 93 at applicants training\n",
      "loss: 1.3880654573440552 at epoch 94 at applicants training\n",
      "loss: 1.3881160020828247 at epoch 95 at applicants training\n",
      "loss: 1.3886364698410034 at epoch 96 at applicants training\n",
      "loss: 1.3890964984893799 at epoch 97 at applicants training\n",
      "loss: 1.391230821609497 at epoch 98 at applicants training\n",
      "loss: 1.3888452053070068 at epoch 99 at applicants training\n",
      "loss: 1.6848324537277222 at epoch 0 at applicants training\n",
      "loss: 1.6039470434188843 at epoch 1 at applicants training\n",
      "loss: 1.6747772693634033 at epoch 2 at applicants training\n",
      "loss: 1.6748324632644653 at epoch 3 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 4 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 5 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 6 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 7 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 8 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 9 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 10 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 11 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 12 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 13 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 14 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 15 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 16 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 17 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 18 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 19 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 20 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 21 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 22 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 23 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 24 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 25 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 26 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 27 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 28 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 29 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 30 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 31 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 32 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 33 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 34 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 35 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 36 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 37 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 38 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 39 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 40 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 41 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 42 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 43 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 44 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 45 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 46 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 47 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 48 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 49 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 50 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 51 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 52 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 53 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 54 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 55 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 56 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 57 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 58 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 59 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 60 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 61 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 62 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 63 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 64 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 65 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 66 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 67 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 68 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 69 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 70 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 71 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 72 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 73 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 74 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 75 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 76 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 77 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 78 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 79 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 80 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 81 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 82 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 83 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 84 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 85 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 86 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 87 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 88 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 89 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 90 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 91 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 92 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 93 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 94 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 95 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 96 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 97 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 98 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 99 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 0 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 1 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 2 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 3 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248259544372559 at epoch 6 at applicants training\n",
      "loss: 1.613020420074463 at epoch 7 at applicants training\n",
      "loss: 1.6201454401016235 at epoch 8 at applicants training\n",
      "loss: 1.5692760944366455 at epoch 9 at applicants training\n",
      "loss: 1.6267009973526 at epoch 10 at applicants training\n",
      "loss: 1.708388328552246 at epoch 11 at applicants training\n",
      "loss: 1.6898623704910278 at epoch 12 at applicants training\n",
      "loss: 1.573075532913208 at epoch 13 at applicants training\n",
      "loss: 1.5973254442214966 at epoch 14 at applicants training\n",
      "loss: 1.614957571029663 at epoch 15 at applicants training\n",
      "loss: 1.6241344213485718 at epoch 16 at applicants training\n",
      "loss: 1.6248290538787842 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.6951807737350464 at epoch 0 at applicants training\n",
      "loss: 1.6848328113555908 at epoch 1 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 2 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 3 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 4 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 5 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 9 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 10 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 11 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 12 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 13 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 14 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 15 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 16 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 17 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 18 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 19 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 20 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 21 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 22 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.6853734254837036 at epoch 0 at applicants training\n",
      "loss: 1.6540108919143677 at epoch 1 at applicants training\n",
      "loss: 1.702015995979309 at epoch 2 at applicants training\n",
      "loss: 1.6128168106079102 at epoch 3 at applicants training\n",
      "loss: 1.5901449918746948 at epoch 4 at applicants training\n",
      "loss: 1.5606228113174438 at epoch 5 at applicants training\n",
      "loss: 1.5459837913513184 at epoch 6 at applicants training\n",
      "loss: 1.5119843482971191 at epoch 7 at applicants training\n",
      "loss: 1.5081017017364502 at epoch 8 at applicants training\n",
      "loss: 1.4834344387054443 at epoch 9 at applicants training\n",
      "loss: 1.4913760423660278 at epoch 10 at applicants training\n",
      "loss: 1.4744055271148682 at epoch 11 at applicants training\n",
      "loss: 1.471422553062439 at epoch 12 at applicants training\n",
      "loss: 1.465602159500122 at epoch 13 at applicants training\n",
      "loss: 1.4618310928344727 at epoch 14 at applicants training\n",
      "loss: 1.4592596292495728 at epoch 15 at applicants training\n",
      "loss: 1.4542611837387085 at epoch 16 at applicants training\n",
      "loss: 1.4425630569458008 at epoch 17 at applicants training\n",
      "loss: 1.4388642311096191 at epoch 18 at applicants training\n",
      "loss: 1.4285521507263184 at epoch 19 at applicants training\n",
      "loss: 1.4236150979995728 at epoch 20 at applicants training\n",
      "loss: 1.4201933145523071 at epoch 21 at applicants training\n",
      "loss: 1.4157931804656982 at epoch 22 at applicants training\n",
      "loss: 1.4121465682983398 at epoch 23 at applicants training\n",
      "loss: 1.4074095487594604 at epoch 24 at applicants training\n",
      "loss: 1.4058781862258911 at epoch 25 at applicants training\n",
      "loss: 1.4046849012374878 at epoch 26 at applicants training\n",
      "loss: 1.4051313400268555 at epoch 27 at applicants training\n",
      "loss: 1.4054149389266968 at epoch 28 at applicants training\n",
      "loss: 1.406054973602295 at epoch 29 at applicants training\n",
      "loss: 1.407561182975769 at epoch 30 at applicants training\n",
      "loss: 1.4070274829864502 at epoch 31 at applicants training\n",
      "loss: 1.4064738750457764 at epoch 32 at applicants training\n",
      "loss: 1.4061280488967896 at epoch 33 at applicants training\n",
      "loss: 1.4041939973831177 at epoch 34 at applicants training\n",
      "loss: 1.4038598537445068 at epoch 35 at applicants training\n",
      "loss: 1.4035005569458008 at epoch 36 at applicants training\n",
      "loss: 1.4026542901992798 at epoch 37 at applicants training\n",
      "loss: 1.4028452634811401 at epoch 38 at applicants training\n",
      "loss: 1.4024937152862549 at epoch 39 at applicants training\n",
      "loss: 1.4025025367736816 at epoch 40 at applicants training\n",
      "loss: 1.40228271484375 at epoch 41 at applicants training\n",
      "loss: 1.4021117687225342 at epoch 42 at applicants training\n",
      "loss: 1.4018322229385376 at epoch 43 at applicants training\n",
      "loss: 1.4015616178512573 at epoch 44 at applicants training\n",
      "loss: 1.4012348651885986 at epoch 45 at applicants training\n",
      "loss: 1.4008773565292358 at epoch 46 at applicants training\n",
      "loss: 1.400637149810791 at epoch 47 at applicants training\n",
      "loss: 1.400246024131775 at epoch 48 at applicants training\n",
      "loss: 1.4001092910766602 at epoch 49 at applicants training\n",
      "loss: 1.3998202085494995 at epoch 50 at applicants training\n",
      "loss: 1.3997360467910767 at epoch 51 at applicants training\n",
      "loss: 1.399497389793396 at epoch 52 at applicants training\n",
      "loss: 1.3993722200393677 at epoch 53 at applicants training\n",
      "loss: 1.399308681488037 at epoch 54 at applicants training\n",
      "loss: 1.3991520404815674 at epoch 55 at applicants training\n",
      "loss: 1.3990625143051147 at epoch 56 at applicants training\n",
      "loss: 1.3990697860717773 at epoch 57 at applicants training\n",
      "loss: 1.399051308631897 at epoch 58 at applicants training\n",
      "loss: 1.3989624977111816 at epoch 59 at applicants training\n",
      "loss: 1.398844599723816 at epoch 60 at applicants training\n",
      "loss: 1.3987330198287964 at epoch 61 at applicants training\n",
      "loss: 1.39864182472229 at epoch 62 at applicants training\n",
      "loss: 1.3986064195632935 at epoch 63 at applicants training\n",
      "loss: 1.3988003730773926 at epoch 64 at applicants training\n",
      "loss: 1.4000300168991089 at epoch 65 at applicants training\n",
      "loss: 1.4028842449188232 at epoch 66 at applicants training\n",
      "loss: 1.403958797454834 at epoch 67 at applicants training\n",
      "loss: 1.3982778787612915 at epoch 68 at applicants training\n",
      "loss: 1.4036709070205688 at epoch 69 at applicants training\n",
      "loss: 1.4058754444122314 at epoch 70 at applicants training\n",
      "loss: 1.4001786708831787 at epoch 71 at applicants training\n",
      "loss: 1.415087103843689 at epoch 72 at applicants training\n",
      "loss: 1.4009536504745483 at epoch 73 at applicants training\n",
      "loss: 1.4046533107757568 at epoch 74 at applicants training\n",
      "loss: 1.3999770879745483 at epoch 75 at applicants training\n",
      "loss: 1.4022959470748901 at epoch 76 at applicants training\n",
      "loss: 1.3988876342773438 at epoch 77 at applicants training\n",
      "loss: 1.4029957056045532 at epoch 78 at applicants training\n",
      "loss: 1.3991143703460693 at epoch 79 at applicants training\n",
      "loss: 1.4008358716964722 at epoch 80 at applicants training\n",
      "loss: 1.4005002975463867 at epoch 81 at applicants training\n",
      "loss: 1.3993873596191406 at epoch 82 at applicants training\n",
      "loss: 1.4013441801071167 at epoch 83 at applicants training\n",
      "loss: 1.3995070457458496 at epoch 84 at applicants training\n",
      "loss: 1.4002593755722046 at epoch 85 at applicants training\n",
      "loss: 1.4003721475601196 at epoch 86 at applicants training\n",
      "loss: 1.3996708393096924 at epoch 87 at applicants training\n",
      "loss: 1.4000853300094604 at epoch 88 at applicants training\n",
      "loss: 1.3997207880020142 at epoch 89 at applicants training\n",
      "loss: 1.3996624946594238 at epoch 90 at applicants training\n",
      "loss: 1.3999626636505127 at epoch 91 at applicants training\n",
      "loss: 1.3995752334594727 at epoch 92 at applicants training\n",
      "loss: 1.3994289636611938 at epoch 93 at applicants training\n",
      "loss: 1.3994885683059692 at epoch 94 at applicants training\n",
      "loss: 1.3992849588394165 at epoch 95 at applicants training\n",
      "loss: 1.3994243144989014 at epoch 96 at applicants training\n",
      "loss: 1.3988968133926392 at epoch 97 at applicants training\n",
      "loss: 1.3992959260940552 at epoch 98 at applicants training\n",
      "loss: 1.3990435600280762 at epoch 99 at applicants training\n",
      "loss: 1.7143564224243164 at epoch 0 at applicants training\n",
      "loss: 1.6840661764144897 at epoch 1 at applicants training\n",
      "loss: 1.6840977668762207 at epoch 2 at applicants training\n",
      "loss: 1.6706308126449585 at epoch 3 at applicants training\n",
      "loss: 1.6497633457183838 at epoch 4 at applicants training\n",
      "loss: 1.6297259330749512 at epoch 5 at applicants training\n",
      "loss: 1.599853515625 at epoch 6 at applicants training\n",
      "loss: 1.5927786827087402 at epoch 7 at applicants training\n",
      "loss: 1.618098497390747 at epoch 8 at applicants training\n",
      "loss: 1.6061993837356567 at epoch 9 at applicants training\n",
      "loss: 1.560747504234314 at epoch 10 at applicants training\n",
      "loss: 1.5849707126617432 at epoch 11 at applicants training\n",
      "loss: 1.5462043285369873 at epoch 12 at applicants training\n",
      "loss: 1.565990924835205 at epoch 13 at applicants training\n",
      "loss: 1.5511410236358643 at epoch 14 at applicants training\n",
      "loss: 1.5318101644515991 at epoch 15 at applicants training\n",
      "loss: 1.549888014793396 at epoch 16 at applicants training\n",
      "loss: 1.531931757926941 at epoch 17 at applicants training\n",
      "loss: 1.521360158920288 at epoch 18 at applicants training\n",
      "loss: 1.5265908241271973 at epoch 19 at applicants training\n",
      "loss: 1.5214756727218628 at epoch 20 at applicants training\n",
      "loss: 1.5157777070999146 at epoch 21 at applicants training\n",
      "loss: 1.505790114402771 at epoch 22 at applicants training\n",
      "loss: 1.5026066303253174 at epoch 23 at applicants training\n",
      "loss: 1.499211072921753 at epoch 24 at applicants training\n",
      "loss: 1.4930901527404785 at epoch 25 at applicants training\n",
      "loss: 1.4814198017120361 at epoch 26 at applicants training\n",
      "loss: 1.476601243019104 at epoch 27 at applicants training\n",
      "loss: 1.4761691093444824 at epoch 28 at applicants training\n",
      "loss: 1.4672515392303467 at epoch 29 at applicants training\n",
      "loss: 1.4638657569885254 at epoch 30 at applicants training\n",
      "loss: 1.4621578454971313 at epoch 31 at applicants training\n",
      "loss: 1.4612128734588623 at epoch 32 at applicants training\n",
      "loss: 1.459496021270752 at epoch 33 at applicants training\n",
      "loss: 1.457203984260559 at epoch 34 at applicants training\n",
      "loss: 1.4562938213348389 at epoch 35 at applicants training\n",
      "loss: 1.4552503824234009 at epoch 36 at applicants training\n",
      "loss: 1.4538674354553223 at epoch 37 at applicants training\n",
      "loss: 1.4532219171524048 at epoch 38 at applicants training\n",
      "loss: 1.4530802965164185 at epoch 39 at applicants training\n",
      "loss: 1.4519858360290527 at epoch 40 at applicants training\n",
      "loss: 1.4518873691558838 at epoch 41 at applicants training\n",
      "loss: 1.451491117477417 at epoch 42 at applicants training\n",
      "loss: 1.4509233236312866 at epoch 43 at applicants training\n",
      "loss: 1.450673222541809 at epoch 44 at applicants training\n",
      "loss: 1.4505163431167603 at epoch 45 at applicants training\n",
      "loss: 1.450097918510437 at epoch 46 at applicants training\n",
      "loss: 1.4497679471969604 at epoch 47 at applicants training\n",
      "loss: 1.4497109651565552 at epoch 48 at applicants training\n",
      "loss: 1.4493218660354614 at epoch 49 at applicants training\n",
      "loss: 1.4490317106246948 at epoch 50 at applicants training\n",
      "loss: 1.4489240646362305 at epoch 51 at applicants training\n",
      "loss: 1.448635220527649 at epoch 52 at applicants training\n",
      "loss: 1.4484370946884155 at epoch 53 at applicants training\n",
      "loss: 1.4480823278427124 at epoch 54 at applicants training\n",
      "loss: 1.4476256370544434 at epoch 55 at applicants training\n",
      "loss: 1.4468650817871094 at epoch 56 at applicants training\n",
      "loss: 1.4459826946258545 at epoch 57 at applicants training\n",
      "loss: 1.4452846050262451 at epoch 58 at applicants training\n",
      "loss: 1.4437447786331177 at epoch 59 at applicants training\n",
      "loss: 1.4422396421432495 at epoch 60 at applicants training\n",
      "loss: 1.4406280517578125 at epoch 61 at applicants training\n",
      "loss: 1.4379570484161377 at epoch 62 at applicants training\n",
      "loss: 1.4359734058380127 at epoch 63 at applicants training\n",
      "loss: 1.4338352680206299 at epoch 64 at applicants training\n",
      "loss: 1.43208909034729 at epoch 65 at applicants training\n",
      "loss: 1.4310133457183838 at epoch 66 at applicants training\n",
      "loss: 1.4305641651153564 at epoch 67 at applicants training\n",
      "loss: 1.429675579071045 at epoch 68 at applicants training\n",
      "loss: 1.4297360181808472 at epoch 69 at applicants training\n",
      "loss: 1.4289790391921997 at epoch 70 at applicants training\n",
      "loss: 1.4288884401321411 at epoch 71 at applicants training\n",
      "loss: 1.4285132884979248 at epoch 72 at applicants training\n",
      "loss: 1.4278514385223389 at epoch 73 at applicants training\n",
      "loss: 1.426973581314087 at epoch 74 at applicants training\n",
      "loss: 1.42533540725708 at epoch 75 at applicants training\n",
      "loss: 1.424428105354309 at epoch 76 at applicants training\n",
      "loss: 1.422865390777588 at epoch 77 at applicants training\n",
      "loss: 1.4220495223999023 at epoch 78 at applicants training\n",
      "loss: 1.421116590499878 at epoch 79 at applicants training\n",
      "loss: 1.420700192451477 at epoch 80 at applicants training\n",
      "loss: 1.4200716018676758 at epoch 81 at applicants training\n",
      "loss: 1.4198408126831055 at epoch 82 at applicants training\n",
      "loss: 1.4196146726608276 at epoch 83 at applicants training\n",
      "loss: 1.4192805290222168 at epoch 84 at applicants training\n",
      "loss: 1.4191827774047852 at epoch 85 at applicants training\n",
      "loss: 1.4190713167190552 at epoch 86 at applicants training\n",
      "loss: 1.4189605712890625 at epoch 87 at applicants training\n",
      "loss: 1.4187697172164917 at epoch 88 at applicants training\n",
      "loss: 1.4186748266220093 at epoch 89 at applicants training\n",
      "loss: 1.4186112880706787 at epoch 90 at applicants training\n",
      "loss: 1.418525218963623 at epoch 91 at applicants training\n",
      "loss: 1.418427586555481 at epoch 92 at applicants training\n",
      "loss: 1.4183250665664673 at epoch 93 at applicants training\n",
      "loss: 1.4182652235031128 at epoch 94 at applicants training\n",
      "loss: 1.4181829690933228 at epoch 95 at applicants training\n",
      "loss: 1.4180865287780762 at epoch 96 at applicants training\n",
      "loss: 1.417953610420227 at epoch 97 at applicants training\n",
      "loss: 1.417842149734497 at epoch 98 at applicants training\n",
      "loss: 1.4177350997924805 at epoch 99 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 0 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 1 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 2 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 3 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 4 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 5 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.684830904006958 at epoch 9 at applicants training\n",
      "loss: 1.681286334991455 at epoch 10 at applicants training\n",
      "loss: 1.6224844455718994 at epoch 11 at applicants training\n",
      "loss: 1.5957430601119995 at epoch 12 at applicants training\n",
      "loss: 1.593592643737793 at epoch 13 at applicants training\n",
      "loss: 1.5664623975753784 at epoch 14 at applicants training\n",
      "loss: 1.5214349031448364 at epoch 15 at applicants training\n",
      "loss: 1.5036802291870117 at epoch 16 at applicants training\n",
      "loss: 1.5125453472137451 at epoch 17 at applicants training\n",
      "loss: 1.4885929822921753 at epoch 18 at applicants training\n",
      "loss: 1.4863494634628296 at epoch 19 at applicants training\n",
      "loss: 1.4747014045715332 at epoch 20 at applicants training\n",
      "loss: 1.4719047546386719 at epoch 21 at applicants training\n",
      "loss: 1.4632447957992554 at epoch 22 at applicants training\n",
      "loss: 1.460182547569275 at epoch 23 at applicants training\n",
      "loss: 1.453244686126709 at epoch 24 at applicants training\n",
      "loss: 1.4494248628616333 at epoch 25 at applicants training\n",
      "loss: 1.4410412311553955 at epoch 26 at applicants training\n",
      "loss: 1.4370585680007935 at epoch 27 at applicants training\n",
      "loss: 1.4231898784637451 at epoch 28 at applicants training\n",
      "loss: 1.428665041923523 at epoch 29 at applicants training\n",
      "loss: 1.4205557107925415 at epoch 30 at applicants training\n",
      "loss: 1.419575810432434 at epoch 31 at applicants training\n",
      "loss: 1.4198293685913086 at epoch 32 at applicants training\n",
      "loss: 1.4137001037597656 at epoch 33 at applicants training\n",
      "loss: 1.414704442024231 at epoch 34 at applicants training\n",
      "loss: 1.4112800359725952 at epoch 35 at applicants training\n",
      "loss: 1.4124131202697754 at epoch 36 at applicants training\n",
      "loss: 1.4099836349487305 at epoch 37 at applicants training\n",
      "loss: 1.4097785949707031 at epoch 38 at applicants training\n",
      "loss: 1.4090157747268677 at epoch 39 at applicants training\n",
      "loss: 1.4074523448944092 at epoch 40 at applicants training\n",
      "loss: 1.406772494316101 at epoch 41 at applicants training\n",
      "loss: 1.4051673412322998 at epoch 42 at applicants training\n",
      "loss: 1.4044618606567383 at epoch 43 at applicants training\n",
      "loss: 1.403968095779419 at epoch 44 at applicants training\n",
      "loss: 1.4028103351593018 at epoch 45 at applicants training\n",
      "loss: 1.403440237045288 at epoch 46 at applicants training\n",
      "loss: 1.4022552967071533 at epoch 47 at applicants training\n",
      "loss: 1.4025025367736816 at epoch 48 at applicants training\n",
      "loss: 1.4032918214797974 at epoch 49 at applicants training\n",
      "loss: 1.4014924764633179 at epoch 50 at applicants training\n",
      "loss: 1.401930332183838 at epoch 51 at applicants training\n",
      "loss: 1.3909192085266113 at epoch 52 at applicants training\n",
      "loss: 1.352159023284912 at epoch 53 at applicants training\n",
      "loss: 1.3777344226837158 at epoch 54 at applicants training\n",
      "loss: 1.3275216817855835 at epoch 55 at applicants training\n",
      "loss: 1.3155486583709717 at epoch 56 at applicants training\n",
      "loss: 1.3042694330215454 at epoch 57 at applicants training\n",
      "loss: 1.2893552780151367 at epoch 58 at applicants training\n",
      "loss: 1.2849785089492798 at epoch 59 at applicants training\n",
      "loss: 1.2697447538375854 at epoch 60 at applicants training\n",
      "loss: 1.2712918519973755 at epoch 61 at applicants training\n",
      "loss: 1.2646708488464355 at epoch 62 at applicants training\n",
      "loss: 1.2540709972381592 at epoch 63 at applicants training\n",
      "loss: 1.254541039466858 at epoch 64 at applicants training\n",
      "loss: 1.2449736595153809 at epoch 65 at applicants training\n",
      "loss: 1.2414189577102661 at epoch 66 at applicants training\n",
      "loss: 1.2354459762573242 at epoch 67 at applicants training\n",
      "loss: 1.227491021156311 at epoch 68 at applicants training\n",
      "loss: 1.2272056341171265 at epoch 69 at applicants training\n",
      "loss: 1.2181645631790161 at epoch 70 at applicants training\n",
      "loss: 1.2126346826553345 at epoch 71 at applicants training\n",
      "loss: 1.2076188325881958 at epoch 72 at applicants training\n",
      "loss: 1.2013556957244873 at epoch 73 at applicants training\n",
      "loss: 1.1977570056915283 at epoch 74 at applicants training\n",
      "loss: 1.194604754447937 at epoch 75 at applicants training\n",
      "loss: 1.189780831336975 at epoch 76 at applicants training\n",
      "loss: 1.188259482383728 at epoch 77 at applicants training\n",
      "loss: 1.1873525381088257 at epoch 78 at applicants training\n",
      "loss: 1.1843342781066895 at epoch 79 at applicants training\n",
      "loss: 1.1852762699127197 at epoch 80 at applicants training\n",
      "loss: 1.1831107139587402 at epoch 81 at applicants training\n",
      "loss: 1.1834421157836914 at epoch 82 at applicants training\n",
      "loss: 1.182591438293457 at epoch 83 at applicants training\n",
      "loss: 1.18180513381958 at epoch 84 at applicants training\n",
      "loss: 1.1818634271621704 at epoch 85 at applicants training\n",
      "loss: 1.1807856559753418 at epoch 86 at applicants training\n",
      "loss: 1.1809340715408325 at epoch 87 at applicants training\n",
      "loss: 1.1799535751342773 at epoch 88 at applicants training\n",
      "loss: 1.1800271272659302 at epoch 89 at applicants training\n",
      "loss: 1.1792830228805542 at epoch 90 at applicants training\n",
      "loss: 1.1791656017303467 at epoch 91 at applicants training\n",
      "loss: 1.1787904500961304 at epoch 92 at applicants training\n",
      "loss: 1.178371548652649 at epoch 93 at applicants training\n",
      "loss: 1.178314447402954 at epoch 94 at applicants training\n",
      "loss: 1.1777018308639526 at epoch 95 at applicants training\n",
      "loss: 1.1776800155639648 at epoch 96 at applicants training\n",
      "loss: 1.177345633506775 at epoch 97 at applicants training\n",
      "loss: 1.1769912242889404 at epoch 98 at applicants training\n",
      "loss: 1.1768794059753418 at epoch 99 at applicants training\n",
      "loss: 1.6248328685760498 at epoch 0 at applicants training\n",
      "loss: 1.6245731115341187 at epoch 1 at applicants training\n",
      "loss: 1.5465080738067627 at epoch 2 at applicants training\n",
      "loss: 1.5771822929382324 at epoch 3 at applicants training\n",
      "loss: 1.519220232963562 at epoch 4 at applicants training\n",
      "loss: 1.4818795919418335 at epoch 5 at applicants training\n",
      "loss: 1.5012813806533813 at epoch 6 at applicants training\n",
      "loss: 1.4335474967956543 at epoch 7 at applicants training\n",
      "loss: 1.3931033611297607 at epoch 8 at applicants training\n",
      "loss: 1.4047740697860718 at epoch 9 at applicants training\n",
      "loss: 1.3714268207550049 at epoch 10 at applicants training\n",
      "loss: 1.3262487649917603 at epoch 11 at applicants training\n",
      "loss: 1.3266491889953613 at epoch 12 at applicants training\n",
      "loss: 1.3271396160125732 at epoch 13 at applicants training\n",
      "loss: 1.3299992084503174 at epoch 14 at applicants training\n",
      "loss: 1.3091380596160889 at epoch 15 at applicants training\n",
      "loss: 1.302727460861206 at epoch 16 at applicants training\n",
      "loss: 1.2816052436828613 at epoch 17 at applicants training\n",
      "loss: 1.2894726991653442 at epoch 18 at applicants training\n",
      "loss: 1.2869895696640015 at epoch 19 at applicants training\n",
      "loss: 1.270514726638794 at epoch 20 at applicants training\n",
      "loss: 1.2691473960876465 at epoch 21 at applicants training\n",
      "loss: 1.2701995372772217 at epoch 22 at applicants training\n",
      "loss: 1.265150785446167 at epoch 23 at applicants training\n",
      "loss: 1.2560901641845703 at epoch 24 at applicants training\n",
      "loss: 1.247714638710022 at epoch 25 at applicants training\n",
      "loss: 1.2442251443862915 at epoch 26 at applicants training\n",
      "loss: 1.2437089681625366 at epoch 27 at applicants training\n",
      "loss: 1.2352558374404907 at epoch 28 at applicants training\n",
      "loss: 1.23198664188385 at epoch 29 at applicants training\n",
      "loss: 1.2285507917404175 at epoch 30 at applicants training\n",
      "loss: 1.2264842987060547 at epoch 31 at applicants training\n",
      "loss: 1.221569538116455 at epoch 32 at applicants training\n",
      "loss: 1.2162463665008545 at epoch 33 at applicants training\n",
      "loss: 1.2138875722885132 at epoch 34 at applicants training\n",
      "loss: 1.2107435464859009 at epoch 35 at applicants training\n",
      "loss: 1.2062063217163086 at epoch 36 at applicants training\n",
      "loss: 1.2055553197860718 at epoch 37 at applicants training\n",
      "loss: 1.201729416847229 at epoch 38 at applicants training\n",
      "loss: 1.2009446620941162 at epoch 39 at applicants training\n",
      "loss: 1.198920726776123 at epoch 40 at applicants training\n",
      "loss: 1.1969319581985474 at epoch 41 at applicants training\n",
      "loss: 1.196204423904419 at epoch 42 at applicants training\n",
      "loss: 1.1943947076797485 at epoch 43 at applicants training\n",
      "loss: 1.1926915645599365 at epoch 44 at applicants training\n",
      "loss: 1.1915396451950073 at epoch 45 at applicants training\n",
      "loss: 1.1899510622024536 at epoch 46 at applicants training\n",
      "loss: 1.188080072402954 at epoch 47 at applicants training\n",
      "loss: 1.1871286630630493 at epoch 48 at applicants training\n",
      "loss: 1.1861094236373901 at epoch 49 at applicants training\n",
      "loss: 1.1853108406066895 at epoch 50 at applicants training\n",
      "loss: 1.1843812465667725 at epoch 51 at applicants training\n",
      "loss: 1.1831767559051514 at epoch 52 at applicants training\n",
      "loss: 1.1822534799575806 at epoch 53 at applicants training\n",
      "loss: 1.181574821472168 at epoch 54 at applicants training\n",
      "loss: 1.1810169219970703 at epoch 55 at applicants training\n",
      "loss: 1.1807150840759277 at epoch 56 at applicants training\n",
      "loss: 1.18040132522583 at epoch 57 at applicants training\n",
      "loss: 1.1796915531158447 at epoch 58 at applicants training\n",
      "loss: 1.178868055343628 at epoch 59 at applicants training\n",
      "loss: 1.1778299808502197 at epoch 60 at applicants training\n",
      "loss: 1.1761418581008911 at epoch 61 at applicants training\n",
      "loss: 1.1748541593551636 at epoch 62 at applicants training\n",
      "loss: 1.1736197471618652 at epoch 63 at applicants training\n",
      "loss: 1.1728601455688477 at epoch 64 at applicants training\n",
      "loss: 1.172217607498169 at epoch 65 at applicants training\n",
      "loss: 1.1718884706497192 at epoch 66 at applicants training\n",
      "loss: 1.1714320182800293 at epoch 67 at applicants training\n",
      "loss: 1.1712067127227783 at epoch 68 at applicants training\n",
      "loss: 1.1708858013153076 at epoch 69 at applicants training\n",
      "loss: 1.170677661895752 at epoch 70 at applicants training\n",
      "loss: 1.1705020666122437 at epoch 71 at applicants training\n",
      "loss: 1.1703801155090332 at epoch 72 at applicants training\n",
      "loss: 1.1703026294708252 at epoch 73 at applicants training\n",
      "loss: 1.1702216863632202 at epoch 74 at applicants training\n",
      "loss: 1.1701176166534424 at epoch 75 at applicants training\n",
      "loss: 1.1700345277786255 at epoch 76 at applicants training\n",
      "loss: 1.169884204864502 at epoch 77 at applicants training\n",
      "loss: 1.1697986125946045 at epoch 78 at applicants training\n",
      "loss: 1.1696637868881226 at epoch 79 at applicants training\n",
      "loss: 1.1695847511291504 at epoch 80 at applicants training\n",
      "loss: 1.1695088148117065 at epoch 81 at applicants training\n",
      "loss: 1.1694480180740356 at epoch 82 at applicants training\n",
      "loss: 1.1693918704986572 at epoch 83 at applicants training\n",
      "loss: 1.1693326234817505 at epoch 84 at applicants training\n",
      "loss: 1.169262409210205 at epoch 85 at applicants training\n",
      "loss: 1.1691893339157104 at epoch 86 at applicants training\n",
      "loss: 1.1691054105758667 at epoch 87 at applicants training\n",
      "loss: 1.1690030097961426 at epoch 88 at applicants training\n",
      "loss: 1.1689218282699585 at epoch 89 at applicants training\n",
      "loss: 1.1688179969787598 at epoch 90 at applicants training\n",
      "loss: 1.1687449216842651 at epoch 91 at applicants training\n",
      "loss: 1.1686699390411377 at epoch 92 at applicants training\n",
      "loss: 1.168609380722046 at epoch 93 at applicants training\n",
      "loss: 1.1685612201690674 at epoch 94 at applicants training\n",
      "loss: 1.1685001850128174 at epoch 95 at applicants training\n",
      "loss: 1.1684499979019165 at epoch 96 at applicants training\n",
      "loss: 1.1683869361877441 at epoch 97 at applicants training\n",
      "loss: 1.1683242321014404 at epoch 98 at applicants training\n",
      "loss: 1.1682634353637695 at epoch 99 at applicants training\n",
      "loss: 1.6731544733047485 at epoch 0 at applicants training\n",
      "loss: 1.6737405061721802 at epoch 1 at applicants training\n",
      "loss: 1.6747839450836182 at epoch 2 at applicants training\n",
      "loss: 1.6743887662887573 at epoch 3 at applicants training\n",
      "loss: 1.650672435760498 at epoch 4 at applicants training\n",
      "loss: 1.5465919971466064 at epoch 5 at applicants training\n",
      "loss: 1.6431260108947754 at epoch 6 at applicants training\n",
      "loss: 1.6376636028289795 at epoch 7 at applicants training\n",
      "loss: 1.5973402261734009 at epoch 8 at applicants training\n",
      "loss: 1.4956244230270386 at epoch 9 at applicants training\n",
      "loss: 1.427189826965332 at epoch 10 at applicants training\n",
      "loss: 1.46089768409729 at epoch 11 at applicants training\n",
      "loss: 1.4783021211624146 at epoch 12 at applicants training\n",
      "loss: 1.463932991027832 at epoch 13 at applicants training\n",
      "loss: 1.4272804260253906 at epoch 14 at applicants training\n",
      "loss: 1.386225700378418 at epoch 15 at applicants training\n",
      "loss: 1.3542966842651367 at epoch 16 at applicants training\n",
      "loss: 1.3515957593917847 at epoch 17 at applicants training\n",
      "loss: 1.2789660692214966 at epoch 18 at applicants training\n",
      "loss: 1.3604501485824585 at epoch 19 at applicants training\n",
      "loss: 1.3371866941452026 at epoch 20 at applicants training\n",
      "loss: 1.2772310972213745 at epoch 21 at applicants training\n",
      "loss: 1.3284049034118652 at epoch 22 at applicants training\n",
      "loss: 1.321807861328125 at epoch 23 at applicants training\n",
      "loss: 1.267575740814209 at epoch 24 at applicants training\n",
      "loss: 1.2771992683410645 at epoch 25 at applicants training\n",
      "loss: 1.2625082731246948 at epoch 26 at applicants training\n",
      "loss: 1.2178616523742676 at epoch 27 at applicants training\n",
      "loss: 1.2457045316696167 at epoch 28 at applicants training\n",
      "loss: 1.2312426567077637 at epoch 29 at applicants training\n",
      "loss: 1.2004878520965576 at epoch 30 at applicants training\n",
      "loss: 1.2212512493133545 at epoch 31 at applicants training\n",
      "loss: 1.229512095451355 at epoch 32 at applicants training\n",
      "loss: 1.2078115940093994 at epoch 33 at applicants training\n",
      "loss: 1.1775972843170166 at epoch 34 at applicants training\n",
      "loss: 1.2005332708358765 at epoch 35 at applicants training\n",
      "loss: 1.179455280303955 at epoch 36 at applicants training\n",
      "loss: 1.1887589693069458 at epoch 37 at applicants training\n",
      "loss: 1.176289439201355 at epoch 38 at applicants training\n",
      "loss: 1.1547831296920776 at epoch 39 at applicants training\n",
      "loss: 1.168879508972168 at epoch 40 at applicants training\n",
      "loss: 1.1646201610565186 at epoch 41 at applicants training\n",
      "loss: 1.1511918306350708 at epoch 42 at applicants training\n",
      "loss: 1.1461572647094727 at epoch 43 at applicants training\n",
      "loss: 1.1561734676361084 at epoch 44 at applicants training\n",
      "loss: 1.141938328742981 at epoch 45 at applicants training\n",
      "loss: 1.1460700035095215 at epoch 46 at applicants training\n",
      "loss: 1.1353003978729248 at epoch 47 at applicants training\n",
      "loss: 1.1388180255889893 at epoch 48 at applicants training\n",
      "loss: 1.1346347332000732 at epoch 49 at applicants training\n",
      "loss: 1.1230524778366089 at epoch 50 at applicants training\n",
      "loss: 1.1279864311218262 at epoch 51 at applicants training\n",
      "loss: 1.122331142425537 at epoch 52 at applicants training\n",
      "loss: 1.1181542873382568 at epoch 53 at applicants training\n",
      "loss: 1.1190913915634155 at epoch 54 at applicants training\n",
      "loss: 1.1138241291046143 at epoch 55 at applicants training\n",
      "loss: 1.1136220693588257 at epoch 56 at applicants training\n",
      "loss: 1.1126006841659546 at epoch 57 at applicants training\n",
      "loss: 1.1084840297698975 at epoch 58 at applicants training\n",
      "loss: 1.1101062297821045 at epoch 59 at applicants training\n",
      "loss: 1.1082026958465576 at epoch 60 at applicants training\n",
      "loss: 1.106353759765625 at epoch 61 at applicants training\n",
      "loss: 1.1073132753372192 at epoch 62 at applicants training\n",
      "loss: 1.106673002243042 at epoch 63 at applicants training\n",
      "loss: 1.1053705215454102 at epoch 64 at applicants training\n",
      "loss: 1.105440378189087 at epoch 65 at applicants training\n",
      "loss: 1.1057795286178589 at epoch 66 at applicants training\n",
      "loss: 1.1048133373260498 at epoch 67 at applicants training\n",
      "loss: 1.1044143438339233 at epoch 68 at applicants training\n",
      "loss: 1.1047654151916504 at epoch 69 at applicants training\n",
      "loss: 1.1044414043426514 at epoch 70 at applicants training\n",
      "loss: 1.1037441492080688 at epoch 71 at applicants training\n",
      "loss: 1.103542685508728 at epoch 72 at applicants training\n",
      "loss: 1.1034877300262451 at epoch 73 at applicants training\n",
      "loss: 1.1027193069458008 at epoch 74 at applicants training\n",
      "loss: 1.101709008216858 at epoch 75 at applicants training\n",
      "loss: 1.1006029844284058 at epoch 76 at applicants training\n",
      "loss: 1.0994348526000977 at epoch 77 at applicants training\n",
      "loss: 1.097845435142517 at epoch 78 at applicants training\n",
      "loss: 1.0963629484176636 at epoch 79 at applicants training\n",
      "loss: 1.0950288772583008 at epoch 80 at applicants training\n",
      "loss: 1.0944058895111084 at epoch 81 at applicants training\n",
      "loss: 1.0939313173294067 at epoch 82 at applicants training\n",
      "loss: 1.0931789875030518 at epoch 83 at applicants training\n",
      "loss: 1.0928421020507812 at epoch 84 at applicants training\n",
      "loss: 1.0927269458770752 at epoch 85 at applicants training\n",
      "loss: 1.092431902885437 at epoch 86 at applicants training\n",
      "loss: 1.091996192932129 at epoch 87 at applicants training\n",
      "loss: 1.0914051532745361 at epoch 88 at applicants training\n",
      "loss: 1.090752363204956 at epoch 89 at applicants training\n",
      "loss: 1.0900839567184448 at epoch 90 at applicants training\n",
      "loss: 1.0891218185424805 at epoch 91 at applicants training\n",
      "loss: 1.0882501602172852 at epoch 92 at applicants training\n",
      "loss: 1.0872578620910645 at epoch 93 at applicants training\n",
      "loss: 1.0863368511199951 at epoch 94 at applicants training\n",
      "loss: 1.085525393486023 at epoch 95 at applicants training\n",
      "loss: 1.085121750831604 at epoch 96 at applicants training\n",
      "loss: 1.0843427181243896 at epoch 97 at applicants training\n",
      "loss: 1.0841286182403564 at epoch 98 at applicants training\n",
      "loss: 1.083903431892395 at epoch 99 at applicants training\n",
      "loss: 1.7618581056594849 at epoch 0 at applicants training\n",
      "loss: 1.6928848028182983 at epoch 1 at applicants training\n",
      "loss: 1.720019817352295 at epoch 2 at applicants training\n",
      "loss: 1.669817328453064 at epoch 3 at applicants training\n",
      "loss: 1.6780282258987427 at epoch 4 at applicants training\n",
      "loss: 1.6813225746154785 at epoch 5 at applicants training\n",
      "loss: 1.666784644126892 at epoch 6 at applicants training\n",
      "loss: 1.6066563129425049 at epoch 7 at applicants training\n",
      "loss: 1.617813229560852 at epoch 8 at applicants training\n",
      "loss: 1.6256625652313232 at epoch 9 at applicants training\n",
      "loss: 1.6272854804992676 at epoch 10 at applicants training\n",
      "loss: 1.5924813747406006 at epoch 11 at applicants training\n",
      "loss: 1.5786889791488647 at epoch 12 at applicants training\n",
      "loss: 1.5948454141616821 at epoch 13 at applicants training\n",
      "loss: 1.5607943534851074 at epoch 14 at applicants training\n",
      "loss: 1.620572566986084 at epoch 15 at applicants training\n",
      "loss: 1.6250319480895996 at epoch 16 at applicants training\n",
      "loss: 1.6047035455703735 at epoch 17 at applicants training\n",
      "loss: 1.5549403429031372 at epoch 18 at applicants training\n",
      "loss: 1.5177662372589111 at epoch 19 at applicants training\n",
      "loss: 1.5757572650909424 at epoch 20 at applicants training\n",
      "loss: 1.6022852659225464 at epoch 21 at applicants training\n",
      "loss: 1.5914644002914429 at epoch 22 at applicants training\n",
      "loss: 1.5426058769226074 at epoch 23 at applicants training\n",
      "loss: 1.4956539869308472 at epoch 24 at applicants training\n",
      "loss: 1.526899814605713 at epoch 25 at applicants training\n",
      "loss: 1.5348858833312988 at epoch 26 at applicants training\n",
      "loss: 1.5278706550598145 at epoch 27 at applicants training\n",
      "loss: 1.505286693572998 at epoch 28 at applicants training\n",
      "loss: 1.482932686805725 at epoch 29 at applicants training\n",
      "loss: 1.4811275005340576 at epoch 30 at applicants training\n",
      "loss: 1.486099362373352 at epoch 31 at applicants training\n",
      "loss: 1.487991213798523 at epoch 32 at applicants training\n",
      "loss: 1.4637770652770996 at epoch 33 at applicants training\n",
      "loss: 1.4513490200042725 at epoch 34 at applicants training\n",
      "loss: 1.4696294069290161 at epoch 35 at applicants training\n",
      "loss: 1.449610710144043 at epoch 36 at applicants training\n",
      "loss: 1.4582759141921997 at epoch 37 at applicants training\n",
      "loss: 1.4607816934585571 at epoch 38 at applicants training\n",
      "loss: 1.4560366868972778 at epoch 39 at applicants training\n",
      "loss: 1.4390034675598145 at epoch 40 at applicants training\n",
      "loss: 1.4369261264801025 at epoch 41 at applicants training\n",
      "loss: 1.4507404565811157 at epoch 42 at applicants training\n",
      "loss: 1.4178290367126465 at epoch 43 at applicants training\n",
      "loss: 1.435958743095398 at epoch 44 at applicants training\n",
      "loss: 1.4296684265136719 at epoch 45 at applicants training\n",
      "loss: 1.4332244396209717 at epoch 46 at applicants training\n",
      "loss: 1.4184116125106812 at epoch 47 at applicants training\n",
      "loss: 1.4215607643127441 at epoch 48 at applicants training\n",
      "loss: 1.4229304790496826 at epoch 49 at applicants training\n",
      "loss: 1.4168046712875366 at epoch 50 at applicants training\n",
      "loss: 1.4227076768875122 at epoch 51 at applicants training\n",
      "loss: 1.417303442955017 at epoch 52 at applicants training\n",
      "loss: 1.411787509918213 at epoch 53 at applicants training\n",
      "loss: 1.4248164892196655 at epoch 54 at applicants training\n",
      "loss: 1.4119269847869873 at epoch 55 at applicants training\n",
      "loss: 1.4160808324813843 at epoch 56 at applicants training\n",
      "loss: 1.4158457517623901 at epoch 57 at applicants training\n",
      "loss: 1.4135959148406982 at epoch 58 at applicants training\n",
      "loss: 1.4163415431976318 at epoch 59 at applicants training\n",
      "loss: 1.4163798093795776 at epoch 60 at applicants training\n",
      "loss: 1.4131393432617188 at epoch 61 at applicants training\n",
      "loss: 1.4132177829742432 at epoch 62 at applicants training\n",
      "loss: 1.4131296873092651 at epoch 63 at applicants training\n",
      "loss: 1.4104498624801636 at epoch 64 at applicants training\n",
      "loss: 1.4115105867385864 at epoch 65 at applicants training\n",
      "loss: 1.4118250608444214 at epoch 66 at applicants training\n",
      "loss: 1.4088228940963745 at epoch 67 at applicants training\n",
      "loss: 1.4089268445968628 at epoch 68 at applicants training\n",
      "loss: 1.409348964691162 at epoch 69 at applicants training\n",
      "loss: 1.4088163375854492 at epoch 70 at applicants training\n",
      "loss: 1.408118486404419 at epoch 71 at applicants training\n",
      "loss: 1.4072444438934326 at epoch 72 at applicants training\n",
      "loss: 1.4076424837112427 at epoch 73 at applicants training\n",
      "loss: 1.4073896408081055 at epoch 74 at applicants training\n",
      "loss: 1.4060362577438354 at epoch 75 at applicants training\n",
      "loss: 1.4064832925796509 at epoch 76 at applicants training\n",
      "loss: 1.406541109085083 at epoch 77 at applicants training\n",
      "loss: 1.405488133430481 at epoch 78 at applicants training\n",
      "loss: 1.4057378768920898 at epoch 79 at applicants training\n",
      "loss: 1.4060161113739014 at epoch 80 at applicants training\n",
      "loss: 1.4051907062530518 at epoch 81 at applicants training\n",
      "loss: 1.4055986404418945 at epoch 82 at applicants training\n",
      "loss: 1.4054441452026367 at epoch 83 at applicants training\n",
      "loss: 1.4052082300186157 at epoch 84 at applicants training\n",
      "loss: 1.405055284500122 at epoch 85 at applicants training\n",
      "loss: 1.4049710035324097 at epoch 86 at applicants training\n",
      "loss: 1.4045000076293945 at epoch 87 at applicants training\n",
      "loss: 1.4042991399765015 at epoch 88 at applicants training\n",
      "loss: 1.4041060209274292 at epoch 89 at applicants training\n",
      "loss: 1.4036035537719727 at epoch 90 at applicants training\n",
      "loss: 1.403775691986084 at epoch 91 at applicants training\n",
      "loss: 1.4033074378967285 at epoch 92 at applicants training\n",
      "loss: 1.4034428596496582 at epoch 93 at applicants training\n",
      "loss: 1.4032455682754517 at epoch 94 at applicants training\n",
      "loss: 1.4031264781951904 at epoch 95 at applicants training\n",
      "loss: 1.403166651725769 at epoch 96 at applicants training\n",
      "loss: 1.402919054031372 at epoch 97 at applicants training\n",
      "loss: 1.4030224084854126 at epoch 98 at applicants training\n",
      "loss: 1.4028373956680298 at epoch 99 at applicants training\n",
      "loss: 1.6747961044311523 at epoch 0 at applicants training\n",
      "loss: 1.6735397577285767 at epoch 1 at applicants training\n",
      "loss: 1.6411008834838867 at epoch 2 at applicants training\n",
      "loss: 1.545119285583496 at epoch 3 at applicants training\n",
      "loss: 1.5672813653945923 at epoch 4 at applicants training\n",
      "loss: 1.5138920545578003 at epoch 5 at applicants training\n",
      "loss: 1.532132863998413 at epoch 6 at applicants training\n",
      "loss: 1.4796643257141113 at epoch 7 at applicants training\n",
      "loss: 1.4269721508026123 at epoch 8 at applicants training\n",
      "loss: 1.4331638813018799 at epoch 9 at applicants training\n",
      "loss: 1.4230743646621704 at epoch 10 at applicants training\n",
      "loss: 1.404423475265503 at epoch 11 at applicants training\n",
      "loss: 1.3835442066192627 at epoch 12 at applicants training\n",
      "loss: 1.3794183731079102 at epoch 13 at applicants training\n",
      "loss: 1.3865103721618652 at epoch 14 at applicants training\n",
      "loss: 1.3700873851776123 at epoch 15 at applicants training\n",
      "loss: 1.3518192768096924 at epoch 16 at applicants training\n",
      "loss: 1.3517440557479858 at epoch 17 at applicants training\n",
      "loss: 1.346157193183899 at epoch 18 at applicants training\n",
      "loss: 1.340500831604004 at epoch 19 at applicants training\n",
      "loss: 1.3363338708877563 at epoch 20 at applicants training\n",
      "loss: 1.3293217420578003 at epoch 21 at applicants training\n",
      "loss: 1.3310760259628296 at epoch 22 at applicants training\n",
      "loss: 1.3250869512557983 at epoch 23 at applicants training\n",
      "loss: 1.3206031322479248 at epoch 24 at applicants training\n",
      "loss: 1.3215782642364502 at epoch 25 at applicants training\n",
      "loss: 1.316296100616455 at epoch 26 at applicants training\n",
      "loss: 1.3160552978515625 at epoch 27 at applicants training\n",
      "loss: 1.3139173984527588 at epoch 28 at applicants training\n",
      "loss: 1.3147364854812622 at epoch 29 at applicants training\n",
      "loss: 1.312841534614563 at epoch 30 at applicants training\n",
      "loss: 1.3138129711151123 at epoch 31 at applicants training\n",
      "loss: 1.3126453161239624 at epoch 32 at applicants training\n",
      "loss: 1.3131860494613647 at epoch 33 at applicants training\n",
      "loss: 1.3117930889129639 at epoch 34 at applicants training\n",
      "loss: 1.3116810321807861 at epoch 35 at applicants training\n",
      "loss: 1.3117973804473877 at epoch 36 at applicants training\n",
      "loss: 1.3109468221664429 at epoch 37 at applicants training\n",
      "loss: 1.3103805780410767 at epoch 38 at applicants training\n",
      "loss: 1.3104814291000366 at epoch 39 at applicants training\n",
      "loss: 1.3104325532913208 at epoch 40 at applicants training\n",
      "loss: 1.3098727464675903 at epoch 41 at applicants training\n",
      "loss: 1.30967378616333 at epoch 42 at applicants training\n",
      "loss: 1.3095543384552002 at epoch 43 at applicants training\n",
      "loss: 1.309450626373291 at epoch 44 at applicants training\n",
      "loss: 1.3092995882034302 at epoch 45 at applicants training\n",
      "loss: 1.3090695142745972 at epoch 46 at applicants training\n",
      "loss: 1.3090193271636963 at epoch 47 at applicants training\n",
      "loss: 1.3089547157287598 at epoch 48 at applicants training\n",
      "loss: 1.3086169958114624 at epoch 49 at applicants training\n",
      "loss: 1.3084514141082764 at epoch 50 at applicants training\n",
      "loss: 1.3083336353302002 at epoch 51 at applicants training\n",
      "loss: 1.3079437017440796 at epoch 52 at applicants training\n",
      "loss: 1.3077698945999146 at epoch 53 at applicants training\n",
      "loss: 1.3076249361038208 at epoch 54 at applicants training\n",
      "loss: 1.3074984550476074 at epoch 55 at applicants training\n",
      "loss: 1.307454228401184 at epoch 56 at applicants training\n",
      "loss: 1.3075637817382812 at epoch 57 at applicants training\n",
      "loss: 1.3074698448181152 at epoch 58 at applicants training\n",
      "loss: 1.3073358535766602 at epoch 59 at applicants training\n",
      "loss: 1.3073091506958008 at epoch 60 at applicants training\n",
      "loss: 1.307072401046753 at epoch 61 at applicants training\n",
      "loss: 1.3068979978561401 at epoch 62 at applicants training\n",
      "loss: 1.3067426681518555 at epoch 63 at applicants training\n",
      "loss: 1.3065788745880127 at epoch 64 at applicants training\n",
      "loss: 1.3064970970153809 at epoch 65 at applicants training\n",
      "loss: 1.3064281940460205 at epoch 66 at applicants training\n",
      "loss: 1.3063360452651978 at epoch 67 at applicants training\n",
      "loss: 1.3063045740127563 at epoch 68 at applicants training\n",
      "loss: 1.3062126636505127 at epoch 69 at applicants training\n",
      "loss: 1.3060476779937744 at epoch 70 at applicants training\n",
      "loss: 1.3059980869293213 at epoch 71 at applicants training\n",
      "loss: 1.305956244468689 at epoch 72 at applicants training\n",
      "loss: 1.3058710098266602 at epoch 73 at applicants training\n",
      "loss: 1.3058252334594727 at epoch 74 at applicants training\n",
      "loss: 1.3057715892791748 at epoch 75 at applicants training\n",
      "loss: 1.3057289123535156 at epoch 76 at applicants training\n",
      "loss: 1.3057124614715576 at epoch 77 at applicants training\n",
      "loss: 1.3057223558425903 at epoch 78 at applicants training\n",
      "loss: 1.3059298992156982 at epoch 79 at applicants training\n",
      "loss: 1.3064688444137573 at epoch 80 at applicants training\n",
      "loss: 1.3071436882019043 at epoch 81 at applicants training\n",
      "loss: 1.3067069053649902 at epoch 82 at applicants training\n",
      "loss: 1.3056776523590088 at epoch 83 at applicants training\n",
      "loss: 1.3054858446121216 at epoch 84 at applicants training\n",
      "loss: 1.30599045753479 at epoch 85 at applicants training\n",
      "loss: 1.3062889575958252 at epoch 86 at applicants training\n",
      "loss: 1.305959939956665 at epoch 87 at applicants training\n",
      "loss: 1.305356502532959 at epoch 88 at applicants training\n",
      "loss: 1.3052352666854858 at epoch 89 at applicants training\n",
      "loss: 1.3055588006973267 at epoch 90 at applicants training\n",
      "loss: 1.306026816368103 at epoch 91 at applicants training\n",
      "loss: 1.3062310218811035 at epoch 92 at applicants training\n",
      "loss: 1.3055932521820068 at epoch 93 at applicants training\n",
      "loss: 1.3051692247390747 at epoch 94 at applicants training\n",
      "loss: 1.3050117492675781 at epoch 95 at applicants training\n",
      "loss: 1.3050357103347778 at epoch 96 at applicants training\n",
      "loss: 1.3052057027816772 at epoch 97 at applicants training\n",
      "loss: 1.3055551052093506 at epoch 98 at applicants training\n",
      "loss: 1.3059589862823486 at epoch 99 at applicants training\n",
      "loss: 1.6770135164260864 at epoch 0 at applicants training\n",
      "loss: 1.6445791721343994 at epoch 1 at applicants training\n",
      "loss: 1.6147677898406982 at epoch 2 at applicants training\n",
      "loss: 1.637465476989746 at epoch 3 at applicants training\n",
      "loss: 1.587066650390625 at epoch 4 at applicants training\n",
      "loss: 1.511724829673767 at epoch 5 at applicants training\n",
      "loss: 1.491896152496338 at epoch 6 at applicants training\n",
      "loss: 1.5627955198287964 at epoch 7 at applicants training\n",
      "loss: 1.5272058248519897 at epoch 8 at applicants training\n",
      "loss: 1.5426827669143677 at epoch 9 at applicants training\n",
      "loss: 1.5095192193984985 at epoch 10 at applicants training\n",
      "loss: 1.5127586126327515 at epoch 11 at applicants training\n",
      "loss: 1.502011775970459 at epoch 12 at applicants training\n",
      "loss: 1.471354365348816 at epoch 13 at applicants training\n",
      "loss: 1.480622410774231 at epoch 14 at applicants training\n",
      "loss: 1.4537205696105957 at epoch 15 at applicants training\n",
      "loss: 1.4572055339813232 at epoch 16 at applicants training\n",
      "loss: 1.4428249597549438 at epoch 17 at applicants training\n",
      "loss: 1.4309841394424438 at epoch 18 at applicants training\n",
      "loss: 1.4171847105026245 at epoch 19 at applicants training\n",
      "loss: 1.4011967182159424 at epoch 20 at applicants training\n",
      "loss: 1.3957600593566895 at epoch 21 at applicants training\n",
      "loss: 1.3997961282730103 at epoch 22 at applicants training\n",
      "loss: 1.387750267982483 at epoch 23 at applicants training\n",
      "loss: 1.3713356256484985 at epoch 24 at applicants training\n",
      "loss: 1.381494164466858 at epoch 25 at applicants training\n",
      "loss: 1.360306739807129 at epoch 26 at applicants training\n",
      "loss: 1.3478715419769287 at epoch 27 at applicants training\n",
      "loss: 1.310149073600769 at epoch 28 at applicants training\n",
      "loss: 1.225570797920227 at epoch 29 at applicants training\n",
      "loss: 1.2712864875793457 at epoch 30 at applicants training\n",
      "loss: 1.210852861404419 at epoch 31 at applicants training\n",
      "loss: 1.2616591453552246 at epoch 32 at applicants training\n",
      "loss: 1.2401375770568848 at epoch 33 at applicants training\n",
      "loss: 1.1722073554992676 at epoch 34 at applicants training\n",
      "loss: 1.2111029624938965 at epoch 35 at applicants training\n",
      "loss: 1.2047241926193237 at epoch 36 at applicants training\n",
      "loss: 1.161666750907898 at epoch 37 at applicants training\n",
      "loss: 1.1840964555740356 at epoch 38 at applicants training\n",
      "loss: 1.15987229347229 at epoch 39 at applicants training\n",
      "loss: 1.1524076461791992 at epoch 40 at applicants training\n",
      "loss: 1.1693027019500732 at epoch 41 at applicants training\n",
      "loss: 1.1502397060394287 at epoch 42 at applicants training\n",
      "loss: 1.1288273334503174 at epoch 43 at applicants training\n",
      "loss: 1.1486531496047974 at epoch 44 at applicants training\n",
      "loss: 1.142404317855835 at epoch 45 at applicants training\n",
      "loss: 1.119655966758728 at epoch 46 at applicants training\n",
      "loss: 1.1373038291931152 at epoch 47 at applicants training\n",
      "loss: 1.1290851831436157 at epoch 48 at applicants training\n",
      "loss: 1.1225398778915405 at epoch 49 at applicants training\n",
      "loss: 1.1220080852508545 at epoch 50 at applicants training\n",
      "loss: 1.1213247776031494 at epoch 51 at applicants training\n",
      "loss: 1.1086609363555908 at epoch 52 at applicants training\n",
      "loss: 1.1154688596725464 at epoch 53 at applicants training\n",
      "loss: 1.1090502738952637 at epoch 54 at applicants training\n",
      "loss: 1.1030348539352417 at epoch 55 at applicants training\n",
      "loss: 1.1068236827850342 at epoch 56 at applicants training\n",
      "loss: 1.1047133207321167 at epoch 57 at applicants training\n",
      "loss: 1.0981709957122803 at epoch 58 at applicants training\n",
      "loss: 1.1007401943206787 at epoch 59 at applicants training\n",
      "loss: 1.1008062362670898 at epoch 60 at applicants training\n",
      "loss: 1.09619140625 at epoch 61 at applicants training\n",
      "loss: 1.0966817140579224 at epoch 62 at applicants training\n",
      "loss: 1.0964494943618774 at epoch 63 at applicants training\n",
      "loss: 1.0956302881240845 at epoch 64 at applicants training\n",
      "loss: 1.0932189226150513 at epoch 65 at applicants training\n",
      "loss: 1.0950349569320679 at epoch 66 at applicants training\n",
      "loss: 1.092620611190796 at epoch 67 at applicants training\n",
      "loss: 1.0921117067337036 at epoch 68 at applicants training\n",
      "loss: 1.0925155878067017 at epoch 69 at applicants training\n",
      "loss: 1.0914642810821533 at epoch 70 at applicants training\n",
      "loss: 1.0904713869094849 at epoch 71 at applicants training\n",
      "loss: 1.0904368162155151 at epoch 72 at applicants training\n",
      "loss: 1.0908125638961792 at epoch 73 at applicants training\n",
      "loss: 1.0894168615341187 at epoch 74 at applicants training\n",
      "loss: 1.0888556241989136 at epoch 75 at applicants training\n",
      "loss: 1.0894052982330322 at epoch 76 at applicants training\n",
      "loss: 1.0887401103973389 at epoch 77 at applicants training\n",
      "loss: 1.0881680250167847 at epoch 78 at applicants training\n",
      "loss: 1.0880753993988037 at epoch 79 at applicants training\n",
      "loss: 1.08803391456604 at epoch 80 at applicants training\n",
      "loss: 1.087668776512146 at epoch 81 at applicants training\n",
      "loss: 1.0872858762741089 at epoch 82 at applicants training\n",
      "loss: 1.0872913599014282 at epoch 83 at applicants training\n",
      "loss: 1.0870976448059082 at epoch 84 at applicants training\n",
      "loss: 1.0867621898651123 at epoch 85 at applicants training\n",
      "loss: 1.0867313146591187 at epoch 86 at applicants training\n",
      "loss: 1.0865715742111206 at epoch 87 at applicants training\n",
      "loss: 1.0863137245178223 at epoch 88 at applicants training\n",
      "loss: 1.0862387418746948 at epoch 89 at applicants training\n",
      "loss: 1.0860916376113892 at epoch 90 at applicants training\n",
      "loss: 1.0859112739562988 at epoch 91 at applicants training\n",
      "loss: 1.0857901573181152 at epoch 92 at applicants training\n",
      "loss: 1.0856671333312988 at epoch 93 at applicants training\n",
      "loss: 1.0855305194854736 at epoch 94 at applicants training\n",
      "loss: 1.0853759050369263 at epoch 95 at applicants training\n",
      "loss: 1.0852340459823608 at epoch 96 at applicants training\n",
      "loss: 1.0851470232009888 at epoch 97 at applicants training\n",
      "loss: 1.0850030183792114 at epoch 98 at applicants training\n",
      "loss: 1.0848264694213867 at epoch 99 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 0 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 1 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 2 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 3 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 4 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 5 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 9 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 10 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 11 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 12 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 13 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 14 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 15 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 16 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 17 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 18 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 19 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 20 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 21 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 22 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.6242977380752563 at epoch 0 at applicants training\n",
      "loss: 1.6030880212783813 at epoch 1 at applicants training\n",
      "loss: 1.6007922887802124 at epoch 2 at applicants training\n",
      "loss: 1.6246017217636108 at epoch 3 at applicants training\n",
      "loss: 1.6248325109481812 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.6454706192016602 at epoch 0 at applicants training\n",
      "loss: 1.624483346939087 at epoch 1 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 2 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 3 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.6743583679199219 at epoch 0 at applicants training\n",
      "loss: 1.6765190362930298 at epoch 1 at applicants training\n",
      "loss: 1.6580305099487305 at epoch 2 at applicants training\n",
      "loss: 1.6336274147033691 at epoch 3 at applicants training\n",
      "loss: 1.60102117061615 at epoch 4 at applicants training\n",
      "loss: 1.5864481925964355 at epoch 5 at applicants training\n",
      "loss: 1.5828709602355957 at epoch 6 at applicants training\n",
      "loss: 1.584816575050354 at epoch 7 at applicants training\n",
      "loss: 1.5772608518600464 at epoch 8 at applicants training\n",
      "loss: 1.5786864757537842 at epoch 9 at applicants training\n",
      "loss: 1.5753693580627441 at epoch 10 at applicants training\n",
      "loss: 1.5787749290466309 at epoch 11 at applicants training\n",
      "loss: 1.5723457336425781 at epoch 12 at applicants training\n",
      "loss: 1.5724859237670898 at epoch 13 at applicants training\n",
      "loss: 1.5723397731781006 at epoch 14 at applicants training\n",
      "loss: 1.5721555948257446 at epoch 15 at applicants training\n",
      "loss: 1.571720838546753 at epoch 16 at applicants training\n",
      "loss: 1.5713406801223755 at epoch 17 at applicants training\n",
      "loss: 1.5708703994750977 at epoch 18 at applicants training\n",
      "loss: 1.570414423942566 at epoch 19 at applicants training\n",
      "loss: 1.5699429512023926 at epoch 20 at applicants training\n",
      "loss: 1.5696606636047363 at epoch 21 at applicants training\n",
      "loss: 1.5692864656448364 at epoch 22 at applicants training\n",
      "loss: 1.5691311359405518 at epoch 23 at applicants training\n",
      "loss: 1.5689637660980225 at epoch 24 at applicants training\n",
      "loss: 1.5688029527664185 at epoch 25 at applicants training\n",
      "loss: 1.568752408027649 at epoch 26 at applicants training\n",
      "loss: 1.568627119064331 at epoch 27 at applicants training\n",
      "loss: 1.5684353113174438 at epoch 28 at applicants training\n",
      "loss: 1.5682880878448486 at epoch 29 at applicants training\n",
      "loss: 1.568169355392456 at epoch 30 at applicants training\n",
      "loss: 1.5680044889450073 at epoch 31 at applicants training\n",
      "loss: 1.5677965879440308 at epoch 32 at applicants training\n",
      "loss: 1.567643165588379 at epoch 33 at applicants training\n",
      "loss: 1.567592978477478 at epoch 34 at applicants training\n",
      "loss: 1.5678857564926147 at epoch 35 at applicants training\n",
      "loss: 1.5690515041351318 at epoch 36 at applicants training\n",
      "loss: 1.5694901943206787 at epoch 37 at applicants training\n",
      "loss: 1.5680527687072754 at epoch 38 at applicants training\n",
      "loss: 1.5667409896850586 at epoch 39 at applicants training\n",
      "loss: 1.566891074180603 at epoch 40 at applicants training\n",
      "loss: 1.5679914951324463 at epoch 41 at applicants training\n",
      "loss: 1.5677448511123657 at epoch 42 at applicants training\n",
      "loss: 1.5666950941085815 at epoch 43 at applicants training\n",
      "loss: 1.5662747621536255 at epoch 44 at applicants training\n",
      "loss: 1.5667316913604736 at epoch 45 at applicants training\n",
      "loss: 1.5673153400421143 at epoch 46 at applicants training\n",
      "loss: 1.5670831203460693 at epoch 47 at applicants training\n",
      "loss: 1.5662431716918945 at epoch 48 at applicants training\n",
      "loss: 1.5659229755401611 at epoch 49 at applicants training\n",
      "loss: 1.566182255744934 at epoch 50 at applicants training\n",
      "loss: 1.5666378736495972 at epoch 51 at applicants training\n",
      "loss: 1.566941499710083 at epoch 52 at applicants training\n",
      "loss: 1.5662527084350586 at epoch 53 at applicants training\n",
      "loss: 1.5657165050506592 at epoch 54 at applicants training\n",
      "loss: 1.5655412673950195 at epoch 55 at applicants training\n",
      "loss: 1.565708041191101 at epoch 56 at applicants training\n",
      "loss: 1.5660731792449951 at epoch 57 at applicants training\n",
      "loss: 1.5661346912384033 at epoch 58 at applicants training\n",
      "loss: 1.5661081075668335 at epoch 59 at applicants training\n",
      "loss: 1.5655931234359741 at epoch 60 at applicants training\n",
      "loss: 1.5652492046356201 at epoch 61 at applicants training\n",
      "loss: 1.565057396888733 at epoch 62 at applicants training\n",
      "loss: 1.5650229454040527 at epoch 63 at applicants training\n",
      "loss: 1.5651233196258545 at epoch 64 at applicants training\n",
      "loss: 1.5653038024902344 at epoch 65 at applicants training\n",
      "loss: 1.5656981468200684 at epoch 66 at applicants training\n",
      "loss: 1.5656349658966064 at epoch 67 at applicants training\n",
      "loss: 1.565737247467041 at epoch 68 at applicants training\n",
      "loss: 1.565088152885437 at epoch 69 at applicants training\n",
      "loss: 1.564713716506958 at epoch 70 at applicants training\n",
      "loss: 1.564515233039856 at epoch 71 at applicants training\n",
      "loss: 1.5645017623901367 at epoch 72 at applicants training\n",
      "loss: 1.564670205116272 at epoch 73 at applicants training\n",
      "loss: 1.5648998022079468 at epoch 74 at applicants training\n",
      "loss: 1.5654889345169067 at epoch 75 at applicants training\n",
      "loss: 1.5653096437454224 at epoch 76 at applicants training\n",
      "loss: 1.565521240234375 at epoch 77 at applicants training\n",
      "loss: 1.5646618604660034 at epoch 78 at applicants training\n",
      "loss: 1.5642387866973877 at epoch 79 at applicants training\n",
      "loss: 1.5639450550079346 at epoch 80 at applicants training\n",
      "loss: 1.5638853311538696 at epoch 81 at applicants training\n",
      "loss: 1.5640099048614502 at epoch 82 at applicants training\n",
      "loss: 1.5642640590667725 at epoch 83 at applicants training\n",
      "loss: 1.564914345741272 at epoch 84 at applicants training\n",
      "loss: 1.5648843050003052 at epoch 85 at applicants training\n",
      "loss: 1.5654062032699585 at epoch 86 at applicants training\n",
      "loss: 1.5642203092575073 at epoch 87 at applicants training\n",
      "loss: 1.5636589527130127 at epoch 88 at applicants training\n",
      "loss: 1.563391923904419 at epoch 89 at applicants training\n",
      "loss: 1.5634284019470215 at epoch 90 at applicants training\n",
      "loss: 1.563738226890564 at epoch 91 at applicants training\n",
      "loss: 1.5641684532165527 at epoch 92 at applicants training\n",
      "loss: 1.5651849508285522 at epoch 93 at applicants training\n",
      "loss: 1.5641770362854004 at epoch 94 at applicants training\n",
      "loss: 1.5636636018753052 at epoch 95 at applicants training\n",
      "loss: 1.5631331205368042 at epoch 96 at applicants training\n",
      "loss: 1.5629810094833374 at epoch 97 at applicants training\n",
      "loss: 1.562989354133606 at epoch 98 at applicants training\n",
      "loss: 1.563086986541748 at epoch 99 at applicants training\n",
      "loss: 1.6320537328720093 at epoch 0 at applicants training\n",
      "loss: 1.647218942642212 at epoch 1 at applicants training\n",
      "loss: 1.5578378438949585 at epoch 2 at applicants training\n",
      "loss: 1.5934994220733643 at epoch 3 at applicants training\n",
      "loss: 1.6053844690322876 at epoch 4 at applicants training\n",
      "loss: 1.6075973510742188 at epoch 5 at applicants training\n",
      "loss: 1.5923590660095215 at epoch 6 at applicants training\n",
      "loss: 1.5386760234832764 at epoch 7 at applicants training\n",
      "loss: 1.5275906324386597 at epoch 8 at applicants training\n",
      "loss: 1.6017379760742188 at epoch 9 at applicants training\n",
      "loss: 1.6116664409637451 at epoch 10 at applicants training\n",
      "loss: 1.565444827079773 at epoch 11 at applicants training\n",
      "loss: 1.4882043600082397 at epoch 12 at applicants training\n",
      "loss: 1.5091148614883423 at epoch 13 at applicants training\n",
      "loss: 1.5484833717346191 at epoch 14 at applicants training\n",
      "loss: 1.5562388896942139 at epoch 15 at applicants training\n",
      "loss: 1.5303257703781128 at epoch 16 at applicants training\n",
      "loss: 1.4652003049850464 at epoch 17 at applicants training\n",
      "loss: 1.4700160026550293 at epoch 18 at applicants training\n",
      "loss: 1.4890437126159668 at epoch 19 at applicants training\n",
      "loss: 1.4940165281295776 at epoch 20 at applicants training\n",
      "loss: 1.4712566137313843 at epoch 21 at applicants training\n",
      "loss: 1.454533576965332 at epoch 22 at applicants training\n",
      "loss: 1.4516983032226562 at epoch 23 at applicants training\n",
      "loss: 1.462561845779419 at epoch 24 at applicants training\n",
      "loss: 1.4626867771148682 at epoch 25 at applicants training\n",
      "loss: 1.4511700868606567 at epoch 26 at applicants training\n",
      "loss: 1.4433578252792358 at epoch 27 at applicants training\n",
      "loss: 1.4301544427871704 at epoch 28 at applicants training\n",
      "loss: 1.4376689195632935 at epoch 29 at applicants training\n",
      "loss: 1.4438148736953735 at epoch 30 at applicants training\n",
      "loss: 1.4319720268249512 at epoch 31 at applicants training\n",
      "loss: 1.4238618612289429 at epoch 32 at applicants training\n",
      "loss: 1.4264030456542969 at epoch 33 at applicants training\n",
      "loss: 1.424172043800354 at epoch 34 at applicants training\n",
      "loss: 1.4176520109176636 at epoch 35 at applicants training\n",
      "loss: 1.4245710372924805 at epoch 36 at applicants training\n",
      "loss: 1.423837423324585 at epoch 37 at applicants training\n",
      "loss: 1.416409969329834 at epoch 38 at applicants training\n",
      "loss: 1.4154715538024902 at epoch 39 at applicants training\n",
      "loss: 1.4209387302398682 at epoch 40 at applicants training\n",
      "loss: 1.4132168292999268 at epoch 41 at applicants training\n",
      "loss: 1.4140292406082153 at epoch 42 at applicants training\n",
      "loss: 1.415913701057434 at epoch 43 at applicants training\n",
      "loss: 1.4148215055465698 at epoch 44 at applicants training\n",
      "loss: 1.4127171039581299 at epoch 45 at applicants training\n",
      "loss: 1.41143798828125 at epoch 46 at applicants training\n",
      "loss: 1.41094970703125 at epoch 47 at applicants training\n",
      "loss: 1.4106898307800293 at epoch 48 at applicants training\n",
      "loss: 1.410086989402771 at epoch 49 at applicants training\n",
      "loss: 1.4076919555664062 at epoch 50 at applicants training\n",
      "loss: 1.404783010482788 at epoch 51 at applicants training\n",
      "loss: 1.4073518514633179 at epoch 52 at applicants training\n",
      "loss: 1.4038021564483643 at epoch 53 at applicants training\n",
      "loss: 1.405362844467163 at epoch 54 at applicants training\n",
      "loss: 1.4042750597000122 at epoch 55 at applicants training\n",
      "loss: 1.4026703834533691 at epoch 56 at applicants training\n",
      "loss: 1.4032005071640015 at epoch 57 at applicants training\n",
      "loss: 1.403135061264038 at epoch 58 at applicants training\n",
      "loss: 1.4022607803344727 at epoch 59 at applicants training\n",
      "loss: 1.4021010398864746 at epoch 60 at applicants training\n",
      "loss: 1.4026774168014526 at epoch 61 at applicants training\n",
      "loss: 1.401940107345581 at epoch 62 at applicants training\n",
      "loss: 1.401667833328247 at epoch 63 at applicants training\n",
      "loss: 1.4020191431045532 at epoch 64 at applicants training\n",
      "loss: 1.4017492532730103 at epoch 65 at applicants training\n",
      "loss: 1.4011828899383545 at epoch 66 at applicants training\n",
      "loss: 1.4014819860458374 at epoch 67 at applicants training\n",
      "loss: 1.4014374017715454 at epoch 68 at applicants training\n",
      "loss: 1.4009170532226562 at epoch 69 at applicants training\n",
      "loss: 1.4013152122497559 at epoch 70 at applicants training\n",
      "loss: 1.4009534120559692 at epoch 71 at applicants training\n",
      "loss: 1.4010080099105835 at epoch 72 at applicants training\n",
      "loss: 1.401100516319275 at epoch 73 at applicants training\n",
      "loss: 1.4007203578948975 at epoch 74 at applicants training\n",
      "loss: 1.401137113571167 at epoch 75 at applicants training\n",
      "loss: 1.400733232498169 at epoch 76 at applicants training\n",
      "loss: 1.400863528251648 at epoch 77 at applicants training\n",
      "loss: 1.4004502296447754 at epoch 78 at applicants training\n",
      "loss: 1.4006646871566772 at epoch 79 at applicants training\n",
      "loss: 1.400335431098938 at epoch 80 at applicants training\n",
      "loss: 1.4000684022903442 at epoch 81 at applicants training\n",
      "loss: 1.400041937828064 at epoch 82 at applicants training\n",
      "loss: 1.3995252847671509 at epoch 83 at applicants training\n",
      "loss: 1.3999191522598267 at epoch 84 at applicants training\n",
      "loss: 1.3997520208358765 at epoch 85 at applicants training\n",
      "loss: 1.3995953798294067 at epoch 86 at applicants training\n",
      "loss: 1.3995716571807861 at epoch 87 at applicants training\n",
      "loss: 1.3996127843856812 at epoch 88 at applicants training\n",
      "loss: 1.3993523120880127 at epoch 89 at applicants training\n",
      "loss: 1.3995766639709473 at epoch 90 at applicants training\n",
      "loss: 1.3992446660995483 at epoch 91 at applicants training\n",
      "loss: 1.3994261026382446 at epoch 92 at applicants training\n",
      "loss: 1.3992302417755127 at epoch 93 at applicants training\n",
      "loss: 1.39923894405365 at epoch 94 at applicants training\n",
      "loss: 1.3991649150848389 at epoch 95 at applicants training\n",
      "loss: 1.399045705795288 at epoch 96 at applicants training\n",
      "loss: 1.399012804031372 at epoch 97 at applicants training\n",
      "loss: 1.398888349533081 at epoch 98 at applicants training\n",
      "loss: 1.3987584114074707 at epoch 99 at applicants training\n",
      "loss: 1.814778447151184 at epoch 0 at applicants training\n",
      "loss: 1.6599922180175781 at epoch 1 at applicants training\n",
      "loss: 1.775729775428772 at epoch 2 at applicants training\n",
      "loss: 1.6524287462234497 at epoch 3 at applicants training\n",
      "loss: 1.6207444667816162 at epoch 4 at applicants training\n",
      "loss: 1.6248282194137573 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.6748342514038086 at epoch 0 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 1 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 2 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 3 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 4 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 5 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 6 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 7 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 8 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 9 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 10 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 11 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 12 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 13 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 14 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 15 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 16 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 17 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 18 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 19 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 20 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 21 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 22 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 23 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 24 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 25 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 26 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 27 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 28 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 29 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 30 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 31 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 32 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 33 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 34 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 35 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 36 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 37 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 38 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 39 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 40 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 41 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 42 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 43 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 44 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 45 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 46 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 47 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 48 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 49 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 50 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 51 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 52 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 53 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 54 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 55 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 56 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 57 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 58 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 59 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 60 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 61 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 62 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 63 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 64 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 65 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 66 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 67 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 68 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 69 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 70 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 71 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 72 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 73 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 74 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 75 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 76 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 77 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 78 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 79 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 80 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 81 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 82 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 83 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 84 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 85 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 86 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 87 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 88 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 89 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 90 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 91 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 92 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 93 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 94 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 95 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 96 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 97 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 98 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 99 at applicants training\n",
      "loss: 1.6510173082351685 at epoch 0 at applicants training\n",
      "loss: 1.6127276420593262 at epoch 1 at applicants training\n",
      "loss: 1.6029952764511108 at epoch 2 at applicants training\n",
      "loss: 1.5727448463439941 at epoch 3 at applicants training\n",
      "loss: 1.5548657178878784 at epoch 4 at applicants training\n",
      "loss: 1.5595299005508423 at epoch 5 at applicants training\n",
      "loss: 1.5246548652648926 at epoch 6 at applicants training\n",
      "loss: 1.5638785362243652 at epoch 7 at applicants training\n",
      "loss: 1.581162691116333 at epoch 8 at applicants training\n",
      "loss: 1.5668251514434814 at epoch 9 at applicants training\n",
      "loss: 1.5071189403533936 at epoch 10 at applicants training\n",
      "loss: 1.4936577081680298 at epoch 11 at applicants training\n",
      "loss: 1.5180943012237549 at epoch 12 at applicants training\n",
      "loss: 1.498374342918396 at epoch 13 at applicants training\n",
      "loss: 1.4670528173446655 at epoch 14 at applicants training\n",
      "loss: 1.48016357421875 at epoch 15 at applicants training\n",
      "loss: 1.488622784614563 at epoch 16 at applicants training\n",
      "loss: 1.4794174432754517 at epoch 17 at applicants training\n",
      "loss: 1.4751956462860107 at epoch 18 at applicants training\n",
      "loss: 1.4582034349441528 at epoch 19 at applicants training\n",
      "loss: 1.447036623954773 at epoch 20 at applicants training\n",
      "loss: 1.4556291103363037 at epoch 21 at applicants training\n",
      "loss: 1.439704179763794 at epoch 22 at applicants training\n",
      "loss: 1.4394192695617676 at epoch 23 at applicants training\n",
      "loss: 1.448738694190979 at epoch 24 at applicants training\n",
      "loss: 1.4317452907562256 at epoch 25 at applicants training\n",
      "loss: 1.4320640563964844 at epoch 26 at applicants training\n",
      "loss: 1.4353318214416504 at epoch 27 at applicants training\n",
      "loss: 1.4312976598739624 at epoch 28 at applicants training\n",
      "loss: 1.42368745803833 at epoch 29 at applicants training\n",
      "loss: 1.4299207925796509 at epoch 30 at applicants training\n",
      "loss: 1.4260119199752808 at epoch 31 at applicants training\n",
      "loss: 1.4216198921203613 at epoch 32 at applicants training\n",
      "loss: 1.4255287647247314 at epoch 33 at applicants training\n",
      "loss: 1.4233143329620361 at epoch 34 at applicants training\n",
      "loss: 1.4202804565429688 at epoch 35 at applicants training\n",
      "loss: 1.4194830656051636 at epoch 36 at applicants training\n",
      "loss: 1.4190574884414673 at epoch 37 at applicants training\n",
      "loss: 1.4169048070907593 at epoch 38 at applicants training\n",
      "loss: 1.4134149551391602 at epoch 39 at applicants training\n",
      "loss: 1.4119117259979248 at epoch 40 at applicants training\n",
      "loss: 1.4106202125549316 at epoch 41 at applicants training\n",
      "loss: 1.4089274406433105 at epoch 42 at applicants training\n",
      "loss: 1.4077633619308472 at epoch 43 at applicants training\n",
      "loss: 1.4068999290466309 at epoch 44 at applicants training\n",
      "loss: 1.405298113822937 at epoch 45 at applicants training\n",
      "loss: 1.4042631387710571 at epoch 46 at applicants training\n",
      "loss: 1.4037574529647827 at epoch 47 at applicants training\n",
      "loss: 1.4027273654937744 at epoch 48 at applicants training\n",
      "loss: 1.4024078845977783 at epoch 49 at applicants training\n",
      "loss: 1.402032732963562 at epoch 50 at applicants training\n",
      "loss: 1.4016679525375366 at epoch 51 at applicants training\n",
      "loss: 1.401648998260498 at epoch 52 at applicants training\n",
      "loss: 1.4013760089874268 at epoch 53 at applicants training\n",
      "loss: 1.401431679725647 at epoch 54 at applicants training\n",
      "loss: 1.4015806913375854 at epoch 55 at applicants training\n",
      "loss: 1.4013845920562744 at epoch 56 at applicants training\n",
      "loss: 1.4013187885284424 at epoch 57 at applicants training\n",
      "loss: 1.40126371383667 at epoch 58 at applicants training\n",
      "loss: 1.4010276794433594 at epoch 59 at applicants training\n",
      "loss: 1.4008026123046875 at epoch 60 at applicants training\n",
      "loss: 1.400704026222229 at epoch 61 at applicants training\n",
      "loss: 1.400429129600525 at epoch 62 at applicants training\n",
      "loss: 1.4002256393432617 at epoch 63 at applicants training\n",
      "loss: 1.4000931978225708 at epoch 64 at applicants training\n",
      "loss: 1.399878978729248 at epoch 65 at applicants training\n",
      "loss: 1.3998602628707886 at epoch 66 at applicants training\n",
      "loss: 1.3997305631637573 at epoch 67 at applicants training\n",
      "loss: 1.3996485471725464 at epoch 68 at applicants training\n",
      "loss: 1.3995542526245117 at epoch 69 at applicants training\n",
      "loss: 1.3994015455245972 at epoch 70 at applicants training\n",
      "loss: 1.3992964029312134 at epoch 71 at applicants training\n",
      "loss: 1.3991161584854126 at epoch 72 at applicants training\n",
      "loss: 1.398989200592041 at epoch 73 at applicants training\n",
      "loss: 1.3988168239593506 at epoch 74 at applicants training\n",
      "loss: 1.3987312316894531 at epoch 75 at applicants training\n",
      "loss: 1.398687720298767 at epoch 76 at applicants training\n",
      "loss: 1.398638129234314 at epoch 77 at applicants training\n",
      "loss: 1.3985657691955566 at epoch 78 at applicants training\n",
      "loss: 1.398452639579773 at epoch 79 at applicants training\n",
      "loss: 1.3983234167099 at epoch 80 at applicants training\n",
      "loss: 1.3981928825378418 at epoch 81 at applicants training\n",
      "loss: 1.3981000185012817 at epoch 82 at applicants training\n",
      "loss: 1.398074746131897 at epoch 83 at applicants training\n",
      "loss: 1.3980190753936768 at epoch 84 at applicants training\n",
      "loss: 1.3979965448379517 at epoch 85 at applicants training\n",
      "loss: 1.3978941440582275 at epoch 86 at applicants training\n",
      "loss: 1.3977866172790527 at epoch 87 at applicants training\n",
      "loss: 1.3977371454238892 at epoch 88 at applicants training\n",
      "loss: 1.3976850509643555 at epoch 89 at applicants training\n",
      "loss: 1.3976327180862427 at epoch 90 at applicants training\n",
      "loss: 1.3975986242294312 at epoch 91 at applicants training\n",
      "loss: 1.3975633382797241 at epoch 92 at applicants training\n",
      "loss: 1.3975006341934204 at epoch 93 at applicants training\n",
      "loss: 1.3974270820617676 at epoch 94 at applicants training\n",
      "loss: 1.3973639011383057 at epoch 95 at applicants training\n",
      "loss: 1.3973110914230347 at epoch 96 at applicants training\n",
      "loss: 1.3972649574279785 at epoch 97 at applicants training\n",
      "loss: 1.3972145318984985 at epoch 98 at applicants training\n",
      "loss: 1.3971703052520752 at epoch 99 at applicants training\n",
      "loss: 1.7178702354431152 at epoch 0 at applicants training\n",
      "loss: 1.660886287689209 at epoch 1 at applicants training\n",
      "loss: 1.6501071453094482 at epoch 2 at applicants training\n",
      "loss: 1.5844452381134033 at epoch 3 at applicants training\n",
      "loss: 1.6360499858856201 at epoch 4 at applicants training\n",
      "loss: 1.5777415037155151 at epoch 5 at applicants training\n",
      "loss: 1.5843218564987183 at epoch 6 at applicants training\n",
      "loss: 1.5687446594238281 at epoch 7 at applicants training\n",
      "loss: 1.472754716873169 at epoch 8 at applicants training\n",
      "loss: 1.4628115892410278 at epoch 9 at applicants training\n",
      "loss: 1.4453202486038208 at epoch 10 at applicants training\n",
      "loss: 1.4051076173782349 at epoch 11 at applicants training\n",
      "loss: 1.3723013401031494 at epoch 12 at applicants training\n",
      "loss: 1.3630659580230713 at epoch 13 at applicants training\n",
      "loss: 1.3556479215621948 at epoch 14 at applicants training\n",
      "loss: 1.3215326070785522 at epoch 15 at applicants training\n",
      "loss: 1.313265085220337 at epoch 16 at applicants training\n",
      "loss: 1.311570405960083 at epoch 17 at applicants training\n",
      "loss: 1.2786293029785156 at epoch 18 at applicants training\n",
      "loss: 1.2805436849594116 at epoch 19 at applicants training\n",
      "loss: 1.2734580039978027 at epoch 20 at applicants training\n",
      "loss: 1.2538690567016602 at epoch 21 at applicants training\n",
      "loss: 1.2607630491256714 at epoch 22 at applicants training\n",
      "loss: 1.2440992593765259 at epoch 23 at applicants training\n",
      "loss: 1.24008047580719 at epoch 24 at applicants training\n",
      "loss: 1.2336201667785645 at epoch 25 at applicants training\n",
      "loss: 1.2304539680480957 at epoch 26 at applicants training\n",
      "loss: 1.2179813385009766 at epoch 27 at applicants training\n",
      "loss: 1.2232356071472168 at epoch 28 at applicants training\n",
      "loss: 1.2089589834213257 at epoch 29 at applicants training\n",
      "loss: 1.2129302024841309 at epoch 30 at applicants training\n",
      "loss: 1.2053463459014893 at epoch 31 at applicants training\n",
      "loss: 1.206639289855957 at epoch 32 at applicants training\n",
      "loss: 1.2022120952606201 at epoch 33 at applicants training\n",
      "loss: 1.2018256187438965 at epoch 34 at applicants training\n",
      "loss: 1.1980400085449219 at epoch 35 at applicants training\n",
      "loss: 1.1981539726257324 at epoch 36 at applicants training\n",
      "loss: 1.1956740617752075 at epoch 37 at applicants training\n",
      "loss: 1.1955512762069702 at epoch 38 at applicants training\n",
      "loss: 1.1930632591247559 at epoch 39 at applicants training\n",
      "loss: 1.1920830011367798 at epoch 40 at applicants training\n",
      "loss: 1.1911816596984863 at epoch 41 at applicants training\n",
      "loss: 1.1887396574020386 at epoch 42 at applicants training\n",
      "loss: 1.1894339323043823 at epoch 43 at applicants training\n",
      "loss: 1.1865159273147583 at epoch 44 at applicants training\n",
      "loss: 1.1858959197998047 at epoch 45 at applicants training\n",
      "loss: 1.1740405559539795 at epoch 46 at applicants training\n",
      "loss: 1.147078275680542 at epoch 47 at applicants training\n",
      "loss: 1.1603679656982422 at epoch 48 at applicants training\n",
      "loss: 1.1256587505340576 at epoch 49 at applicants training\n",
      "loss: 1.1390776634216309 at epoch 50 at applicants training\n",
      "loss: 1.1458280086517334 at epoch 51 at applicants training\n",
      "loss: 1.1392426490783691 at epoch 52 at applicants training\n",
      "loss: 1.1201814413070679 at epoch 53 at applicants training\n",
      "loss: 1.1250979900360107 at epoch 54 at applicants training\n",
      "loss: 1.1232341527938843 at epoch 55 at applicants training\n",
      "loss: 1.1117249727249146 at epoch 56 at applicants training\n",
      "loss: 1.1188666820526123 at epoch 57 at applicants training\n",
      "loss: 1.1185147762298584 at epoch 58 at applicants training\n",
      "loss: 1.1098631620407104 at epoch 59 at applicants training\n",
      "loss: 1.10694420337677 at epoch 60 at applicants training\n",
      "loss: 1.1128227710723877 at epoch 61 at applicants training\n",
      "loss: 1.1056135892868042 at epoch 62 at applicants training\n",
      "loss: 1.1022528409957886 at epoch 63 at applicants training\n",
      "loss: 1.1061863899230957 at epoch 64 at applicants training\n",
      "loss: 1.1025325059890747 at epoch 65 at applicants training\n",
      "loss: 1.0971016883850098 at epoch 66 at applicants training\n",
      "loss: 1.1014310121536255 at epoch 67 at applicants training\n",
      "loss: 1.097050666809082 at epoch 68 at applicants training\n",
      "loss: 1.094468593597412 at epoch 69 at applicants training\n",
      "loss: 1.0969294309616089 at epoch 70 at applicants training\n",
      "loss: 1.093672513961792 at epoch 71 at applicants training\n",
      "loss: 1.0915204286575317 at epoch 72 at applicants training\n",
      "loss: 1.0934474468231201 at epoch 73 at applicants training\n",
      "loss: 1.0912836790084839 at epoch 74 at applicants training\n",
      "loss: 1.0901089906692505 at epoch 75 at applicants training\n",
      "loss: 1.0909994840621948 at epoch 76 at applicants training\n",
      "loss: 1.0895880460739136 at epoch 77 at applicants training\n",
      "loss: 1.0886136293411255 at epoch 78 at applicants training\n",
      "loss: 1.0889350175857544 at epoch 79 at applicants training\n",
      "loss: 1.0887953042984009 at epoch 80 at applicants training\n",
      "loss: 1.088065266609192 at epoch 81 at applicants training\n",
      "loss: 1.0877727270126343 at epoch 82 at applicants training\n",
      "loss: 1.0876564979553223 at epoch 83 at applicants training\n",
      "loss: 1.08744478225708 at epoch 84 at applicants training\n",
      "loss: 1.0869823694229126 at epoch 85 at applicants training\n",
      "loss: 1.0864810943603516 at epoch 86 at applicants training\n",
      "loss: 1.0863896608352661 at epoch 87 at applicants training\n",
      "loss: 1.0863052606582642 at epoch 88 at applicants training\n",
      "loss: 1.0858005285263062 at epoch 89 at applicants training\n",
      "loss: 1.085545301437378 at epoch 90 at applicants training\n",
      "loss: 1.085551142692566 at epoch 91 at applicants training\n",
      "loss: 1.0853255987167358 at epoch 92 at applicants training\n",
      "loss: 1.0850059986114502 at epoch 93 at applicants training\n",
      "loss: 1.0847817659378052 at epoch 94 at applicants training\n",
      "loss: 1.0846887826919556 at epoch 95 at applicants training\n",
      "loss: 1.0845520496368408 at epoch 96 at applicants training\n",
      "loss: 1.0843391418457031 at epoch 97 at applicants training\n",
      "loss: 1.084149956703186 at epoch 98 at applicants training\n",
      "loss: 1.0840487480163574 at epoch 99 at applicants training\n",
      "loss: 1.6818867921829224 at epoch 0 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 1 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 2 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 3 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 4 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 5 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 9 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 10 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 11 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 12 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 13 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 14 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 15 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 16 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 17 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 18 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 19 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 20 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 21 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 22 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.6687580347061157 at epoch 0 at applicants training\n",
      "loss: 1.563342571258545 at epoch 1 at applicants training\n",
      "loss: 1.5541703701019287 at epoch 2 at applicants training\n",
      "loss: 1.6058725118637085 at epoch 3 at applicants training\n",
      "loss: 1.6248195171356201 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.732223629951477 at epoch 0 at applicants training\n",
      "loss: 1.7374600172042847 at epoch 1 at applicants training\n",
      "loss: 1.7248098850250244 at epoch 2 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 3 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 4 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 5 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 6 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 7 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 8 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 9 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 10 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 11 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 12 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 13 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 14 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 15 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 16 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 17 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 18 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 19 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 20 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 21 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 22 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 23 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 24 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 25 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 26 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 27 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 28 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 29 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 30 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 31 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 32 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 33 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 34 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 35 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 36 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 37 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 38 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 39 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 40 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 41 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 42 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 43 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 44 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 45 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 46 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 47 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 48 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 49 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 50 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 51 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 52 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 53 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 54 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 55 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 56 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 57 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 58 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 59 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 60 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 61 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 62 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 63 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 64 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 65 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 66 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 67 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 68 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 69 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 70 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 71 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 72 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 73 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 74 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 75 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 76 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 77 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 78 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 79 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 80 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 81 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 82 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 83 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 84 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 85 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 86 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 87 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 88 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 89 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 90 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 91 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 92 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 93 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 94 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 95 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 96 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 97 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 98 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 99 at applicants training\n",
      "loss: 1.7273684740066528 at epoch 0 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 1 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 2 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 3 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 4 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 5 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 6 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 7 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 8 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 9 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 10 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 11 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 12 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 13 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 14 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 15 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 16 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 17 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 18 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 19 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 20 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 21 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 22 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 23 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 24 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 25 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 26 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 27 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 28 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 29 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 30 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 31 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 32 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 33 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 34 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 35 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 36 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 37 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 38 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 39 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 40 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 41 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 42 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 43 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 44 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 45 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 46 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 47 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 48 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 49 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 50 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 51 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 52 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 53 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 54 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 55 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 56 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 57 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 58 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 59 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 60 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 61 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 62 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 63 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 64 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 65 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 66 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 67 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 68 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 69 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 70 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 71 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 72 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 73 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 74 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 75 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 76 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 77 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 78 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 79 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 80 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 81 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 82 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 83 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 84 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 85 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 86 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 87 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 88 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 89 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 90 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 91 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 92 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 93 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 94 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 95 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 96 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 97 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 98 at applicants training\n",
      "loss: 1.7248326539993286 at epoch 99 at applicants training\n",
      "loss: 1.6721147298812866 at epoch 0 at applicants training\n",
      "loss: 1.6748290061950684 at epoch 1 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 2 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 3 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 4 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 5 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 6 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 7 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 8 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 9 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 10 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 11 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 12 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 13 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 14 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 15 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 16 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 17 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 18 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 19 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 20 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 21 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 22 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 23 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 24 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 25 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 26 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 27 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 28 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 29 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 30 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 31 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 32 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 33 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 34 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 35 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 36 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 37 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 38 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 39 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 40 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 41 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 42 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 43 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 44 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 45 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 46 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 47 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 48 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 49 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 50 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 51 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 52 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 53 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 54 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 55 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 56 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 57 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 58 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 59 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 60 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 61 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 62 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 63 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 64 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 65 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 66 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 67 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 68 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 69 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 70 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 71 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 72 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 73 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 74 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 75 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 76 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 77 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 78 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 79 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 80 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 81 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 82 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 83 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 84 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 85 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 86 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 87 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 88 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 89 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 90 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 91 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 92 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 93 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 94 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 95 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 96 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 97 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 98 at applicants training\n",
      "loss: 1.6748325824737549 at epoch 99 at applicants training\n",
      "loss: 1.759528398513794 at epoch 0 at applicants training\n",
      "loss: 1.7216758728027344 at epoch 1 at applicants training\n",
      "loss: 1.7135220766067505 at epoch 2 at applicants training\n",
      "loss: 1.7041871547698975 at epoch 3 at applicants training\n",
      "loss: 1.6954954862594604 at epoch 4 at applicants training\n",
      "loss: 1.6928361654281616 at epoch 5 at applicants training\n",
      "loss: 1.6848846673965454 at epoch 6 at applicants training\n",
      "loss: 1.676735281944275 at epoch 7 at applicants training\n",
      "loss: 1.6892039775848389 at epoch 8 at applicants training\n",
      "loss: 1.6707563400268555 at epoch 9 at applicants training\n",
      "loss: 1.6773737668991089 at epoch 10 at applicants training\n",
      "loss: 1.676965594291687 at epoch 11 at applicants training\n",
      "loss: 1.6724416017532349 at epoch 12 at applicants training\n",
      "loss: 1.6679182052612305 at epoch 13 at applicants training\n",
      "loss: 1.6646487712860107 at epoch 14 at applicants training\n",
      "loss: 1.6629618406295776 at epoch 15 at applicants training\n",
      "loss: 1.6601247787475586 at epoch 16 at applicants training\n",
      "loss: 1.652543544769287 at epoch 17 at applicants training\n",
      "loss: 1.6572918891906738 at epoch 18 at applicants training\n",
      "loss: 1.6488932371139526 at epoch 19 at applicants training\n",
      "loss: 1.6522881984710693 at epoch 20 at applicants training\n",
      "loss: 1.650541067123413 at epoch 21 at applicants training\n",
      "loss: 1.6462146043777466 at epoch 22 at applicants training\n",
      "loss: 1.649114966392517 at epoch 23 at applicants training\n",
      "loss: 1.6445709466934204 at epoch 24 at applicants training\n",
      "loss: 1.6463333368301392 at epoch 25 at applicants training\n",
      "loss: 1.6456507444381714 at epoch 26 at applicants training\n",
      "loss: 1.6437400579452515 at epoch 27 at applicants training\n",
      "loss: 1.6449023485183716 at epoch 28 at applicants training\n",
      "loss: 1.6441651582717896 at epoch 29 at applicants training\n",
      "loss: 1.643483281135559 at epoch 30 at applicants training\n",
      "loss: 1.644110083580017 at epoch 31 at applicants training\n",
      "loss: 1.6441000699996948 at epoch 32 at applicants training\n",
      "loss: 1.6433073282241821 at epoch 33 at applicants training\n",
      "loss: 1.6433136463165283 at epoch 34 at applicants training\n",
      "loss: 1.6437127590179443 at epoch 35 at applicants training\n",
      "loss: 1.643220067024231 at epoch 36 at applicants training\n",
      "loss: 1.6428881883621216 at epoch 37 at applicants training\n",
      "loss: 1.6430609226226807 at epoch 38 at applicants training\n",
      "loss: 1.6430658102035522 at epoch 39 at applicants training\n",
      "loss: 1.6427127122879028 at epoch 40 at applicants training\n",
      "loss: 1.6425683498382568 at epoch 41 at applicants training\n",
      "loss: 1.642491340637207 at epoch 42 at applicants training\n",
      "loss: 1.6424665451049805 at epoch 43 at applicants training\n",
      "loss: 1.6422910690307617 at epoch 44 at applicants training\n",
      "loss: 1.642025351524353 at epoch 45 at applicants training\n",
      "loss: 1.6418920755386353 at epoch 46 at applicants training\n",
      "loss: 1.6417442560195923 at epoch 47 at applicants training\n",
      "loss: 1.6415292024612427 at epoch 48 at applicants training\n",
      "loss: 1.641385793685913 at epoch 49 at applicants training\n",
      "loss: 1.6413198709487915 at epoch 50 at applicants training\n",
      "loss: 1.6411038637161255 at epoch 51 at applicants training\n",
      "loss: 1.6409932374954224 at epoch 52 at applicants training\n",
      "loss: 1.6409051418304443 at epoch 53 at applicants training\n",
      "loss: 1.6406697034835815 at epoch 54 at applicants training\n",
      "loss: 1.6406463384628296 at epoch 55 at applicants training\n",
      "loss: 1.6404353380203247 at epoch 56 at applicants training\n",
      "loss: 1.6404510736465454 at epoch 57 at applicants training\n",
      "loss: 1.6403021812438965 at epoch 58 at applicants training\n",
      "loss: 1.6403523683547974 at epoch 59 at applicants training\n",
      "loss: 1.6402838230133057 at epoch 60 at applicants training\n",
      "loss: 1.6402660608291626 at epoch 61 at applicants training\n",
      "loss: 1.6402075290679932 at epoch 62 at applicants training\n",
      "loss: 1.6400874853134155 at epoch 63 at applicants training\n",
      "loss: 1.6400619745254517 at epoch 64 at applicants training\n",
      "loss: 1.6399372816085815 at epoch 65 at applicants training\n",
      "loss: 1.6399040222167969 at epoch 66 at applicants training\n",
      "loss: 1.6398745775222778 at epoch 67 at applicants training\n",
      "loss: 1.6397732496261597 at epoch 68 at applicants training\n",
      "loss: 1.639736294746399 at epoch 69 at applicants training\n",
      "loss: 1.639763355255127 at epoch 70 at applicants training\n",
      "loss: 1.6397678852081299 at epoch 71 at applicants training\n",
      "loss: 1.63972806930542 at epoch 72 at applicants training\n",
      "loss: 1.6397159099578857 at epoch 73 at applicants training\n",
      "loss: 1.6397311687469482 at epoch 74 at applicants training\n",
      "loss: 1.639723777770996 at epoch 75 at applicants training\n",
      "loss: 1.6396926641464233 at epoch 76 at applicants training\n",
      "loss: 1.6396414041519165 at epoch 77 at applicants training\n",
      "loss: 1.6396044492721558 at epoch 78 at applicants training\n",
      "loss: 1.6395885944366455 at epoch 79 at applicants training\n",
      "loss: 1.6395912170410156 at epoch 80 at applicants training\n",
      "loss: 1.6396136283874512 at epoch 81 at applicants training\n",
      "loss: 1.6396533250808716 at epoch 82 at applicants training\n",
      "loss: 1.6397333145141602 at epoch 83 at applicants training\n",
      "loss: 1.6397987604141235 at epoch 84 at applicants training\n",
      "loss: 1.6398910284042358 at epoch 85 at applicants training\n",
      "loss: 1.6397995948791504 at epoch 86 at applicants training\n",
      "loss: 1.6396899223327637 at epoch 87 at applicants training\n",
      "loss: 1.6395529508590698 at epoch 88 at applicants training\n",
      "loss: 1.6394965648651123 at epoch 89 at applicants training\n",
      "loss: 1.639520287513733 at epoch 90 at applicants training\n",
      "loss: 1.63958740234375 at epoch 91 at applicants training\n",
      "loss: 1.6396534442901611 at epoch 92 at applicants training\n",
      "loss: 1.6395667791366577 at epoch 93 at applicants training\n",
      "loss: 1.639490008354187 at epoch 94 at applicants training\n",
      "loss: 1.63944673538208 at epoch 95 at applicants training\n",
      "loss: 1.6394630670547485 at epoch 96 at applicants training\n",
      "loss: 1.63950514793396 at epoch 97 at applicants training\n",
      "loss: 1.6394929885864258 at epoch 98 at applicants training\n",
      "loss: 1.639458179473877 at epoch 99 at applicants training\n",
      "loss: 1.7961244583129883 at epoch 0 at applicants training\n",
      "loss: 1.684820532798767 at epoch 1 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 2 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 3 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 4 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 5 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 6 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 7 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 8 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 9 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 10 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 11 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 12 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 13 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 14 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 15 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 16 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 17 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 18 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 19 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 20 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 21 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 22 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 23 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 24 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 25 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 26 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 27 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 28 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 29 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 30 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 31 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 32 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 33 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 34 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 35 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 36 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 37 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 38 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 39 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 40 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 41 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 42 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 43 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 44 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 45 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 46 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 47 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 48 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 49 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 50 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 51 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 52 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 53 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 54 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 55 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 56 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 57 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 58 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 59 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 60 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 61 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 62 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 63 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 64 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 65 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 66 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 67 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 68 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 69 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 70 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 71 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 72 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 73 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 74 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 75 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 76 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 77 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 78 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 79 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 80 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 81 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 82 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 83 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 84 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 85 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 86 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 87 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 88 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 89 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 90 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 91 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 92 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 93 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 94 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 95 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 96 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 97 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 98 at applicants training\n",
      "loss: 1.6848325729370117 at epoch 99 at applicants training\n",
      "loss: 1.6839176416397095 at epoch 0 at applicants training\n",
      "loss: 1.724939227104187 at epoch 1 at applicants training\n",
      "loss: 1.7067970037460327 at epoch 2 at applicants training\n",
      "loss: 1.7024867534637451 at epoch 3 at applicants training\n",
      "loss: 1.6919152736663818 at epoch 4 at applicants training\n",
      "loss: 1.6828628778457642 at epoch 5 at applicants training\n",
      "loss: 1.668311595916748 at epoch 6 at applicants training\n",
      "loss: 1.6851364374160767 at epoch 7 at applicants training\n",
      "loss: 1.6782965660095215 at epoch 8 at applicants training\n",
      "loss: 1.6759257316589355 at epoch 9 at applicants training\n",
      "loss: 1.659429907798767 at epoch 10 at applicants training\n",
      "loss: 1.6480461359024048 at epoch 11 at applicants training\n",
      "loss: 1.6335372924804688 at epoch 12 at applicants training\n",
      "loss: 1.6282305717468262 at epoch 13 at applicants training\n",
      "loss: 1.6180087327957153 at epoch 14 at applicants training\n",
      "loss: 1.6024670600891113 at epoch 15 at applicants training\n",
      "loss: 1.5813707113265991 at epoch 16 at applicants training\n",
      "loss: 1.5941941738128662 at epoch 17 at applicants training\n",
      "loss: 1.5807127952575684 at epoch 18 at applicants training\n",
      "loss: 1.553999423980713 at epoch 19 at applicants training\n",
      "loss: 1.5474755764007568 at epoch 20 at applicants training\n",
      "loss: 1.5294580459594727 at epoch 21 at applicants training\n",
      "loss: 1.5295851230621338 at epoch 22 at applicants training\n",
      "loss: 1.5010725259780884 at epoch 23 at applicants training\n",
      "loss: 1.4978960752487183 at epoch 24 at applicants training\n",
      "loss: 1.469515085220337 at epoch 25 at applicants training\n",
      "loss: 1.4498788118362427 at epoch 26 at applicants training\n",
      "loss: 1.437833309173584 at epoch 27 at applicants training\n",
      "loss: 1.4187566041946411 at epoch 28 at applicants training\n",
      "loss: 1.4097115993499756 at epoch 29 at applicants training\n",
      "loss: 1.4047378301620483 at epoch 30 at applicants training\n",
      "loss: 1.3923327922821045 at epoch 31 at applicants training\n",
      "loss: 1.3852248191833496 at epoch 32 at applicants training\n",
      "loss: 1.385117769241333 at epoch 33 at applicants training\n",
      "loss: 1.3801774978637695 at epoch 34 at applicants training\n",
      "loss: 1.377238154411316 at epoch 35 at applicants training\n",
      "loss: 1.3765301704406738 at epoch 36 at applicants training\n",
      "loss: 1.3732130527496338 at epoch 37 at applicants training\n",
      "loss: 1.372585654258728 at epoch 38 at applicants training\n",
      "loss: 1.3700546026229858 at epoch 39 at applicants training\n",
      "loss: 1.3695663213729858 at epoch 40 at applicants training\n",
      "loss: 1.3701781034469604 at epoch 41 at applicants training\n",
      "loss: 1.363890528678894 at epoch 42 at applicants training\n",
      "loss: 1.3646163940429688 at epoch 43 at applicants training\n",
      "loss: 1.36076021194458 at epoch 44 at applicants training\n",
      "loss: 1.3604336977005005 at epoch 45 at applicants training\n",
      "loss: 1.358405590057373 at epoch 46 at applicants training\n",
      "loss: 1.356951117515564 at epoch 47 at applicants training\n",
      "loss: 1.3564178943634033 at epoch 48 at applicants training\n",
      "loss: 1.356175184249878 at epoch 49 at applicants training\n",
      "loss: 1.3546100854873657 at epoch 50 at applicants training\n",
      "loss: 1.3549799919128418 at epoch 51 at applicants training\n",
      "loss: 1.3536545038223267 at epoch 52 at applicants training\n",
      "loss: 1.3541969060897827 at epoch 53 at applicants training\n",
      "loss: 1.3531690835952759 at epoch 54 at applicants training\n",
      "loss: 1.3531100749969482 at epoch 55 at applicants training\n",
      "loss: 1.353161334991455 at epoch 56 at applicants training\n",
      "loss: 1.352340579032898 at epoch 57 at applicants training\n",
      "loss: 1.3526376485824585 at epoch 58 at applicants training\n",
      "loss: 1.3518519401550293 at epoch 59 at applicants training\n",
      "loss: 1.3519761562347412 at epoch 60 at applicants training\n",
      "loss: 1.3512604236602783 at epoch 61 at applicants training\n",
      "loss: 1.3511103391647339 at epoch 62 at applicants training\n",
      "loss: 1.350425124168396 at epoch 63 at applicants training\n",
      "loss: 1.3501940965652466 at epoch 64 at applicants training\n",
      "loss: 1.3494735956192017 at epoch 65 at applicants training\n",
      "loss: 1.349302053451538 at epoch 66 at applicants training\n",
      "loss: 1.3485451936721802 at epoch 67 at applicants training\n",
      "loss: 1.3484084606170654 at epoch 68 at applicants training\n",
      "loss: 1.347784399986267 at epoch 69 at applicants training\n",
      "loss: 1.3476898670196533 at epoch 70 at applicants training\n",
      "loss: 1.3473156690597534 at epoch 71 at applicants training\n",
      "loss: 1.3473020792007446 at epoch 72 at applicants training\n",
      "loss: 1.3470242023468018 at epoch 73 at applicants training\n",
      "loss: 1.3469656705856323 at epoch 74 at applicants training\n",
      "loss: 1.3466782569885254 at epoch 75 at applicants training\n",
      "loss: 1.3465332984924316 at epoch 76 at applicants training\n",
      "loss: 1.3462677001953125 at epoch 77 at applicants training\n",
      "loss: 1.3461461067199707 at epoch 78 at applicants training\n",
      "loss: 1.3459692001342773 at epoch 79 at applicants training\n",
      "loss: 1.3457248210906982 at epoch 80 at applicants training\n",
      "loss: 1.3455026149749756 at epoch 81 at applicants training\n",
      "loss: 1.345305323600769 at epoch 82 at applicants training\n",
      "loss: 1.3453084230422974 at epoch 83 at applicants training\n",
      "loss: 1.3452038764953613 at epoch 84 at applicants training\n",
      "loss: 1.3450208902359009 at epoch 85 at applicants training\n",
      "loss: 1.3449134826660156 at epoch 86 at applicants training\n",
      "loss: 1.3448638916015625 at epoch 87 at applicants training\n",
      "loss: 1.3447662591934204 at epoch 88 at applicants training\n",
      "loss: 1.3446274995803833 at epoch 89 at applicants training\n",
      "loss: 1.3445024490356445 at epoch 90 at applicants training\n",
      "loss: 1.344457745552063 at epoch 91 at applicants training\n",
      "loss: 1.3444210290908813 at epoch 92 at applicants training\n",
      "loss: 1.3443362712860107 at epoch 93 at applicants training\n",
      "loss: 1.344223141670227 at epoch 94 at applicants training\n",
      "loss: 1.3441325426101685 at epoch 95 at applicants training\n",
      "loss: 1.3440747261047363 at epoch 96 at applicants training\n",
      "loss: 1.343986988067627 at epoch 97 at applicants training\n",
      "loss: 1.343889594078064 at epoch 98 at applicants training\n",
      "loss: 1.343827486038208 at epoch 99 at applicants training\n",
      "loss: 1.5541044473648071 at epoch 0 at applicants training\n",
      "loss: 1.5287758111953735 at epoch 1 at applicants training\n",
      "loss: 1.5083520412445068 at epoch 2 at applicants training\n",
      "loss: 1.4674471616744995 at epoch 3 at applicants training\n",
      "loss: 1.4803751707077026 at epoch 4 at applicants training\n",
      "loss: 1.4736993312835693 at epoch 5 at applicants training\n",
      "loss: 1.450425624847412 at epoch 6 at applicants training\n",
      "loss: 1.4418573379516602 at epoch 7 at applicants training\n",
      "loss: 1.4444619417190552 at epoch 8 at applicants training\n",
      "loss: 1.4440033435821533 at epoch 9 at applicants training\n",
      "loss: 1.4367001056671143 at epoch 10 at applicants training\n",
      "loss: 1.4306745529174805 at epoch 11 at applicants training\n",
      "loss: 1.4316413402557373 at epoch 12 at applicants training\n",
      "loss: 1.429634690284729 at epoch 13 at applicants training\n",
      "loss: 1.4222830533981323 at epoch 14 at applicants training\n",
      "loss: 1.4148571491241455 at epoch 15 at applicants training\n",
      "loss: 1.4127236604690552 at epoch 16 at applicants training\n",
      "loss: 1.4045242071151733 at epoch 17 at applicants training\n",
      "loss: 1.3910577297210693 at epoch 18 at applicants training\n",
      "loss: 1.3682211637496948 at epoch 19 at applicants training\n",
      "loss: 1.3434247970581055 at epoch 20 at applicants training\n",
      "loss: 1.3520818948745728 at epoch 21 at applicants training\n",
      "loss: 1.3169277906417847 at epoch 22 at applicants training\n",
      "loss: 1.3121676445007324 at epoch 23 at applicants training\n",
      "loss: 1.3074357509613037 at epoch 24 at applicants training\n",
      "loss: 1.2946057319641113 at epoch 25 at applicants training\n",
      "loss: 1.2809803485870361 at epoch 26 at applicants training\n",
      "loss: 1.3016295433044434 at epoch 27 at applicants training\n",
      "loss: 1.2701404094696045 at epoch 28 at applicants training\n",
      "loss: 1.2716131210327148 at epoch 29 at applicants training\n",
      "loss: 1.2720659971237183 at epoch 30 at applicants training\n",
      "loss: 1.2566747665405273 at epoch 31 at applicants training\n",
      "loss: 1.2448246479034424 at epoch 32 at applicants training\n",
      "loss: 1.2419159412384033 at epoch 33 at applicants training\n",
      "loss: 1.2346673011779785 at epoch 34 at applicants training\n",
      "loss: 1.2227962017059326 at epoch 35 at applicants training\n",
      "loss: 1.2215172052383423 at epoch 36 at applicants training\n",
      "loss: 1.2111923694610596 at epoch 37 at applicants training\n",
      "loss: 1.204584002494812 at epoch 38 at applicants training\n",
      "loss: 1.2021771669387817 at epoch 39 at applicants training\n",
      "loss: 1.19192373752594 at epoch 40 at applicants training\n",
      "loss: 1.1912027597427368 at epoch 41 at applicants training\n",
      "loss: 1.1895661354064941 at epoch 42 at applicants training\n",
      "loss: 1.1860363483428955 at epoch 43 at applicants training\n",
      "loss: 1.1846013069152832 at epoch 44 at applicants training\n",
      "loss: 1.1837936639785767 at epoch 45 at applicants training\n",
      "loss: 1.1825125217437744 at epoch 46 at applicants training\n",
      "loss: 1.1824028491973877 at epoch 47 at applicants training\n",
      "loss: 1.1798820495605469 at epoch 48 at applicants training\n",
      "loss: 1.1814346313476562 at epoch 49 at applicants training\n",
      "loss: 1.1789886951446533 at epoch 50 at applicants training\n",
      "loss: 1.1789207458496094 at epoch 51 at applicants training\n",
      "loss: 1.1784435510635376 at epoch 52 at applicants training\n",
      "loss: 1.1770055294036865 at epoch 53 at applicants training\n",
      "loss: 1.1765034198760986 at epoch 54 at applicants training\n",
      "loss: 1.1762266159057617 at epoch 55 at applicants training\n",
      "loss: 1.1750332117080688 at epoch 56 at applicants training\n",
      "loss: 1.1748909950256348 at epoch 57 at applicants training\n",
      "loss: 1.1746339797973633 at epoch 58 at applicants training\n",
      "loss: 1.1738677024841309 at epoch 59 at applicants training\n",
      "loss: 1.1735392808914185 at epoch 60 at applicants training\n",
      "loss: 1.1734611988067627 at epoch 61 at applicants training\n",
      "loss: 1.1730430126190186 at epoch 62 at applicants training\n",
      "loss: 1.1726535558700562 at epoch 63 at applicants training\n",
      "loss: 1.172590732574463 at epoch 64 at applicants training\n",
      "loss: 1.1722432374954224 at epoch 65 at applicants training\n",
      "loss: 1.1720521450042725 at epoch 66 at applicants training\n",
      "loss: 1.1720175743103027 at epoch 67 at applicants training\n",
      "loss: 1.1716859340667725 at epoch 68 at applicants training\n",
      "loss: 1.1714980602264404 at epoch 69 at applicants training\n",
      "loss: 1.1713427305221558 at epoch 70 at applicants training\n",
      "loss: 1.1710104942321777 at epoch 71 at applicants training\n",
      "loss: 1.170958161354065 at epoch 72 at applicants training\n",
      "loss: 1.1707934141159058 at epoch 73 at applicants training\n",
      "loss: 1.1706575155258179 at epoch 74 at applicants training\n",
      "loss: 1.1705975532531738 at epoch 75 at applicants training\n",
      "loss: 1.1705483198165894 at epoch 76 at applicants training\n",
      "loss: 1.170390009880066 at epoch 77 at applicants training\n",
      "loss: 1.1703529357910156 at epoch 78 at applicants training\n",
      "loss: 1.1702492237091064 at epoch 79 at applicants training\n",
      "loss: 1.170158863067627 at epoch 80 at applicants training\n",
      "loss: 1.1700271368026733 at epoch 81 at applicants training\n",
      "loss: 1.169956088066101 at epoch 82 at applicants training\n",
      "loss: 1.1698122024536133 at epoch 83 at applicants training\n",
      "loss: 1.1696383953094482 at epoch 84 at applicants training\n",
      "loss: 1.1694971323013306 at epoch 85 at applicants training\n",
      "loss: 1.1692856550216675 at epoch 86 at applicants training\n",
      "loss: 1.1691162586212158 at epoch 87 at applicants training\n",
      "loss: 1.1689386367797852 at epoch 88 at applicants training\n",
      "loss: 1.168744444847107 at epoch 89 at applicants training\n",
      "loss: 1.1685950756072998 at epoch 90 at applicants training\n",
      "loss: 1.1684565544128418 at epoch 91 at applicants training\n",
      "loss: 1.1683297157287598 at epoch 92 at applicants training\n",
      "loss: 1.1682054996490479 at epoch 93 at applicants training\n",
      "loss: 1.168074369430542 at epoch 94 at applicants training\n",
      "loss: 1.1679563522338867 at epoch 95 at applicants training\n",
      "loss: 1.1678239107131958 at epoch 96 at applicants training\n",
      "loss: 1.1676928997039795 at epoch 97 at applicants training\n",
      "loss: 1.1675575971603394 at epoch 98 at applicants training\n",
      "loss: 1.1674062013626099 at epoch 99 at applicants training\n",
      "loss: 1.7564622163772583 at epoch 0 at applicants training\n",
      "loss: 1.6822941303253174 at epoch 1 at applicants training\n",
      "loss: 1.68134343624115 at epoch 2 at applicants training\n",
      "loss: 1.6704188585281372 at epoch 3 at applicants training\n",
      "loss: 1.6384782791137695 at epoch 4 at applicants training\n",
      "loss: 1.6225160360336304 at epoch 5 at applicants training\n",
      "loss: 1.5953553915023804 at epoch 6 at applicants training\n",
      "loss: 1.6064214706420898 at epoch 7 at applicants training\n",
      "loss: 1.5843327045440674 at epoch 8 at applicants training\n",
      "loss: 1.56795072555542 at epoch 9 at applicants training\n",
      "loss: 1.5780291557312012 at epoch 10 at applicants training\n",
      "loss: 1.5506329536437988 at epoch 11 at applicants training\n",
      "loss: 1.5496361255645752 at epoch 12 at applicants training\n",
      "loss: 1.554021954536438 at epoch 13 at applicants training\n",
      "loss: 1.5337350368499756 at epoch 14 at applicants training\n",
      "loss: 1.5339182615280151 at epoch 15 at applicants training\n",
      "loss: 1.533623218536377 at epoch 16 at applicants training\n",
      "loss: 1.5184746980667114 at epoch 17 at applicants training\n",
      "loss: 1.5156575441360474 at epoch 18 at applicants training\n",
      "loss: 1.513911247253418 at epoch 19 at applicants training\n",
      "loss: 1.5017848014831543 at epoch 20 at applicants training\n",
      "loss: 1.5029929876327515 at epoch 21 at applicants training\n",
      "loss: 1.499361276626587 at epoch 22 at applicants training\n",
      "loss: 1.4928076267242432 at epoch 23 at applicants training\n",
      "loss: 1.4949979782104492 at epoch 24 at applicants training\n",
      "loss: 1.488997459411621 at epoch 25 at applicants training\n",
      "loss: 1.486114501953125 at epoch 26 at applicants training\n",
      "loss: 1.4869029521942139 at epoch 27 at applicants training\n",
      "loss: 1.480323076248169 at epoch 28 at applicants training\n",
      "loss: 1.4820138216018677 at epoch 29 at applicants training\n",
      "loss: 1.477908730506897 at epoch 30 at applicants training\n",
      "loss: 1.4759736061096191 at epoch 31 at applicants training\n",
      "loss: 1.4751063585281372 at epoch 32 at applicants training\n",
      "loss: 1.4703599214553833 at epoch 33 at applicants training\n",
      "loss: 1.4697211980819702 at epoch 34 at applicants training\n",
      "loss: 1.4659907817840576 at epoch 35 at applicants training\n",
      "loss: 1.462396502494812 at epoch 36 at applicants training\n",
      "loss: 1.4616339206695557 at epoch 37 at applicants training\n",
      "loss: 1.4574812650680542 at epoch 38 at applicants training\n",
      "loss: 1.4580318927764893 at epoch 39 at applicants training\n",
      "loss: 1.4542850255966187 at epoch 40 at applicants training\n",
      "loss: 1.4554798603057861 at epoch 41 at applicants training\n",
      "loss: 1.4536645412445068 at epoch 42 at applicants training\n",
      "loss: 1.4542617797851562 at epoch 43 at applicants training\n",
      "loss: 1.453080415725708 at epoch 44 at applicants training\n",
      "loss: 1.4539281129837036 at epoch 45 at applicants training\n",
      "loss: 1.4525893926620483 at epoch 46 at applicants training\n",
      "loss: 1.4533779621124268 at epoch 47 at applicants training\n",
      "loss: 1.4519518613815308 at epoch 48 at applicants training\n",
      "loss: 1.4526423215866089 at epoch 49 at applicants training\n",
      "loss: 1.451387882232666 at epoch 50 at applicants training\n",
      "loss: 1.451575756072998 at epoch 51 at applicants training\n",
      "loss: 1.4506754875183105 at epoch 52 at applicants training\n",
      "loss: 1.4506295919418335 at epoch 53 at applicants training\n",
      "loss: 1.450339913368225 at epoch 54 at applicants training\n",
      "loss: 1.4501698017120361 at epoch 55 at applicants training\n",
      "loss: 1.4500222206115723 at epoch 56 at applicants training\n",
      "loss: 1.4497442245483398 at epoch 57 at applicants training\n",
      "loss: 1.449642300605774 at epoch 58 at applicants training\n",
      "loss: 1.4493818283081055 at epoch 59 at applicants training\n",
      "loss: 1.4492738246917725 at epoch 60 at applicants training\n",
      "loss: 1.4489822387695312 at epoch 61 at applicants training\n",
      "loss: 1.4489558935165405 at epoch 62 at applicants training\n",
      "loss: 1.4486421346664429 at epoch 63 at applicants training\n",
      "loss: 1.4486323595046997 at epoch 64 at applicants training\n",
      "loss: 1.4483648538589478 at epoch 65 at applicants training\n",
      "loss: 1.4483897686004639 at epoch 66 at applicants training\n",
      "loss: 1.4481632709503174 at epoch 67 at applicants training\n",
      "loss: 1.448216438293457 at epoch 68 at applicants training\n",
      "loss: 1.4480112791061401 at epoch 69 at applicants training\n",
      "loss: 1.4480297565460205 at epoch 70 at applicants training\n",
      "loss: 1.447861671447754 at epoch 71 at applicants training\n",
      "loss: 1.4478510618209839 at epoch 72 at applicants training\n",
      "loss: 1.4477194547653198 at epoch 73 at applicants training\n",
      "loss: 1.4476908445358276 at epoch 74 at applicants training\n",
      "loss: 1.447572946548462 at epoch 75 at applicants training\n",
      "loss: 1.4475326538085938 at epoch 76 at applicants training\n",
      "loss: 1.4474595785140991 at epoch 77 at applicants training\n",
      "loss: 1.4474104642868042 at epoch 78 at applicants training\n",
      "loss: 1.4473551511764526 at epoch 79 at applicants training\n",
      "loss: 1.4473074674606323 at epoch 80 at applicants training\n",
      "loss: 1.4472600221633911 at epoch 81 at applicants training\n",
      "loss: 1.4472174644470215 at epoch 82 at applicants training\n",
      "loss: 1.4471763372421265 at epoch 83 at applicants training\n",
      "loss: 1.4471490383148193 at epoch 84 at applicants training\n",
      "loss: 1.4470981359481812 at epoch 85 at applicants training\n",
      "loss: 1.4470759630203247 at epoch 86 at applicants training\n",
      "loss: 1.4470086097717285 at epoch 87 at applicants training\n",
      "loss: 1.4469853639602661 at epoch 88 at applicants training\n",
      "loss: 1.4469218254089355 at epoch 89 at applicants training\n",
      "loss: 1.446882963180542 at epoch 90 at applicants training\n",
      "loss: 1.446837306022644 at epoch 91 at applicants training\n",
      "loss: 1.4467747211456299 at epoch 92 at applicants training\n",
      "loss: 1.4467509984970093 at epoch 93 at applicants training\n",
      "loss: 1.4466826915740967 at epoch 94 at applicants training\n",
      "loss: 1.4466549158096313 at epoch 95 at applicants training\n",
      "loss: 1.4466205835342407 at epoch 96 at applicants training\n",
      "loss: 1.4465795755386353 at epoch 97 at applicants training\n",
      "loss: 1.4465543031692505 at epoch 98 at applicants training\n",
      "loss: 1.446516990661621 at epoch 99 at applicants training\n",
      "loss: 1.6289410591125488 at epoch 0 at applicants training\n",
      "loss: 1.561753273010254 at epoch 1 at applicants training\n",
      "loss: 1.4948530197143555 at epoch 2 at applicants training\n",
      "loss: 1.4076694250106812 at epoch 3 at applicants training\n",
      "loss: 1.520188808441162 at epoch 4 at applicants training\n",
      "loss: 1.4014211893081665 at epoch 5 at applicants training\n",
      "loss: 1.4173095226287842 at epoch 6 at applicants training\n",
      "loss: 1.3911482095718384 at epoch 7 at applicants training\n",
      "loss: 1.3814685344696045 at epoch 8 at applicants training\n",
      "loss: 1.3599148988723755 at epoch 9 at applicants training\n",
      "loss: 1.332547664642334 at epoch 10 at applicants training\n",
      "loss: 1.287115454673767 at epoch 11 at applicants training\n",
      "loss: 1.3015927076339722 at epoch 12 at applicants training\n",
      "loss: 1.3102257251739502 at epoch 13 at applicants training\n",
      "loss: 1.2982310056686401 at epoch 14 at applicants training\n",
      "loss: 1.2786451578140259 at epoch 15 at applicants training\n",
      "loss: 1.2704790830612183 at epoch 16 at applicants training\n",
      "loss: 1.259858250617981 at epoch 17 at applicants training\n",
      "loss: 1.2417901754379272 at epoch 18 at applicants training\n",
      "loss: 1.232540249824524 at epoch 19 at applicants training\n",
      "loss: 1.2511414289474487 at epoch 20 at applicants training\n",
      "loss: 1.2411277294158936 at epoch 21 at applicants training\n",
      "loss: 1.2290339469909668 at epoch 22 at applicants training\n",
      "loss: 1.2203792333602905 at epoch 23 at applicants training\n",
      "loss: 1.2157459259033203 at epoch 24 at applicants training\n",
      "loss: 1.215464472770691 at epoch 25 at applicants training\n",
      "loss: 1.203107237815857 at epoch 26 at applicants training\n",
      "loss: 1.1896737813949585 at epoch 27 at applicants training\n",
      "loss: 1.1780377626419067 at epoch 28 at applicants training\n",
      "loss: 1.1954847574234009 at epoch 29 at applicants training\n",
      "loss: 1.1735527515411377 at epoch 30 at applicants training\n",
      "loss: 1.160486102104187 at epoch 31 at applicants training\n",
      "loss: 1.1675406694412231 at epoch 32 at applicants training\n",
      "loss: 1.1485074758529663 at epoch 33 at applicants training\n",
      "loss: 1.1416534185409546 at epoch 34 at applicants training\n",
      "loss: 1.14463472366333 at epoch 35 at applicants training\n",
      "loss: 1.1308979988098145 at epoch 36 at applicants training\n",
      "loss: 1.1291627883911133 at epoch 37 at applicants training\n",
      "loss: 1.1312556266784668 at epoch 38 at applicants training\n",
      "loss: 1.1202282905578613 at epoch 39 at applicants training\n",
      "loss: 1.120091438293457 at epoch 40 at applicants training\n",
      "loss: 1.1207813024520874 at epoch 41 at applicants training\n",
      "loss: 1.1120641231536865 at epoch 42 at applicants training\n",
      "loss: 1.1125656366348267 at epoch 43 at applicants training\n",
      "loss: 1.1102855205535889 at epoch 44 at applicants training\n",
      "loss: 1.1042968034744263 at epoch 45 at applicants training\n",
      "loss: 1.105556845664978 at epoch 46 at applicants training\n",
      "loss: 1.1013803482055664 at epoch 47 at applicants training\n",
      "loss: 1.1005988121032715 at epoch 48 at applicants training\n",
      "loss: 1.099114179611206 at epoch 49 at applicants training\n",
      "loss: 1.098832607269287 at epoch 50 at applicants training\n",
      "loss: 1.0962389707565308 at epoch 51 at applicants training\n",
      "loss: 1.0972647666931152 at epoch 52 at applicants training\n",
      "loss: 1.0951323509216309 at epoch 53 at applicants training\n",
      "loss: 1.0945582389831543 at epoch 54 at applicants training\n",
      "loss: 1.094577670097351 at epoch 55 at applicants training\n",
      "loss: 1.092514157295227 at epoch 56 at applicants training\n",
      "loss: 1.0930694341659546 at epoch 57 at applicants training\n",
      "loss: 1.0912225246429443 at epoch 58 at applicants training\n",
      "loss: 1.0913478136062622 at epoch 59 at applicants training\n",
      "loss: 1.0902366638183594 at epoch 60 at applicants training\n",
      "loss: 1.0895329713821411 at epoch 61 at applicants training\n",
      "loss: 1.0894629955291748 at epoch 62 at applicants training\n",
      "loss: 1.0881401300430298 at epoch 63 at applicants training\n",
      "loss: 1.0882720947265625 at epoch 64 at applicants training\n",
      "loss: 1.0874085426330566 at epoch 65 at applicants training\n",
      "loss: 1.0869930982589722 at epoch 66 at applicants training\n",
      "loss: 1.0867103338241577 at epoch 67 at applicants training\n",
      "loss: 1.0860992670059204 at epoch 68 at applicants training\n",
      "loss: 1.0859684944152832 at epoch 69 at applicants training\n",
      "loss: 1.0854533910751343 at epoch 70 at applicants training\n",
      "loss: 1.0852055549621582 at epoch 71 at applicants training\n",
      "loss: 1.085015892982483 at epoch 72 at applicants training\n",
      "loss: 1.0846251249313354 at epoch 73 at applicants training\n",
      "loss: 1.0845379829406738 at epoch 74 at applicants training\n",
      "loss: 1.084283471107483 at epoch 75 at applicants training\n",
      "loss: 1.0840575695037842 at epoch 76 at applicants training\n",
      "loss: 1.0839120149612427 at epoch 77 at applicants training\n",
      "loss: 1.0836617946624756 at epoch 78 at applicants training\n",
      "loss: 1.0835280418395996 at epoch 79 at applicants training\n",
      "loss: 1.0832855701446533 at epoch 80 at applicants training\n",
      "loss: 1.083130121231079 at epoch 81 at applicants training\n",
      "loss: 1.0829668045043945 at epoch 82 at applicants training\n",
      "loss: 1.0827275514602661 at epoch 83 at applicants training\n",
      "loss: 1.0826506614685059 at epoch 84 at applicants training\n",
      "loss: 1.0824503898620605 at epoch 85 at applicants training\n",
      "loss: 1.0823172330856323 at epoch 86 at applicants training\n",
      "loss: 1.0822277069091797 at epoch 87 at applicants training\n",
      "loss: 1.082066297531128 at epoch 88 at applicants training\n",
      "loss: 1.0819875001907349 at epoch 89 at applicants training\n",
      "loss: 1.081875205039978 at epoch 90 at applicants training\n",
      "loss: 1.0817780494689941 at epoch 91 at applicants training\n",
      "loss: 1.0816853046417236 at epoch 92 at applicants training\n",
      "loss: 1.081602692604065 at epoch 93 at applicants training\n",
      "loss: 1.081512689590454 at epoch 94 at applicants training\n",
      "loss: 1.0814268589019775 at epoch 95 at applicants training\n",
      "loss: 1.0813630819320679 at epoch 96 at applicants training\n",
      "loss: 1.0812709331512451 at epoch 97 at applicants training\n",
      "loss: 1.0812166929244995 at epoch 98 at applicants training\n",
      "loss: 1.0811439752578735 at epoch 99 at applicants training\n",
      "loss: 1.6747887134552002 at epoch 0 at applicants training\n",
      "loss: 1.6062005758285522 at epoch 1 at applicants training\n",
      "loss: 1.6628286838531494 at epoch 2 at applicants training\n",
      "loss: 1.6654208898544312 at epoch 3 at applicants training\n",
      "loss: 1.6222535371780396 at epoch 4 at applicants training\n",
      "loss: 1.577518105506897 at epoch 5 at applicants training\n",
      "loss: 1.6135815382003784 at epoch 6 at applicants training\n",
      "loss: 1.5378694534301758 at epoch 7 at applicants training\n",
      "loss: 1.6154258251190186 at epoch 8 at applicants training\n",
      "loss: 1.644599199295044 at epoch 9 at applicants training\n",
      "loss: 1.6380195617675781 at epoch 10 at applicants training\n",
      "loss: 1.592850685119629 at epoch 11 at applicants training\n",
      "loss: 1.5153923034667969 at epoch 12 at applicants training\n",
      "loss: 1.5479846000671387 at epoch 13 at applicants training\n",
      "loss: 1.5617642402648926 at epoch 14 at applicants training\n",
      "loss: 1.5529890060424805 at epoch 15 at applicants training\n",
      "loss: 1.512607216835022 at epoch 16 at applicants training\n",
      "loss: 1.5057096481323242 at epoch 17 at applicants training\n",
      "loss: 1.5416792631149292 at epoch 18 at applicants training\n",
      "loss: 1.527199387550354 at epoch 19 at applicants training\n",
      "loss: 1.4903271198272705 at epoch 20 at applicants training\n",
      "loss: 1.5088791847229004 at epoch 21 at applicants training\n",
      "loss: 1.531801700592041 at epoch 22 at applicants training\n",
      "loss: 1.5042229890823364 at epoch 23 at applicants training\n",
      "loss: 1.4854220151901245 at epoch 24 at applicants training\n",
      "loss: 1.500594139099121 at epoch 25 at applicants training\n",
      "loss: 1.514302372932434 at epoch 26 at applicants training\n",
      "loss: 1.5086660385131836 at epoch 27 at applicants training\n",
      "loss: 1.4789631366729736 at epoch 28 at applicants training\n",
      "loss: 1.4818605184555054 at epoch 29 at applicants training\n",
      "loss: 1.4919055700302124 at epoch 30 at applicants training\n",
      "loss: 1.4889531135559082 at epoch 31 at applicants training\n",
      "loss: 1.4690579175949097 at epoch 32 at applicants training\n",
      "loss: 1.4724915027618408 at epoch 33 at applicants training\n",
      "loss: 1.474989414215088 at epoch 34 at applicants training\n",
      "loss: 1.4713746309280396 at epoch 35 at applicants training\n",
      "loss: 1.466935157775879 at epoch 36 at applicants training\n",
      "loss: 1.4701042175292969 at epoch 37 at applicants training\n",
      "loss: 1.4693527221679688 at epoch 38 at applicants training\n",
      "loss: 1.4629162549972534 at epoch 39 at applicants training\n",
      "loss: 1.4644715785980225 at epoch 40 at applicants training\n",
      "loss: 1.4630545377731323 at epoch 41 at applicants training\n",
      "loss: 1.4612385034561157 at epoch 42 at applicants training\n",
      "loss: 1.4603338241577148 at epoch 43 at applicants training\n",
      "loss: 1.461539626121521 at epoch 44 at applicants training\n",
      "loss: 1.4579635858535767 at epoch 45 at applicants training\n",
      "loss: 1.4589015245437622 at epoch 46 at applicants training\n",
      "loss: 1.457188367843628 at epoch 47 at applicants training\n",
      "loss: 1.4550248384475708 at epoch 48 at applicants training\n",
      "loss: 1.455593228340149 at epoch 49 at applicants training\n",
      "loss: 1.4536274671554565 at epoch 50 at applicants training\n",
      "loss: 1.455077052116394 at epoch 51 at applicants training\n",
      "loss: 1.4524390697479248 at epoch 52 at applicants training\n",
      "loss: 1.4533919095993042 at epoch 53 at applicants training\n",
      "loss: 1.4523729085922241 at epoch 54 at applicants training\n",
      "loss: 1.4517043828964233 at epoch 55 at applicants training\n",
      "loss: 1.4521872997283936 at epoch 56 at applicants training\n",
      "loss: 1.4508672952651978 at epoch 57 at applicants training\n",
      "loss: 1.4513493776321411 at epoch 58 at applicants training\n",
      "loss: 1.4506570100784302 at epoch 59 at applicants training\n",
      "loss: 1.4508826732635498 at epoch 60 at applicants training\n",
      "loss: 1.4505338668823242 at epoch 61 at applicants training\n",
      "loss: 1.4506067037582397 at epoch 62 at applicants training\n",
      "loss: 1.4507626295089722 at epoch 63 at applicants training\n",
      "loss: 1.4503594636917114 at epoch 64 at applicants training\n",
      "loss: 1.4506616592407227 at epoch 65 at applicants training\n",
      "loss: 1.4503175020217896 at epoch 66 at applicants training\n",
      "loss: 1.4503393173217773 at epoch 67 at applicants training\n",
      "loss: 1.4502776861190796 at epoch 68 at applicants training\n",
      "loss: 1.4500226974487305 at epoch 69 at applicants training\n",
      "loss: 1.4501234292984009 at epoch 70 at applicants training\n",
      "loss: 1.4497815370559692 at epoch 71 at applicants training\n",
      "loss: 1.4498817920684814 at epoch 72 at applicants training\n",
      "loss: 1.4496281147003174 at epoch 73 at applicants training\n",
      "loss: 1.4496526718139648 at epoch 74 at applicants training\n",
      "loss: 1.4496574401855469 at epoch 75 at applicants training\n",
      "loss: 1.4494744539260864 at epoch 76 at applicants training\n",
      "loss: 1.4496406316757202 at epoch 77 at applicants training\n",
      "loss: 1.4494476318359375 at epoch 78 at applicants training\n",
      "loss: 1.4494420289993286 at epoch 79 at applicants training\n",
      "loss: 1.449507474899292 at epoch 80 at applicants training\n",
      "loss: 1.449312448501587 at epoch 81 at applicants training\n",
      "loss: 1.4493238925933838 at epoch 82 at applicants training\n",
      "loss: 1.4494061470031738 at epoch 83 at applicants training\n",
      "loss: 1.449272871017456 at epoch 84 at applicants training\n",
      "loss: 1.4491610527038574 at epoch 85 at applicants training\n",
      "loss: 1.4475871324539185 at epoch 86 at applicants training\n",
      "loss: 1.445796012878418 at epoch 87 at applicants training\n",
      "loss: 1.444045901298523 at epoch 88 at applicants training\n",
      "loss: 1.4378536939620972 at epoch 89 at applicants training\n",
      "loss: 1.4191794395446777 at epoch 90 at applicants training\n",
      "loss: 1.4005475044250488 at epoch 91 at applicants training\n",
      "loss: 1.4233005046844482 at epoch 92 at applicants training\n",
      "loss: 1.3930296897888184 at epoch 93 at applicants training\n",
      "loss: 1.389655590057373 at epoch 94 at applicants training\n",
      "loss: 1.4203004837036133 at epoch 95 at applicants training\n",
      "loss: 1.4032424688339233 at epoch 96 at applicants training\n",
      "loss: 1.4190685749053955 at epoch 97 at applicants training\n",
      "loss: 1.3843145370483398 at epoch 98 at applicants training\n",
      "loss: 1.3521490097045898 at epoch 99 at applicants training\n",
      "loss: 1.6792629957199097 at epoch 0 at applicants training\n",
      "loss: 1.6179486513137817 at epoch 1 at applicants training\n",
      "loss: 1.5394271612167358 at epoch 2 at applicants training\n",
      "loss: 1.5108941793441772 at epoch 3 at applicants training\n",
      "loss: 1.5025606155395508 at epoch 4 at applicants training\n",
      "loss: 1.4908298254013062 at epoch 5 at applicants training\n",
      "loss: 1.4833118915557861 at epoch 6 at applicants training\n",
      "loss: 1.475122094154358 at epoch 7 at applicants training\n",
      "loss: 1.4870367050170898 at epoch 8 at applicants training\n",
      "loss: 1.4681636095046997 at epoch 9 at applicants training\n",
      "loss: 1.4740691184997559 at epoch 10 at applicants training\n",
      "loss: 1.4648106098175049 at epoch 11 at applicants training\n",
      "loss: 1.4718095064163208 at epoch 12 at applicants training\n",
      "loss: 1.4653156995773315 at epoch 13 at applicants training\n",
      "loss: 1.4661673307418823 at epoch 14 at applicants training\n",
      "loss: 1.4697948694229126 at epoch 15 at applicants training\n",
      "loss: 1.4607925415039062 at epoch 16 at applicants training\n",
      "loss: 1.465755820274353 at epoch 17 at applicants training\n",
      "loss: 1.4633632898330688 at epoch 18 at applicants training\n",
      "loss: 1.4594371318817139 at epoch 19 at applicants training\n",
      "loss: 1.4614046812057495 at epoch 20 at applicants training\n",
      "loss: 1.4592316150665283 at epoch 21 at applicants training\n",
      "loss: 1.4598993062973022 at epoch 22 at applicants training\n",
      "loss: 1.4565449953079224 at epoch 23 at applicants training\n",
      "loss: 1.4579901695251465 at epoch 24 at applicants training\n",
      "loss: 1.4551513195037842 at epoch 25 at applicants training\n",
      "loss: 1.4569510221481323 at epoch 26 at applicants training\n",
      "loss: 1.4543806314468384 at epoch 27 at applicants training\n",
      "loss: 1.4560348987579346 at epoch 28 at applicants training\n",
      "loss: 1.4541542530059814 at epoch 29 at applicants training\n",
      "loss: 1.4544610977172852 at epoch 30 at applicants training\n",
      "loss: 1.4543325901031494 at epoch 31 at applicants training\n",
      "loss: 1.4535553455352783 at epoch 32 at applicants training\n",
      "loss: 1.4541772603988647 at epoch 33 at applicants training\n",
      "loss: 1.4531660079956055 at epoch 34 at applicants training\n",
      "loss: 1.4529696702957153 at epoch 35 at applicants training\n",
      "loss: 1.453006386756897 at epoch 36 at applicants training\n",
      "loss: 1.4521023035049438 at epoch 37 at applicants training\n",
      "loss: 1.452781319618225 at epoch 38 at applicants training\n",
      "loss: 1.4519915580749512 at epoch 39 at applicants training\n",
      "loss: 1.4527366161346436 at epoch 40 at applicants training\n",
      "loss: 1.4517467021942139 at epoch 41 at applicants training\n",
      "loss: 1.4521989822387695 at epoch 42 at applicants training\n",
      "loss: 1.4514188766479492 at epoch 43 at applicants training\n",
      "loss: 1.4518215656280518 at epoch 44 at applicants training\n",
      "loss: 1.4511194229125977 at epoch 45 at applicants training\n",
      "loss: 1.4513510465621948 at epoch 46 at applicants training\n",
      "loss: 1.4509488344192505 at epoch 47 at applicants training\n",
      "loss: 1.451108455657959 at epoch 48 at applicants training\n",
      "loss: 1.4507458209991455 at epoch 49 at applicants training\n",
      "loss: 1.4507560729980469 at epoch 50 at applicants training\n",
      "loss: 1.450486421585083 at epoch 51 at applicants training\n",
      "loss: 1.4503189325332642 at epoch 52 at applicants training\n",
      "loss: 1.4500224590301514 at epoch 53 at applicants training\n",
      "loss: 1.4498754739761353 at epoch 54 at applicants training\n",
      "loss: 1.449555516242981 at epoch 55 at applicants training\n",
      "loss: 1.4495701789855957 at epoch 56 at applicants training\n",
      "loss: 1.4492377042770386 at epoch 57 at applicants training\n",
      "loss: 1.449022650718689 at epoch 58 at applicants training\n",
      "loss: 1.4489856958389282 at epoch 59 at applicants training\n",
      "loss: 1.4487473964691162 at epoch 60 at applicants training\n",
      "loss: 1.4484078884124756 at epoch 61 at applicants training\n",
      "loss: 1.4483698606491089 at epoch 62 at applicants training\n",
      "loss: 1.4485673904418945 at epoch 63 at applicants training\n",
      "loss: 1.448561191558838 at epoch 64 at applicants training\n",
      "loss: 1.448702335357666 at epoch 65 at applicants training\n",
      "loss: 1.4486373662948608 at epoch 66 at applicants training\n",
      "loss: 1.4489924907684326 at epoch 67 at applicants training\n",
      "loss: 1.4494085311889648 at epoch 68 at applicants training\n",
      "loss: 1.4501484632492065 at epoch 69 at applicants training\n",
      "loss: 1.4500858783721924 at epoch 70 at applicants training\n",
      "loss: 1.4493886232376099 at epoch 71 at applicants training\n",
      "loss: 1.448655366897583 at epoch 72 at applicants training\n",
      "loss: 1.4484251737594604 at epoch 73 at applicants training\n",
      "loss: 1.44830322265625 at epoch 74 at applicants training\n",
      "loss: 1.4483524560928345 at epoch 75 at applicants training\n",
      "loss: 1.4483815431594849 at epoch 76 at applicants training\n",
      "loss: 1.4485750198364258 at epoch 77 at applicants training\n",
      "loss: 1.448803424835205 at epoch 78 at applicants training\n",
      "loss: 1.4489670991897583 at epoch 79 at applicants training\n",
      "loss: 1.4489363431930542 at epoch 80 at applicants training\n",
      "loss: 1.4486514329910278 at epoch 81 at applicants training\n",
      "loss: 1.4483065605163574 at epoch 82 at applicants training\n",
      "loss: 1.448122262954712 at epoch 83 at applicants training\n",
      "loss: 1.4480313062667847 at epoch 84 at applicants training\n",
      "loss: 1.4481399059295654 at epoch 85 at applicants training\n",
      "loss: 1.4484875202178955 at epoch 86 at applicants training\n",
      "loss: 1.4490270614624023 at epoch 87 at applicants training\n",
      "loss: 1.449477195739746 at epoch 88 at applicants training\n",
      "loss: 1.4486056566238403 at epoch 89 at applicants training\n",
      "loss: 1.448179006576538 at epoch 90 at applicants training\n",
      "loss: 1.448021411895752 at epoch 91 at applicants training\n",
      "loss: 1.4481834173202515 at epoch 92 at applicants training\n",
      "loss: 1.4485069513320923 at epoch 93 at applicants training\n",
      "loss: 1.448805570602417 at epoch 94 at applicants training\n",
      "loss: 1.448663353919983 at epoch 95 at applicants training\n",
      "loss: 1.4482518434524536 at epoch 96 at applicants training\n",
      "loss: 1.4479973316192627 at epoch 97 at applicants training\n",
      "loss: 1.447924017906189 at epoch 98 at applicants training\n",
      "loss: 1.4480966329574585 at epoch 99 at applicants training\n",
      "loss: 1.6825573444366455 at epoch 0 at applicants training\n",
      "loss: 1.7979600429534912 at epoch 1 at applicants training\n",
      "loss: 1.6606504917144775 at epoch 2 at applicants training\n",
      "loss: 1.6692906618118286 at epoch 3 at applicants training\n",
      "loss: 1.6590901613235474 at epoch 4 at applicants training\n",
      "loss: 1.6270121335983276 at epoch 5 at applicants training\n",
      "loss: 1.6113581657409668 at epoch 6 at applicants training\n",
      "loss: 1.6007990837097168 at epoch 7 at applicants training\n",
      "loss: 1.5917059183120728 at epoch 8 at applicants training\n",
      "loss: 1.5791186094284058 at epoch 9 at applicants training\n",
      "loss: 1.583375096321106 at epoch 10 at applicants training\n",
      "loss: 1.5610136985778809 at epoch 11 at applicants training\n",
      "loss: 1.5597723722457886 at epoch 12 at applicants training\n",
      "loss: 1.5570331811904907 at epoch 13 at applicants training\n",
      "loss: 1.5337917804718018 at epoch 14 at applicants training\n",
      "loss: 1.5284136533737183 at epoch 15 at applicants training\n",
      "loss: 1.4994781017303467 at epoch 16 at applicants training\n",
      "loss: 1.5021448135375977 at epoch 17 at applicants training\n",
      "loss: 1.487351655960083 at epoch 18 at applicants training\n",
      "loss: 1.4670199155807495 at epoch 19 at applicants training\n",
      "loss: 1.4555673599243164 at epoch 20 at applicants training\n",
      "loss: 1.44379460811615 at epoch 21 at applicants training\n",
      "loss: 1.4198756217956543 at epoch 22 at applicants training\n",
      "loss: 1.4047660827636719 at epoch 23 at applicants training\n",
      "loss: 1.4045413732528687 at epoch 24 at applicants training\n",
      "loss: 1.3846875429153442 at epoch 25 at applicants training\n",
      "loss: 1.378546953201294 at epoch 26 at applicants training\n",
      "loss: 1.3792104721069336 at epoch 27 at applicants training\n",
      "loss: 1.370771050453186 at epoch 28 at applicants training\n",
      "loss: 1.3672631978988647 at epoch 29 at applicants training\n",
      "loss: 1.346096396446228 at epoch 30 at applicants training\n",
      "loss: 1.3585368394851685 at epoch 31 at applicants training\n",
      "loss: 1.3380507230758667 at epoch 32 at applicants training\n",
      "loss: 1.3400757312774658 at epoch 33 at applicants training\n",
      "loss: 1.327041506767273 at epoch 34 at applicants training\n",
      "loss: 1.3315566778182983 at epoch 35 at applicants training\n",
      "loss: 1.3252441883087158 at epoch 36 at applicants training\n",
      "loss: 1.317596435546875 at epoch 37 at applicants training\n",
      "loss: 1.3200749158859253 at epoch 38 at applicants training\n",
      "loss: 1.3045388460159302 at epoch 39 at applicants training\n",
      "loss: 1.2753962278366089 at epoch 40 at applicants training\n",
      "loss: 1.2695103883743286 at epoch 41 at applicants training\n",
      "loss: 1.2326749563217163 at epoch 42 at applicants training\n",
      "loss: 1.2291651964187622 at epoch 43 at applicants training\n",
      "loss: 1.1861708164215088 at epoch 44 at applicants training\n",
      "loss: 1.1872568130493164 at epoch 45 at applicants training\n",
      "loss: 1.1603080034255981 at epoch 46 at applicants training\n",
      "loss: 1.1635372638702393 at epoch 47 at applicants training\n",
      "loss: 1.1482810974121094 at epoch 48 at applicants training\n",
      "loss: 1.1407252550125122 at epoch 49 at applicants training\n",
      "loss: 1.1369621753692627 at epoch 50 at applicants training\n",
      "loss: 1.1271579265594482 at epoch 51 at applicants training\n",
      "loss: 1.1285911798477173 at epoch 52 at applicants training\n",
      "loss: 1.117425799369812 at epoch 53 at applicants training\n",
      "loss: 1.1190146207809448 at epoch 54 at applicants training\n",
      "loss: 1.1163386106491089 at epoch 55 at applicants training\n",
      "loss: 1.1104542016983032 at epoch 56 at applicants training\n",
      "loss: 1.1114600896835327 at epoch 57 at applicants training\n",
      "loss: 1.1061670780181885 at epoch 58 at applicants training\n",
      "loss: 1.1042529344558716 at epoch 59 at applicants training\n",
      "loss: 1.1009477376937866 at epoch 60 at applicants training\n",
      "loss: 1.0966709852218628 at epoch 61 at applicants training\n",
      "loss: 1.096793532371521 at epoch 62 at applicants training\n",
      "loss: 1.0933765172958374 at epoch 63 at applicants training\n",
      "loss: 1.0930016040802002 at epoch 64 at applicants training\n",
      "loss: 1.0907443761825562 at epoch 65 at applicants training\n",
      "loss: 1.0894873142242432 at epoch 66 at applicants training\n",
      "loss: 1.0898239612579346 at epoch 67 at applicants training\n",
      "loss: 1.0876051187515259 at epoch 68 at applicants training\n",
      "loss: 1.0871104001998901 at epoch 69 at applicants training\n",
      "loss: 1.0864644050598145 at epoch 70 at applicants training\n",
      "loss: 1.0854991674423218 at epoch 71 at applicants training\n",
      "loss: 1.0853521823883057 at epoch 72 at applicants training\n",
      "loss: 1.0846705436706543 at epoch 73 at applicants training\n",
      "loss: 1.0837714672088623 at epoch 74 at applicants training\n",
      "loss: 1.0837490558624268 at epoch 75 at applicants training\n",
      "loss: 1.0835044384002686 at epoch 76 at applicants training\n",
      "loss: 1.0826963186264038 at epoch 77 at applicants training\n",
      "loss: 1.082463264465332 at epoch 78 at applicants training\n",
      "loss: 1.082322597503662 at epoch 79 at applicants training\n",
      "loss: 1.081847071647644 at epoch 80 at applicants training\n",
      "loss: 1.0816221237182617 at epoch 81 at applicants training\n",
      "loss: 1.0813385248184204 at epoch 82 at applicants training\n",
      "loss: 1.0810514688491821 at epoch 83 at applicants training\n",
      "loss: 1.0809406042099 at epoch 84 at applicants training\n",
      "loss: 1.0806938409805298 at epoch 85 at applicants training\n",
      "loss: 1.080400824546814 at epoch 86 at applicants training\n",
      "loss: 1.0803308486938477 at epoch 87 at applicants training\n",
      "loss: 1.080208659172058 at epoch 88 at applicants training\n",
      "loss: 1.0799301862716675 at epoch 89 at applicants training\n",
      "loss: 1.079818606376648 at epoch 90 at applicants training\n",
      "loss: 1.0797799825668335 at epoch 91 at applicants training\n",
      "loss: 1.0795692205429077 at epoch 92 at applicants training\n",
      "loss: 1.079446792602539 at epoch 93 at applicants training\n",
      "loss: 1.0794000625610352 at epoch 94 at applicants training\n",
      "loss: 1.0792665481567383 at epoch 95 at applicants training\n",
      "loss: 1.0791518688201904 at epoch 96 at applicants training\n",
      "loss: 1.0790895223617554 at epoch 97 at applicants training\n",
      "loss: 1.0790016651153564 at epoch 98 at applicants training\n",
      "loss: 1.0789070129394531 at epoch 99 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 0 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 1 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 2 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 3 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 4 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.624830961227417 at epoch 10 at applicants training\n",
      "loss: 1.619329571723938 at epoch 11 at applicants training\n",
      "loss: 1.6248301267623901 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "loss: 1.7198392152786255 at epoch 0 at applicants training\n",
      "loss: 1.6748313903808594 at epoch 1 at applicants training\n",
      "loss: 1.6748323440551758 at epoch 2 at applicants training\n",
      "loss: 1.6748324632644653 at epoch 3 at applicants training\n",
      "loss: 1.6748324632644653 at epoch 4 at applicants training\n",
      "loss: 1.6748319864273071 at epoch 5 at applicants training\n",
      "loss: 1.674829363822937 at epoch 6 at applicants training\n",
      "loss: 1.6748013496398926 at epoch 7 at applicants training\n",
      "loss: 1.673536777496338 at epoch 8 at applicants training\n",
      "loss: 1.5754104852676392 at epoch 9 at applicants training\n",
      "loss: 1.5943533182144165 at epoch 10 at applicants training\n",
      "loss: 1.5839694738388062 at epoch 11 at applicants training\n",
      "loss: 1.4759838581085205 at epoch 12 at applicants training\n",
      "loss: 1.544090986251831 at epoch 13 at applicants training\n",
      "loss: 1.521438479423523 at epoch 14 at applicants training\n",
      "loss: 1.4795286655426025 at epoch 15 at applicants training\n",
      "loss: 1.4927536249160767 at epoch 16 at applicants training\n",
      "loss: 1.4839582443237305 at epoch 17 at applicants training\n",
      "loss: 1.4525933265686035 at epoch 18 at applicants training\n",
      "loss: 1.4583207368850708 at epoch 19 at applicants training\n",
      "loss: 1.4451310634613037 at epoch 20 at applicants training\n",
      "loss: 1.4297658205032349 at epoch 21 at applicants training\n",
      "loss: 1.4415897130966187 at epoch 22 at applicants training\n",
      "loss: 1.4349966049194336 at epoch 23 at applicants training\n",
      "loss: 1.4092719554901123 at epoch 24 at applicants training\n",
      "loss: 1.421952247619629 at epoch 25 at applicants training\n",
      "loss: 1.4061555862426758 at epoch 26 at applicants training\n",
      "loss: 1.410823106765747 at epoch 27 at applicants training\n",
      "loss: 1.414806842803955 at epoch 28 at applicants training\n",
      "loss: 1.4065568447113037 at epoch 29 at applicants training\n",
      "loss: 1.4087682962417603 at epoch 30 at applicants training\n",
      "loss: 1.4096280336380005 at epoch 31 at applicants training\n",
      "loss: 1.4035289287567139 at epoch 32 at applicants training\n",
      "loss: 1.408131718635559 at epoch 33 at applicants training\n",
      "loss: 1.4019689559936523 at epoch 34 at applicants training\n",
      "loss: 1.402112603187561 at epoch 35 at applicants training\n",
      "loss: 1.4044239521026611 at epoch 36 at applicants training\n",
      "loss: 1.4030206203460693 at epoch 37 at applicants training\n",
      "loss: 1.4010663032531738 at epoch 38 at applicants training\n",
      "loss: 1.4005138874053955 at epoch 39 at applicants training\n",
      "loss: 1.4030758142471313 at epoch 40 at applicants training\n",
      "loss: 1.4011712074279785 at epoch 41 at applicants training\n",
      "loss: 1.400331735610962 at epoch 42 at applicants training\n",
      "loss: 1.401466727256775 at epoch 43 at applicants training\n",
      "loss: 1.4018795490264893 at epoch 44 at applicants training\n",
      "loss: 1.4008980989456177 at epoch 45 at applicants training\n",
      "loss: 1.4000523090362549 at epoch 46 at applicants training\n",
      "loss: 1.4006434679031372 at epoch 47 at applicants training\n",
      "loss: 1.4008837938308716 at epoch 48 at applicants training\n",
      "loss: 1.400179386138916 at epoch 49 at applicants training\n",
      "loss: 1.3995635509490967 at epoch 50 at applicants training\n",
      "loss: 1.399712085723877 at epoch 51 at applicants training\n",
      "loss: 1.3997259140014648 at epoch 52 at applicants training\n",
      "loss: 1.398982048034668 at epoch 53 at applicants training\n",
      "loss: 1.3975905179977417 at epoch 54 at applicants training\n",
      "loss: 1.3972774744033813 at epoch 55 at applicants training\n",
      "loss: 1.3956278562545776 at epoch 56 at applicants training\n",
      "loss: 1.3943495750427246 at epoch 57 at applicants training\n",
      "loss: 1.3930436372756958 at epoch 58 at applicants training\n",
      "loss: 1.391660213470459 at epoch 59 at applicants training\n",
      "loss: 1.3905390501022339 at epoch 60 at applicants training\n",
      "loss: 1.3905329704284668 at epoch 61 at applicants training\n",
      "loss: 1.38969087600708 at epoch 62 at applicants training\n",
      "loss: 1.39048171043396 at epoch 63 at applicants training\n",
      "loss: 1.3897819519042969 at epoch 64 at applicants training\n",
      "loss: 1.3901842832565308 at epoch 65 at applicants training\n",
      "loss: 1.3896807432174683 at epoch 66 at applicants training\n",
      "loss: 1.3897123336791992 at epoch 67 at applicants training\n",
      "loss: 1.3892757892608643 at epoch 68 at applicants training\n",
      "loss: 1.389040470123291 at epoch 69 at applicants training\n",
      "loss: 1.3888053894042969 at epoch 70 at applicants training\n",
      "loss: 1.3883049488067627 at epoch 71 at applicants training\n",
      "loss: 1.3882267475128174 at epoch 72 at applicants training\n",
      "loss: 1.387747049331665 at epoch 73 at applicants training\n",
      "loss: 1.3877654075622559 at epoch 74 at applicants training\n",
      "loss: 1.3874412775039673 at epoch 75 at applicants training\n",
      "loss: 1.3875879049301147 at epoch 76 at applicants training\n",
      "loss: 1.3875365257263184 at epoch 77 at applicants training\n",
      "loss: 1.3875632286071777 at epoch 78 at applicants training\n",
      "loss: 1.3876957893371582 at epoch 79 at applicants training\n",
      "loss: 1.3875219821929932 at epoch 80 at applicants training\n",
      "loss: 1.387606143951416 at epoch 81 at applicants training\n",
      "loss: 1.387465000152588 at epoch 82 at applicants training\n",
      "loss: 1.387349247932434 at epoch 83 at applicants training\n",
      "loss: 1.3873945474624634 at epoch 84 at applicants training\n",
      "loss: 1.3872270584106445 at epoch 85 at applicants training\n",
      "loss: 1.387233853340149 at epoch 86 at applicants training\n",
      "loss: 1.3871831893920898 at epoch 87 at applicants training\n",
      "loss: 1.3870774507522583 at epoch 88 at applicants training\n",
      "loss: 1.3870980739593506 at epoch 89 at applicants training\n",
      "loss: 1.3869810104370117 at epoch 90 at applicants training\n",
      "loss: 1.3869174718856812 at epoch 91 at applicants training\n",
      "loss: 1.3869316577911377 at epoch 92 at applicants training\n",
      "loss: 1.3868615627288818 at epoch 93 at applicants training\n",
      "loss: 1.3867835998535156 at epoch 94 at applicants training\n",
      "loss: 1.38679838180542 at epoch 95 at applicants training\n",
      "loss: 1.3868414163589478 at epoch 96 at applicants training\n",
      "loss: 1.3867994546890259 at epoch 97 at applicants training\n",
      "loss: 1.3867003917694092 at epoch 98 at applicants training\n",
      "loss: 1.386602520942688 at epoch 99 at applicants training\n",
      "loss: 1.814314603805542 at epoch 0 at applicants training\n",
      "loss: 1.6530320644378662 at epoch 1 at applicants training\n",
      "loss: 1.7101397514343262 at epoch 2 at applicants training\n",
      "loss: 1.4707026481628418 at epoch 3 at applicants training\n",
      "loss: 1.5113482475280762 at epoch 4 at applicants training\n",
      "loss: 1.5035651922225952 at epoch 5 at applicants training\n",
      "loss: 1.4957427978515625 at epoch 6 at applicants training\n",
      "loss: 1.5191526412963867 at epoch 7 at applicants training\n",
      "loss: 1.4814594984054565 at epoch 8 at applicants training\n",
      "loss: 1.4764317274093628 at epoch 9 at applicants training\n",
      "loss: 1.4845179319381714 at epoch 10 at applicants training\n",
      "loss: 1.470442771911621 at epoch 11 at applicants training\n",
      "loss: 1.476466417312622 at epoch 12 at applicants training\n",
      "loss: 1.467427372932434 at epoch 13 at applicants training\n",
      "loss: 1.4686381816864014 at epoch 14 at applicants training\n",
      "loss: 1.4642837047576904 at epoch 15 at applicants training\n",
      "loss: 1.4618241786956787 at epoch 16 at applicants training\n",
      "loss: 1.4610503911972046 at epoch 17 at applicants training\n",
      "loss: 1.4577996730804443 at epoch 18 at applicants training\n",
      "loss: 1.4580459594726562 at epoch 19 at applicants training\n",
      "loss: 1.4544456005096436 at epoch 20 at applicants training\n",
      "loss: 1.4548704624176025 at epoch 21 at applicants training\n",
      "loss: 1.4522490501403809 at epoch 22 at applicants training\n",
      "loss: 1.4507317543029785 at epoch 23 at applicants training\n",
      "loss: 1.4451059103012085 at epoch 24 at applicants training\n",
      "loss: 1.4460971355438232 at epoch 25 at applicants training\n",
      "loss: 1.4494163990020752 at epoch 26 at applicants training\n",
      "loss: 1.442255973815918 at epoch 27 at applicants training\n",
      "loss: 1.4326789379119873 at epoch 28 at applicants training\n",
      "loss: 1.4493234157562256 at epoch 29 at applicants training\n",
      "loss: 1.427786111831665 at epoch 30 at applicants training\n",
      "loss: 1.447229266166687 at epoch 31 at applicants training\n",
      "loss: 1.421576976776123 at epoch 32 at applicants training\n",
      "loss: 1.444672703742981 at epoch 33 at applicants training\n",
      "loss: 1.4256995916366577 at epoch 34 at applicants training\n",
      "loss: 1.4331320524215698 at epoch 35 at applicants training\n",
      "loss: 1.4368226528167725 at epoch 36 at applicants training\n",
      "loss: 1.4143012762069702 at epoch 37 at applicants training\n",
      "loss: 1.4349431991577148 at epoch 38 at applicants training\n",
      "loss: 1.4223394393920898 at epoch 39 at applicants training\n",
      "loss: 1.41717529296875 at epoch 40 at applicants training\n",
      "loss: 1.4464889764785767 at epoch 41 at applicants training\n",
      "loss: 1.4158238172531128 at epoch 42 at applicants training\n",
      "loss: 1.4147562980651855 at epoch 43 at applicants training\n",
      "loss: 1.4315298795700073 at epoch 44 at applicants training\n",
      "loss: 1.4154261350631714 at epoch 45 at applicants training\n",
      "loss: 1.413422703742981 at epoch 46 at applicants training\n",
      "loss: 1.4200688600540161 at epoch 47 at applicants training\n",
      "loss: 1.421453833580017 at epoch 48 at applicants training\n",
      "loss: 1.4143471717834473 at epoch 49 at applicants training\n",
      "loss: 1.407325029373169 at epoch 50 at applicants training\n",
      "loss: 1.4188076257705688 at epoch 51 at applicants training\n",
      "loss: 1.4106806516647339 at epoch 52 at applicants training\n",
      "loss: 1.4064533710479736 at epoch 53 at applicants training\n",
      "loss: 1.4096864461898804 at epoch 54 at applicants training\n",
      "loss: 1.4054304361343384 at epoch 55 at applicants training\n",
      "loss: 1.4024070501327515 at epoch 56 at applicants training\n",
      "loss: 1.4064242839813232 at epoch 57 at applicants training\n",
      "loss: 1.4033154249191284 at epoch 58 at applicants training\n",
      "loss: 1.4027507305145264 at epoch 59 at applicants training\n",
      "loss: 1.4042948484420776 at epoch 60 at applicants training\n",
      "loss: 1.4045196771621704 at epoch 61 at applicants training\n",
      "loss: 1.4030402898788452 at epoch 62 at applicants training\n",
      "loss: 1.4026756286621094 at epoch 63 at applicants training\n",
      "loss: 1.402743935585022 at epoch 64 at applicants training\n",
      "loss: 1.4024602174758911 at epoch 65 at applicants training\n",
      "loss: 1.4018769264221191 at epoch 66 at applicants training\n",
      "loss: 1.4035868644714355 at epoch 67 at applicants training\n",
      "loss: 1.401900291442871 at epoch 68 at applicants training\n",
      "loss: 1.402248740196228 at epoch 69 at applicants training\n",
      "loss: 1.4022166728973389 at epoch 70 at applicants training\n",
      "loss: 1.4022771120071411 at epoch 71 at applicants training\n",
      "loss: 1.4023422002792358 at epoch 72 at applicants training\n",
      "loss: 1.4022040367126465 at epoch 73 at applicants training\n",
      "loss: 1.4016603231430054 at epoch 74 at applicants training\n",
      "loss: 1.3999202251434326 at epoch 75 at applicants training\n",
      "loss: 1.3994675874710083 at epoch 76 at applicants training\n",
      "loss: 1.3976719379425049 at epoch 77 at applicants training\n",
      "loss: 1.3980482816696167 at epoch 78 at applicants training\n",
      "loss: 1.3970489501953125 at epoch 79 at applicants training\n",
      "loss: 1.396838665008545 at epoch 80 at applicants training\n",
      "loss: 1.3952549695968628 at epoch 81 at applicants training\n",
      "loss: 1.3956257104873657 at epoch 82 at applicants training\n",
      "loss: 1.394521951675415 at epoch 83 at applicants training\n",
      "loss: 1.3950337171554565 at epoch 84 at applicants training\n",
      "loss: 1.3939701318740845 at epoch 85 at applicants training\n",
      "loss: 1.394264578819275 at epoch 86 at applicants training\n",
      "loss: 1.3937814235687256 at epoch 87 at applicants training\n",
      "loss: 1.393852949142456 at epoch 88 at applicants training\n",
      "loss: 1.3936129808425903 at epoch 89 at applicants training\n",
      "loss: 1.3935270309448242 at epoch 90 at applicants training\n",
      "loss: 1.3935776948928833 at epoch 91 at applicants training\n",
      "loss: 1.3933030366897583 at epoch 92 at applicants training\n",
      "loss: 1.3933089971542358 at epoch 93 at applicants training\n",
      "loss: 1.393204927444458 at epoch 94 at applicants training\n",
      "loss: 1.3930854797363281 at epoch 95 at applicants training\n",
      "loss: 1.393139362335205 at epoch 96 at applicants training\n",
      "loss: 1.392952561378479 at epoch 97 at applicants training\n",
      "loss: 1.3929823637008667 at epoch 98 at applicants training\n",
      "loss: 1.39286470413208 at epoch 99 at applicants training\n",
      "loss: 1.609751582145691 at epoch 0 at applicants training\n",
      "loss: 1.6138027906417847 at epoch 1 at applicants training\n",
      "loss: 1.5720086097717285 at epoch 2 at applicants training\n",
      "loss: 1.6095515489578247 at epoch 3 at applicants training\n",
      "loss: 1.6245787143707275 at epoch 4 at applicants training\n",
      "loss: 1.6248325109481812 at epoch 5 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 6 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 7 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 8 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 9 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 10 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 11 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 12 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 13 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 14 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 15 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 16 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 17 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 18 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 19 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 20 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 21 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 22 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 23 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 24 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 25 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 26 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 27 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 28 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 29 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 30 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 31 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 32 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 33 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 34 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 35 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 36 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 37 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 38 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 39 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 40 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 41 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 42 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 43 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 44 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 45 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 46 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 47 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 48 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 49 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 50 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 51 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 52 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 53 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 54 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 55 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 56 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 57 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 58 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 59 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 60 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 61 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 62 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 63 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 64 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 65 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 66 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 67 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 68 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 69 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 70 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 71 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 72 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 73 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 74 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 75 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 76 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 77 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 78 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 79 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 80 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 81 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 82 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 83 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 84 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 85 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 86 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 87 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 88 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 89 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 90 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 91 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 92 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 93 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 94 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 95 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 96 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 97 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 98 at applicants training\n",
      "loss: 1.6248326301574707 at epoch 99 at applicants training\n",
      "faculty_vectors: [[0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.1278972  0.05040207 0.13980323 0.12742627 0.14581134 0.07736603\n",
      "  0.01951673 0.08872223 0.13024412 0.09281079]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.01041314 0.20592686 0.0942512  0.05190548 0.04768495 0.17827562\n",
      "  0.05192086 0.17280017 0.09880626 0.08801546]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]\n",
      " [0.07111928 0.12959674 0.01126222 0.1219592  0.1294873  0.10510422\n",
      "  0.12417133 0.14035369 0.13333587 0.03361014]\n",
      " [0.11644829 0.11834479 0.02361647 0.0021561  0.144906   0.0264752\n",
      "  0.12280246 0.12720759 0.17644736 0.14159574]\n",
      " [0.05086573 0.14674769 0.0997167  0.17813519 0.07876923 0.08814894\n",
      "  0.11002026 0.01049029 0.14363795 0.09346802]]\n",
      "student_features: [[ 84.61845413  91.35264284  92.39754531  71.01904582  60.89586403\n",
      "   70.06210319  74.31994144  91.288768    67.76846361  95.77581402]\n",
      " [ 74.00766447  40.          73.03810339 100.          57.11482107\n",
      "   64.69250744  45.91662421  92.00521698  87.13765175  52.26246932]\n",
      " [ 75.58787673  78.73637625  71.66543405  55.46607965  47.04532845\n",
      "   46.26268487  85.78305825  90.76534213  69.268758    84.0931705 ]\n",
      " [ 40.          67.80305652  73.02954062  46.38359958 100.\n",
      "   70.14714595  40.          58.56592078  46.44045257  54.66770408]\n",
      " [ 50.77550384  61.98133937  74.99888775  82.04910518 100.\n",
      "   79.17374644  54.34710766  50.71616297  99.97424934  40.        ]\n",
      " [100.          92.98874062  77.7401636   69.393147    82.21710657\n",
      "   59.23432419  99.88372338  40.          60.68590607  69.99318953]\n",
      " [ 64.72237755  71.46787477  40.          40.          98.52992196\n",
      "   89.45628569  73.63316825  59.01343913  42.41772006  89.70935076]\n",
      " [ 97.16513824  91.82727849  75.24368712 100.          41.73152409\n",
      "   64.92429265  69.09490561  47.65676947  99.2741422   63.22046585]\n",
      " [ 73.56873208  51.42792401  40.          40.          56.85164849\n",
      "  100.          61.82850355  40.          42.1355724   49.57398378]\n",
      " [ 73.49321619  96.21154072  62.70646887  93.36634238  74.00050729\n",
      "   86.48110079  73.86531471  53.83966423  91.70682498  79.2697792 ]\n",
      " [ 65.92661164  58.78960969  77.94343286 100.          93.12400165\n",
      "   99.61405641  40.          53.00218843  60.11957901  40.        ]\n",
      " [ 63.71628004  95.11152835  83.49106654  55.8946345  100.\n",
      "   94.78461992  93.86240715  93.24379408  40.          51.58559784]\n",
      " [ 87.80878885  50.89985299  40.          48.19827352  49.60542495\n",
      "   95.81278791 100.          62.71177451  78.03125628 100.        ]\n",
      " [ 82.76095318  79.76216001  65.59682822  72.66100447  80.46673791\n",
      "   40.16234623  61.53172941  46.4647612   67.54132376 100.        ]\n",
      " [ 93.5675656   52.08303855  40.          71.9020371   89.06158967\n",
      "   96.66817499 100.          56.26456897  80.54080694  75.39620165]\n",
      " [ 70.78134173 100.          99.00558812  87.6721703   85.57683426\n",
      "   40.          64.67998279 100.          92.0965641   90.55772362]\n",
      " [100.          40.84202169  48.34979668  67.62614423  86.18220371\n",
      "   82.57080584  62.78085205  57.17390201  77.32229104  63.87048326]\n",
      " [ 40.          40.          54.59356868  40.          75.18928774\n",
      "   56.65211791  40.         100.          69.19079661  62.39221226]\n",
      " [ 40.          40.          40.          69.62224312  80.75107206\n",
      "   40.          49.58905236  40.          71.09052742  68.4714313 ]\n",
      " [ 48.66465616 100.          40.         100.          63.15682836\n",
      "  100.          76.13591529  54.24798815  76.5176318   80.50790011]\n",
      " [ 72.60729253  68.45105873  76.5981102   40.          66.58895497\n",
      "   67.89295041  85.41914874  40.          67.73883367  40.        ]\n",
      " [ 48.87899748 100.          57.21509061  81.97723319 100.\n",
      "   88.5183361   70.70359672  82.26359483  59.64205646  60.21249312]\n",
      " [ 55.87127636  89.37875909  72.96175584  61.18328706  52.00066992\n",
      "   53.04117003  73.68063228  43.05948796 100.          75.28276803]\n",
      " [100.          40.          89.80390362  93.16542661 100.\n",
      "   77.00202389  80.39801995  89.78466852  54.86817116  41.98083614]\n",
      " [ 56.09741093  40.          95.52773993  50.18342488 100.\n",
      "   40.          97.0226332   75.21242    100.          44.65865855]\n",
      " [ 64.73723979  48.69368899  75.72514479  46.17989932  81.77943836\n",
      "   50.26541281  40.         100.          40.          69.42905835]\n",
      " [100.          66.33727076  79.99297744  89.98833197  47.4860802\n",
      "   51.79865091  94.68602347  89.14290433  42.5776541   69.43491215]\n",
      " [ 50.26182375  60.73155832  73.25639497 100.          80.62897902\n",
      "   40.          40.          92.99309677  40.          69.77599226]\n",
      " [ 40.          40.          69.24484614  45.71336967  46.20621856\n",
      "   64.0115027   89.49406286  99.11264485  89.34444011  49.25023103]\n",
      " [ 76.59845676  85.5699106   66.86253612  52.67217511  60.73987866\n",
      "   79.70667064  93.58700774  91.90117451  60.64499898  71.62957287]\n",
      " [ 65.74527881  59.2695485  100.          65.33449794  40.\n",
      "   73.48064606 100.          47.29175807  89.78002393  82.76945159]\n",
      " [100.          56.33182501  70.02700145  41.55993426  40.\n",
      "   41.14515898  75.88581027 100.          47.92219515  58.14509843]\n",
      " [ 89.1456465   74.25764253  95.99434061  78.51180156  51.54841077\n",
      "   58.15216669  54.77679554  66.55157248  47.08701959  87.9379949 ]\n",
      " [ 45.5190952  100.          71.91042718  74.8234997   76.78495607\n",
      "   91.26062544  47.10796785  77.20419542  70.85560237  76.10371441]\n",
      " [ 40.          73.5570972   90.73868065  65.70015476  50.42935657\n",
      "   94.17186765 100.         100.          40.         100.        ]\n",
      " [ 79.55517687  65.77162827  85.52393595  74.17770591  40.\n",
      "   62.60503502  57.7139171   49.44284728  50.35593091  48.80200385]\n",
      " [ 40.          68.31612368  40.          80.52235093  60.07281323\n",
      "   57.53363618  65.93152917  78.3071343   40.          62.29791503]\n",
      " [ 55.0584441   93.14987782  57.44861604  67.97433365 100.\n",
      "   40.         100.          90.7031937  100.          82.97726474]\n",
      " [ 66.54346394  78.58754913  57.8304291  100.          78.57929193\n",
      "   40.          70.1224765   67.11747989  69.48880788 100.        ]\n",
      " [ 71.58589862  49.0022444   50.09776102  40.          45.72758284\n",
      "   45.78876399  68.238485   100.          40.          71.57413637]\n",
      " [ 65.45869267  41.02434426  56.64728467  93.55522884  56.48050185\n",
      "   78.38661147 100.          43.74870791 100.          90.2141261 ]\n",
      " [ 64.13951342  68.77382778  91.8860268  100.          92.17802855\n",
      "  100.          40.          91.89357994  60.21207334  48.7051747 ]\n",
      " [ 83.25579158 100.          80.02794194 100.          82.93799143\n",
      "  100.         100.          98.93212748  58.85776627  70.50058479]\n",
      " [ 82.32486984  66.86502537  46.17221642  53.13801166  73.98906036\n",
      "   41.85133753  40.          55.45499991  96.31385642  83.68454169]\n",
      " [ 40.          59.93871385  72.20861718  40.          57.70132175\n",
      "   61.8707027   49.93686269  68.31246725  66.34110923  40.        ]\n",
      " [ 99.21629875  40.          70.60786333 100.          76.28071557\n",
      "   53.25751239  40.         100.          41.35958298  71.71676794]\n",
      " [ 76.56480368 100.          44.07170689  75.47279448  40.90567806\n",
      "   81.54740499  71.59381153  56.84314802  59.02923332  40.        ]\n",
      " [ 69.64916892  69.21783328  57.42469462  46.00828337  78.02549078\n",
      "   91.54011126  40.          86.95476509  62.938461    59.29185255]\n",
      " [ 76.88549748 100.          40.47755506  90.42692464  66.27120155\n",
      "  100.          80.58960489  65.581809    75.19263416  49.6372305 ]\n",
      " [ 40.          43.57638382  90.4483717   65.9605415   40.\n",
      "   57.20428385  51.1137973   52.81241398  91.99534431  64.0422889 ]\n",
      " [ 50.6940453   40.          51.83102636  97.88805452  84.7425806\n",
      "   75.30653016  66.84596411  40.          70.09223151  88.38976218]\n",
      " [ 54.84743156  40.          89.02192329  76.21287083  55.8857874\n",
      "   69.68968378  85.96767963  84.09545702  73.96637149  44.543224  ]\n",
      " [ 78.82587292  75.3036614  100.         100.          70.38859372\n",
      "  100.          76.36191592  86.69536474  72.40879199 100.        ]\n",
      " [ 60.40027132  40.          63.71186877  50.73970059  71.21900929\n",
      "   67.19286726  65.42743952  81.49758431  79.5834907   66.98714105]\n",
      " [ 98.2681082   56.24225537 100.          90.22545307  84.36598515\n",
      "   57.51747591  67.24602352  91.38580283 100.          78.6260286 ]\n",
      " [ 91.11204456  40.          40.          40.          64.72317986\n",
      "   68.53634299  57.08352484  66.23186257  40.          52.00392868]\n",
      " [100.          63.72614066  74.73269454  76.77814277  54.23181728\n",
      "   69.98185191  62.35004912  40.          55.42319207  63.65497477]\n",
      " [100.          53.89165584  78.3626459   61.55105528  65.97556641\n",
      "  100.         100.          50.34534168  90.41742568  93.99484804]\n",
      " [ 61.54719582  98.40116655  40.01752782  67.98111875  81.47220082\n",
      "   40.          78.69838109  89.15867661  40.          84.12327883]\n",
      " [ 40.         100.          85.53651607 100.          89.50181342\n",
      "   99.23072341  53.03566809  91.94068653  98.45900181  96.71099179]\n",
      " [ 76.18325319  89.4413036   92.94494417  81.6923632   86.17300894\n",
      "   69.18953645  94.68658279  96.62242111  51.46434237  59.74227792]\n",
      " [ 84.53173971  46.27283086  40.         100.          74.45239072\n",
      "   72.07469945 100.          57.88445242  91.75457638 100.        ]\n",
      " [ 98.68619774  71.68859002  50.7751799   91.33413881  40.\n",
      "   47.98136405  57.38029493  89.99028148  96.63491898  53.4746838 ]\n",
      " [ 71.98414992  56.45621063  44.94297955  75.39166346  46.70991946\n",
      "   44.1795635   48.29824384  64.77332033  83.73365815  63.21285097]\n",
      " [ 94.09892398  40.          40.          99.15938734  50.82474856\n",
      "   40.          56.01246471  88.89325251  79.35974821  78.55557533]\n",
      " [ 60.46947062 100.          96.38178696  92.83911974 100.\n",
      "   64.23252607  64.12072864  78.68311653  63.65713224  93.60197845]\n",
      " [ 84.89855635 100.          40.         100.          87.98872142\n",
      "   95.86107772  82.06480557  89.46876114  95.59519843  57.78801818]\n",
      " [ 40.47664815  76.7402503   79.68915864  40.          48.65079976\n",
      "   72.83498329 100.          79.18500262  45.66387455  84.62567534]\n",
      " [ 92.23178657  74.91806876  40.          44.35876143  40.\n",
      "   71.17576478  88.516289    57.76720189  81.81763639  60.58863662]\n",
      " [ 60.97044162  64.63621196  87.74056003  85.04545107  40.\n",
      "   45.51515699  48.46492408  40.          54.12254044  52.56517004]\n",
      " [ 85.40872194  70.44333865  80.99767103  91.06783969  86.23373505\n",
      "   63.88192866 100.          73.97344812  44.18807042  40.        ]\n",
      " [ 72.01723359  40.          74.2332979  100.          57.86307574\n",
      "   76.37148418  66.26994926  79.46002311  85.20886871  63.34462473]\n",
      " [100.          40.          63.98391649  94.69392774 100.\n",
      "   77.12749811  92.91405106  45.35910706  70.91981569  98.36665083]\n",
      " [100.          40.         100.         100.          49.34682929\n",
      "   58.03963916  40.          58.55538319  44.59220904  65.9210438 ]\n",
      " [ 47.78194257  71.15857335  85.92961555  75.27865913  53.22172338\n",
      "   75.56649158  59.07694613  40.          92.94109191  40.        ]\n",
      " [ 40.          93.00557445  41.5118509   86.14949708  89.17529596\n",
      "   79.85936772  68.82950877  95.39412228  90.13060235  87.72041448]\n",
      " [ 92.47209348  90.49946366  73.4106111   87.01710609 100.\n",
      "   80.73207078  56.83134038  88.89544259  67.51331652  77.49829276]\n",
      " [ 90.86120203  84.96489498  40.          40.          99.11562004\n",
      "   65.18687723 100.          40.          60.4392017   94.1714819 ]\n",
      " [ 70.60144458  76.17286544  40.          40.          43.64476798\n",
      "   40.         100.          88.28240023  66.01564655 100.        ]\n",
      " [ 83.83455275  40.38663755 100.          68.17780276  40.\n",
      "   57.33694487  40.          64.11938952  51.55757741  90.57005904]\n",
      " [ 73.37896722  53.20884485  81.32988069  98.11141202  56.38104198\n",
      "  100.          96.84940124 100.         100.          92.20825529]\n",
      " [ 46.77565117  70.19919685  55.40051459  70.37817291  40.\n",
      "   98.14136365  83.08951692 100.          64.78727505  54.00980699]\n",
      " [ 86.55006732  95.75970138 100.          79.05421606  71.67863241\n",
      "   93.97207405  70.82091524  68.64854135  87.6739928   40.        ]\n",
      " [ 66.36984382  91.90907045  90.04331181  55.27799771  66.4279366\n",
      "   70.51873955 100.          83.35311649  65.90467361  85.84983994]\n",
      " [ 75.91160422 100.          64.90858001  66.40159968  40.\n",
      "  100.          50.52661472  65.31101154 100.          44.02946875]\n",
      " [ 66.11444325  98.90759176  69.8124073   45.39319637  79.25086718\n",
      "   57.70204434  87.25254155  71.26442024  97.04626004  71.69661943]\n",
      " [ 44.13195351  40.          72.33187288  50.50129212  40.\n",
      "   56.59437788  40.          54.06079527  73.65035551 100.        ]\n",
      " [ 55.56189194  58.78354754  75.81831248  49.84691887  40.\n",
      "   68.71894544  40.          67.93247844  55.08332829  66.85558723]\n",
      " [ 79.39050947  77.8348638   57.40642314  62.97805026  65.49739445\n",
      "   75.61810999  51.80730851 100.          68.24343406  73.27434582]\n",
      " [ 44.04052339  56.00373846  72.1116636  100.          82.01581284\n",
      "   87.76463323 100.          58.93975578  66.12688069  40.        ]\n",
      " [ 40.          66.68750557  77.2820855   65.0940996   90.25173273\n",
      "   98.24750319  89.98167272  60.50252724  40.          40.        ]\n",
      " [ 76.4293528   68.76636502  62.84063199  47.52222662  67.24221816\n",
      "   48.17845087  60.96693514 100.         100.          40.        ]\n",
      " [ 95.96306209  83.90955344  58.82028297  44.67105348  61.27462015\n",
      "   65.7127185   51.02938105  40.         100.          49.63064646]\n",
      " [ 98.09745855  52.48463161 100.          68.01907927 100.\n",
      "   60.21813959  53.73998321  89.76721371  96.06952885  68.50616617]\n",
      " [ 40.          80.03183332  68.50137366  82.77728805  50.86045969\n",
      "   83.56431139  65.81261179  64.3965796   40.          61.67629181]\n",
      " [ 40.          63.48715985  64.34964704  46.07842804 100.\n",
      "   80.22945289  60.25257722  93.17962412  46.98241007  49.02446714]\n",
      " [ 74.44717705  95.63669567  84.33989655  80.19290582  61.9663379\n",
      "   49.96026967  75.80448533  81.6504805   49.31956502  70.05760176]\n",
      " [ 87.54422282  72.46551101  64.39068319  77.05627524  79.28042153\n",
      "  100.          78.18804506  69.94050178  57.71777363  59.13832891]\n",
      " [ 44.49801338  40.          78.33230567  49.48974001  65.78953437\n",
      "  100.          94.25521783  56.5630999   96.25794353 100.        ]\n",
      " [ 44.6807331   40.          40.          99.61115897  40.\n",
      "  100.          95.48208099  40.          73.54523715  66.15840367]]\n",
      "\n",
      "Results Comparison:\n",
      "\n",
      "Iteration 0 (No Gaming):\n",
      "Mean grade: 73.53\n",
      "Faculty distribution: [28  9 18 23 22]\n",
      "Students who got desired faculty: 22 (22.0%)\n",
      "\n",
      "Iteration 1 (With Gaming):\n",
      "Mean grade: 73.52\n",
      "Faculty distribution: [24 10 12 26 28]\n",
      "Students who got desired faculty: 28 (28.0%)\n",
      "\n",
      "Detailed results for first 5 applicants:\n",
      "\n",
      "Applicant 0:\n",
      "Desired faculty: 2.0\n",
      "Iteration 0 faculty: 3\n",
      "Iteration 0 grade: 81.89\n",
      "Final faculty: 3\n",
      "Final grade: 81.41\n",
      "\n",
      "Applicant 1:\n",
      "Desired faculty: 2.0\n",
      "Iteration 0 faculty: 1\n",
      "Iteration 0 grade: 69.07\n",
      "Final faculty: 2\n",
      "Final grade: 73.56\n",
      "\n",
      "Applicant 2:\n",
      "Desired faculty: 4.0\n",
      "Iteration 0 faculty: 0\n",
      "Iteration 0 grade: 73.40\n",
      "Final faculty: 0\n",
      "Final grade: 73.52\n",
      "\n",
      "Applicant 3:\n",
      "Desired faculty: 1.0\n",
      "Iteration 0 faculty: 3\n",
      "Iteration 0 grade: 65.94\n",
      "Final faculty: 3\n",
      "Final grade: 58.65\n",
      "\n",
      "Applicant 4:\n",
      "Desired faculty: 2.0\n",
      "Iteration 0 faculty: 4\n",
      "Iteration 0 grade: 70.20\n",
      "Final faculty: 4\n",
      "Final grade: 76.02\n"
     ]
    }
   ],
   "source": [
    "run_multi_iteration_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs236781-hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
