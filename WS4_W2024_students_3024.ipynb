{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashalem/ML_Human/blob/main/WS4_W2024_students_3024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj4TYqYunijM"
      },
      "source": [
        "\n",
        "<div>Machine Learning and Human Behavior - 236667 - Winter 2024-2025</div>\n",
        "<font size=\"6\">Workshop #4 - Retraining Dynamics ðŸ¦”</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ9lfDp0oCLH"
      },
      "source": [
        "# Instructions and submission guidelines\n",
        "\n",
        "* Clone this notebook and complete the exercise:\n",
        "    * Aim for clear and concise solutions.\n",
        "    * Indicate clearly with a text block the sections of your solutions.\n",
        "    * Answer dry questions in markdown blocks, and wet questions in code blocks.\n",
        "* Submission guidelines:\n",
        "    * Add a text block in the beginning of your notebook with your IDs.\n",
        "    * When you're done, restart the notebook and make sure that everything runs smoothly (Runtime->\"Restart and Run All\")\n",
        "    * Export your notebook as ipynb (File->Download->\"Download .ipynb\")\n",
        "    * If you need to attach additional files to your submission (e.g images), add them to a zip file together with the notebook ipynb file.\n",
        "    * Submit through the course website. Remember to list partner IDs when you submit.\n",
        "* **Due date**: Sunday 19/01/2025, 10:00\n",
        "* For any questions regarding this workshop, contact [Eden](mailto:edens@campus.technion.ac.il)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeeN8kfPohnT"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this workshop, we will be using a simulation framework for evaluating different recommendation algorithms. The framework consists of 3 main components - **environments** and **recommenders**, coupled together in the **simulation**.\n",
        "\n",
        "Today's workshop has two parts:\n",
        "* **Part 1**: Guided exercises on retraining dynamics\n",
        "* **Part 2**: Exploratory exercise on recommending with a lookahead step\n",
        "\n",
        "\n",
        "We will make heavy use of agent-oriented simulation. This has lots of benefits, but also some peculiarities you should be aware of. A few tips:\n",
        "\n",
        "* **Code smartly and plan ahead**\n",
        "  * Some simulations take time (up to 1-2 min). Given the workshop's time constraints, try to reduce the number of unnecessary simulations you run.\n",
        "* **Expect noisy outcomes**\n",
        "  * In addition to computation time, some simulations are also sensitive to initial conditions and settings. This will be especially pronounced since we will use fairly small datasets, and since we won't be averaging over multiple runs (again, time constraints).\n",
        "  * As in all workshops so far, our goal is to study phenomena. Noisy results make this very tricky since we all have a tendancy to find signal in noise (and tell nice stories about it). Beware of the noise-as-signal fallacy!\n",
        "  * As a guidline, for any finding you have, try to understand if it's:\n",
        "   * true signal (i.e., the phenomena you're looking for)\n",
        "   * just noise (due to parameters, initialization, randomization, etc.)\n",
        "   * artifacts of the setup (e.g., running out of items to recommend)\n",
        "  * As always, be sure to control randomization by setting seeds.\n",
        "* **Exploratory section**\n",
        "  * This type of task may be new to you as we have not undertaken this sort of exercise before - but it's a good time to try it out, as it will prepare you well for the final projects (which will be similar in spirit, though more advanced).\n",
        "  * Try to reach Part 2 after roughly 1.5 hours. Make sure you reach it *well before the end of the workshop*, so that you can use us for guidance.\n",
        "\n",
        "**Good luck!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX0wtICNIqXu"
      },
      "source": [
        "## Notations\n",
        "\n",
        "As in HW4, we use the following notations:\n",
        "* Time is assumed to be discrete and denoted by $t\\in\\{0,1,\\dots\\}$\n",
        "* The set of users is denoted by $U$.\n",
        "* At each time $t$, the set of \"online users\" requesting recommendation is denoted by $U_t \\subseteq U$.\n",
        "* The set of all items is denoted by $X$.\n",
        "* Rating given by user $u\\in U$ to item $x \\in X$ at time $t$ is denoted by $r_t(u,x)\\in[1,5]$. User and item are not explicity specified when they are clear from context.\n",
        "* Predicted ratings are denoted by $\\hat{r}_t (u,x)$.\n",
        "* Recommendations at time $t$ are denoted by $\\{(u,x_u)\\}_{u\\in U_t}$.\n",
        "* The Average Rating of Recommended Items (*ARRI*) at time $t$ is defined as:\n",
        "$$\n",
        "\\mathrm{ARRI} =\n",
        "\\frac{1}{\\left|U_t\\right|}\n",
        "\\sum_{u\\in U_t}\n",
        "r_t (u, x_u)\n",
        "$$\n",
        "* Rating RMSE at time $t$ is defined as:\n",
        "$$\n",
        "\\mathrm{RMSE} =\n",
        "\\left(\n",
        "\\frac{1}{\\left|U_t\\right|}\n",
        "\\sum_{u\\in U_t}\n",
        "\\left(\\hat{r}_t (u, x_u) - r_t (u, x_u) \\right)^2\n",
        "\\right)^{0.5}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDd5vQRWIsKO"
      },
      "source": [
        "## Environment\n",
        "An *environment* defines the population of users and the collection of available items. It specifies the users' behavior, their preferences and the way they change over time, how the users rate items, etc. In particular, the environments we use here are **stateful**.\n",
        "\n",
        "In this workshop, environments expose the following interface:\n",
        "\n",
        "* `__init__(...)` - Initialize environment using given parameters\n",
        "* `get_initial_ratings()` - Returns a pandas DataFrame with initial ratings $\\{(u_i, x_i, r_i)\\}$. Useful for bootstrapping the recommendation algorithm and avoiding the \"[cold-start](https://en.wikipedia.org/wiki/Cold_start_(recommender_systems)\" problem.\n",
        "* `get_online_users()` - Returns the set of online users $U_t\\subseteq U$ that queried the system at the current time step $t$.\n",
        "* `recommend(recommendations)` - Receives as input a list of tuples $R_t = \\{ (u_i,x_i) \\mid u_i \\in U_t \\} $, where each $u_i \\in U_t$ is an online user, and $x_i$ is the item being recommended. Note that only unseen items can be recommended, and all online users must receive recommendations. The function returns the true ratings given by the users.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGZO-kFkIvGz"
      },
      "source": [
        "## Recommender\n",
        "\n",
        "A *recommender* generates item recommendations to users based on past ratings.\n",
        "\n",
        "In this workshop, recommendation algorithms use the [Surprise](https://surpriselib.com/) framework.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy-4eGaRIwJe"
      },
      "source": [
        "## Simulation\n",
        "The simulation works in discrete time steps $t\\in\\{1,2,\\dots\\}$. At each step $t$:\n",
        "* The environent is queried for the current set of online users: $$U_t\\subseteq U$$\n",
        "* Recommendations are selected based on predictions:\n",
        "$$ \\{(u,x_u)\\}_{u\\in U_t} $$\n",
        "* Explicit feedback (true ratings) is returned:\n",
        "$$\\{(u, x_u, r_t(u,x_u)\\}_{u\\in U_t}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install scikit-surprise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT3EhuJf-P9J"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "re3ZbC04CIei"
      },
      "outputs": [],
      "source": [
        "\n",
        "import itertools\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import surprise\n",
        "\n",
        "from tqdm.auto import tqdm "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjLclotwIxz5"
      },
      "source": [
        "# Homework recap\n",
        "\n",
        "In this task we, will get ourselves familiar with the framework by exploring a static recommendation environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmOqWaQdJpkQ"
      },
      "source": [
        "## The `TopicsStatic` environment\n",
        "\n",
        "(Introduced in HW4)\n",
        "\n",
        "The following environment is based on the `topics-static` environment introduced by [Krauth et. al. 2020](https://arxiv.org/abs/2011.07931).\n",
        "\n",
        "* In the `TopicsStatic` environment, each item is assigned to one of $K$ topics and users prefer certain topics.\n",
        "The preference of user $u$ for items of topic $k$\n",
        "is initialized as $\\pi(u, k) \\sim \\mathrm{Uniform}(0.5, 5.5)$, while the topic\n",
        "$k_x$ of item $x$ is chosen randomly from the set of all topics.\n",
        "\n",
        "* When user $u$ is recommended item $x $ of topic $k_x$, they will rate the item as:\n",
        "$$r_t(u,x) = \\mathrm{clip}_{[1,5]}\\left(\\pi(u, k_x) + \\epsilon\\right)$$\n",
        "where $\\epsilon \\sim N(0, \\sigma^2)$ represents exogenous noise not modeled by the simulation, and $\\mathrm{clip}_{[1,5]}$ truncates values to be between $1$ and $5$.\n",
        "\n",
        "\n",
        "Implementation of the `TopicsStatic` environment is given by the following class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8U3Lv9cMMRn7"
      },
      "outputs": [],
      "source": [
        "class TopicsStatic:\n",
        "    topic_affinity_params = dict(\n",
        "        low=0.5,\n",
        "        high=5.5,\n",
        "    )\n",
        "    decision_noise_params = dict(\n",
        "        scale=0.5,\n",
        "    )\n",
        "    rating_frequency = 0.2\n",
        "\n",
        "    def __init__(self, n_users, n_items, n_topics, n_initial_ratings, random_state=None):\n",
        "        \"\"\"\n",
        "        Initialize the environment.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_users : int\n",
        "            Number of users in the environment.\n",
        "\n",
        "        n_items : int\n",
        "            Number of items in the environment.\n",
        "\n",
        "        n_topics : int\n",
        "            Number of latent topics in the environment.\n",
        "\n",
        "        n_initial_ratings : int\n",
        "            Number of initial ratings available to the recommender before the\n",
        "            simulation starts.\n",
        "\n",
        "        random_state : int, default=None\n",
        "            Random seed to use, if none is specified, a seed provided by the\n",
        "            OS will be used.\n",
        "        \"\"\"\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_users\n",
        "        self.all_users = [f'usr_{i}' for i in range(self.n_users)]\n",
        "        self.all_items = [f'itm_{i}' for i in range(self.n_items)]\n",
        "        self.n_topics = n_topics\n",
        "        self.n_initial_ratings = n_initial_ratings\n",
        "        self.rng = np.random.default_rng(random_state)\n",
        "        # Assign topics to items\n",
        "        self._item_topics = self.rng.integers(\n",
        "            low=0,\n",
        "            high=n_topics,\n",
        "            size=n_items\n",
        "        )\n",
        "        # Initialize topics affinity matrix\n",
        "        self._topic_affinity = self.rng.uniform(\n",
        "            **self.topic_affinity_params,\n",
        "            size=(n_users, n_topics),\n",
        "        )\n",
        "        # Initialize environment state\n",
        "        self.t = 0\n",
        "        self.last_online_users = None\n",
        "        self.seen_items = {user_id: list() for user_id in self.all_users}\n",
        "        self.initial_ratings_shown = False\n",
        "\n",
        "    def _get_rating(self, user_id, item_id):\n",
        "        \"\"\"\n",
        "        Calculate rating r_t(user_id, item_id).\n",
        "        \"\"\"\n",
        "        assert item_id not in self.seen_items[user_id], (\n",
        "            'Each item can only be shown once to each user'\n",
        "        )\n",
        "        user_internal_id = self.all_users.index(user_id)\n",
        "        item_internal_id = self.all_items.index(item_id)\n",
        "        self.seen_items[user_id].append(item_id)\n",
        "        item_topic = self._item_topics[item_internal_id]\n",
        "        affinity = self._topic_affinity[user_internal_id, item_topic]\n",
        "        noise = self.rng.normal(**self.decision_noise_params)\n",
        "        rating = np.clip(affinity+noise, 1, 5)\n",
        "        return rating\n",
        "\n",
        "    def _get_latent_topic(self, item_id):\n",
        "        item_internal_id = self.all_items.index(item_id)\n",
        "        return self._item_topics[item_internal_id]\n",
        "\n",
        "    def get_initial_ratings(self):\n",
        "        \"\"\"\n",
        "        Get initial ratings, to be used when initializing the recommender.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ratings_df : pandas.DataFrame\n",
        "            DataFrame with columns: user_id, item_id, rating, timestamp.\n",
        "            Timestamp for initial data is set to 0.\n",
        "        \"\"\"\n",
        "        assert not self.initial_ratings_shown, (\n",
        "            'Initial ratings can only be calculated once',\n",
        "        )\n",
        "        all_pairs = list(itertools.product(\n",
        "            self.all_users,\n",
        "            self.all_items,\n",
        "        ))\n",
        "        selected_pairs = self.rng.choice(\n",
        "            a=all_pairs,\n",
        "            size=self.n_initial_ratings,\n",
        "            replace=False,\n",
        "        )\n",
        "        ratings = [\n",
        "            (user_id, item_id, self._get_rating(user_id, item_id))\n",
        "            for user_id, item_id in selected_pairs\n",
        "        ]\n",
        "        self.initial_ratings_shown = True\n",
        "        return pd.DataFrame(\n",
        "            data=ratings,\n",
        "            columns=('user_id','item_id','rating')\n",
        "        ).assign(timestamp=self.t)\n",
        "\n",
        "    def get_online_users(self):\n",
        "        \"\"\"\n",
        "        Returns the set of online users that queried the system at the\n",
        "        current time step.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        online_users: array of user_ids\n",
        "        \"\"\"\n",
        "        assert self.last_online_users is None, (\n",
        "            'Previous batch of online users must get recommendations'\n",
        "        )\n",
        "        n_online = self.rng.binomial(\n",
        "            n=self.n_users,\n",
        "            p=self.rating_frequency,\n",
        "        )\n",
        "        online_users = self.rng.choice(\n",
        "            a=self.all_users,\n",
        "            size=n_online,\n",
        "            replace=False,\n",
        "        )\n",
        "        self.last_online_users = set(online_users)\n",
        "        return online_users\n",
        "\n",
        "    def recommend(self, recommendations):\n",
        "        \"\"\"\n",
        "        Recommend items to users.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        recommendations : list of (user_id, item_id) tuples.\n",
        "            Each (user_id, item_id) tuple corresponds to an item recommended\n",
        "            to an online user. Note that only unseen items can be recommended,\n",
        "            and all online users must receive recommendations.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ratings_df : pandas.DataFrame\n",
        "            True ratings given by the users.\n",
        "            DataFrame with columns: user_id, item_id, rating, timestamp.\n",
        "            Timestamp for recommendations is >= 1.\n",
        "        \"\"\"\n",
        "        assert self.last_online_users is not None, (\n",
        "            'Online users must be selected by calling get_online_users()'\n",
        "        )\n",
        "        assert len(recommendations)==len(self.last_online_users), (\n",
        "            'Number of recommendations must match number of online users'\n",
        "        )\n",
        "        assert {user_id for user_id, _ in recommendations}==self.last_online_users, (\n",
        "            'Users given recommendations must match online users'\n",
        "        )\n",
        "        assert all(item_id not in self.seen_items[user_id] for user_id, item_id in recommendations), (\n",
        "            'Only unseen items can be recommended'\n",
        "        )\n",
        "        ratings = [\n",
        "            (user_id, item_id, self._get_rating(user_id, item_id))\n",
        "            for user_id, item_id in recommendations\n",
        "        ]\n",
        "        self.last_online_users = None\n",
        "        self.t += 1\n",
        "        return pd.DataFrame(\n",
        "            data=ratings,\n",
        "            columns=('user_id','item_id','rating')\n",
        "        ).assign(timestamp=self.t)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8dee1W2U0T8"
      },
      "source": [
        "## Surprise\n",
        "\n",
        "[Surprise](https://surpriselib.com/) is a Python library for building and analyzing recommender systems that deal with explicit rating data.\n",
        "\n",
        "In particular, Surprise provides various ready-to-use prediction algorithms such as baseline algorithms, neighborhood methods, matrix factorization-based, and many others.\n",
        "\n",
        "1. Go over the introduction to Surprise in its [main homepage](https://surpriselib.com/) (\"Overview\", \"Getting started\" sections).\n",
        "2. Go over the \"[Basic usage](https://surprise.readthedocs.io/en/stable/getting_started.html#basic-usage)\" section in the Surprise documentation.\n",
        "3. The `surprise` library is already available in this notebook. (Surprise! ðŸ¥³)\n",
        "\n",
        "The following function may be useful for converting a `pandas.DataFrame` to a `surprise.Trainset`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dHgDmBuVag2e"
      },
      "outputs": [],
      "source": [
        "def trainset_from_df(df):\n",
        "    \"\"\"\n",
        "    Convert DataFrame to Surprise training set.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame with columns [user_id, item_id, rating]\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    trainset : surprise.Trainset\n",
        "    \"\"\"\n",
        "    dataset = surprise.Dataset.load_from_df(\n",
        "        df=df[['user_id','item_id','rating']],\n",
        "        reader=surprise.Reader(rating_scale=(1,5)),\n",
        "    )\n",
        "    return dataset.build_full_trainset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jLYUeROsPnF"
      },
      "source": [
        "# Part 1: Retraining dynamics\n",
        "\n",
        "\n",
        "In this task we, will investigate a dynamic environment under retraining.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAd4zO87SxOe"
      },
      "source": [
        "## The `TopicsDynamic` environment\n",
        "\n",
        "In `TopicsDynamic`, items are rated in the same way as `TopicsStatic`. In this setting however, user preferences change as a result of content consumption.\n",
        "In particular, if item $x$ is recommended to user $u$ at time $t$, then their preferences are updated as:\n",
        "\n",
        "$$\n",
        "\\pi_{t+1}(u,k)\n",
        "= \\begin{cases}\n",
        "\\mathrm{clip}_{[0.5,5.5]}\\left(\\pi_t(u,k) + a\\right) & k = k_x \\\\\n",
        "\\mathrm{clip}_{[0.5,5.5]}\\left(\\pi_t(u,k) - \\frac{a}{K-1}\\right) & k \\neq k_x\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where $a \\ge 0$ is a fixed affinity parameter.\n",
        "\n",
        "Implementation of the `TopicsDynamic` environment is provided below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F4yTNBUDVXAs"
      },
      "outputs": [],
      "source": [
        "class TopicsDynamic(TopicsStatic):\n",
        "    def __init__(self, a, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.a = a\n",
        "\n",
        "    def recommend(self, recommendations):\n",
        "        # Recommend and get ratings\n",
        "        ratings_df = super().recommend(recommendations)\n",
        "        # Update affinity\n",
        "        for row in ratings_df.itertuples():\n",
        "            user_internal_id = self.all_users.index(row.user_id)\n",
        "            item_internal_id = self.all_items.index(row.item_id)\n",
        "\n",
        "            # increase affinity for selected item topic\n",
        "            selected_topic = self._item_topics[item_internal_id]\n",
        "            self._topic_affinity[user_internal_id, selected_topic] += self.a\n",
        "            # decrease affinity for other topics\n",
        "            not_selected = np.arange(self.n_topics) != selected_topic\n",
        "            self._topic_affinity[user_internal_id, not_selected] -= (\n",
        "                self.a / (self.n_topics-1)\n",
        "            )\n",
        "            # clip\n",
        "            self._topic_affinity[user_internal_id] = (\n",
        "                self._topic_affinity[user_internal_id].clip(min=0.5, max=5.5)\n",
        "            )\n",
        "        # Return ratings dataframe\n",
        "        return ratings_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smuVpkSu2uKJ"
      },
      "source": [
        "**Question**: Looking at the definition of the `TopicsDynamic` environment, what is the role of the parameter $a$? In your answer, relate to the following aspects:\n",
        "* What are the effects of setting $a=0$? What is the effect of setting $a$ to a high value (e.g $a=2$)?\n",
        "* What is the behavioral interpretation of the parameter $a$?\n",
        "\n",
        "ðŸ”µ **Solution**:\n",
        "\n",
        "1. When $a=0$, the environment is static and the user preferences do not change.\n",
        "2. When $a$ is high, the environment is highly dynamic and the user preferences change significantly.\n",
        "3. The parameter $a$ represents the rate of change in user preferences, meaning how much the user preferences change based on previous consumption.\n",
        "The higher the $a$ value, the more the user preferences items with the same topic as the previous item consumed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7bDsKNyWYGH"
      },
      "source": [
        "## Parameters\n",
        "\n",
        "For the `TopicsDynamic` simulations below, we will use the following parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hMoaFjFWfTl3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_users': 100,\n",
              " 'n_items': 1000,\n",
              " 'n_topics': 4,\n",
              " 'n_initial_ratings': 200,\n",
              " 'random_state': 1234}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simulation_random_state = 1234\n",
        "\n",
        "topics_dynamic_params = {\n",
        "    'n_users': 100,\n",
        "    'n_items': 1000,\n",
        "    'n_topics': 4,\n",
        "    'n_initial_ratings': 200,\n",
        "    'random_state': simulation_random_state,\n",
        "}\n",
        "topics_dynamic_params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PU7Lhyeg0I0"
      },
      "source": [
        "For recommendation, use `surprise.SVD` with the following parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_pwXagmnhHvJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_factors': 8, 'random_state': 1234}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svd_model_params = {\n",
        "    'n_factors': 8,\n",
        "    'random_state': simulation_random_state,\n",
        "}\n",
        "svd_model_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhH_AyB_OKgx"
      },
      "source": [
        "Hint: The parameters are specified in dictionaries for convenience. The statement `TopicsDynamic(**topics_dynamic_params)` initializes a `TopicsDynamic` environment with the given parameters. Similarly for `surprise.SVD(**svd_model_params)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuHKHHhTZj8z"
      },
      "source": [
        "## Exercise 1.1: Training once\n",
        "Since users preferences are bound to change over time, it may seem appealing to employ **retraining**.\n",
        "\n",
        "However, before we look at how retraining dynamics behave in the current environment, let's see what happens when we only train once at the start and fix the model from that point on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyDtx2CJT0Jk"
      },
      "source": [
        "Run simulations for three affinity values, $a \\in \\{0, 0.3, 1.2\\}$,\n",
        "to study:\n",
        "* a static environment ($a=0$)\n",
        "* a moderately dynamic environment (moderate $a$)\n",
        "* a highly dynamic environment (largest $a$)\n",
        "\n",
        "For each simulation, initiate a new environment with the given parameters,\n",
        "train the collaborative filtering model **once** at the beggining using the initial ratings (i.e using the output of `env.get_initial_ratings()`), and\n",
        "recommend the item with highest predicted rating.\n",
        "\n",
        "Run the simulation for $100$ timesteps.\n",
        "\n",
        "For each recommendation, record the:\n",
        "* Relevant environment parameters (i.e. the value of $a$)\n",
        "* Time $t$\n",
        "* User id receiving recommendation\n",
        "* Item id being recommended\n",
        "* Rating predicted by the system\n",
        "* Rating given by the user\n",
        "* Latent topic of the item (obtained via `env._get_latent_topic(item_id)`)\n",
        "\n",
        "**Notes**:\n",
        "* Start with short simulations to debug the code, then move to longer time horizons. The complete simulation shouldn't take more than a couple of minutes.\n",
        "* The argument to ``TopicsDynamic`` corresponding to $a$ is called `a` (see code above).\n",
        "* If you feel your results are noisy, run the experiments using different randomization seeds and plot average results.\n",
        "* pandas DataFrames are very convenient for storing and analyzing experiment results.\n",
        "\n",
        "ðŸ”µ **Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GqQpAFFkgLvg"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Length mismatch: Expected axis has 28 elements, new values have 7 elements",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m         env\u001b[38;5;241m.\u001b[39mrecommend(recommendations)\n\u001b[1;32m     28\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(recommendations_recording)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_rating\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactual_rating\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     30\u001b[0m df\n",
            "File \u001b[0;32m~/miniconda3/envs/cs236781-hw2/lib/python3.8/site-packages/pandas/core/generic.py:6002\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   6001\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[0;32m-> 6002\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6003\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m   6004\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/cs236781-hw2/lib/python3.8/site-packages/pandas/_libs/properties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/miniconda3/envs/cs236781-hw2/lib/python3.8/site-packages/pandas/core/generic.py:730\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    729\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
            "File \u001b[0;32m~/miniconda3/envs/cs236781-hw2/lib/python3.8/site-packages/pandas/core/internals/managers.py:225\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
            "File \u001b[0;32m~/miniconda3/envs/cs236781-hw2/lib/python3.8/site-packages/pandas/core/internals/base.py:70\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 28 elements, new values have 7 elements"
          ]
        }
      ],
      "source": [
        "a_values = [0, 0.3, 1.2]\n",
        "num_steps = 10\n",
        "\n",
        "## YOUR SOLUTION\n",
        "recommendations_recording = []\n",
        "for a in a_values:\n",
        "    env = TopicsDynamic(a=a, **topics_dynamic_params)\n",
        "    initial_ratings = env.get_initial_ratings()\n",
        "    all_items = env.all_items \n",
        "    algo = surprise.SVD(**svd_model_params)\n",
        "    algo.fit(trainset_from_df(initial_ratings))\n",
        "    for t in range(num_steps):\n",
        "        # Get online users for this timestep\n",
        "        online_users = env.get_online_users()\n",
        "        \n",
        "        # Get recommendations for each online user\n",
        "        predictions = [\n",
        "            max([algo.predict(user, item) for item in all_items if item not in env.seen_items[user]], key=lambda x: x.est)\n",
        "            for user in online_users\n",
        "        ]\n",
        "        recommendations = [(pred.uid, pred.iid) for pred in predictions]\n",
        "        recommendations_recording.append([\n",
        "            (a, t, user, item, pred.est, pred.r_ui, env._get_latent_topic(item))\n",
        "            for (user, item), pred in zip(recommendations, predictions)\n",
        "        ])\n",
        "        env.recommend(recommendations)\n",
        "        \n",
        "df = pd.DataFrame(recommendations_recording)\n",
        "df.columns = ['a', 't', 'user', 'item', 'predicted_rating', 'actual_rating', 'topic']\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clP8V0KMiw2d"
      },
      "source": [
        "Plot and compare two performance measures: ARRI and RMSE, as defined in the \"Technical Preliminaries\" section above, as a function of time $t$\n",
        "\n",
        "Create two plots, one for ARRI and one for RMSE. Each plot should contain three lines, corresponding to the different values of $a$.\n",
        "\n",
        "To make results less noisy, take a [moving average](https://en.wikipedia.org/wiki/Moving_average) of 10 time units. The pandas function [`rolling`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html) is useful for calculating the moving average (e.g using `df.rolling(10).mean()`).\n",
        "\n",
        "ðŸ”µ **Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waLBcZ8PqCLW"
      },
      "outputs": [],
      "source": [
        "averaging_window_size = 10\n",
        "\n",
        "## YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6im8R_X4pBkW"
      },
      "source": [
        "Explain the results. What do you observe in the graphs? In your answer, relate to:\n",
        "* Overall trends in the plots.\n",
        "* Are there values of $a$ for which RMSE/ARRI is significantly higher/lower? If so, why?\n",
        "\n",
        "ðŸ”µ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwbCtDByyk48"
      },
      "source": [
        "## Exercise 1.2: Retraining\n",
        "\n",
        "Next, we will explore what happens when the recommender is retrained after, and compare this to the single training setting above.\n",
        "\n",
        "Repeat the experiment described in the previous exercise, but this time, retrain the prediction model using all available predictions gathered from the environment so far (i.e initial ratings returned from `get_initial_ratings` + all ratings returned by the `recommend` method until the current time $t$).\n",
        "\n",
        "Simulate two variations:\n",
        "* **Retraining once**: Retrain the model once at the middle of the simulation (after $50$ simulation steps) using all available data, including initial observations.\n",
        "* **Retraining every step**: Retrain the model after each simulation step using all data collected so far, including initial observations.\n",
        "\n",
        "Use the simulation logic from Exercise 1.1.\n",
        "\n",
        "For each variation, plot the RMSE and ARRI graphs for all three experimental conditions (the different values of $a$). Show all three of the RMSE lines the same plot, and all three of the ARRI lines on the same plot. Use rolling average with `window_size=10` to smooth results.\n",
        "\n",
        "For the \"retraining once\" plots, mark the retraining point with the following command: `ax.axvline(retrain_after,color='tab:gray',linestyle=':',zorder=-1)`\n",
        "\n",
        "\n",
        "ðŸ”µ **Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBHUdac5ze2j"
      },
      "outputs": [],
      "source": [
        "retrain_after = num_steps//2\n",
        "\n",
        "## YOUR SOLUTION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7b3i_eAdBEc"
      },
      "source": [
        "Interpret your results.\n",
        "\n",
        "What do you observe in the graphs?\n",
        "\n",
        "ðŸ”µ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM7So3ntyKuv"
      },
      "source": [
        "## Exercise 1.3: Aggregate comparison\n",
        "Analyze your results from the previous exercises.\n",
        "\n",
        "For a given simulation run, the average ARRI is defined as:\n",
        "\n",
        "$$\n",
        "\\bar{\\mathrm{ARRI}} =\n",
        "\\frac{\n",
        "    \\sum_{t=1}^T\n",
        "    \\sum_{u\\in U_t}\n",
        "    r_t (u, x_u)\n",
        "}{\n",
        "    \\sum_{t=1}^T |U_t|\n",
        "}\n",
        "$$\n",
        "\n",
        "and the average RMSE is defined as:\n",
        "\n",
        "$$\n",
        "\\bar{\\mathrm{RMSE}} = \\left(\n",
        "\\frac{\n",
        "    \\sum_{t=1}^T\n",
        "    \\sum_{u\\in U_t}\n",
        "    \\left(\\hat{r}_t (u, x_u) - r_t (u, x_u) \\right)^2\n",
        "}{\n",
        "    \\sum_{t=1}^T |U_t|\n",
        "}\n",
        "\\right)^{0.5}\n",
        "$$\n",
        "\n",
        "\n",
        "Create a bar plot that compares retraining vs. not retraining in terms of **RMSE** for each $a$ value and experiment variation. Is there a general trend? Explain.\n",
        "\n",
        "ðŸ”µ **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgcY6qnmymEH"
      },
      "outputs": [],
      "source": [
        "## YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhqIxaoD4H-Z"
      },
      "source": [
        "ðŸ”µ **Explain your results**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-KQSm_h-wWa"
      },
      "source": [
        "Now, create a plot that compares training vs. not training in terms of **ARRI** for each $a$ value (use your results from exercises 1.1.1 and 1.1.2). Is there a general trend? Explain.\n",
        "\n",
        "ðŸ”µ **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp5LOrEF-14i"
      },
      "outputs": [],
      "source": [
        "## YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iAMEpoa-2TK"
      },
      "source": [
        "ðŸ”µ **Explain your results**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIMsWuayNqX0"
      },
      "source": [
        "## Exercise 1.4: Analyzing diversity\n",
        "\n",
        "For a user $u$ being shown $n_u$ items throughout the simulation run, denote by $k_i\\in[K]$ the topic of the item shown to the user on their $i$-th interaction with the system.\n",
        "\n",
        "The *empirical topic distribution* for user $u$ is the [empirical distribution](https://en.wikipedia.org/wiki/Empirical_distribution_function) of topics shown to $u$ throughout the simulation run:\n",
        "\n",
        "$$\n",
        "p_u(k)\n",
        "= \\frac{\\sum_{i=1}^{n_u} \\mathbb{I}[k_i=k]}{n_u}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JYLM39DI4h8"
      },
      "source": [
        "\n",
        "### 1.4.1: Entropy\n",
        "\n",
        "The [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) of a discrete random variable defined as:\n",
        "\n",
        "$$\n",
        "H(p) = -\\sum_k p(k) \\log_2 p(k)\n",
        "$$\n",
        "\n",
        "The entropy is a property of the random variable given by $p$:\n",
        "* Entropy is maximized when the probability is uniform (i.e. each query of the random variable returns a value with uniform probability over outcomes)\n",
        "* Entropy minimized when the random variable is a singleton (i.e. each query returns the same value)\n",
        "\n",
        "\n",
        "Implement the function `shannon_entropy(lst)` which receives a list of integers and returns the Shannon entropy of their empirical distribution.\n",
        "\n",
        "ðŸ”µ **Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53kXRUCUId4Q"
      },
      "outputs": [],
      "source": [
        "## YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVpGdKkQIRx3"
      },
      "source": [
        "\n",
        "\n",
        "For each variation of the experiment (retraining: none/once/every step, different values of $a$), calculate the topic entropy for each user using the formula above. Create a bar plot comparing the average entropy for each variation.\n",
        "\n",
        "ðŸ”µ **Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kopUATA8Nz8k"
      },
      "outputs": [],
      "source": [
        "## YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcDsU_8YX5cp"
      },
      "source": [
        "Explain your results:\n",
        "\n",
        "ðŸ”µ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI8KUL13ZBca"
      },
      "source": [
        "### 1.4.2: Trade-off?\n",
        "\n",
        "Is there a trade-off between diversity and item rating?\n",
        "\n",
        "Create a scatter plot to visualize the relation between average topic entropy and average item rating for each experiment variation. Make sure that each point in the scatterplot is annotated appropriately using text or shape/color. You can annotate points using the matplotlib [`ax.annotate(...)`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.annotate.html) function.\n",
        "\n",
        "ðŸ”µ **Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdUsgj3YaMmu"
      },
      "outputs": [],
      "source": [
        "## YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmhAGuS1bCbG"
      },
      "source": [
        "Explain your results. Is there a trade-off? What do you conclude about the relation between diversity and average rating?\n",
        "\n",
        "ðŸ”µ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl60rEpudVwN"
      },
      "source": [
        "## Exercise 1.5: Thinking about experimentation\n",
        "Above, we experimented with an ideal setting where we could compare different retraining scenarios side by side. From this, we were able to draw conclusions about the effects of retraining.\n",
        "In the real-world, however, we can only choose one retraining policy at any given time, and any evaluation method has caveats.\n",
        "\n",
        "Let's assume a slightly more realistic setting where you only have access to the data from the \"retrain once\" simulation in Exercise 1.2, and your goal is to measure the change in the ARRI metric as a result of retraining.\n",
        "\n",
        "Two initial solutions have been proposed:\n",
        "1. Measure ARRI at the step before retraining, and compare to the ARRI at the step after retraining.\n",
        "2. Measure ARRI 10 steps before retraining, and compare to the average ARRI 10 steps after retraining.\n",
        "\n",
        "Given these proposed solutions, answer the following questions:\n",
        "\n",
        "\n",
        "What are the the advantages and disadvantages of each approach?\n",
        "\n",
        "ðŸ”µ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWK6nNanGyEV"
      },
      "source": [
        "\n",
        "Are there limitations which are common to both approaches?\n",
        "\n",
        "ðŸ”µ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E45tcmAWGxGs"
      },
      "source": [
        "\n",
        "How would you measure the change in ARRI as a result of retraining? What are the limitations of your approach?\n",
        "\n",
        "ðŸ”µ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEoKUc0MT1tg"
      },
      "source": [
        "# Part 2: Fighting feedback using one-step â€œlookaheadâ€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_xkQAfVUe81"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This section is **exploratory** - you are encouraged to think, experiment, and try out different approaches. Concentrate on thinking things through and proposing a sound idea (worry more about what you propose, and less about whether it works exactly as expected or not). **Be creative!**\n",
        "\n",
        "As we've seen so far in this workshop, recommendation is a dynamic process that is prone to feedback effects, due both to the actions of the system (e.g., retraining) and the behavioral tendencies of users (e.g., preference change with exposure, susceptibility to social signals). Feedback can sometimes help, but at other times - can promote undesired outcomes, either in terms of performance (predictive or recommendation qulity) or other important aspects (homogenization, echo chambers, filter bubbles, etc.).\n",
        "\n",
        "Ideally, learning to recommend should be done in a way that takes feedback into account and optimizes various criteria for the *long term*. However, this is an exceptionally challenging task. One milder alternative is to instead use a greedy approach and recommend in a way that considers the effects of recommendation only **one time-step ahead**.\n",
        "\n",
        "The idea here is for the system--*before* it recommends--to simulate how users are likely to act in response to recommendations. By \"looking ahead\" one step into the future, and by anticipating user responses, the system can in principal be smarter about what it plans to recommend, ideally in a way that compensates for any possible negative affects.\n",
        "\n",
        "In reallity, the system must \"guess\" how users will act in the next timestep. In this exercise, we will make life easier for you, and allow you to use the simulation environment itself to simulate what will happen next. This will focus the challenge on planning ahead correctly (rather than on estimating user responses). Technically, do this using ``copy.deepcopy(env)`` - this copies the environment and lets you perform a simulated step on it, without affecting the natural flow of the simulation itself.\n",
        "\n",
        "Restrictions: In this exercise, we assume that the environment can be cloned, but in practical scenarios we don't have complete knowledge about the environment, and therefore the structure of the environment can only be estimated, and at relatively high cost. Therefore, in your solution you should assume that the environment can only be cloned once (or a few times) for every time step. In addition, you may only interact with the environment through its public interface (e.g. `env.recommend`), and may not assume knowledge of the environment's latent parameters  (e.g. you are not allowed to use `env._get_latent_topic`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDv56JFSeoos"
      },
      "source": [
        "\n",
        "Example for using `copy.deepcopy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vRmFodwdnBO"
      },
      "outputs": [],
      "source": [
        "# Initialize environment\n",
        "example_env = TopicsDynamic(a=0.5, **topics_dynamic_params)\n",
        "\n",
        "# Copy it using deepcopy\n",
        "import copy\n",
        "example_env_copy = copy.deepcopy(example_env)\n",
        "\n",
        "# Get a random list of online users from the original and the copy\n",
        "example_online_users = example_env.get_online_users()\n",
        "example_online_users_copy = example_env_copy.get_online_users()\n",
        "\n",
        "# Lists of online users should be identical\n",
        "assert (example_online_users==example_online_users_copy).all()\n",
        "\n",
        "# You are allowed to use example_env_copy.recommend(...) to probe how\n",
        "# users will react, and use this to inform your recommendation policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tJEnTpFubnn"
      },
      "source": [
        "## Step 2.1: Choose your problem\n",
        "\n",
        "**Plan ahead!** Begin by deciding what *negative effect* of prediction-only recommendation you wish to combat using a lookahead strategy. This could be one of the effects we've discussed above (predictive performance, ratings/welfare, homogenization/diversity, popularity/fat-tails, echo-chambers), mentioned in class (filter bubbles, polarization), or something else (ask us!).\n",
        "\n",
        "Then, define an environment and recommendation policy that you feel are appropriate for working with the effect you've chosen. Think carefully about what properties these should have for you to (i) observe the effect, and (ii) be able to mitigate it.\n",
        "\n",
        "Finally, determine how you will need to extend the existing simulation your experimental setup. For example, you may need to control user preferences, update stored data, expose latent features, or (re)train the model in certain ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O_OZp2t4hfT"
      },
      "source": [
        "ðŸ”µ **Write these down**:\n",
        "\n",
        "* **Chosen effect**: [answer]\n",
        "\n",
        "* **Environment**: [answer]\n",
        "\n",
        "* **Recommendation policy**: [answer]\n",
        "\n",
        "* **Simulation**: [answer]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlMA4iBT4hiT"
      },
      "source": [
        "ðŸ”µ **Briefly explain your choices**:\n",
        "\n",
        "[answer]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Uk_UNWO4r7A"
      },
      "source": [
        "## Step 2.2: State your lookahead strategy\n",
        "\n",
        "Imagine the system has just received a new batch of data, retrained on it, and is about to send recommendations to its users. What is \"good\" about these recomendations, and what can be \"bad\"? If the system had the ability to anticipate what would happen *after* it recommends - would it change its recommendations, and how?\n",
        "\n",
        "Decide on a lookahead strategy - briefly explain:\n",
        "* What do you plan to look ahead for?\n",
        "* How will you do this? (remember you can \"cheat\" by using a copy of the simulation environment)\n",
        "* What will you be looking for and what you will measure?\n",
        "* How will you use this to change your recommendations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdxOPpms5xHE"
      },
      "source": [
        "ðŸ”µ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0SATqLTe2cj"
      },
      "source": [
        "## Step 2.3: Research question\n",
        "\n",
        "Phrase your intentions as a **research question**.\n",
        "\n",
        "Ideally, a research question is **concise**, **testable**, and has a **simple answer** that we still don't know.\n",
        "\n",
        "Your research question should refer to the \"problem variables\" that you modify, and the \"result\" you expect to measure.\n",
        "\n",
        "ðŸ”µ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kg5A7qS6d4K"
      },
      "source": [
        "## Step 2.4: Implement and analyze\n",
        "Create a new simulation flow (similar to the simulation flow in Part 1), which simulates a recommendation system and its affect on the environment.\n",
        "Implement within this function your lookahead step.\n",
        "\n",
        "Run an experiment that compares a your lookahead approach to a myopic baseline (that does not lookahead). Think carefully about how to set up the environment and dynamics, what to measure, and how.\n",
        "\n",
        "Plot your results, and discuss them. You are encouraged to use multiple code cells to separate between the different stages of simulation and analysis (possibly with Markdown cells between them expaining the process).\n",
        "\n",
        "ðŸ”µ **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZt8s5wjv4Ba"
      },
      "outputs": [],
      "source": [
        "## YOUR SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPnToPt57Su6"
      },
      "source": [
        "ðŸ”µ **Discussion**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3RSX5D17YXn"
      },
      "source": [
        "Finally, describe briefly the challenges you expect to encounter if your lookahead approach would be used in a realistic setting (in which you would not be able to a simulator to correctly anticipate the outcomes of recommending at the next step).\n",
        "\n",
        "ðŸ”µ **Answer**:\n",
        "\n",
        "(YOUR SOLUTION)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tGZO-kFkIvGz",
        "Wy-4eGaRIwJe"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
