{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashalem/ML_Human/blob/main/WS3_2024_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>Machine Learning and Human Behavior - 236667 - Winter 2024-2025</div>\n",
        "<font size=\"6\">Workshop #3 - Biased inputs, biased outputs ðŸ¦¡</font>"
      ],
      "metadata": {
        "id": "btQtvZxxgZKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions and submission guidelines\n",
        "\n",
        "* Clone this notebook and complete the exercise:\n",
        "    * Aim for clear and concise solutions.\n",
        "    * Indicate clearly with a text block the sections of your solutions.\n",
        "    * Answer dry questions in text (markdown) blocks, and wet questions in code blocks.\n",
        "* Submission guidelines:\n",
        "    * When you're done, restart the notebook, and make sure that everything runs smoothly (Runtime->\"Restart and Run All\")\n",
        "    * Add a text block in the beginning of your notebook with your IDs.\n",
        "    * Export your notebook as ipynb (File->Download->\"Download .ipynb\")\n",
        "    * If you need to attach additional files to your submission (e.g images), add them to a zip file together with the notebook ipynb file.\n",
        "    * Submit through the course website. Remember to list partner IDs when you submit.\n",
        "* **Due date**: Sunday 5/1/2025, 10:00\n",
        "* For any questions regarding this workshop task, contact [Lotan](mailto:lotan.amit@campus.technion.ac.il).\n"
      ],
      "metadata": {
        "id": "iG2scxOIXpjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries\n",
        "Run these cells to load into memory interface objects and functions that will be used throughtout today's workshop.\n",
        "\n",
        "No need to read the actual code."
      ],
      "metadata": {
        "id": "vjUh-k9NvYwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterable, Set, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import ndcg_score, precision_score, recall_score, mean_squared_error\n",
        "from scipy.stats import bernoulli, binom, norm\n",
        "import uuid\n",
        "from typing import List, Tuple\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "CLICKS_NOISE_SCALE = 5\n",
        "RATINGS_NOISE_SCALE = 2\n",
        "LATENT_DIMENSION = 30"
      ],
      "metadata": {
        "id": "MY7jN43R7qlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RateClickEnvironment\n",
        "\n",
        "class RateClickEnvironment(object):\n",
        "  FEATURE_COLUMNS = [f'x{i+1}' for i in range(8)]\n",
        "\n",
        "  def __init__(self, thresh_click=2, eps_click=0.4,\n",
        "               eps_rating=1, user_mu=0, user_sigma=1):\n",
        "    self.thresh_click = thresh_click\n",
        "    self.eps_click = eps_click\n",
        "    self.eps_rating = eps_rating\n",
        "    self._latent_dim = len(self.FEATURE_COLUMNS)\n",
        "    self.top_k_idxs = np.arange(self._latent_dim)<5\n",
        "\n",
        "    self._user_features = np.abs(np.random.normal(user_mu, user_sigma, self._latent_dim))# np.abs(np.random.randn(self._latent_dim))\n",
        "    self._user_features /= np.linalg.norm(self._user_features)\n",
        "    self._user_features_topk = self._user_features*self.top_k_idxs\n",
        "\n",
        "    self.ndcg_10 = lambda y_true, y_est: ndcg_score(\n",
        "    np.expand_dims(y_true, 0),\n",
        "    np.expand_dims(y_est, 0),\n",
        "    k=10,\n",
        "    )\n",
        "\n",
        "  def _generate_dataframe(self, n_samples, seed=None) -> pd.DataFrame:\n",
        "    if seed is not None:\n",
        "      np.random.seed(seed)\n",
        "      print('in')\n",
        "    return pd.DataFrame(\n",
        "        index=pd.Index(\n",
        "            data=[str(uuid.uuid4()) for _ in range(n_samples)],\n",
        "            name='item_id',\n",
        "        ),\n",
        "        data={\n",
        "            feature_name: np.random.randn(n_samples)\n",
        "            for feature_name in self.FEATURE_COLUMNS\n",
        "        }\n",
        "    )\n",
        "\n",
        "  @staticmethod\n",
        "  def _click_transform(x):\n",
        "    return x>2\n",
        "\n",
        "  @staticmethod\n",
        "  def _rating_transform(x):\n",
        "    sigmoid = lambda x: 1/(1+np.exp(-x))\n",
        "    return np.round((sigmoid(x*1.5)-0.5)*4 + 3).astype(int)\n",
        "\n",
        "  def generate_evaluation_set(self, n_samples: int, seed: int=None) -> pd.DataFrame:\n",
        "    df = self._generate_dataframe(n_samples, seed)\n",
        "    X = df[self.FEATURE_COLUMNS].to_numpy()\n",
        "    scores = X@self._user_features\n",
        "    y = self._rating_transform(scores)\n",
        "    return df.assign(true_relevance=y)\n",
        "\n",
        "  def generate_training_set(self, n_samples: int, seed: int=None):\n",
        "    df = self._generate_dataframe(n_samples, seed)\n",
        "    X = df[self.FEATURE_COLUMNS].to_numpy()\n",
        "    for _ in range(1000):  # rejection sampling\n",
        "      noisy_scores = X@self._user_features_topk + self.eps_click*np.random.logistic(size=X.shape[0])\n",
        "      y_click = pd.array(\n",
        "          (noisy_scores > self.thresh_click),\n",
        "          dtype=pd.Int64Dtype()\n",
        "      )\n",
        "      if len(y_click.unique())>1 and y_click.value_counts().min()>1:\n",
        "        break\n",
        "    for _ in range(1000):  # rejection sampling\n",
        "      noisy_scores = X@self._user_features + self.eps_rating*np.random.logistic(size=X.shape[0])\n",
        "      y_rating = pd.array(\n",
        "          self._rating_transform(noisy_scores),\n",
        "          dtype=pd.Int64Dtype(),\n",
        "      )\n",
        "      y_rating[y_click==0] = None\n",
        "      if len(y_rating.dropna().unique())>1:\n",
        "        break\n",
        "\n",
        "    return df.assign(click=y_click, rating=y_rating)\n",
        "\n",
        "  def build_X_y_matrices(\n",
        "    self,\n",
        "    df: pd.DataFrame,\n",
        "    label_column: str,\n",
        "    feature_columns: List[str]=FEATURE_COLUMNS,\n",
        "    get_valid_entries:bool=False ) -> Tuple[np.ndarray, np.array]:\n",
        "    valid_entries = pd.notna(df[label_column])\n",
        "    X = df.loc[valid_entries][feature_columns].to_numpy()\n",
        "    y = df.loc[valid_entries][label_column].to_numpy()\n",
        "    if get_valid_entries:\n",
        "      return X,y,valid_entries\n",
        "    else:\n",
        "      return X,y\n",
        "\n",
        "def generate_population(utype_1='ranting', user_mu_1=-1, user_sigma_1=0.5,\n",
        "                        thresh_click_1=1, ratio=4, utype_2='extreme',\n",
        "                        user_mu_2=2, user_sigma_2=2, thresh_click_2=2\n",
        "                        ):\n",
        "# utype_1='ranting'\n",
        "# user_mu_1=-1\n",
        "# user_sigma_1=0.5\n",
        "# thresh_click_1=1\n",
        "# ratio=4\n",
        "# utype_2='extreme'\n",
        "# user_mu_2=2\n",
        "# user_sigma_2=2\n",
        "# thresh_click_2=2.5\n",
        "\n",
        "\n",
        "  train_size_ma = 10000 * 3\n",
        "  train_size_min = 10000\n",
        "\n",
        "  rc_env_rant = RateClickEnvironment(user_mu=user_mu_1, user_sigma=user_sigma_1,\n",
        "                                    thresh_click=thresh_click_1)\n",
        "  train_df = rc_env_rant.generate_training_set(train_size_ma)\n",
        "  items_train_rant, ratings_train_rant = rc_env_rant.build_X_y_matrices(train_df, 'rating')\n",
        "  test_df = rc_env_rant.generate_evaluation_set(500)\n",
        "  items_test_rant, true_relevance_rant = rc_env_rant.build_X_y_matrices(test_df, 'true_relevance')\n",
        "  ratings_train_rant, items_train_rant = selective_ratings(ratings_train_rant,\n",
        "                                                          utype_1,items_train_rant)\n",
        "\n",
        "  # extreme reporters\n",
        "  rc_env_over = RateClickEnvironment(user_mu=user_mu_2, user_sigma=user_sigma_2,\n",
        "                                    thresh_click=thresh_click_2)\n",
        "  train_df = rc_env_over.generate_training_set(train_size_min)\n",
        "  items_train, ratings_train = rc_env_over.build_X_y_matrices(train_df, 'rating')\n",
        "  test_df = rc_env_over.generate_evaluation_set(500)\n",
        "\n",
        "  items_test_over, true_relevance_over = rc_env_over.build_X_y_matrices(test_df, 'true_relevance')\n",
        "  ratings_train_over, items_train_over = selective_ratings(ratings_train,\n",
        "                                                          utype_2,items_train)\n",
        "\n",
        "  # merge:\n",
        "  rant_df = pd.DataFrame(data=np.hstack([items_train_rant,\n",
        "                                      np.expand_dims(ratings_train_rant,1)]),\n",
        "                      columns=[f'x_{i}' for i in range(items_train_rant.shape[1])] + ['rating'])\n",
        "\n",
        "  over_df = pd.DataFrame(data=np.hstack([items_train_over,\n",
        "                                        np.expand_dims(ratings_train_over,1)]),\n",
        "                        columns=[f'x_{i}' for i in range(items_train_over.shape[1])] + ['rating'])\n",
        "\n",
        "  train_df = pd.concat([rant_df, over_df])\n",
        "  train_df['user_type'] = np.hstack([np.ones(ratings_train_rant.shape),\n",
        "                                    np.zeros(ratings_train_over.shape)])\n",
        "\n",
        "  rant_df = pd.DataFrame(data=np.hstack([items_test_rant,\n",
        "                                        np.expand_dims(true_relevance_rant,1)]),\n",
        "                        columns=[f'x_{i}' for i in range(items_train_rant.shape[1])] + ['true_relevance'])\n",
        "\n",
        "  over_df = pd.DataFrame(data=np.hstack([items_test_over,\n",
        "                                        np.expand_dims(true_relevance_over,1)]),\n",
        "                        columns=[f'x_{i}' for i in range(items_test_over.shape[1])] + ['true_relevance'])\n",
        "\n",
        "  test_df = pd.concat([rant_df, over_df])\n",
        "  test_df['user_type'] = np.hstack([np.ones(true_relevance_rant.shape), np.zeros(true_relevance_over.shape)])\n",
        "\n",
        "  return train_df, test_df\n"
      ],
      "metadata": {
        "id": "mFnQRPRUr8Co",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Predictions through the funnel"
      ],
      "metadata": {
        "id": "cOH5FF6cggSl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vlvDYcn1XAm"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this exercise, we will simulate and analyze a hypothetical recommender system. For simplicty, our system will have a single user, and the goal of the system is to provide her with relevant content.\n",
        "The user is described by a (latent) vector $u\\in \\mathbb{R}^d$, which is *unknown* to the system. For a given item $i$ with features $x_i \\in \\mathbb{R}^d$ (which are *known* to the system), relevance to the user is an integer $y \\in \\{1,\\dots,5\\}$, given by the following formula:\n",
        "$$\n",
        "y=\\texttt{round}(\\sigma(u^\\top x_i)-0.5) \\quad \\text{where} \\quad\n",
        "\\sigma(a)=1+\\frac{4}{1+e^{-1.5a}}\n",
        "$$\n",
        "Intuitively, the formula computes an inner product, \"squashes\" the result into $[1,5]$ using a scaled sigmoid function, and rounds.\n",
        "\n",
        "To make its recommendations, **the system is given access to two types of data**, generated by the user as a response to past recommendations: clicks, and (possibly) ratings.\n",
        "\n",
        "Given a training set of previously-recommended items, for each of these items, the system observes:\n",
        "  1. A binary *click* label $c \\in \\{0,1\\}$. The user clicks on a recommended item only if its *perceived* relevance $\\tilde{y}$ is higher than some threshold `thresh_click`. Perceived relevance differs from true relevance in two ways:\n",
        "    1. It is based only on partial information (e.g think of how search results include only a short summary of an item). In our case, the user only gets to observe some of the features.\n",
        "    2. It includes additive noise `eps_click` (this is applied to the inner product $u^\\top x_i$).\n",
        "  2. An integer *rating* label $r \\in \\{1,\\dots,5\\}$. Ratings are based on items relevancies, and are reported *only after items have been consumed*. This means that:\n",
        "    1. They are based on the entire feature vector (i.e., consuming an items means the user is exposed to the entire feature vector).\n",
        "    2. They are also affected by additive noise `eps_rating`.\n",
        "    3. Importantly, they are observed **only for items that have been clicked**.\n",
        "\n",
        "None of the parameters `thresh_click`, `eps_click`, and `eps_rating` are known to the system.\n",
        "\n",
        "In this exercise, we will explore how the interaction between these two types of user feedback affect learning and recommendation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  `RateClickEnvironment` Interface\n",
        "\n",
        "\n",
        "`RateClickEnvironment` is an interface which generates synthetic datasets of clicks and ratings according to the paradigm above. The constructor includes three arguments: `thresh_click`, `eps_click`, and `eps_rating`, as described above. Calling `RateClickEnvironment()` results in default arguments.\n",
        "\n",
        "Additionally, it has two public instance methods that we're going to use:\n",
        "\n",
        "* `generate_evaluation_set(n_samples)` - Generate evaluation set of the given size. This function returns a `pandas.DataFrame` object. The dataframe is indexed by `item_id`, which is an arbitrary random string assigned to each item. The dataframe has columns for each of the $d$ features, and additional `true_relevance` column that stores the true (un-noised) relevancies of all items.\n",
        "\n",
        "* `generate_training_set(n_samples)` - Generate a training set of the given size. Similarly, this function returns a `pandas.DataFrame` object with the same type of index, and with feature columns. The dataframe does not include true relevancies; instead it includes two additional columns:\n",
        " * The column `click` is an indicator of whether the user clicked an item ($c_{i}\\in\\{0,1\\}$ as in the definitions above).\n",
        " * The column `rating` contains the item rating ($r_{i}\\in\\{1,\\dots,5\\}$ in the definitions above), or `N/A` if the rating is not observed.\n"
      ],
      "metadata": {
        "id": "qDTp6mIIrtFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Exploratory analysis: Warmup\n",
        "\n",
        "`rc_env` is an instance of the `RateClickEnvironment` class:\n"
      ],
      "metadata": {
        "id": "p-V2ZDbT3iiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rc_env = RateClickEnvironment()"
      ],
      "metadata": {
        "id": "6ouPSZ_F4LfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the `rc_env.generate_training_set` method to generate a training set of size 1000 and print the first 10 rows. Note that values of `rating` are only observed for items that were clicked."
      ],
      "metadata": {
        "id": "9BQZf5u3s5_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "_cTu8P19tq3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the proportion of items that were clicked in the training set (result should be a number between 0 and 1):\n",
        "\n",
        "(try to do this in one line of pandas code!)"
      ],
      "metadata": {
        "id": "St4bkPen6v5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "36qpGiub67se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot a bar chart showing the number of times each rating value (1,2,...,5) was observed (histogram). Use `df.value_counts()`.\n",
        "\n",
        "(try to do this in one line of pandas code!)"
      ],
      "metadata": {
        "id": "HCzHw7517Xnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "NqPuAcWQ7vR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the `rc_env.generate_evaluation_set` method with input 1000 to generate an evaluation set with 1000 samples, print the first 5 rows.\n"
      ],
      "metadata": {
        "id": "zHIVkGrksefM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "UxKQ2VP3shIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot an histogram of true (un-noised) values. Use `df.value_counts()`.\n"
      ],
      "metadata": {
        "id": "aKNqqsAq4_Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "ObULNNwjtCRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How is this histogram different than the one plotted for the training set ratings? Why is that? Explain your answer."
      ],
      "metadata": {
        "id": "WjmySM0G8qNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Answer:...\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "r4ilLMia37w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Basic model training\n",
        "\n",
        "The utility method `rc_env.build_X_y_matrices`\n",
        "extracts training matrices and labels from a given dataset DataFrame.\n",
        "Entries for which the labels are missing (`N/A`) are removed from the dataset.\n",
        "\n",
        "Arguments:\n",
        "- `label_column (str)`: Name of column to be used as label (e.g 'click')\n",
        "- `feature_columns (List[str])`: Columns to be used as features\n",
        "\n"
      ],
      "metadata": {
        "id": "KrJMDigCavQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `rc_env.build_X_y_matrices` together with previous methods to:\n",
        "* Create a train set of size 1000 that includes ratings as labels.\n",
        "* Train a linear regression model (use scikit-learn).\n",
        "* Create an evaluation set of size 500 (that includes true relevancies $y$).\n",
        "* Use the learned model to generate predicted relevancies $\\hat{y}$\n",
        "* Create a violin plot comparing $\\hat{y}$ to $\\hat{y}$ (code provided).\n",
        "\n",
        "**Note:** You may try to re-run with different seeds.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8P3Xg8ev_-NG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "iz1XUS9KAZTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [plot correlations]\n",
        "\n",
        "# eval_r: true ratings(1d numpy.ndarray)\n",
        "# pred_r: predicted ratings(1d numpy.ndarray)\n",
        "\n",
        "plt.figure(figsize=(8,6));\n",
        "rs = np.sort(np.unique(eval_r))\n",
        "pred_per_r = [pred_r[np.where(eval_r==r)] for r in rs]\n",
        "plt.violinplot(pred_per_r, rs, showmeans=True);\n",
        "plt.plot([1,5],[1,5],'k--');\n",
        "plt.text(5.05, 5.05, 'Perfect \\n Pred');\n",
        "plt.xlabel(\"True\");\n",
        "plt.ylabel(\"Predicted\");"
      ],
      "metadata": {
        "id": "JGDiSGqfAeni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain the plot -** Does the linear regression model capture the true relevance?"
      ],
      "metadata": {
        "id": "Mo0oZq39AiGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Answer:...\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "rWqHGWFd3slB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b04INZI34HC8"
      },
      "source": [
        "## 1.3 The effect of dataset size\n",
        "\n",
        "* Create a `RateClickEnvironment` object with default arguments, and use it to generate an evaluation set of size 500.\n",
        "* For each n in `geomspace(50,20000,8)`, generate a training dataset of size n.\n",
        "* Train a click-prediction model using logistic regression.\n",
        "* Train a rating-prediction model using linear regression.\n",
        "* Evaluate and store the NDCG of each model on the evaluation set: use the following `rc_nev.ndcg_10` method that wraps scikit-learn's `ndcg_score` for top 10:\n",
        "\n",
        "  * Input: 2 arrays, true relevance and predicted relevance score.\n",
        "  * Output: float, score\n",
        "\n",
        "* For each value of n, **repeat the above procedure 20 times**.\n",
        "* Generate a results DataFrame with the following columns: `n_samples, repetition, click_ndcg, rating_ndcg`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate evaluation set (to be shared across simulation repetitions):"
      ],
      "metadata": {
        "id": "ZsrFXpxlk1Yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "## YOUR SOLUTION\n"
      ],
      "metadata": {
        "id": "kJMp5dP5uQgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run simulations and output results DataFrame:"
      ],
      "metadata": {
        "id": "m1aUHhbek_yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "num_repeats = 20\n",
        "\n",
        "n_values_lst = np.geomspace(50,20000,8).astype(int)\n",
        "\n",
        "## YOUR SOLUTION\n"
      ],
      "metadata": {
        "id": "VTkffpQQ0GWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot a line-plot showing the average NDCG per n for each of the two models. Include error bars.\n",
        "\n",
        "Guidance:\n",
        "\n",
        "*   Group by trainset size\n",
        "*   Calculate aggregated mean and std values for columns `click_ndcg` and `rating_ndcg`.\n",
        "*   Use mean values for the series plots (the lines) and std for the error bars.\n",
        "\n",
        "Technical tip:\n",
        "*   After aggregation, your pandas DataFrame may contain multi-level columns, the methods `df.swaplevel(axis=1)` or `df.xs(...,axis=1,level=2)` may be convenient for reshaping the DataFrame before plotting.\n",
        "* The pandas `plot.line` function has an optinal argument `yerr` for specfying error bar size.\n",
        "\n"
      ],
      "metadata": {
        "id": "WZjr6zEvU9zj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3ARVvPR6lLk"
      },
      "source": [
        "## YOUR SOLUTION\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain the plot -** What can be observed from the graph? In What is the relation between the two lines?"
      ],
      "metadata": {
        "id": "hnnlxG9Bm-Iy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Answer: ...\n",
        "```"
      ],
      "metadata": {
        "id": "vqUJuL1GnVkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Varying click threshold\n",
        "Repeat the experiment, but this time:\n",
        "* Keep the number of training samples fixed at 1000.\n",
        "* Vary the thershold $\\mathtt{thresh\\_click} \\in[0,5]$. Pass this as a parameter to the `RateClickEnvironment` constructor (note this means that now the environment object should be constructed *inside* the loop).\n",
        "* For every threshold parameter run 20 random repeats and average the results.\n",
        "\n",
        "(Feel free to reuse code from the above question)"
      ],
      "metadata": {
        "id": "iLNHsp71apSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "click_thresholds = np.linspace(0, 5, 20)\n",
        "\n",
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "AlMlKqCnawYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Results - a line graph including clicks and ratings ndcg, similar to 1.3 but with threshold vs.ndcg. Again, feel free to reuse code."
      ],
      "metadata": {
        "id": "8noYzaTbdSFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION\n"
      ],
      "metadata": {
        "id": "1GjDSQzndvW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "r9dY42slroIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain the plot -** What can be observed from the graph? In What is the relation between the two lines?"
      ],
      "metadata": {
        "id": "51cX65qLsBkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Answer: ...\n",
        "```"
      ],
      "metadata": {
        "id": "t_g_75lBsBkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Selective rating"
      ],
      "metadata": {
        "id": "29L-GzloeBaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In real life, users rarely rate all items they consume. Moreover - both the choice of *which* items to rate, and the actual *reported* ratings, are prone to behavioral biases. In this question we will examine the effects of different reporting schemes (i.e., ways in which uesrs report or fail to report ratings, and how they modify these ratings) on learning.\n",
        "\n",
        "\n",
        "We will examine five types of users:\n",
        "- Rational (random): Each rating $r$ is reported with a fixed probability $p=0.4$. Note this serves as a benchmark since it does not add bias to how ratings are reported.\n",
        "- Ranting: Only reports bad ratings, defined as $r \\le 2$.\n",
        "- Extreme: Only reports extreme ratings, defined as $r=1$ or $r=5$.\n",
        "- Optimistic: Reports only if ratings are high, $r \\ge 4$, but for each item with true relevance $r=4$, misreports it as $r=5$ with probability $p=0.8$.\n",
        "- Overreacts: Reports ratings for items with low ratings $r \\le 2$ and with high ratings $r \\ge 4$, and for these, reports them with probability $p=0.5$. However, he extremifies his ratings by misreporting $r=2$ as $r=1$ and $r=4$ as $r=5$.\n",
        "\n",
        "Note: the above criteria and probabilities ensure that for all types, ~40% of ratings are reported, so all types generate the same (expected) number of ratings.\n",
        "\n"
      ],
      "metadata": {
        "id": "jit8xG5xMI-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.1 Implementing user types\n",
        "Implement a function `selective_ratings` that takes as input an array `r_arr` of ratings, and simulates user behavior by returning a subset of `r_arr` matching one of the above types of user reporting behaviors, as determined by the parameter `utype`. The function also takes as input the set of item features `items_train`, and returns the subset of items corresponding to the reported ratings.\n",
        "\n",
        "You may use `rc_env._user_features` to access actual user values per features."
      ],
      "metadata": {
        "id": "EEppQqZk5QfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def selective_ratings(r_arr, utype='rational',items_train=None):\n",
        "  \"\"\"\n",
        "  input:\n",
        "  r_arr: ratings array - numpy\n",
        "  utype: string\n",
        "\n",
        "  return:\n",
        "    - revealed_ratings: numpy array of revealed ratings\n",
        "    - revealed_items: features of the corresponding items\n",
        "  \"\"\"\n",
        "\n",
        "  if utype=='rational':\n",
        "\n",
        "    ## YOUR SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "  if utype=='ranting':\n",
        "    ## YOUR SOLUTION\n",
        "\n",
        "\n",
        "  if utype=='extreme':\n",
        "    ## YOUR SOLUTION\n",
        "\n",
        "\n",
        "  if utype=='optimistic':\n",
        "    ## YOUR SOLUTION\n",
        "\n",
        "\n",
        "  if utype=='overreacts':\n",
        "    ## YOUR SOLUTION\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dAVrI3SXk-_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1.1 New user types\n",
        "Implement 2 data revelation models of your own, and explain the behavioral rationale behind them. Make sure that ~40% of the data is revealed. You may edit the function above."
      ],
      "metadata": {
        "id": "L5Yri0HB4cS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Use next block of code to experiment with your ratings filter models"
      ],
      "metadata": {
        "id": "uwFYXlBq36aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Try your code -\n",
        "You may use ratings generated below of the evaluation..\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "j_YRoFig9B_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the rationale behind the two models? How did you make sure that ~40% of ratings are reported?\n",
        "```\n",
        "Answer: ...\n",
        "```"
      ],
      "metadata": {
        "id": "yNOFsO2wuRl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Training and evaluation\n",
        "\n",
        "In this section we will:\n",
        "* generates train and test data\n",
        "* filter rating for each user type based on your implementation of `selective_ratings`\n",
        "* Train and evaluate the performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "8A0uYFOz8MyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.1 **Before actually running the code** - give an educated guess for NDCG and RMSE values for each user type (if values are hard for you to guess, state relations, e.g. 'higher', 'lower', 'much lower'):"
      ],
      "metadata": {
        "id": "08jvusbISxDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Answer:...\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "nWLkH8OX4IzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.2 Generate traning set of size 5000 and evaluation set of size 1000. Use `build_X_y_matrices` to get `rating` as label for training and `true_relevance` for the evaluation."
      ],
      "metadata": {
        "id": "KX_DFMeO95hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "04lt7iQEVWDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each user type train and evaluate the results. Print ndcg and rmse."
      ],
      "metadata": {
        "id": "bbrEgV-zWbOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "utypes = ['rational', 'ranting', 'extreme', 'optimistic', 'overreacts'] #todo: add your own\n",
        "\n",
        "ndcgs = []\n",
        "rmses = []\n",
        "preds = []\n",
        "\n",
        "for utype in utypes:\n",
        "\n",
        "  ## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "279T8v0S77ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Reflection\n",
        "#### Comparing guesses and results\n",
        "Do these results match your guesses? Why, or why not?\n",
        "\n",
        "\n",
        "To answer fully run the following cell first.\n",
        "It will generate a violin plots of per user type.\n",
        "\n",
        "Use these distributions to explain ndcg and rmse preformance."
      ],
      "metadata": {
        "id": "RAvOuEprXc7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for utype_, pred_, ndcg_, rmse_ in zip(utypes, preds, ndcgs, rmses):\n",
        "  rs = np.sort(np.unique(true_relevance))\n",
        "  pred_per_r = [pred_[np.where(true_relevance==r)] for r in rs]\n",
        "  plt.violinplot(pred_per_r, rs, showmeans=True)\n",
        "  plt.plot([1,5],[1,5],'k--');\n",
        "  plt.xlabel('true relevance')\n",
        "  plt.ylabel('predicted rating')\n",
        "  plt.title('{}: ndcg={:.3f}, rmse={:.3f}'.format(utype_, ndcg_, rmse_))\n",
        "  plt.text(3.2, 3.8, 'Perfect \\n Pred');\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "GO8R3Rk6jAt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain the results:"
      ],
      "metadata": {
        "id": "cO7nueiQvL27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Answer:...\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "LCW8UNH6T436"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Sub Populations and IPW\n",
        "\n",
        "In this segment, our dataset encompasses two distinct user groups: *Ranters*, which like to complain and *Extreme Rankers* which report extreme enjoyment or disappointment. These groups vary not only in their rating policies but also in user features. Your objective with this imbalanced training set is to learn \"true_relevance\" based on rated items.\n",
        "\n",
        "To improve the baseline performance, you'll explore using inverse propensity weighting (IPW). In practical scenarios, you might not know the user type during testing. Propensity weighting can help account for this uncertainty and capture the information effectively.\n"
      ],
      "metadata": {
        "id": "IErPXM_VUAil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `generate_population` generates train and test dataframes which include items' feature columns and either received `rating` or `true_relevance`. In addition, each dataset has a `user_type` column. A binary column which indicates if the item displayed was rated by a *Ranter* or not.\n",
        "You may **not** use this column directly in your training process.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z8c66YbIV2DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = generate_population()\n",
        "\n",
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "gpEIN4XqKI_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed in the lecture, leverage `LogisticRegression` to calculate propensity weights for each item. Using `user_type` is allowed for weights calculation.\n",
        "\n",
        "Note that`LogisticRegression`default settings includes regularization."
      ],
      "metadata": {
        "id": "h20kIFM1lpE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "uN4iUb0_zue7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `RandomForestRegressor` model to fit to ratings and predict true relevance.\n",
        "Calculate baseline preformance and weighted preformance.\n",
        "\n",
        "Save overall test NDCG, and for each the sub population (e.g NDCG for *Ranters*).\n"
      ],
      "metadata": {
        "id": "ygbXcHe-mDj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "3UPugFlbRQBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a bar graph to plot results in trems of NDCG. Use to series, to represent weighted and un-weighted preformace for each population (Ranters, Extreme reporters and overall)."
      ],
      "metadata": {
        "id": "Ewdcz0n8ndxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: changes in preformance don't have to be major."
      ],
      "metadata": {
        "id": "C9_se2MkSmK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR SOLUTION"
      ],
      "metadata": {
        "id": "Zd9Dz12OTgEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain the results:**"
      ],
      "metadata": {
        "id": "JxXftofloGsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Answer:...\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "NavsJyGPoOzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain:** Why would IPW be a viable strategy in this secenario?"
      ],
      "metadata": {
        "id": "NKrPciX9T6pM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Answer:...\n",
        "```"
      ],
      "metadata": {
        "id": "8S3m_bliUMIw"
      }
    }
  ]
}